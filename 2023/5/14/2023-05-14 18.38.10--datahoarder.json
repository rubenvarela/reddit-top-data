{"kind": "Listing", "data": {"after": "t3_13hdres", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We need a ton of help right now, there are too many new images coming in for all of them to be archived by tomorrow. We've done [760 million and there are another 250 million waiting to be done](https://tracker.archiveteam.org/imgur/). Can you spare 5 minutes for archiving Imgur?\n\n### [Choose the \"host\" that matches your current PC, probably Windows or macOS](https://www.virtualbox.org/wiki/Downloads)\n\n### [Download ArchiveTeam Warrior](https://tracker.archiveteam.org/)\n\n1. In VirtualBox, click File &gt; Import Appliance and open the file.\n2. Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.\n\nOnce you\u2019ve started your warrior:\n\n1. Go to http://localhost:8001/ and check the Settings page.\n2. Choose a username \u2014 we\u2019ll show your progress on the leaderboard.\n3. Go to the All projects tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Imgur).\n\nTakes 5 minutes.\n\nTell your friends!\n\n[The megathread is stickied](https://old.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/), but I think it's worth noting that despite everyone's valiant efforts there are just too many images out there. The only way we're saving everything is if you run ArchiveTeam Warrior and get the word out to other people.", "author_fullname": "t2_84crybq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ArchiveTeam has saved 760 MILLION Imgur files, but it's not enough. We need YOU to run ArchiveTeam Warrior!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hex6p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 279, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 279, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684083297.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684077614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We need a ton of help right now, there are too many new images coming in for all of them to be archived by tomorrow. We&amp;#39;ve done &lt;a href=\"https://tracker.archiveteam.org/imgur/\"&gt;760 million and there are another 250 million waiting to be done&lt;/a&gt;. Can you spare 5 minutes for archiving Imgur?&lt;/p&gt;\n\n&lt;h3&gt;&lt;a href=\"https://www.virtualbox.org/wiki/Downloads\"&gt;Choose the &amp;quot;host&amp;quot; that matches your current PC, probably Windows or macOS&lt;/a&gt;&lt;/h3&gt;\n\n&lt;h3&gt;&lt;a href=\"https://tracker.archiveteam.org/\"&gt;Download ArchiveTeam Warrior&lt;/a&gt;&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In VirtualBox, click File &amp;gt; Import Appliance and open the file.&lt;/li&gt;\n&lt;li&gt;Start the virtual machine. It will fetch the latest updates and will eventually tell you to start your web browser.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Once you\u2019ve started your warrior:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go to http://localhost:8001/ and check the Settings page.&lt;/li&gt;\n&lt;li&gt;Choose a username \u2014 we\u2019ll show your progress on the leaderboard.&lt;/li&gt;\n&lt;li&gt;Go to the All projects tab and select ArchiveTeam\u2019s Choice to let your warrior work on the most urgent project. (This will be Imgur).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Takes 5 minutes.&lt;/p&gt;\n\n&lt;p&gt;Tell your friends!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://old.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/\"&gt;The megathread is stickied&lt;/a&gt;, but I think it&amp;#39;s worth noting that despite everyone&amp;#39;s valiant efforts there are just too many images out there. The only way we&amp;#39;re saving everything is if you run ArchiveTeam Warrior and get the word out to other people.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hex6p", "is_robot_indexable": true, "report_reasons": null, "author": "Seglegs", "discussion_type": null, "num_comments": 82, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hex6p/archiveteam_has_saved_760_million_imgur_files_but/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hex6p/archiveteam_has_saved_760_million_imgur_files_but/", "subreddit_subscribers": 682478, "created_utc": 1684077614.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,  \n\n\nMy friend passed few years ago and I need to archive his tweets before his account gets deleted (thanks elon). He was an artist so there is some media (his drawings) but I'd like to archive his entire profile even with his old posts, replies, retweets etc., and that's why I'd like to have some UI to read everything like it's on twitter.\n\nI've already used snscrape to get every tweet link, but I miss the \"visual representation of the data\". Do anyone of you know which tool can I use?\n\nThanks &lt;3", "author_fullname": "t2_9egzy7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with archiving my old friend's tweets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gohrs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684001349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,  &lt;/p&gt;\n\n&lt;p&gt;My friend passed few years ago and I need to archive his tweets before his account gets deleted (thanks elon). He was an artist so there is some media (his drawings) but I&amp;#39;d like to archive his entire profile even with his old posts, replies, retweets etc., and that&amp;#39;s why I&amp;#39;d like to have some UI to read everything like it&amp;#39;s on twitter.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve already used snscrape to get every tweet link, but I miss the &amp;quot;visual representation of the data&amp;quot;. Do anyone of you know which tool can I use?&lt;/p&gt;\n\n&lt;p&gt;Thanks &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "200TB (2x100TB) ZFS RAID6", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gohrs", "is_robot_indexable": true, "report_reasons": null, "author": "Arturro43", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13gohrs/help_with_archiving_my_old_friends_tweets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gohrs/help_with_archiving_my_old_friends_tweets/", "subreddit_subscribers": 682478, "created_utc": 1684001349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Some tips for manually finding and archiving all the Imgur links in your reddit post history:\n\n1. Download the Reddit Enhancement Suite for its autoscrolling feature\n\n2. Go to old.reddit.com/user/YOUR_USERNAME , which is your Overview page sorted by \"New\"\n\n3. Scroll down until it won't load anymore posts (due to the 1000 post API limit)\n\n4. Run this console command: `document.querySelectorAll('.usertext-body .md a').forEach(function(element, index){element.text = element.href});`\n\n5. Ctrl+F search for `imgur.` and go through the results\n\n6. Run each URL through the latest archived image checker: https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png\n\n7. If a URL is missing, run it through the Save Page Now tool: https://web.archive.org/save\n\n8. After you've finished processing the results, redo the process for the Overview Top, Comments New, Comments Top, Submitted New, and Submitted Top results. This is important if you have a long reddit history because they'll help you get around the 1000 post limit due to them loading the posts in a different order.", "author_fullname": "t2_4bc1l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Resource] How to manually find and archive almost all the Imgur links in your reddit post history and avoid the 1000 post API limit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hdv65", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684074985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some tips for manually finding and archiving all the Imgur links in your reddit post history:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Download the Reddit Enhancement Suite for its autoscrolling feature&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Go to old.reddit.com/user/YOUR_USERNAME , which is your Overview page sorted by &amp;quot;New&amp;quot;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Scroll down until it won&amp;#39;t load anymore posts (due to the 1000 post API limit)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run this console command: &lt;code&gt;document.querySelectorAll(&amp;#39;.usertext-body .md a&amp;#39;).forEach(function(element, index){element.text = element.href});&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ctrl+F search for &lt;code&gt;imgur.&lt;/code&gt; and go through the results&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run each URL through the latest archived image checker: &lt;a href=\"https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png\"&gt;https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If a URL is missing, run it through the Save Page Now tool: &lt;a href=\"https://web.archive.org/save\"&gt;https://web.archive.org/save&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;After you&amp;#39;ve finished processing the results, redo the process for the Overview Top, Comments New, Comments Top, Submitted New, and Submitted Top results. This is important if you have a long reddit history because they&amp;#39;ll help you get around the 1000 post limit due to them loading the posts in a different order.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?auto=webp&amp;v=enabled&amp;s=03f6f54b024049096bac8b6984c80c3bd5669e4d", "width": 448, "height": 310}, "resolutions": [{"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebfa371e414b141502de697ab78ff8c8ef644ab4", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1d0f2af124cd3fb85d38e60b6391d608a94761a", "width": 216, "height": 149}, {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5419c7771818bf926687520bb178125b9fad7e46", "width": 320, "height": 221}], "variants": {}, "id": "L23lfyVQ0rA1Gxs5Ds3_qLTfvRjLtonmKPj-Zd2YhT4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hdv65", "is_robot_indexable": true, "report_reasons": null, "author": "Pikamander2", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/", "subreddit_subscribers": 682478, "created_utc": 1684074985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If you are looking for Clouds to expand your storage or simply to backup, dont go with google!\n\nIm using Google Drive on Windows for \\~1 month now and im really asking myself how a big company like Google can make such a bad Application for it.\n\nBefore i get into the problems i have with it i want to say that i also use OneDrive and have used Dropbox before that. But they never had any issues like Google Drive has.\n\n&amp;#x200B;\n\n1. **Syncing Errors** \\- In comparison to OneDrive &amp; Dropbox, which occassionally throw an error like \"the Filename includes chars that are not allowed\", Google Drive throws errors that shouldnt even happen. ex.: \"You have no permission to upload File 'X5' into the Directory 'X'\" after uploading 'X1'-'X4' to Directory 'X' without a problem...\n\n2. **Lost &amp; Found** \\- When Google Drive has a problem while syncing a file, it wont just try again later (that would be too easy &amp; user friendly), it will throw the File into the \"Lost &amp; Found\" Directory. Not only is this a local Directory, which wont get synced, you will also have to move the file back into the place it belongs manually. Oh, and if you uploaded lets say \\~5000 Directories containing \\~200k Files &amp; \\~500 Files got moved into L&amp;F... have fun finding out where each file belongs, as there is no information where it was before.\n\n3. **Account Connection** \\- in the 1 month im using Google Drive i got the \"cant connect to account, please login again\" 4 times. And now the real fun begins. Im using the Setting where Drive will act as a volume in your pc. The last \"cant connect\" (today) happened midsync, now all files that havent been uploaded yet are **GONE**. The Lost&amp;Found Directory, **GONE**. I couldnt even recover anything with recuva...\n\n&amp;#x200B;\n\n**TLDR:** Google Drive for Windows has so many Problems that its not just a pain to use, it also caused the loss of around 10% of the 1,5TB Files i tried to upload.\n\nAnd before someone asks: No, Google Drive &amp; OneDrive havent tried to use the same files. My OneDrive is on a different SSD, doesnt auto-backup anything, just uploads what i put into the OneDrive directory.\n\nIm kinda mad that such a huge amount of Files has been lost by a service that is there to \"keep your files well protected in the cloud\"... Luckily i used Google Drive only for not so important Stuff while putting the important Stuff on OneDrive, where it is really \"well protected in the cloud\".", "author_fullname": "t2_3lrsk4yj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive for Windows is TERRIBLE, here is why.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hd1br", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684072896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are looking for Clouds to expand your storage or simply to backup, dont go with google!&lt;/p&gt;\n\n&lt;p&gt;Im using Google Drive on Windows for ~1 month now and im really asking myself how a big company like Google can make such a bad Application for it.&lt;/p&gt;\n\n&lt;p&gt;Before i get into the problems i have with it i want to say that i also use OneDrive and have used Dropbox before that. But they never had any issues like Google Drive has.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Syncing Errors&lt;/strong&gt; - In comparison to OneDrive &amp;amp; Dropbox, which occassionally throw an error like &amp;quot;the Filename includes chars that are not allowed&amp;quot;, Google Drive throws errors that shouldnt even happen. ex.: &amp;quot;You have no permission to upload File &amp;#39;X5&amp;#39; into the Directory &amp;#39;X&amp;#39;&amp;quot; after uploading &amp;#39;X1&amp;#39;-&amp;#39;X4&amp;#39; to Directory &amp;#39;X&amp;#39; without a problem...&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lost &amp;amp; Found&lt;/strong&gt; - When Google Drive has a problem while syncing a file, it wont just try again later (that would be too easy &amp;amp; user friendly), it will throw the File into the &amp;quot;Lost &amp;amp; Found&amp;quot; Directory. Not only is this a local Directory, which wont get synced, you will also have to move the file back into the place it belongs manually. Oh, and if you uploaded lets say ~5000 Directories containing ~200k Files &amp;amp; ~500 Files got moved into L&amp;amp;F... have fun finding out where each file belongs, as there is no information where it was before.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Account Connection&lt;/strong&gt; - in the 1 month im using Google Drive i got the &amp;quot;cant connect to account, please login again&amp;quot; 4 times. And now the real fun begins. Im using the Setting where Drive will act as a volume in your pc. The last &amp;quot;cant connect&amp;quot; (today) happened midsync, now all files that havent been uploaded yet are &lt;strong&gt;GONE&lt;/strong&gt;. The Lost&amp;amp;Found Directory, &lt;strong&gt;GONE&lt;/strong&gt;. I couldnt even recover anything with recuva...&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Google Drive for Windows has so many Problems that its not just a pain to use, it also caused the loss of around 10% of the 1,5TB Files i tried to upload.&lt;/p&gt;\n\n&lt;p&gt;And before someone asks: No, Google Drive &amp;amp; OneDrive havent tried to use the same files. My OneDrive is on a different SSD, doesnt auto-backup anything, just uploads what i put into the OneDrive directory.&lt;/p&gt;\n\n&lt;p&gt;Im kinda mad that such a huge amount of Files has been lost by a service that is there to &amp;quot;keep your files well protected in the cloud&amp;quot;... Luckily i used Google Drive only for not so important Stuff while putting the important Stuff on OneDrive, where it is really &amp;quot;well protected in the cloud&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hd1br", "is_robot_indexable": true, "report_reasons": null, "author": "TriQancer", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hd1br/google_drive_for_windows_is_terrible_here_is_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hd1br/google_drive_for_windows_is_terrible_here_is_why/", "subreddit_subscribers": 682478, "created_utc": 1684072896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I forgot to do this earlier and, while I would have time to do it manually, it would probably lead to hours of frustration and a broken mouse wheel. Is there a tool which does this, since it's unfortunately not an album?\n\nBy the way, I am aware of the archival project. I mainly want to save old, obscure sfw posts that will probably get nuked because they were uploaded anonymously - and not necessarily on Reddit.\n\nThanks.\n\n**EDIT: this seems to work:** [**Imgur To Folder**](https://github.com/santosderek/Imgur-To-Folder)and while we're at it, if anyone else has problems installing it because of Python writing permission issues, check out [this post!](https://stackoverflow.com/questions/22551461/how-to-avoid-permission-denied-while-installing-package-for-python-without-sudo)  \n\n\nEdit 2:  use this command\n\n    itf --download-favorites [username] --max-downloads [NUMBER LARGER THAN THE AMOUNT OF FAVORITES]", "author_fullname": "t2_766bk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to (automatically) download imgur favourites/bookmarks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hfesk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684088710.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684078820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I forgot to do this earlier and, while I would have time to do it manually, it would probably lead to hours of frustration and a broken mouse wheel. Is there a tool which does this, since it&amp;#39;s unfortunately not an album?&lt;/p&gt;\n\n&lt;p&gt;By the way, I am aware of the archival project. I mainly want to save old, obscure sfw posts that will probably get nuked because they were uploaded anonymously - and not necessarily on Reddit.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT: this seems to work:&lt;/strong&gt; &lt;a href=\"https://github.com/santosderek/Imgur-To-Folder\"&gt;&lt;strong&gt;Imgur To Folder&lt;/strong&gt;&lt;/a&gt;and while we&amp;#39;re at it, if anyone else has problems installing it because of Python writing permission issues, check out &lt;a href=\"https://stackoverflow.com/questions/22551461/how-to-avoid-permission-denied-while-installing-package-for-python-without-sudo\"&gt;this post!&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Edit 2:  use this command&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;itf --download-favorites [username] --max-downloads [NUMBER LARGER THAN THE AMOUNT OF FAVORITES]\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?auto=webp&amp;v=enabled&amp;s=3d7eb255cd3293678e379a47901a019c0125d263", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=093170ff63839e776e4ffdebfd5e9b3a60840e15", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=587da2bad60702e265ed179c448dd2f2ce5d3717", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b3714a32f029176bf5f6c6558a9da2e5a2efddb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7452d437fc0f2f53960dabbcf746cdc5ec4c540", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b662d4d1cb754f5805920fc520420ec923d2d8a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/D0yLIOFXbV0EGC4riqplqfpvsJPsQT-2Ja3sk0P7__Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ed1207cd19b44a3ea3f53a515c9ecba80a4016", "width": 1080, "height": 540}], "variants": {}, "id": "O0EyWO840AK6vFLLoWxug2mAnxhbLamTr8PWLrEavSc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hfesk", "is_robot_indexable": true, "report_reasons": null, "author": "RedFlame99", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hfesk/is_there_a_way_to_automatically_download_imgur/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hfesk/is_there_a_way_to_automatically_download_imgur/", "subreddit_subscribers": 682478, "created_utc": 1684078820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey. since tomorrow is the day when Imgur wipes out its non-logged in images, I am wondering if any of you in here, or elsewhere like on Archive Team, have been archiving imgur links not just from reddit, but also from other sites.\n\nIn particular, if this is still possible, I would like to request [imgur links from the alternate history forum](https://www.alternatehistory.com/forum/) to be archived, I am an amateur cartographer, and possibly thousands of maps, flags, photoshopped, and historical photos could be gone.", "author_fullname": "t2_b9uquxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On the Imgur archival, have any of you been searching for forums outside of reddit? can I also request the archival of Imgur links from the Alternate History forum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h15el", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": "", "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684034513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey. since tomorrow is the day when Imgur wipes out its non-logged in images, I am wondering if any of you in here, or elsewhere like on Archive Team, have been archiving imgur links not just from reddit, but also from other sites.&lt;/p&gt;\n\n&lt;p&gt;In particular, if this is still possible, I would like to request &lt;a href=\"https://www.alternatehistory.com/forum/\"&gt;imgur links from the alternate history forum&lt;/a&gt; to be archived, I am an amateur cartographer, and possibly thousands of maps, flags, photoshopped, and historical photos could be gone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h15el", "is_robot_indexable": true, "report_reasons": null, "author": "wq1119", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13h15el/on_the_imgur_archival_have_any_of_you_been/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h15el/on_the_imgur_archival_have_any_of_you_been/", "subreddit_subscribers": 682478, "created_utc": 1684034513.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Which means most of reddit images from 2009-2016 (when reddit started hosting their own images) will be dead. The digital dark age grows around us.\n\n----\n____\nPlease contribute by following along here:\n\nhttps://www.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/", "author_fullname": "t2_6efmq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tomorrow is the big day! Imgur is deleting all images uploaded by anonymous users", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hgi64", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684081475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which means most of reddit images from 2009-2016 (when reddit started hosting their own images) will be dead. The digital dark age grows around us.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Please contribute by following along here:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/\"&gt;https://www.reddit.com/r/DataHoarder/comments/12sbch3/imgur_is_updating_their_tos_on_may_15_2023_all/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hgi64", "is_robot_indexable": true, "report_reasons": null, "author": "gabefair", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hgi64/tomorrow_is_the_big_day_imgur_is_deleting_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hgi64/tomorrow_is_the_big_day_imgur_is_deleting_all/", "subreddit_subscribers": 682478, "created_utc": 1684081475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello to all good people on this sub!\n\nA little backstory *- skip to* ***below*** *if you're not interesting in reading some fluff.*\n\nRecently, a friend of mine have given me a couple of old drives he had no use for, of which 3 HDDs turned out to be working and even passed the full read scans in Victoria. 2 of them were formatted already and have no data, but one 2.5\" 320 GB WD HDD has the previous owner's files intact, which include photos and other private stuff (though I haven't looked that much 'cuz that's rude lol). The friend looked at those and told me that they likely aren't needed anymore and I can format everything. But, to be honest, it just feels wrong to me.\n\nNow, one of the other working two HDDs is a mid-00s 3.5\" 7200RPM 320GB WD monstrosity, which, while works, gives of a constant hum like from a power grid station or something, which gets on my nerves (as I have a fairly quiet PC), so I decided to not use it. But since it can still store data, I decided that maybe I'll use it for backing up the data from the aforementioned drive. (And yes, I understand that that drive can fail any time now due to how old it is, but the data are apparently not needed anymore anyway and it's the only storage medium I'll have no use for, so a flimsy backup would be better than no backup at all, right?)\n\n***-- backstory end --***\n\nSo, I need to copy the entirety of a 320 GB drive (of which \\~160 GB is occupied) to another drive as a compressed file. Ideally, I would have the following requirements:\n\n* The backed up copy should be able to be restored in a way that the disk would be exactly the same as it was at the time of backup - MBR/boot and all partitions with data intact.\n* The backed up copy should be browseable (ideally - with programs other than the one that made the backup, too), so I could look up and extract individual files without having to restore the whole backup.\n* The resulting disk image needs to be compressed, or at least not have the empty parts of the disk's partitions take space in the image (so the backup file would be of those \\~160 GB or less in size instead of full 320 GB as the source disk is).\n\nI googled around and found some useful suggestions in [this thread](https://www.reddit.com/r/DataHoarder/comments/c47wwn/best_software_for_backing_upcloning_entire_hard/), such as dd, Clonezilla, Acronis and some others. However, I'm not sure if any of those can make a backup that would fit all three of my requirements. For example, `dd if=/dev/sda status=progress | lz4 -c &gt; ~/my_disk.lz4` looks like the easiest way (I have an external SSD with Linux on it), but it likely fails the browseability requirement if compressed, and if not the compression one is failed instead, same with Clonezilla (as I understand, both copy everything exactly including the empty space, which I want to avoid). Acronis is popular enough that I think it will probably meet all 3 requirements, but I want to know for sure. TeraByte Image looks really interesting in its ability to omit some unnecessary data (such as hibernation data, pagefile and logs, saw on [this screenshot](https://www.terabyteunlimited.com/wp-content/uploads/2021/09/ss_ifw_04-min.png)), and it has compression, but I don't know about the browseability. Same with MiniTool ShadowMaker, and lots of others.\n\nMy main problem here is uncertainty - I just don't know which software should I pick for the task, as they all seem to mostly do what I need them to, except for my requirements. I think I could've just try every solution for myself, but I don't really want to spend my Sunday evening (or any evening, per se) on finding out the best way to copy some not that important data to an old, slow, and *HUMMING* drive. I want to just make that backup, disconnect the target drive, and be done with it lol.\n\nSo, what software would fit my requirements? Or if none does, which suggestions for my use case would be? Any responses are welcome!\n\n^(And thank you if you've read this wall of text :\\))", "author_fullname": "t2_13zs23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to make a compressed backup of an HDD before formatting it, with an ability to browse the backed up image's contents and be able to restore everything exactly to how it was, just in case. What software would be the best for the task?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hcziy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684072783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello to all good people on this sub!&lt;/p&gt;\n\n&lt;p&gt;A little backstory &lt;em&gt;- skip to&lt;/em&gt; &lt;strong&gt;&lt;em&gt;below&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;if you&amp;#39;re not interesting in reading some fluff.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Recently, a friend of mine have given me a couple of old drives he had no use for, of which 3 HDDs turned out to be working and even passed the full read scans in Victoria. 2 of them were formatted already and have no data, but one 2.5&amp;quot; 320 GB WD HDD has the previous owner&amp;#39;s files intact, which include photos and other private stuff (though I haven&amp;#39;t looked that much &amp;#39;cuz that&amp;#39;s rude lol). The friend looked at those and told me that they likely aren&amp;#39;t needed anymore and I can format everything. But, to be honest, it just feels wrong to me.&lt;/p&gt;\n\n&lt;p&gt;Now, one of the other working two HDDs is a mid-00s 3.5&amp;quot; 7200RPM 320GB WD monstrosity, which, while works, gives of a constant hum like from a power grid station or something, which gets on my nerves (as I have a fairly quiet PC), so I decided to not use it. But since it can still store data, I decided that maybe I&amp;#39;ll use it for backing up the data from the aforementioned drive. (And yes, I understand that that drive can fail any time now due to how old it is, but the data are apparently not needed anymore anyway and it&amp;#39;s the only storage medium I&amp;#39;ll have no use for, so a flimsy backup would be better than no backup at all, right?)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;-- backstory end --&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So, I need to copy the entirety of a 320 GB drive (of which ~160 GB is occupied) to another drive as a compressed file. Ideally, I would have the following requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The backed up copy should be able to be restored in a way that the disk would be exactly the same as it was at the time of backup - MBR/boot and all partitions with data intact.&lt;/li&gt;\n&lt;li&gt;The backed up copy should be browseable (ideally - with programs other than the one that made the backup, too), so I could look up and extract individual files without having to restore the whole backup.&lt;/li&gt;\n&lt;li&gt;The resulting disk image needs to be compressed, or at least not have the empty parts of the disk&amp;#39;s partitions take space in the image (so the backup file would be of those ~160 GB or less in size instead of full 320 GB as the source disk is).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I googled around and found some useful suggestions in &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/c47wwn/best_software_for_backing_upcloning_entire_hard/\"&gt;this thread&lt;/a&gt;, such as dd, Clonezilla, Acronis and some others. However, I&amp;#39;m not sure if any of those can make a backup that would fit all three of my requirements. For example, &lt;code&gt;dd if=/dev/sda status=progress | lz4 -c &amp;gt; ~/my_disk.lz4&lt;/code&gt; looks like the easiest way (I have an external SSD with Linux on it), but it likely fails the browseability requirement if compressed, and if not the compression one is failed instead, same with Clonezilla (as I understand, both copy everything exactly including the empty space, which I want to avoid). Acronis is popular enough that I think it will probably meet all 3 requirements, but I want to know for sure. TeraByte Image looks really interesting in its ability to omit some unnecessary data (such as hibernation data, pagefile and logs, saw on &lt;a href=\"https://www.terabyteunlimited.com/wp-content/uploads/2021/09/ss_ifw_04-min.png\"&gt;this screenshot&lt;/a&gt;), and it has compression, but I don&amp;#39;t know about the browseability. Same with MiniTool ShadowMaker, and lots of others.&lt;/p&gt;\n\n&lt;p&gt;My main problem here is uncertainty - I just don&amp;#39;t know which software should I pick for the task, as they all seem to mostly do what I need them to, except for my requirements. I think I could&amp;#39;ve just try every solution for myself, but I don&amp;#39;t really want to spend my Sunday evening (or any evening, per se) on finding out the best way to copy some not that important data to an old, slow, and &lt;em&gt;HUMMING&lt;/em&gt; drive. I want to just make that backup, disconnect the target drive, and be done with it lol.&lt;/p&gt;\n\n&lt;p&gt;So, what software would fit my requirements? Or if none does, which suggestions for my use case would be? Any responses are welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;And thank you if you&amp;#39;ve read this wall of text :\\&lt;/sup&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?auto=webp&amp;v=enabled&amp;s=0c3965c3168cf02ba36d0a843a40556c9382b69f", "width": 688, "height": 539}, "resolutions": [{"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04ba787f24f535c39cb3ee62c063d2b4b7cd12cf", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6013acb016d77465e1a5e8aa11a2f4af7e31b90a", "width": 216, "height": 169}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9d4e2f687e375000bf562ec4eae41718f7cb3db", "width": 320, "height": 250}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da7d0805daa602cedc14f8c3a8134e891b865dcf", "width": 640, "height": 501}], "variants": {}, "id": "lMVX8-JabBvuaas6wAJ7e3HAGDfG-Ea7elA1S0S6mj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hcziy", "is_robot_indexable": true, "report_reasons": null, "author": "AGTS10k", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hcziy/need_to_make_a_compressed_backup_of_an_hdd_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hcziy/need_to_make_a_compressed_backup_of_an_hdd_before/", "subreddit_subscribers": 682478, "created_utc": 1684072783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! Is using old NAS for backups safe? I wouldn't use it for any file sharing, clouds etc. Just simply connected to router to regularly backup files. I read few topics that overall using old NAS'es isn't the best idea but not sure if they were talking about just data storage or something else.  \n\n\nEdit  \n\n\nBy old I mean devices like 10 years old, with no support nor new software", "author_fullname": "t2_bty3k7xm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old NAS as backup server - is it safe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gyc8h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684026485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Is using old NAS for backups safe? I wouldn&amp;#39;t use it for any file sharing, clouds etc. Just simply connected to router to regularly backup files. I read few topics that overall using old NAS&amp;#39;es isn&amp;#39;t the best idea but not sure if they were talking about just data storage or something else.  &lt;/p&gt;\n\n&lt;p&gt;Edit  &lt;/p&gt;\n\n&lt;p&gt;By old I mean devices like 10 years old, with no support nor new software&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gyc8h", "is_robot_indexable": true, "report_reasons": null, "author": "luki52721", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gyc8h/old_nas_as_backup_server_is_it_safe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gyc8h/old_nas_as_backup_server_is_it_safe/", "subreddit_subscribers": 682478, "created_utc": 1684026485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_cajg81c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these easily shuckable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_13hh6pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/9rGFoWlbbJEloT5-Um7zXUrVCXZUk3rgP8bf1up_-j4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684083123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/udknslirbvza1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/udknslirbvza1.jpg?auto=webp&amp;v=enabled&amp;s=2647f0b72a63c5f0a7c9a0f4902356d2bada0046", "width": 1080, "height": 2400}, "resolutions": [{"url": "https://preview.redd.it/udknslirbvza1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d7a0f01957ec36f488538436d9ca61022bcf14c7", "width": 108, "height": 216}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e598d1f259cec14e30455decf97504c9e9fe3ba1", "width": 216, "height": 432}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9a1c69d7847de1f9810b59626670e50fcd6e6bfa", "width": 320, "height": 640}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7150a44e7bed0676c5233075ebea23f201e6ce9c", "width": 640, "height": 1280}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed5baa9ef3c395cc978b71350e24351aa771e4cc", "width": 960, "height": 1920}, {"url": "https://preview.redd.it/udknslirbvza1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b06b69ea905a7339c243abf895148ecfe8a80727", "width": 1080, "height": 2160}], "variants": {}, "id": "hs4C4e1mJWc0TuQhDGNB1zDr2rjAdzxgUWMJIDLhk14"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hh6pe", "is_robot_indexable": true, "report_reasons": null, "author": "umairshariff23", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hh6pe/are_these_easily_shuckable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/udknslirbvza1.jpg", "subreddit_subscribers": 682478, "created_utc": 1684083123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellas. If similar to me, you like collecting comic books and \"Getcomics\" doesn't cut it for you, you might have an answer to my problem.\n\nI live in Iran, and we have nearly zero access to purchasing comic books in any format. So as a comic book enthusiast, I could turn to nowhere but piracy for accessing the books I liked.\nGetcomics has been a great help, and if I can't find something there, Libgen might have PDF files of the stuff I want. But other than that I had no major resources, until I randomly found Comicslady.\n\nI was trying to learn a few other languages, and I'm really interested in French comics from France and Belgium. To my amazement, Comicslady was filled with original AND officially translated works in French, and they had something around 40,000 webpages/entries for their comics. Aside from French, they had a really good archive for Italian &amp; Spanish comics too.\n\nUnfortunately I didn't have time to hoard the archive at the time, and today when I went back to get a few books, I found out that the website has apparently been closed.\n\nI can't find any good alternatives to that archive, and a lot of the stuff they had is just not on the public internet for free.\n\nI was wondering if any of you knew an online archive like that which I could use. Mostly for French comics, preferably in CBR or CBZ formats and with good or average quality, and spanning from the old works like M\u00e9tal Hurlant and the works of M\u0153bius &amp; M\u00e9zi\u00e8res, to works published in the recent years.\n\nThank you in advance.", "author_fullname": "t2_4temyu59", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacements for Comicslady (non-English comic book archive)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hd9wy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684074025.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684073500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellas. If similar to me, you like collecting comic books and &amp;quot;Getcomics&amp;quot; doesn&amp;#39;t cut it for you, you might have an answer to my problem.&lt;/p&gt;\n\n&lt;p&gt;I live in Iran, and we have nearly zero access to purchasing comic books in any format. So as a comic book enthusiast, I could turn to nowhere but piracy for accessing the books I liked.\nGetcomics has been a great help, and if I can&amp;#39;t find something there, Libgen might have PDF files of the stuff I want. But other than that I had no major resources, until I randomly found Comicslady.&lt;/p&gt;\n\n&lt;p&gt;I was trying to learn a few other languages, and I&amp;#39;m really interested in French comics from France and Belgium. To my amazement, Comicslady was filled with original AND officially translated works in French, and they had something around 40,000 webpages/entries for their comics. Aside from French, they had a really good archive for Italian &amp;amp; Spanish comics too.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately I didn&amp;#39;t have time to hoard the archive at the time, and today when I went back to get a few books, I found out that the website has apparently been closed.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t find any good alternatives to that archive, and a lot of the stuff they had is just not on the public internet for free.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if any of you knew an online archive like that which I could use. Mostly for French comics, preferably in CBR or CBZ formats and with good or average quality, and spanning from the old works like M\u00e9tal Hurlant and the works of M\u0153bius &amp;amp; M\u00e9zi\u00e8res, to works published in the recent years.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hd9wy", "is_robot_indexable": true, "report_reasons": null, "author": "Mehrider", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hd9wy/replacements_for_comicslady_nonenglish_comic_book/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hd9wy/replacements_for_comicslady_nonenglish_comic_book/", "subreddit_subscribers": 682478, "created_utc": 1684073500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am locking into creating a music server i need suggestions on which client should i use with good ui and performance. I am also looking for a service that downloads the music for me. For example something that can download and save it from YouTube in mp3 cause i wanna be able to download songs of different languages. \nIf requesting music and client can be one client that would ge great.\nJust looking for suggestions\nThanks already", "author_fullname": "t2_acv70nwi5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A music server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h2p5m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684039202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am locking into creating a music server i need suggestions on which client should i use with good ui and performance. I am also looking for a service that downloads the music for me. For example something that can download and save it from YouTube in mp3 cause i wanna be able to download songs of different languages. \nIf requesting music and client can be one client that would ge great.\nJust looking for suggestions\nThanks already&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h2p5m", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable_Middle_711", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13h2p5m/a_music_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h2p5m/a_music_server/", "subreddit_subscribers": 682478, "created_utc": 1684039202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone! For my work, i'm recording about 80 hours (\\~350gb) webinars per month and I need a cost effective way to store this data. I record the videos on either my iphone or chromebook and then my virtual assistant edits these videos and sends smaller clips back to me.\n\n&amp;#x200B;\n\nI do NOT want to pay for both/ do more work using both. It's important that the transferring process is simple and quick!\n\nShould I use cloud storage or external hard-drive, or maybe something else?? \n\nIf you recommend cloud storage, what plan should i go with?", "author_fullname": "t2_ap36uryh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Store Weekly Videos On Cloud OR External Drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h16ad", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684082213.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684034589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! For my work, i&amp;#39;m recording about 80 hours (~350gb) webinars per month and I need a cost effective way to store this data. I record the videos on either my iphone or chromebook and then my virtual assistant edits these videos and sends smaller clips back to me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I do NOT want to pay for both/ do more work using both. It&amp;#39;s important that the transferring process is simple and quick!&lt;/p&gt;\n\n&lt;p&gt;Should I use cloud storage or external hard-drive, or maybe something else?? &lt;/p&gt;\n\n&lt;p&gt;If you recommend cloud storage, what plan should i go with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h16ad", "is_robot_indexable": true, "report_reasons": null, "author": "Zestyclose_Froyo_558", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13h16ad/store_weekly_videos_on_cloud_or_external_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h16ad/store_weekly_videos_on_cloud_or_external_drive/", "subreddit_subscribers": 682478, "created_utc": 1684034589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm sure this question has been asked before, but I personally couldn't find a clear answer for my case so I'd like a bit of advice. As a preventative measure, I would like to back up all my files to the cloud in case of a drive failure. Ideally, I don't want to spend my whole life savings on it but I don't mind spending a bit to have all my data backed up. I have around 2.5TB to back up, so it's not a huge amount.\n\nWhat are your recommendations for a cloud backup provider that lets me upload my drives and restore in case of a drive failure?\n\nThanks", "author_fullname": "t2_12sl74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup data to cloud in case of drive failure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gupg4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684016866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure this question has been asked before, but I personally couldn&amp;#39;t find a clear answer for my case so I&amp;#39;d like a bit of advice. As a preventative measure, I would like to back up all my files to the cloud in case of a drive failure. Ideally, I don&amp;#39;t want to spend my whole life savings on it but I don&amp;#39;t mind spending a bit to have all my data backed up. I have around 2.5TB to back up, so it&amp;#39;s not a huge amount.&lt;/p&gt;\n\n&lt;p&gt;What are your recommendations for a cloud backup provider that lets me upload my drives and restore in case of a drive failure?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gupg4", "is_robot_indexable": true, "report_reasons": null, "author": "DiamoNNNd1337", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gupg4/backup_data_to_cloud_in_case_of_drive_failure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gupg4/backup_data_to_cloud_in_case_of_drive_failure/", "subreddit_subscribers": 682478, "created_utc": 1684016866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I'm a big fan of [thecoverproject.net](https://thecoverproject.net/),  a website that offers high-quality printable scans of video game covers for  free.  However, I'm wondering if there's a similar website that provides  scans  of DVD and Blu-ray covers, without charging any money.  \n\nI've  looked around online, but  most of the websites I've found require a subscription or payment to  access their collection of scans.\n\nAny recommendations or suggestions would be greatly appreciated.", "author_fullname": "t2_8wn2xapeo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Cover Project but for Movies and Tv Shows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hgfri", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684081311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a big fan of &lt;a href=\"https://thecoverproject.net/\"&gt;thecoverproject.net&lt;/a&gt;,  a website that offers high-quality printable scans of video game covers for  free.  However, I&amp;#39;m wondering if there&amp;#39;s a similar website that provides  scans  of DVD and Blu-ray covers, without charging any money.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve  looked around online, but  most of the websites I&amp;#39;ve found require a subscription or payment to  access their collection of scans.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations or suggestions would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hgfri", "is_robot_indexable": true, "report_reasons": null, "author": "ReasonableHorse10", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hgfri/the_cover_project_but_for_movies_and_tv_shows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hgfri/the_cover_project_but_for_movies_and_tv_shows/", "subreddit_subscribers": 682478, "created_utc": 1684081311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I intend to use the several external HDDs that I already have to archive some data. Which filesystem do I use for this purpose? I like ZFS's checksumming, but that seems to be geared more towards use on a running server rather than a cold archive. (E.g., it causes performance issues to fill up the filesystem beyond a certain threshold because of the way it allocates space. (Same for Btrfs?) For my purpose it would actually be optimal to allocate space left to right with no gaps because no file will ever be resized.)\n\nTransparent compression and encryption would be nice, but I can also take care of that manually if need be. (I know of Parchive, with which I could do the checksumming manually as well, however, this wouldn't protect the filesystem structures. **By the way, are there more modern alternatives to Parchive, like Zstandard to gzip?**)\n\nI'm aware that checksumming is not enough of a failsafe and that I therefore should also make redundant copies and ideally keep them elsewhere.", "author_fullname": "t2_b9ussxlpm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which filesystem for cold archive? (Checksumming, compression, encryption)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hf1lq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684077906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I intend to use the several external HDDs that I already have to archive some data. Which filesystem do I use for this purpose? I like ZFS&amp;#39;s checksumming, but that seems to be geared more towards use on a running server rather than a cold archive. (E.g., it causes performance issues to fill up the filesystem beyond a certain threshold because of the way it allocates space. (Same for Btrfs?) For my purpose it would actually be optimal to allocate space left to right with no gaps because no file will ever be resized.)&lt;/p&gt;\n\n&lt;p&gt;Transparent compression and encryption would be nice, but I can also take care of that manually if need be. (I know of Parchive, with which I could do the checksumming manually as well, however, this wouldn&amp;#39;t protect the filesystem structures. &lt;strong&gt;By the way, are there more modern alternatives to Parchive, like Zstandard to gzip?&lt;/strong&gt;)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware that checksumming is not enough of a failsafe and that I therefore should also make redundant copies and ideally keep them elsewhere.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hf1lq", "is_robot_indexable": true, "report_reasons": null, "author": "_sunderlord_", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hf1lq/which_filesystem_for_cold_archive_checksumming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hf1lq/which_filesystem_for_cold_archive_checksumming/", "subreddit_subscribers": 682478, "created_utc": 1684077906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a problem that I'm trying to find a solution to. I have several hundred files of movie names that I need to rename. The current format is:  \n\n\nThe Movie name (CCYY).file \n\n&amp;#x200B;\n\nI'm trying to rename them en masse to make my life easier. The filenames vary from .mkv, .mp4, .avi, etc and the year also varies file to file, so using the MacOS file rename function isn't working well other than to take the \"The\" off the front. I really want them to end up like this:\n\n&amp;#x200B;\n\nMovie name, The (CCYY).file\n\n&amp;#x200B;\n\nAnyone know of a reliable way to make this happen on macOS?", "author_fullname": "t2_9irjj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mass Renaming in MacOS with complicated titles?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hejv2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684076676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a problem that I&amp;#39;m trying to find a solution to. I have several hundred files of movie names that I need to rename. The current format is:  &lt;/p&gt;\n\n&lt;p&gt;The Movie name (CCYY).file &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to rename them en masse to make my life easier. The filenames vary from .mkv, .mp4, .avi, etc and the year also varies file to file, so using the MacOS file rename function isn&amp;#39;t working well other than to take the &amp;quot;The&amp;quot; off the front. I really want them to end up like this:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Movie name, The (CCYY).file&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyone know of a reliable way to make this happen on macOS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hejv2", "is_robot_indexable": true, "report_reasons": null, "author": "Zxello5", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hejv2/mass_renaming_in_macos_with_complicated_titles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hejv2/mass_renaming_in_macos_with_complicated_titles/", "subreddit_subscribers": 682478, "created_utc": 1684076676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI do things with lots of heavy data (read: video, audio) and have amassed about 10 hard drives of projects over the years. Some of them are local backups, but I balance multiple projects simultaneously and find myself having to swap out drives/reorder files throughout the day. I also work on three different machines and have to swap drives between them. All this data management gets in the way of my workflow and I'd like to streamline a bit...\n\nIs there a solution whereby I can have the majority of my drives docked and access them from both machines, without taking a massive hit on speed when I am working with data directly from the drives?\nAnd ideally without spending a ton of money?\n\nMajority of my drives are 2.5 inch SSD, so if I have to choose a form factor it would be 2.5. \n\nMy intuition tells me I need to build a NAS but I don't really know much about them. Do I have to compromise on speed? Where do I start? Am I going to have to spend a lot of money?\n\nThanks!", "author_fullname": "t2_769qe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Management strategies for 10 hard drives and multiple computers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13he528", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684075661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I do things with lots of heavy data (read: video, audio) and have amassed about 10 hard drives of projects over the years. Some of them are local backups, but I balance multiple projects simultaneously and find myself having to swap out drives/reorder files throughout the day. I also work on three different machines and have to swap drives between them. All this data management gets in the way of my workflow and I&amp;#39;d like to streamline a bit...&lt;/p&gt;\n\n&lt;p&gt;Is there a solution whereby I can have the majority of my drives docked and access them from both machines, without taking a massive hit on speed when I am working with data directly from the drives?\nAnd ideally without spending a ton of money?&lt;/p&gt;\n\n&lt;p&gt;Majority of my drives are 2.5 inch SSD, so if I have to choose a form factor it would be 2.5. &lt;/p&gt;\n\n&lt;p&gt;My intuition tells me I need to build a NAS but I don&amp;#39;t really know much about them. Do I have to compromise on speed? Where do I start? Am I going to have to spend a lot of money?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13he528", "is_robot_indexable": true, "report_reasons": null, "author": "something_anonymous", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13he528/management_strategies_for_10_hard_drives_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13he528/management_strategies_for_10_hard_drives_and/", "subreddit_subscribers": 682478, "created_utc": 1684075661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Game preservation guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gplcz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_u3tdlu5o", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "GamePreservationists", "selftext": "Hi, some time ago I was thinking whether I am doing my back ups right, but I could not find a good guide, which would explain what devices are best for archiving, how to deal with corruption and so on. So, I ended up making my own broad guide on how to properly archive games.\n\nIt does not explain everything in complete detail, but I am sure there are some other people who will find it useful, so I uploaded it here:\n\ngameobservatory. neocities. org/ guides/ guide2", "author_fullname": "t2_u3tdlu5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Game preservation guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/GamePreservationists", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13az47q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683485346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.GamePreservationists", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, some time ago I was thinking whether I am doing my back ups right, but I could not find a good guide, which would explain what devices are best for archiving, how to deal with corruption and so on. So, I ended up making my own broad guide on how to properly archive games.&lt;/p&gt;\n\n&lt;p&gt;It does not explain everything in complete detail, but I am sure there are some other people who will find it useful, so I uploaded it here:&lt;/p&gt;\n\n&lt;p&gt;gameobservatory. neocities. org/ guides/ guide2&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2gx4id", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13az47q", "is_robot_indexable": true, "report_reasons": null, "author": "Gamobser", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/GamePreservationists/comments/13az47q/game_preservation_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/GamePreservationists/comments/13az47q/game_preservation_guide/", "subreddit_subscribers": 3900, "created_utc": 1683485346.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1684004064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.GamePreservationists", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/GamePreservationists/comments/13az47q/game_preservation_guide/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gplcz", "is_robot_indexable": true, "report_reasons": null, "author": "Gamobser", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13az47q", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gplcz/game_preservation_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/GamePreservationists/comments/13az47q/game_preservation_guide/", "subreddit_subscribers": 682478, "created_utc": 1684004064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got new hdds that atm I can't use because of server space. I would like to make them work but I don't know why. Maybe seeding?\n\nWhat would you do if you had spare drives of uneven capacity? what would you use them for? ideas?\n\nIn my case I got 2x3TB, 1x4TB, 2x1TB, 1x500GB", "author_fullname": "t2_3gdxkeob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what would you do with spare drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hhu98", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684084757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got new hdds that atm I can&amp;#39;t use because of server space. I would like to make them work but I don&amp;#39;t know why. Maybe seeding?&lt;/p&gt;\n\n&lt;p&gt;What would you do if you had spare drives of uneven capacity? what would you use them for? ideas?&lt;/p&gt;\n\n&lt;p&gt;In my case I got 2x3TB, 1x4TB, 2x1TB, 1x500GB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13hhu98", "is_robot_indexable": true, "report_reasons": null, "author": "JoaGamo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hhu98/what_would_you_do_with_spare_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hhu98/what_would_you_do_with_spare_drives/", "subreddit_subscribers": 682478, "created_utc": 1684084757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What block size should I choose for my file system on a RAID 6 LUN with 64KB stripes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hdtvr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_5fjqh0yg", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "homelab", "selftext": "Hello,\n\nThe whole question is in the title but here is the full context :\n\nI am using a Dell PowerEdge R720XD with PERC H710P, 12x 6TB 512e HDD in RAID 6 and Debian 11 as my only homelab server. This server stores a various data (18TB of videos, 6TB of binary data from the STORJ network, 2TB of miscellaneous files (home NAS), etc...) so I chose 64KB RAID stripes (default value).\n\nGiven the proportion of large files, some of you will certainly tell me that I should have used larger RAID bands.\u00a0It's probably right but, when I installed the server, I had no idea that I'll get this data distribution, these proportions are likely to change over time and now I can't change this without formatting the server. Whatever, any advice is welcome.\n\nHere's what my storage stack looks like:\n\nhttps://preview.redd.it/1ywd4col5tza1.png?width=1835&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d09725b3c718f61de8b29d4706023d1107ca59bb\n\nhttps://preview.redd.it/fqfb6t9m5tza1.png?width=1035&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=63ceb240ef82ec2cfd707efeae09ac6be1ab75d3\n\nMy objective is to get the maximum performance while keeping this maximum available storage (I don't want to switch to a RAID 10). From what I understood, the RAID controller will only write to disk every \\[stripe size\\] x \\[number of data drives\\] = 64KB x 10 (12 drives - 2 parity drives) = 640KB in my case\n\nIs this correct?\n\nIf yes :\n\n* then ideally I should instruct LVM and filesystems to use 64KB or 128KB (only divisors of 640KB which are &gt;=64KB) blocks to fill those 640KB flushed to disk as best as possible?\n* then why is the default value,\u00a0returned by the RAID controller to the OS, 512B and not 64KB?\n\nThanks in advance for your help and explanations !", "author_fullname": "t2_5fjqh0yg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What block size should I choose for my file system on a RAID 6 LUN with 64KB stripes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/homelab", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "media_metadata": {"fqfb6t9m5tza1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 40, "x": 108, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57d0e83867403539daea624d1b8521d887dc993a"}, {"y": 81, "x": 216, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d37180c5dfa5807280931f11219517405cb9644"}, {"y": 121, "x": 320, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad8ab265cf878b99ac9eafa521327973608a3960"}, {"y": 242, "x": 640, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f90c4b7ad41e17a9c296ad699bdc5bd6bbf91d91"}, {"y": 363, "x": 960, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a68ab64b7b3a6842b3ea8088dbf8339e59bf1833"}], "s": {"y": 392, "x": 1035, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=1035&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=63ceb240ef82ec2cfd707efeae09ac6be1ab75d3"}, "id": "fqfb6t9m5tza1"}, "1ywd4col5tza1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 21, "x": 108, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c6ad9c18c0d83f1c2dc7868a5a311161dd31928"}, {"y": 42, "x": 216, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9003581bb318bba67429b01a4ed3e13372bee96e"}, {"y": 63, "x": 320, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fb49675d317644304dd1f7ae118553db7d2cec5"}, {"y": 126, "x": 640, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30b5f06b3dcb7a1dced79e92c07a42b42a24a695"}, {"y": 189, "x": 960, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a70316f7645e26232851e25a15ed4aaa587e12c7"}, {"y": 213, "x": 1080, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee2766d50d52f3e37fe07048714a795bf671fbd1"}], "s": {"y": 363, "x": 1835, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=1835&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d09725b3c718f61de8b29d4706023d1107ca59bb"}, "id": "1ywd4col5tza1"}}, "name": "t3_13hdtd8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "8fc1a448-bbcf-11e4-9649-22000b2b8291", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684086623.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684074866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;The whole question is in the title but here is the full context :&lt;/p&gt;\n\n&lt;p&gt;I am using a Dell PowerEdge R720XD with PERC H710P, 12x 6TB 512e HDD in RAID 6 and Debian 11 as my only homelab server. This server stores a various data (18TB of videos, 6TB of binary data from the STORJ network, 2TB of miscellaneous files (home NAS), etc...) so I chose 64KB RAID stripes (default value).&lt;/p&gt;\n\n&lt;p&gt;Given the proportion of large files, some of you will certainly tell me that I should have used larger RAID bands.\u00a0It&amp;#39;s probably right but, when I installed the server, I had no idea that I&amp;#39;ll get this data distribution, these proportions are likely to change over time and now I can&amp;#39;t change this without formatting the server. Whatever, any advice is welcome.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what my storage stack looks like:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1ywd4col5tza1.png?width=1835&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d09725b3c718f61de8b29d4706023d1107ca59bb\"&gt;https://preview.redd.it/1ywd4col5tza1.png?width=1835&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d09725b3c718f61de8b29d4706023d1107ca59bb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fqfb6t9m5tza1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=63ceb240ef82ec2cfd707efeae09ac6be1ab75d3\"&gt;https://preview.redd.it/fqfb6t9m5tza1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=63ceb240ef82ec2cfd707efeae09ac6be1ab75d3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My objective is to get the maximum performance while keeping this maximum available storage (I don&amp;#39;t want to switch to a RAID 10). From what I understood, the RAID controller will only write to disk every [stripe size] x [number of data drives] = 64KB x 10 (12 drives - 2 parity drives) = 640KB in my case&lt;/p&gt;\n\n&lt;p&gt;Is this correct?&lt;/p&gt;\n\n&lt;p&gt;If yes :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;then ideally I should instruct LVM and filesystems to use 64KB or 128KB (only divisors of 640KB which are &amp;gt;=64KB) blocks to fill those 640KB flushed to disk as best as possible?&lt;/li&gt;\n&lt;li&gt;then why is the default value,\u00a0returned by the RAID controller to the OS, 512B and not 64KB?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance for your help and explanations !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "664a26e4-322a-11e6-80ae-0e0378709321", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Dell PowerEdge R720XD | Debian 11 | LVM + VDO | Ansible", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2ubz7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff6347", "id": "13hdtd8", "is_robot_indexable": true, "report_reasons": null, "author": "tigerblue77", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/homelab/comments/13hdtd8/what_block_size_should_i_choose_for_my_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/homelab/comments/13hdtd8/what_block_size_should_i_choose_for_my_file/", "subreddit_subscribers": 571357, "created_utc": 1684074866.0, "num_crossposts": 4, "media": null, "is_video": false}], "created": 1684074902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/homelab/comments/13hdtd8/what_block_size_should_i_choose_for_my_file/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "72TB R720XD  + 16TB R720 + 3,5TB homemade PC", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hdtvr", "is_robot_indexable": true, "report_reasons": null, "author": "tigerblue77", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13hdtd8", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hdtvr/what_block_size_should_i_choose_for_my_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/homelab/comments/13hdtd8/what_block_size_should_i_choose_for_my_file/", "subreddit_subscribers": 682478, "created_utc": 1684074902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, I currently have one zip where I have multiple folders with combined 50k files and photos. This zip is encrypted via 7zip. When I put a new folder in it I do see that it does a \"replicating\". This basically copies everything to a new zip (if I got it right), but this is starting to get tricky especially as my drive gets fuller.\n\n**So my question is**: Is it better to zip per folder with encryption, and then put all the encrypted zips into one big zip without encryption?", "author_fullname": "t2_f9v5z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategy for zipping folders and encryption", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h9lhc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684063357.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684063104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I currently have one zip where I have multiple folders with combined 50k files and photos. This zip is encrypted via 7zip. When I put a new folder in it I do see that it does a &amp;quot;replicating&amp;quot;. This basically copies everything to a new zip (if I got it right), but this is starting to get tricky especially as my drive gets fuller.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So my question is&lt;/strong&gt;: Is it better to zip per folder with encryption, and then put all the encrypted zips into one big zip without encryption?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h9lhc", "is_robot_indexable": true, "report_reasons": null, "author": "FlyingChinesePanda", "discussion_type": null, "num_comments": 8, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13h9lhc/strategy_for_zipping_folders_and_encryption/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h9lhc/strategy_for_zipping_folders_and_encryption/", "subreddit_subscribers": 682478, "created_utc": 1684063104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a Drobo 5D3 that I've been using for 5 years or so and quite liked it, even if it was a bit finnicky at times.  That being said, aside from it being time for an upgrade we all know where Drobo is going, so the sooner I can get off it the better.\n\nI'm looking for a DAS as so far the Drobo is populated with standard desktop drives that I don't trust running 24/7 in a NAS, and I already have a NAS project I'm building with a bunch of  smaller NAS drives.\n\nI'd like to be able to use it with both my M1 Mac and Windows PCs, so would prefer a hardware RAID solution.  I'd also like to have it be Thunderbolt and 10GB/s USB-C compatible -- M1 Mac has Thunderbolt and, while the current PC doesn't my planned upgrade will have Thunderbolt.\n\nI have 5 drives in the Drobo and 3 more drives new in box, so 8 bay would be fantastic but smaller is also fine.\n\nSo far I've seen the following:\n\n[OWC Thunderbay 8](https://www.amazon.com/OWC-ThunderBay-8-Bay-External-Thunderbolt/dp/B084S3S8JC?spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFCMVgzMlZXR0JZU1kmZW5jcnlwdGVkSWQ9QTA4Njc3ODlCVlU5UzNVRzVNREgmZW5jcnlwdGVkQWRJZD1BMDk4NjQyMDFaQ0Y3R0FBRE9DR0gmd2lkZ2V0TmFtZT1zcF9hdGYmYWN0aW9uPWNsaWNrUmVkaXJlY3QmZG9Ob3RMb2dDbGljaz10cnVl&amp;linkId=e5455e043ae2e9f5ce392b051ea24ac8&amp;language=en_US) \\-- SoftRAID but at least it is cross platform\n\n[TerraMaster D5](https://www.amazon.com/TerraMaster-Thunderbolt-Professional-Grade-External-Enclosure/dp/B07BHMCWMS?spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFQM1NLTDA0TDQ3MTgmZW5jcnlwdGVkSWQ9QTA2ODMwMTQxTDVBM01YNkRTMllFJmVuY3J5cHRlZEFkSWQ9QTAzOTAwNTIxUkU2TlhBSkVQTEpDJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ&amp;linkId=4bc4c586cd1c3d9e72f91ad0ef379829&amp;language=en_US)\n\nAny other thoughts?", "author_fullname": "t2_3sdmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drobo DAS alternative?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gp4ec", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684002902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a Drobo 5D3 that I&amp;#39;ve been using for 5 years or so and quite liked it, even if it was a bit finnicky at times.  That being said, aside from it being time for an upgrade we all know where Drobo is going, so the sooner I can get off it the better.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a DAS as so far the Drobo is populated with standard desktop drives that I don&amp;#39;t trust running 24/7 in a NAS, and I already have a NAS project I&amp;#39;m building with a bunch of  smaller NAS drives.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to be able to use it with both my M1 Mac and Windows PCs, so would prefer a hardware RAID solution.  I&amp;#39;d also like to have it be Thunderbolt and 10GB/s USB-C compatible -- M1 Mac has Thunderbolt and, while the current PC doesn&amp;#39;t my planned upgrade will have Thunderbolt.&lt;/p&gt;\n\n&lt;p&gt;I have 5 drives in the Drobo and 3 more drives new in box, so 8 bay would be fantastic but smaller is also fine.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve seen the following:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/OWC-ThunderBay-8-Bay-External-Thunderbolt/dp/B084S3S8JC?spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFCMVgzMlZXR0JZU1kmZW5jcnlwdGVkSWQ9QTA4Njc3ODlCVlU5UzNVRzVNREgmZW5jcnlwdGVkQWRJZD1BMDk4NjQyMDFaQ0Y3R0FBRE9DR0gmd2lkZ2V0TmFtZT1zcF9hdGYmYWN0aW9uPWNsaWNrUmVkaXJlY3QmZG9Ob3RMb2dDbGljaz10cnVl&amp;amp;linkId=e5455e043ae2e9f5ce392b051ea24ac8&amp;amp;language=en_US\"&gt;OWC Thunderbay 8&lt;/a&gt; -- SoftRAID but at least it is cross platform&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/TerraMaster-Thunderbolt-Professional-Grade-External-Enclosure/dp/B07BHMCWMS?spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFQM1NLTDA0TDQ3MTgmZW5jcnlwdGVkSWQ9QTA2ODMwMTQxTDVBM01YNkRTMllFJmVuY3J5cHRlZEFkSWQ9QTAzOTAwNTIxUkU2TlhBSkVQTEpDJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ&amp;amp;linkId=4bc4c586cd1c3d9e72f91ad0ef379829&amp;amp;language=en_US\"&gt;TerraMaster D5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any other thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gp4ec", "is_robot_indexable": true, "report_reasons": null, "author": "fuzzycuffs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gp4ec/drobo_das_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gp4ec/drobo_das_alternative/", "subreddit_subscribers": 682478, "created_utc": 1684002902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Read that verbatim have been rebranding their Blu-ray\u2019s as M-Discs so I\u2019m looking for a place to purchase reliably from another manufacturer. I need to start backing up some personal files forever. I\u2019m based in the US at NYC if anybody knows a local place as well.", "author_fullname": "t2_xwllr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reliable source of M-Discs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hhpl0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684084437.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Read that verbatim have been rebranding their Blu-ray\u2019s as M-Discs so I\u2019m looking for a place to purchase reliably from another manufacturer. I need to start backing up some personal files forever. I\u2019m based in the US at NYC if anybody knows a local place as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hhpl0", "is_robot_indexable": true, "report_reasons": null, "author": "Vatican87", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hhpl0/reliable_source_of_mdiscs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hhpl0/reliable_source_of_mdiscs/", "subreddit_subscribers": 682478, "created_utc": 1684084437.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I created a PowerShell script to automate the downloading process of update files from the Microsoft Update Catalog (www.catalog.update.microsoft.com), this way you can grab them for convenience or archiving purposes. My script supports advanced workarounds and options to make it as effective, flexible and seamless as possible to the user. Batch downloading, as well as language and NT version filters are available. This project started in 2021, but i have recently updated it to the 1.02 release with more advanced workarounds, features, as well as fixes for multiple language download.\n\nIf you are interested in hoarding or archiving windows updates, you will probably find this script handy. It is available on: https://github.com/blueclouds8666/msupdate-dl\n\nLet me know if you find it useful.", "author_fullname": "t2_8ff5t1fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Script for batch downloading windows updates, release 1.02", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hdres", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Windows", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684074734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created a PowerShell script to automate the downloading process of update files from the Microsoft Update Catalog (&lt;a href=\"http://www.catalog.update.microsoft.com\"&gt;www.catalog.update.microsoft.com&lt;/a&gt;), this way you can grab them for convenience or archiving purposes. My script supports advanced workarounds and options to make it as effective, flexible and seamless as possible to the user. Batch downloading, as well as language and NT version filters are available. This project started in 2021, but i have recently updated it to the 1.02 release with more advanced workarounds, features, as well as fixes for multiple language download.&lt;/p&gt;\n\n&lt;p&gt;If you are interested in hoarding or archiving windows updates, you will probably find this script handy. It is available on: &lt;a href=\"https://github.com/blueclouds8666/msupdate-dl\"&gt;https://github.com/blueclouds8666/msupdate-dl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know if you find it useful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13hdres", "is_robot_indexable": true, "report_reasons": null, "author": "blueclouds8666", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hdres/script_for_batch_downloading_windows_updates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hdres/script_for_batch_downloading_windows_updates/", "subreddit_subscribers": 682478, "created_utc": 1684074734.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}