{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ucst1pa3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We have backed up the world\u2019s largest comics shadow library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13gn07s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 778, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 778, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hmXUnF_Q3dc-xFXtgtWStrwbNDsjn9dakdnqyIVwZlU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683997738.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "annas-blog.org", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://annas-blog.org/backed-up-the-worlds-largest-comics-shadow-lib.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/40n9LDd7IdrWGJNvbmuthNDd_0fP0tbWFAPipHBCNPM.jpg?auto=webp&amp;v=enabled&amp;s=03095ab28bac0d537d47574e7e167893a3616e0c", "width": 513, "height": 270}, "resolutions": [{"url": "https://external-preview.redd.it/40n9LDd7IdrWGJNvbmuthNDd_0fP0tbWFAPipHBCNPM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba14eba794b44a8f4b164b67200f60da5e4622ca", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/40n9LDd7IdrWGJNvbmuthNDd_0fP0tbWFAPipHBCNPM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1dc460c2dae35dc23a82f1e6cb7a776f2c5cd84d", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/40n9LDd7IdrWGJNvbmuthNDd_0fP0tbWFAPipHBCNPM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81a9cf73b54b0655cc611cf2f5e5b762524a3912", "width": 320, "height": 168}], "variants": {}, "id": "PPuw_FdXbP9HAeq2KFtJRhMY2cwn9i3h5pWIQndrkBM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gn07s", "is_robot_indexable": true, "report_reasons": null, "author": "AnnaArchivist", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gn07s/we_have_backed_up_the_worlds_largest_comics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://annas-blog.org/backed-up-the-worlds-largest-comics-shadow-lib.html", "subreddit_subscribers": 682393, "created_utc": 1683997738.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,  \n\n\nMy friend passed few years ago and I need to archive his tweets before his account gets deleted (thanks elon). He was an artist so there is some media (his drawings) but I'd like to archive his entire profile even with his old posts, replies, retweets etc., and that's why I'd like to have some UI to read everything like it's on twitter.\n\nI've already used snscrape to get every tweet link, but I miss the \"visual representation of the data\". Do anyone of you know which tool can I use?\n\nThanks &lt;3", "author_fullname": "t2_9egzy7p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with archiving my old friend's tweets", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gohrs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 31, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 31, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684001349.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,  &lt;/p&gt;\n\n&lt;p&gt;My friend passed few years ago and I need to archive his tweets before his account gets deleted (thanks elon). He was an artist so there is some media (his drawings) but I&amp;#39;d like to archive his entire profile even with his old posts, replies, retweets etc., and that&amp;#39;s why I&amp;#39;d like to have some UI to read everything like it&amp;#39;s on twitter.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve already used snscrape to get every tweet link, but I miss the &amp;quot;visual representation of the data&amp;quot;. Do anyone of you know which tool can I use?&lt;/p&gt;\n\n&lt;p&gt;Thanks &amp;lt;3&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "200TB (2x100TB) ZFS RAID6", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gohrs", "is_robot_indexable": true, "report_reasons": null, "author": "Arturro43", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13gohrs/help_with_archiving_my_old_friends_tweets/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gohrs/help_with_archiving_my_old_friends_tweets/", "subreddit_subscribers": 682393, "created_utc": 1684001349.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! Is using old NAS for backups safe? I wouldn't use it for any file sharing, clouds etc. Just simply connected to router to regularly backup files. I read few topics that overall using old NAS'es isn't the best idea but not sure if they were talking about just data storage or something else.  \n\n\nEdit  \n\n\nBy old I mean devices like 10 years old, with no support nor new software", "author_fullname": "t2_bty3k7xm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old NAS as backup server - is it safe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gyc8h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684026485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Is using old NAS for backups safe? I wouldn&amp;#39;t use it for any file sharing, clouds etc. Just simply connected to router to regularly backup files. I read few topics that overall using old NAS&amp;#39;es isn&amp;#39;t the best idea but not sure if they were talking about just data storage or something else.  &lt;/p&gt;\n\n&lt;p&gt;Edit  &lt;/p&gt;\n\n&lt;p&gt;By old I mean devices like 10 years old, with no support nor new software&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gyc8h", "is_robot_indexable": true, "report_reasons": null, "author": "luki52721", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gyc8h/old_nas_as_backup_server_is_it_safe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gyc8h/old_nas_as_backup_server_is_it_safe/", "subreddit_subscribers": 682393, "created_utc": 1684026485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey. since tomorrow is the day when Imgur wipes out its non-logged in images, I am wondering if any of you in here, or elsewhere like on Archive Team, have been archiving imgur links not just from reddit, but also from other sites.\n\nIn particular, if this is still possible, I would like to request [imgur links from the alternate history forum](https://www.alternatehistory.com/forum/) to be archived, I am an amateur cartographer, and possibly thousands of maps, flags, photoshopped, and historical photos could be gone.", "author_fullname": "t2_b9uquxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On the Imgur archival, have any of you been searching for forums outside of reddit? can I also request the archival of Imgur links from the Alternate History forum?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h15el", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": "", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684034513.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey. since tomorrow is the day when Imgur wipes out its non-logged in images, I am wondering if any of you in here, or elsewhere like on Archive Team, have been archiving imgur links not just from reddit, but also from other sites.&lt;/p&gt;\n\n&lt;p&gt;In particular, if this is still possible, I would like to request &lt;a href=\"https://www.alternatehistory.com/forum/\"&gt;imgur links from the alternate history forum&lt;/a&gt; to be archived, I am an amateur cartographer, and possibly thousands of maps, flags, photoshopped, and historical photos could be gone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h15el", "is_robot_indexable": true, "report_reasons": null, "author": "wq1119", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13h15el/on_the_imgur_archival_have_any_of_you_been/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h15el/on_the_imgur_archival_have_any_of_you_been/", "subreddit_subscribers": 682393, "created_utc": 1684034513.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello to all good people on this sub!\n\nA little backstory *- skip to* ***below*** *if you're not interesting in reading some fluff.*\n\nRecently, a friend of mine have given me a couple of old drives he had no use for, of which 3 HDDs turned out to be working and even passed the full read scans in Victoria. 2 of them were formatted already and have no data, but one 2.5\" 320 GB WD HDD has the previous owner's files intact, which include photos and other private stuff (though I haven't looked that much 'cuz that's rude lol). The friend looked at those and told me that they likely aren't needed anymore and I can format everything. But, to be honest, it just feels wrong to me.\n\nNow, one of the other working two HDDs is a mid-00s 3.5\" 7200RPM 320GB WD monstrosity, which, while works, gives of a constant hum like from a power grid station or something, which gets on my nerves (as I have a fairly quiet PC), so I decided to not use it. But since it can still store data, I decided that maybe I'll use it for backing up the data from the aforementioned drive. (And yes, I understand that that drive can fail any time now due to how old it is, but the data are apparently not needed anymore anyway and it's the only storage medium I'll have no use for, so a flimsy backup would be better than no backup at all, right?)\n\n***-- backstory end --***\n\nSo, I need to copy the entirety of a 320 GB drive (of which \\~160 GB is occupied) to another drive as a compressed file. Ideally, I would have the following requirements:\n\n* The backed up copy should be able to be restored in a way that the disk would be exactly the same as it was at the time of backup - MBR/boot and all partitions with data intact.\n* The backed up copy should be browseable (ideally - with programs other than the one that made the backup, too), so I could look up and extract individual files without having to restore the whole backup.\n* The resulting disk image needs to be compressed, or at least not have the empty parts of the disk's partitions take space in the image (so the backup file would be of those \\~160 GB or less in size instead of full 320 GB as the source disk is).\n\nI googled around and found some useful suggestions in [this thread](https://www.reddit.com/r/DataHoarder/comments/c47wwn/best_software_for_backing_upcloning_entire_hard/), such as dd, Clonezilla, Acronis and some others. However, I'm not sure if any of those can make a backup that would fit all three of my requirements. For example, `dd if=/dev/sda status=progress | lz4 -c &gt; ~/my_disk.lz4` looks like the easiest way (I have an external SSD with Linux on it), but it likely fails the browseability requirement if compressed, and if not the compression one is failed instead, same with Clonezilla (as I understand, both copy everything exactly including the empty space, which I want to avoid). Acronis is popular enough that I think it will probably meet all 3 requirements, but I want to know for sure. TeraByte Image looks really interesting in its ability to omit some unnecessary data (such as hibernation data, pagefile and logs, saw on [this screenshot](https://www.terabyteunlimited.com/wp-content/uploads/2021/09/ss_ifw_04-min.png)), and it has compression, but I don't know about the browseability. Same with MiniTool ShadowMaker, and lots of others.\n\nMy main problem here is uncertainty - I just don't know which software should I pick for the task, as they all seem to mostly do what I need them to, except for my requirements. I think I could've just try every solution for myself, but I don't really want to spend my Sunday evening (or any evening, per se) on finding out the best way to copy some not that important data to an old, slow, and *HUMMING* drive. I want to just make that backup, disconnect the target drive, and be done with it lol.\n\nSo, what software would fit my requirements? Or if none does, which suggestions for my use case would be? Any responses are welcome!\n\n^(And thank you if you've read this wall of text :\\))", "author_fullname": "t2_13zs23", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to make a compressed backup of an HDD before formatting it, with an ability to browse the backed up image's contents and be able to restore everything exactly to how it was, just in case. What software would be the best for the task?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hcziy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684072783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello to all good people on this sub!&lt;/p&gt;\n\n&lt;p&gt;A little backstory &lt;em&gt;- skip to&lt;/em&gt; &lt;strong&gt;&lt;em&gt;below&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;if you&amp;#39;re not interesting in reading some fluff.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Recently, a friend of mine have given me a couple of old drives he had no use for, of which 3 HDDs turned out to be working and even passed the full read scans in Victoria. 2 of them were formatted already and have no data, but one 2.5&amp;quot; 320 GB WD HDD has the previous owner&amp;#39;s files intact, which include photos and other private stuff (though I haven&amp;#39;t looked that much &amp;#39;cuz that&amp;#39;s rude lol). The friend looked at those and told me that they likely aren&amp;#39;t needed anymore and I can format everything. But, to be honest, it just feels wrong to me.&lt;/p&gt;\n\n&lt;p&gt;Now, one of the other working two HDDs is a mid-00s 3.5&amp;quot; 7200RPM 320GB WD monstrosity, which, while works, gives of a constant hum like from a power grid station or something, which gets on my nerves (as I have a fairly quiet PC), so I decided to not use it. But since it can still store data, I decided that maybe I&amp;#39;ll use it for backing up the data from the aforementioned drive. (And yes, I understand that that drive can fail any time now due to how old it is, but the data are apparently not needed anymore anyway and it&amp;#39;s the only storage medium I&amp;#39;ll have no use for, so a flimsy backup would be better than no backup at all, right?)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;-- backstory end --&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;So, I need to copy the entirety of a 320 GB drive (of which ~160 GB is occupied) to another drive as a compressed file. Ideally, I would have the following requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The backed up copy should be able to be restored in a way that the disk would be exactly the same as it was at the time of backup - MBR/boot and all partitions with data intact.&lt;/li&gt;\n&lt;li&gt;The backed up copy should be browseable (ideally - with programs other than the one that made the backup, too), so I could look up and extract individual files without having to restore the whole backup.&lt;/li&gt;\n&lt;li&gt;The resulting disk image needs to be compressed, or at least not have the empty parts of the disk&amp;#39;s partitions take space in the image (so the backup file would be of those ~160 GB or less in size instead of full 320 GB as the source disk is).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I googled around and found some useful suggestions in &lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/c47wwn/best_software_for_backing_upcloning_entire_hard/\"&gt;this thread&lt;/a&gt;, such as dd, Clonezilla, Acronis and some others. However, I&amp;#39;m not sure if any of those can make a backup that would fit all three of my requirements. For example, &lt;code&gt;dd if=/dev/sda status=progress | lz4 -c &amp;gt; ~/my_disk.lz4&lt;/code&gt; looks like the easiest way (I have an external SSD with Linux on it), but it likely fails the browseability requirement if compressed, and if not the compression one is failed instead, same with Clonezilla (as I understand, both copy everything exactly including the empty space, which I want to avoid). Acronis is popular enough that I think it will probably meet all 3 requirements, but I want to know for sure. TeraByte Image looks really interesting in its ability to omit some unnecessary data (such as hibernation data, pagefile and logs, saw on &lt;a href=\"https://www.terabyteunlimited.com/wp-content/uploads/2021/09/ss_ifw_04-min.png\"&gt;this screenshot&lt;/a&gt;), and it has compression, but I don&amp;#39;t know about the browseability. Same with MiniTool ShadowMaker, and lots of others.&lt;/p&gt;\n\n&lt;p&gt;My main problem here is uncertainty - I just don&amp;#39;t know which software should I pick for the task, as they all seem to mostly do what I need them to, except for my requirements. I think I could&amp;#39;ve just try every solution for myself, but I don&amp;#39;t really want to spend my Sunday evening (or any evening, per se) on finding out the best way to copy some not that important data to an old, slow, and &lt;em&gt;HUMMING&lt;/em&gt; drive. I want to just make that backup, disconnect the target drive, and be done with it lol.&lt;/p&gt;\n\n&lt;p&gt;So, what software would fit my requirements? Or if none does, which suggestions for my use case would be? Any responses are welcome!&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;And thank you if you&amp;#39;ve read this wall of text :\\&lt;/sup&gt;)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?auto=webp&amp;v=enabled&amp;s=0c3965c3168cf02ba36d0a843a40556c9382b69f", "width": 688, "height": 539}, "resolutions": [{"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04ba787f24f535c39cb3ee62c063d2b4b7cd12cf", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6013acb016d77465e1a5e8aa11a2f4af7e31b90a", "width": 216, "height": 169}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d9d4e2f687e375000bf562ec4eae41718f7cb3db", "width": 320, "height": 250}, {"url": "https://external-preview.redd.it/skKAa7-ZwoXP_HFzh50Xfq4MWbaMLRYHYXtnV8AV_ao.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da7d0805daa602cedc14f8c3a8134e891b865dcf", "width": 640, "height": 501}], "variants": {}, "id": "lMVX8-JabBvuaas6wAJ7e3HAGDfG-Ea7elA1S0S6mj4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hcziy", "is_robot_indexable": true, "report_reasons": null, "author": "AGTS10k", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hcziy/need_to_make_a_compressed_backup_of_an_hdd_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hcziy/need_to_make_a_compressed_backup_of_an_hdd_before/", "subreddit_subscribers": 682393, "created_utc": 1684072783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Some tips for manually finding and archiving all the Imgur links in your reddit post history:\n\n1. Download the Reddit Enhancement Suite for its autoscrolling feature\n\n2. Go to old.reddit.com/user/YOUR_USERNAME , which is your Overview page sorted by \"New\"\n\n3. Scroll down until it won't load anymore posts (due to the 1000 post API limit)\n\n4. Run this console command: `document.querySelectorAll('.usertext-body .md a').forEach(function(element, index){element.text = element.href});`\n\n5. Ctrl+F search for `imgur.` and go through the results\n\n6. Run each URL through the latest archived image checker: https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png\n\n7. If a URL is missing, run it through the Save Page Now tool: https://web.archive.org/save\n\n8. After you've finished processing the results, redo the process for the Overview Top, Comments New, Comments Top, Submitted New, and Submitted Top results. This is important if you have a long reddit history because they'll help you get around the 1000 post limit due to them loading the posts in a different order.", "author_fullname": "t2_4bc1l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Resource] How to manually find and archive almost all the Imgur links in your reddit post history and avoid the 1000 post API limit", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hdv65", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684074985.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some tips for manually finding and archiving all the Imgur links in your reddit post history:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Download the Reddit Enhancement Suite for its autoscrolling feature&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Go to old.reddit.com/user/YOUR_USERNAME , which is your Overview page sorted by &amp;quot;New&amp;quot;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Scroll down until it won&amp;#39;t load anymore posts (due to the 1000 post API limit)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run this console command: &lt;code&gt;document.querySelectorAll(&amp;#39;.usertext-body .md a&amp;#39;).forEach(function(element, index){element.text = element.href});&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Ctrl+F search for &lt;code&gt;imgur.&lt;/code&gt; and go through the results&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run each URL through the latest archived image checker: &lt;a href=\"https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png\"&gt;https://web.archive.org/web/20290403101433/https://i.imgur.com/jwuDhEW.png&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If a URL is missing, run it through the Save Page Now tool: &lt;a href=\"https://web.archive.org/save\"&gt;https://web.archive.org/save&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;After you&amp;#39;ve finished processing the results, redo the process for the Overview Top, Comments New, Comments Top, Submitted New, and Submitted Top results. This is important if you have a long reddit history because they&amp;#39;ll help you get around the 1000 post limit due to them loading the posts in a different order.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?auto=webp&amp;v=enabled&amp;s=03f6f54b024049096bac8b6984c80c3bd5669e4d", "width": 448, "height": 310}, "resolutions": [{"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebfa371e414b141502de697ab78ff8c8ef644ab4", "width": 108, "height": 74}, {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1d0f2af124cd3fb85d38e60b6391d608a94761a", "width": 216, "height": 149}, {"url": "https://external-preview.redd.it/w31OeNkzmB_xmgOfO8FrYLjGQrnyUswoYgZJ4TKKXNw.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5419c7771818bf926687520bb178125b9fad7e46", "width": 320, "height": 221}], "variants": {}, "id": "L23lfyVQ0rA1Gxs5Ds3_qLTfvRjLtonmKPj-Zd2YhT4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hdv65", "is_robot_indexable": true, "report_reasons": null, "author": "Pikamander2", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hdv65/resource_how_to_manually_find_and_archive_almost/", "subreddit_subscribers": 682393, "created_utc": 1684074985.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I recently did a new build and it has the typical power light LED on the front of the tower. \n\nWell, when building I got to thinking about HDD activity lights.\n\nI know, I'm a geek...but I want to connect an LED of some sort to the motherboard HDD activity header. I have the Fractal Meshify so there's plenty of options on where to mount them, behind the front mesh - it's what I need to buy is what I need help with. \n\nSo, does anyone have experience with this and know what I might need, does it need a resistor or can I literally hookup any LED, once I obtain the proper cable?\n\nI'm sure one of you has added HDD activity to your build since it's not typically included on a lot of newer towers. Your advice is as always, appreciated.\n\nP.S. Any pics of your build (or addition of HDD LED's would be cool to see.", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Adding some blinking lights (HDD Activity)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hd4xp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684073144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I recently did a new build and it has the typical power light LED on the front of the tower. &lt;/p&gt;\n\n&lt;p&gt;Well, when building I got to thinking about HDD activity lights.&lt;/p&gt;\n\n&lt;p&gt;I know, I&amp;#39;m a geek...but I want to connect an LED of some sort to the motherboard HDD activity header. I have the Fractal Meshify so there&amp;#39;s plenty of options on where to mount them, behind the front mesh - it&amp;#39;s what I need to buy is what I need help with. &lt;/p&gt;\n\n&lt;p&gt;So, does anyone have experience with this and know what I might need, does it need a resistor or can I literally hookup any LED, once I obtain the proper cable?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure one of you has added HDD activity to your build since it&amp;#39;s not typically included on a lot of newer towers. Your advice is as always, appreciated.&lt;/p&gt;\n\n&lt;p&gt;P.S. Any pics of your build (or addition of HDD LED&amp;#39;s would be cool to see.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13hd4xp", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hd4xp/adding_some_blinking_lights_hdd_activity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hd4xp/adding_some_blinking_lights_hdd_activity/", "subreddit_subscribers": 682393, "created_utc": 1684073144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "If you are looking for Clouds to expand your storage or simply to backup, dont go with google!\n\nIm using Google Drive on Windows for \\~1 month now and im really asking myself how a big company like Google can make such a bad Application for it.\n\nBefore i get into the problems i have with it i want to say that i also use OneDrive and have used Dropbox before that. But they never had any issues like Google Drive has.\n\n&amp;#x200B;\n\n1. **Syncing Errors** \\- In comparison to OneDrive &amp; Dropbox, which occassionally throw an error like \"the Filename includes chars that are not allowed\", Google Drive throws errors that shouldnt even happen. ex.: \"You have no permission to upload File 'X5' into the Directory 'X'\" after uploading 'X1'-'X4' to Directory 'X' without a problem...\n\n2. **Lost &amp; Found** \\- When Google Drive has a problem while syncing a file, it wont just try again later (that would be too easy &amp; user friendly), it will throw the File into the \"Lost &amp; Found\" Directory. Not only is this a local Directory, which wont get synced, you will also have to move the file back into the place it belongs manually. Oh, and if you uploaded lets say \\~5000 Directories containing \\~200k Files &amp; \\~500 Files got moved into L&amp;F... have fun finding out where each file belongs, as there is no information where it was before.\n\n3. **Account Connection** \\- in the 1 month im using Google Drive i got the \"cant connect to account, please login again\" 4 times. And now the real fun begins. Im using the Setting where Drive will act as a volume in your pc. The last \"cant connect\" (today) happened midsync, now all files that havent been uploaded yet are **GONE**. The Lost&amp;Found Directory, **GONE**. I couldnt even recover anything with recuva...\n\n&amp;#x200B;\n\n**TLDR:** Google Drive for Windows has so many Problems that its not just a pain to use, it also caused the loss of around 10% of the 1,5TB Files i tried to upload.\n\nAnd before someone asks: No, Google Drive &amp; OneDrive havent tried to use the same files. My OneDrive is on a different SSD, doesnt auto-backup anything, just uploads what i put into the OneDrive directory.\n\nIm kinda mad that such a huge amount of Files has been lost by a service that is there to \"keep your files well protected in the cloud\"... Luckily i used Google Drive only for not so important Stuff while putting the important Stuff on OneDrive, where it is really \"well protected in the cloud\".", "author_fullname": "t2_3lrsk4yj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive for Windows is TERRIBLE, here is why.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hd1br", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684072896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are looking for Clouds to expand your storage or simply to backup, dont go with google!&lt;/p&gt;\n\n&lt;p&gt;Im using Google Drive on Windows for ~1 month now and im really asking myself how a big company like Google can make such a bad Application for it.&lt;/p&gt;\n\n&lt;p&gt;Before i get into the problems i have with it i want to say that i also use OneDrive and have used Dropbox before that. But they never had any issues like Google Drive has.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Syncing Errors&lt;/strong&gt; - In comparison to OneDrive &amp;amp; Dropbox, which occassionally throw an error like &amp;quot;the Filename includes chars that are not allowed&amp;quot;, Google Drive throws errors that shouldnt even happen. ex.: &amp;quot;You have no permission to upload File &amp;#39;X5&amp;#39; into the Directory &amp;#39;X&amp;#39;&amp;quot; after uploading &amp;#39;X1&amp;#39;-&amp;#39;X4&amp;#39; to Directory &amp;#39;X&amp;#39; without a problem...&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lost &amp;amp; Found&lt;/strong&gt; - When Google Drive has a problem while syncing a file, it wont just try again later (that would be too easy &amp;amp; user friendly), it will throw the File into the &amp;quot;Lost &amp;amp; Found&amp;quot; Directory. Not only is this a local Directory, which wont get synced, you will also have to move the file back into the place it belongs manually. Oh, and if you uploaded lets say ~5000 Directories containing ~200k Files &amp;amp; ~500 Files got moved into L&amp;amp;F... have fun finding out where each file belongs, as there is no information where it was before.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Account Connection&lt;/strong&gt; - in the 1 month im using Google Drive i got the &amp;quot;cant connect to account, please login again&amp;quot; 4 times. And now the real fun begins. Im using the Setting where Drive will act as a volume in your pc. The last &amp;quot;cant connect&amp;quot; (today) happened midsync, now all files that havent been uploaded yet are &lt;strong&gt;GONE&lt;/strong&gt;. The Lost&amp;amp;Found Directory, &lt;strong&gt;GONE&lt;/strong&gt;. I couldnt even recover anything with recuva...&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Google Drive for Windows has so many Problems that its not just a pain to use, it also caused the loss of around 10% of the 1,5TB Files i tried to upload.&lt;/p&gt;\n\n&lt;p&gt;And before someone asks: No, Google Drive &amp;amp; OneDrive havent tried to use the same files. My OneDrive is on a different SSD, doesnt auto-backup anything, just uploads what i put into the OneDrive directory.&lt;/p&gt;\n\n&lt;p&gt;Im kinda mad that such a huge amount of Files has been lost by a service that is there to &amp;quot;keep your files well protected in the cloud&amp;quot;... Luckily i used Google Drive only for not so important Stuff while putting the important Stuff on OneDrive, where it is really &amp;quot;well protected in the cloud&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "9.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hd1br", "is_robot_indexable": true, "report_reasons": null, "author": "TriQancer", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hd1br/google_drive_for_windows_is_terrible_here_is_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hd1br/google_drive_for_windows_is_terrible_here_is_why/", "subreddit_subscribers": 682393, "created_utc": 1684072896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone! For my work, i'm recording about 80 hours (\\~350gb) webinars per month and I need a cost effective way to store this data. I record the videos on either my iphone or chromebook and then my virtual assistant edits these videos and sends smaller clips back to me.\n\n&amp;#x200B;\n\nI do NOT want to pay for both/ do more work using both. It's important that the transferring process is simple and quick!\n\nCan someone help me? \n\nIf you recommend cloud storage, what plan should i go with?", "author_fullname": "t2_ap36uryh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Store Weekly Videos On Cloud OR External Drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h16ad", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684041472.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684034589.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! For my work, i&amp;#39;m recording about 80 hours (~350gb) webinars per month and I need a cost effective way to store this data. I record the videos on either my iphone or chromebook and then my virtual assistant edits these videos and sends smaller clips back to me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I do NOT want to pay for both/ do more work using both. It&amp;#39;s important that the transferring process is simple and quick!&lt;/p&gt;\n\n&lt;p&gt;Can someone help me? &lt;/p&gt;\n\n&lt;p&gt;If you recommend cloud storage, what plan should i go with?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h16ad", "is_robot_indexable": true, "report_reasons": null, "author": "Zestyclose_Froyo_558", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13h16ad/store_weekly_videos_on_cloud_or_external_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h16ad/store_weekly_videos_on_cloud_or_external_drive/", "subreddit_subscribers": 682393, "created_utc": 1684034589.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI'm sure this question has been asked before, but I personally couldn't find a clear answer for my case so I'd like a bit of advice. As a preventative measure, I would like to back up all my files to the cloud in case of a drive failure. Ideally, I don't want to spend my whole life savings on it but I don't mind spending a bit to have all my data backed up. I have around 2.5TB to back up, so it's not a huge amount.\n\nWhat are your recommendations for a cloud backup provider that lets me upload my drives and restore in case of a drive failure?\n\nThanks", "author_fullname": "t2_12sl74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup data to cloud in case of drive failure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gupg4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684016866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure this question has been asked before, but I personally couldn&amp;#39;t find a clear answer for my case so I&amp;#39;d like a bit of advice. As a preventative measure, I would like to back up all my files to the cloud in case of a drive failure. Ideally, I don&amp;#39;t want to spend my whole life savings on it but I don&amp;#39;t mind spending a bit to have all my data backed up. I have around 2.5TB to back up, so it&amp;#39;s not a huge amount.&lt;/p&gt;\n\n&lt;p&gt;What are your recommendations for a cloud backup provider that lets me upload my drives and restore in case of a drive failure?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gupg4", "is_robot_indexable": true, "report_reasons": null, "author": "DiamoNNNd1337", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gupg4/backup_data_to_cloud_in_case_of_drive_failure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gupg4/backup_data_to_cloud_in_case_of_drive_failure/", "subreddit_subscribers": 682393, "created_utc": 1684016866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello!\n\nI do things with lots of heavy data (read: video, audio) and have amassed about 10 hard drives of projects over the years. Some of them are local backups, but I balance multiple projects simultaneously and find myself having to swap out drives/reorder files throughout the day. I also work on three different machines and have to swap drives between them. All this data management gets in the way of my workflow and I'd like to streamline a bit...\n\nIs there a solution whereby I can have the majority of my drives docked and access them from both machines, without taking a massive hit on speed when I am working with data directly from the drives?\nAnd ideally without spending a ton of money?\n\nMajority of my drives are 2.5 inch SSD, so if I have to choose a form factor it would be 2.5. \n\nMy intuition tells me I need to build a NAS but I don't really know much about them. Do I have to compromise on speed? Where do I start? Am I going to have to spend a lot of money?\n\nThanks!", "author_fullname": "t2_769qe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Management strategies for 10 hard drives and multiple computers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13he528", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684075661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I do things with lots of heavy data (read: video, audio) and have amassed about 10 hard drives of projects over the years. Some of them are local backups, but I balance multiple projects simultaneously and find myself having to swap out drives/reorder files throughout the day. I also work on three different machines and have to swap drives between them. All this data management gets in the way of my workflow and I&amp;#39;d like to streamline a bit...&lt;/p&gt;\n\n&lt;p&gt;Is there a solution whereby I can have the majority of my drives docked and access them from both machines, without taking a massive hit on speed when I am working with data directly from the drives?\nAnd ideally without spending a ton of money?&lt;/p&gt;\n\n&lt;p&gt;Majority of my drives are 2.5 inch SSD, so if I have to choose a form factor it would be 2.5. &lt;/p&gt;\n\n&lt;p&gt;My intuition tells me I need to build a NAS but I don&amp;#39;t really know much about them. Do I have to compromise on speed? Where do I start? Am I going to have to spend a lot of money?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13he528", "is_robot_indexable": true, "report_reasons": null, "author": "something_anonymous", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13he528/management_strategies_for_10_hard_drives_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13he528/management_strategies_for_10_hard_drives_and/", "subreddit_subscribers": 682393, "created_utc": 1684075661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What block size should I choose for my file system on a RAID 6 LUN with 64KB stripes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hdtvr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "author_fullname": "t2_5fjqh0yg", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "homelab", "selftext": "Hello,\n\nThe whole question is in the title but here is the full context :\n\nI am using a Dell PowerEdge R720XD with PERC H710P, 12x 6TB 512e HDD in RAID 6 and Debian 11 as my only homelab server. This server stores a various data (18TB of videos, 6TB of binary data from the STORJ network, 2TB of miscellaneous files (home NAS), etc...) so I chose 64KB RAID stripes (default value).\n\nGiven the proportion of large files, some of you will certainly tell me that I should have used larger RAID bands.\u00a0It's probably right but, when I installed the server, I had no idea that I'll get this data distribution, these proportions are likely to change over time and now I can't change this without formatting the server. Whatever, any advice is welcome.\n\nHere's what my storage stack looks like:\n\nhttps://preview.redd.it/1ywd4col5tza1.png?width=1835&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d09725b3c718f61de8b29d4706023d1107ca59bb\n\nhttps://preview.redd.it/fqfb6t9m5tza1.png?width=1035&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=63ceb240ef82ec2cfd707efeae09ac6be1ab75d3\n\nMy objective is to get the maximum performance while keeping this maximum available storage (I don't want to switch to a RAID 10). From what I understood, the RAID controller will only write to disk every \\[stripe size\\] x \\[number of data drives\\] = 64KB x 10 (12 drives - 2 parity drives) = 640KB in my case\n\nIs this correct?\n\nIf yes :\n\n* then ideally I should instruct LVM and filesystems to use 64KB or 128KB (only divisors of 640KB which are &gt;=64KB) blocks to fill those 640KB flushed to disk as best as possible?\n* then why is the default value,\u00a0returned by the RAID controller to the OS, 512B?\n\nThanks in advance for your help and explanations !", "author_fullname": "t2_5fjqh0yg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What block size should I choose for my file system on a RAID 6 LUN with 64KB stripes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/homelab", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "media_metadata": {"fqfb6t9m5tza1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 40, "x": 108, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57d0e83867403539daea624d1b8521d887dc993a"}, {"y": 81, "x": 216, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d37180c5dfa5807280931f11219517405cb9644"}, {"y": 121, "x": 320, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ad8ab265cf878b99ac9eafa521327973608a3960"}, {"y": 242, "x": 640, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f90c4b7ad41e17a9c296ad699bdc5bd6bbf91d91"}, {"y": 363, "x": 960, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a68ab64b7b3a6842b3ea8088dbf8339e59bf1833"}], "s": {"y": 392, "x": 1035, "u": "https://preview.redd.it/fqfb6t9m5tza1.png?width=1035&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=63ceb240ef82ec2cfd707efeae09ac6be1ab75d3"}, "id": "fqfb6t9m5tza1"}, "1ywd4col5tza1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 21, "x": 108, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c6ad9c18c0d83f1c2dc7868a5a311161dd31928"}, {"y": 42, "x": 216, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9003581bb318bba67429b01a4ed3e13372bee96e"}, {"y": 63, "x": 320, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fb49675d317644304dd1f7ae118553db7d2cec5"}, {"y": 126, "x": 640, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30b5f06b3dcb7a1dced79e92c07a42b42a24a695"}, {"y": 189, "x": 960, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a70316f7645e26232851e25a15ed4aaa587e12c7"}, {"y": 213, "x": 1080, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee2766d50d52f3e37fe07048714a795bf671fbd1"}], "s": {"y": 363, "x": 1835, "u": "https://preview.redd.it/1ywd4col5tza1.png?width=1835&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d09725b3c718f61de8b29d4706023d1107ca59bb"}, "id": "1ywd4col5tza1"}}, "name": "t3_13hdtd8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "8fc1a448-bbcf-11e4-9649-22000b2b8291", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684074866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;The whole question is in the title but here is the full context :&lt;/p&gt;\n\n&lt;p&gt;I am using a Dell PowerEdge R720XD with PERC H710P, 12x 6TB 512e HDD in RAID 6 and Debian 11 as my only homelab server. This server stores a various data (18TB of videos, 6TB of binary data from the STORJ network, 2TB of miscellaneous files (home NAS), etc...) so I chose 64KB RAID stripes (default value).&lt;/p&gt;\n\n&lt;p&gt;Given the proportion of large files, some of you will certainly tell me that I should have used larger RAID bands.\u00a0It&amp;#39;s probably right but, when I installed the server, I had no idea that I&amp;#39;ll get this data distribution, these proportions are likely to change over time and now I can&amp;#39;t change this without formatting the server. Whatever, any advice is welcome.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s what my storage stack looks like:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1ywd4col5tza1.png?width=1835&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d09725b3c718f61de8b29d4706023d1107ca59bb\"&gt;https://preview.redd.it/1ywd4col5tza1.png?width=1835&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=d09725b3c718f61de8b29d4706023d1107ca59bb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fqfb6t9m5tza1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=63ceb240ef82ec2cfd707efeae09ac6be1ab75d3\"&gt;https://preview.redd.it/fqfb6t9m5tza1.png?width=1035&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=63ceb240ef82ec2cfd707efeae09ac6be1ab75d3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;My objective is to get the maximum performance while keeping this maximum available storage (I don&amp;#39;t want to switch to a RAID 10). From what I understood, the RAID controller will only write to disk every [stripe size] x [number of data drives] = 64KB x 10 (12 drives - 2 parity drives) = 640KB in my case&lt;/p&gt;\n\n&lt;p&gt;Is this correct?&lt;/p&gt;\n\n&lt;p&gt;If yes :&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;then ideally I should instruct LVM and filesystems to use 64KB or 128KB (only divisors of 640KB which are &amp;gt;=64KB) blocks to fill those 640KB flushed to disk as best as possible?&lt;/li&gt;\n&lt;li&gt;then why is the default value,\u00a0returned by the RAID controller to the OS, 512B?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance for your help and explanations !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "664a26e4-322a-11e6-80ae-0e0378709321", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Dell PowerEdge R720XD | Debian 11 | LVM + VDO | Ansible", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2ubz7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff6347", "id": "13hdtd8", "is_robot_indexable": true, "report_reasons": null, "author": "tigerblue77", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/homelab/comments/13hdtd8/what_block_size_should_i_choose_for_my_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/homelab/comments/13hdtd8/what_block_size_should_i_choose_for_my_file/", "subreddit_subscribers": 571332, "created_utc": 1684074866.0, "num_crossposts": 4, "media": null, "is_video": false}], "created": 1684074902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/homelab/comments/13hdtd8/what_block_size_should_i_choose_for_my_file/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "72TB R720XD  + 16TB R720 + 3,5TB homemade PC", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hdtvr", "is_robot_indexable": true, "report_reasons": null, "author": "tigerblue77", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13hdtd8", "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13hdtvr/what_block_size_should_i_choose_for_my_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/homelab/comments/13hdtd8/what_block_size_should_i_choose_for_my_file/", "subreddit_subscribers": 682393, "created_utc": 1684074902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello fellas. If similar to me, you like collecting comic books and \"Getcomics\" doesn't cut it for you, you might have an answer to my problem.\n\nI live in Iran, and we have nearly zero access to purchasing comic books in any format. So as a comic book enthusiast, I could turn to nowhere but piracy for accessing the books I liked.\nGetcomics has been a great help, and if I can't find something there, Libgen might have PDF files of the stuff I want. But other than that I had no major resources, until I randomly found Comicslady.\n\nI was trying to learn a few other languages, and I'm really interested in French comics from France and Belgium. To my amazement, Comicslady was filled with original AND officially translated works in French, and they had something around 40,000 webpages/entries for their comics. Aside from French, they had a really good archive for Italian &amp; Spanish comics too.\n\nUnfortunately I didn't have time to hoard the archive at the time, and today when I went back to get a few books, I found out that the website has apparently been closed.\n\nI can't find any good alternatives to that archive, and a lot of the stuff they had is just not on the public internet for free.\n\nI was wondering if any of you knew an online archive like that which I could use. Mostly for French comics, preferably in CBR or CBZ formats and with good or average quality, and spanning from the old works like M\u00e9tal Hurlant and the works of M\u0153bius &amp; M\u00e9zi\u00e8res, to works published in the recent years.\n\nThank you in advance.", "author_fullname": "t2_4temyu59", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replacements for Comicslady (non-English comic book archive)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hd9wy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684074025.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684073500.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellas. If similar to me, you like collecting comic books and &amp;quot;Getcomics&amp;quot; doesn&amp;#39;t cut it for you, you might have an answer to my problem.&lt;/p&gt;\n\n&lt;p&gt;I live in Iran, and we have nearly zero access to purchasing comic books in any format. So as a comic book enthusiast, I could turn to nowhere but piracy for accessing the books I liked.\nGetcomics has been a great help, and if I can&amp;#39;t find something there, Libgen might have PDF files of the stuff I want. But other than that I had no major resources, until I randomly found Comicslady.&lt;/p&gt;\n\n&lt;p&gt;I was trying to learn a few other languages, and I&amp;#39;m really interested in French comics from France and Belgium. To my amazement, Comicslady was filled with original AND officially translated works in French, and they had something around 40,000 webpages/entries for their comics. Aside from French, they had a really good archive for Italian &amp;amp; Spanish comics too.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately I didn&amp;#39;t have time to hoard the archive at the time, and today when I went back to get a few books, I found out that the website has apparently been closed.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t find any good alternatives to that archive, and a lot of the stuff they had is just not on the public internet for free.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if any of you knew an online archive like that which I could use. Mostly for French comics, preferably in CBR or CBZ formats and with good or average quality, and spanning from the old works like M\u00e9tal Hurlant and the works of M\u0153bius &amp;amp; M\u00e9zi\u00e8res, to works published in the recent years.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13hd9wy", "is_robot_indexable": true, "report_reasons": null, "author": "Mehrider", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hd9wy/replacements_for_comicslady_nonenglish_comic_book/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hd9wy/replacements_for_comicslady_nonenglish_comic_book/", "subreddit_subscribers": 682393, "created_utc": 1684073500.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, I currently have one zip where I have multiple folders with combined 50k files and photos. This zip is encrypted via 7zip. When I put a new folder in it I do see that it does a \"replicating\". This basically copies everything to a new zip (if I got it right), but this is starting to get tricky especially as my drive gets fuller.\n\n**So my question is**: Is it better to zip per folder with encryption, and then put all the encrypted zips into one big zip without encryption?", "author_fullname": "t2_f9v5z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategy for zipping folders and encryption", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h9lhc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684063357.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684063104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I currently have one zip where I have multiple folders with combined 50k files and photos. This zip is encrypted via 7zip. When I put a new folder in it I do see that it does a &amp;quot;replicating&amp;quot;. This basically copies everything to a new zip (if I got it right), but this is starting to get tricky especially as my drive gets fuller.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;So my question is&lt;/strong&gt;: Is it better to zip per folder with encryption, and then put all the encrypted zips into one big zip without encryption?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h9lhc", "is_robot_indexable": true, "report_reasons": null, "author": "FlyingChinesePanda", "discussion_type": null, "num_comments": 7, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13h9lhc/strategy_for_zipping_folders_and_encryption/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h9lhc/strategy_for_zipping_folders_and_encryption/", "subreddit_subscribers": 682393, "created_utc": 1684063104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Game preservation guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gplcz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_u3tdlu5o", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "GamePreservationists", "selftext": "Hi, some time ago I was thinking whether I am doing my back ups right, but I could not find a good guide, which would explain what devices are best for archiving, how to deal with corruption and so on. So, I ended up making my own broad guide on how to properly archive games.\n\nIt does not explain everything in complete detail, but I am sure there are some other people who will find it useful, so I uploaded it here:\n\ngameobservatory. neocities. org/ guides/ guide2", "author_fullname": "t2_u3tdlu5o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Game preservation guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/GamePreservationists", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13az47q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683485346.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.GamePreservationists", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, some time ago I was thinking whether I am doing my back ups right, but I could not find a good guide, which would explain what devices are best for archiving, how to deal with corruption and so on. So, I ended up making my own broad guide on how to properly archive games.&lt;/p&gt;\n\n&lt;p&gt;It does not explain everything in complete detail, but I am sure there are some other people who will find it useful, so I uploaded it here:&lt;/p&gt;\n\n&lt;p&gt;gameobservatory. neocities. org/ guides/ guide2&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2gx4id", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13az47q", "is_robot_indexable": true, "report_reasons": null, "author": "Gamobser", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/GamePreservationists/comments/13az47q/game_preservation_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/GamePreservationists/comments/13az47q/game_preservation_guide/", "subreddit_subscribers": 3900, "created_utc": 1683485346.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1684004064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.GamePreservationists", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/GamePreservationists/comments/13az47q/game_preservation_guide/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gplcz", "is_robot_indexable": true, "report_reasons": null, "author": "Gamobser", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13az47q", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gplcz/game_preservation_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/GamePreservationists/comments/13az47q/game_preservation_guide/", "subreddit_subscribers": 682393, "created_utc": 1684004064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've got a Drobo 5D3 that I've been using for 5 years or so and quite liked it, even if it was a bit finnicky at times.  That being said, aside from it being time for an upgrade we all know where Drobo is going, so the sooner I can get off it the better.\n\nI'm looking for a DAS as so far the Drobo is populated with standard desktop drives that I don't trust running 24/7 in a NAS, and I already have a NAS project I'm building with a bunch of  smaller NAS drives.\n\nI'd like to be able to use it with both my M1 Mac and Windows PCs, so would prefer a hardware RAID solution.  I'd also like to have it be Thunderbolt and 10GB/s USB-C compatible -- M1 Mac has Thunderbolt and, while the current PC doesn't my planned upgrade will have Thunderbolt.\n\nI have 5 drives in the Drobo and 3 more drives new in box, so 8 bay would be fantastic but smaller is also fine.\n\nSo far I've seen the following:\n\n[OWC Thunderbay 8](https://www.amazon.com/OWC-ThunderBay-8-Bay-External-Thunderbolt/dp/B084S3S8JC?spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFCMVgzMlZXR0JZU1kmZW5jcnlwdGVkSWQ9QTA4Njc3ODlCVlU5UzNVRzVNREgmZW5jcnlwdGVkQWRJZD1BMDk4NjQyMDFaQ0Y3R0FBRE9DR0gmd2lkZ2V0TmFtZT1zcF9hdGYmYWN0aW9uPWNsaWNrUmVkaXJlY3QmZG9Ob3RMb2dDbGljaz10cnVl&amp;linkId=e5455e043ae2e9f5ce392b051ea24ac8&amp;language=en_US) \\-- SoftRAID but at least it is cross platform\n\n[TerraMaster D5](https://www.amazon.com/TerraMaster-Thunderbolt-Professional-Grade-External-Enclosure/dp/B07BHMCWMS?spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFQM1NLTDA0TDQ3MTgmZW5jcnlwdGVkSWQ9QTA2ODMwMTQxTDVBM01YNkRTMllFJmVuY3J5cHRlZEFkSWQ9QTAzOTAwNTIxUkU2TlhBSkVQTEpDJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ&amp;linkId=4bc4c586cd1c3d9e72f91ad0ef379829&amp;language=en_US)\n\nAny other thoughts?", "author_fullname": "t2_3sdmz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drobo DAS alternative?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gp4ec", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684002902.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a Drobo 5D3 that I&amp;#39;ve been using for 5 years or so and quite liked it, even if it was a bit finnicky at times.  That being said, aside from it being time for an upgrade we all know where Drobo is going, so the sooner I can get off it the better.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a DAS as so far the Drobo is populated with standard desktop drives that I don&amp;#39;t trust running 24/7 in a NAS, and I already have a NAS project I&amp;#39;m building with a bunch of  smaller NAS drives.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to be able to use it with both my M1 Mac and Windows PCs, so would prefer a hardware RAID solution.  I&amp;#39;d also like to have it be Thunderbolt and 10GB/s USB-C compatible -- M1 Mac has Thunderbolt and, while the current PC doesn&amp;#39;t my planned upgrade will have Thunderbolt.&lt;/p&gt;\n\n&lt;p&gt;I have 5 drives in the Drobo and 3 more drives new in box, so 8 bay would be fantastic but smaller is also fine.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve seen the following:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/OWC-ThunderBay-8-Bay-External-Thunderbolt/dp/B084S3S8JC?spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFCMVgzMlZXR0JZU1kmZW5jcnlwdGVkSWQ9QTA4Njc3ODlCVlU5UzNVRzVNREgmZW5jcnlwdGVkQWRJZD1BMDk4NjQyMDFaQ0Y3R0FBRE9DR0gmd2lkZ2V0TmFtZT1zcF9hdGYmYWN0aW9uPWNsaWNrUmVkaXJlY3QmZG9Ob3RMb2dDbGljaz10cnVl&amp;amp;linkId=e5455e043ae2e9f5ce392b051ea24ac8&amp;amp;language=en_US\"&gt;OWC Thunderbay 8&lt;/a&gt; -- SoftRAID but at least it is cross platform&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/TerraMaster-Thunderbolt-Professional-Grade-External-Enclosure/dp/B07BHMCWMS?spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFQM1NLTDA0TDQ3MTgmZW5jcnlwdGVkSWQ9QTA2ODMwMTQxTDVBM01YNkRTMllFJmVuY3J5cHRlZEFkSWQ9QTAzOTAwNTIxUkU2TlhBSkVQTEpDJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ&amp;amp;linkId=4bc4c586cd1c3d9e72f91ad0ef379829&amp;amp;language=en_US\"&gt;TerraMaster D5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Any other thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gp4ec", "is_robot_indexable": true, "report_reasons": null, "author": "fuzzycuffs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gp4ec/drobo_das_alternative/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gp4ec/drobo_das_alternative/", "subreddit_subscribers": 682393, "created_utc": 1684002902.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I created a PowerShell script to automate the downloading process of update files from the Microsoft Update Catalog (www.catalog.update.microsoft.com), this way you can grab them for convenience or archiving purposes. My script supports advanced workarounds and options to make it as effective, flexible and seamless as possible to the user. Batch downloading, as well as language and NT version filters are available. This project started in 2021, but i have recently updated it to the 1.02 release with more advanced workarounds, features, as well as fixes for multiple language download.\n\nIf you are interested in hoarding or archiving windows updates, you will probably find this script handy. It is available on: https://github.com/blueclouds8666/msupdate-dl\n\nLet me know if you find it useful.", "author_fullname": "t2_8ff5t1fz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Script for batch downloading windows updates, release 1.02", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hdres", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Windows", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684074734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created a PowerShell script to automate the downloading process of update files from the Microsoft Update Catalog (&lt;a href=\"http://www.catalog.update.microsoft.com\"&gt;www.catalog.update.microsoft.com&lt;/a&gt;), this way you can grab them for convenience or archiving purposes. My script supports advanced workarounds and options to make it as effective, flexible and seamless as possible to the user. Batch downloading, as well as language and NT version filters are available. This project started in 2021, but i have recently updated it to the 1.02 release with more advanced workarounds, features, as well as fixes for multiple language download.&lt;/p&gt;\n\n&lt;p&gt;If you are interested in hoarding or archiving windows updates, you will probably find this script handy. It is available on: &lt;a href=\"https://github.com/blueclouds8666/msupdate-dl\"&gt;https://github.com/blueclouds8666/msupdate-dl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know if you find it useful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13hdres", "is_robot_indexable": true, "report_reasons": null, "author": "blueclouds8666", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13hdres/script_for_batch_downloading_windows_updates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13hdres/script_for_batch_downloading_windows_updates/", "subreddit_subscribers": 682393, "created_utc": 1684074734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nLong story short, I've had some crazy people come after me recently &amp; try to assassinate my character with a bunch of false accusations. I don't think anything will come of it &amp; I'm just ignoring them for now, but I'm also collecting receipts just to be on the safe side. If they somehow gain traction &amp; I need to write up a doc addressing their claims, I don't want to be caught with my pants down.\n\nAs the title suggests, the main two sources of receipts are Discord &amp; Twitter.\n\nOn the Discord side of things, I've been using DiscordChatExporter to grab all the relevant conversations on that platform, which has been super convenient, because it preserves in a HTML file that duplicates how they would look on Discord. This means I can save everything locally, and screenshot later at my leisure.\n\nTwitter, meanwhile, is proving to be more of a headache. Their tweets are a vast ocean of madness, and having them all archived would be more than enough evidence to defend myself, if I could get them all in one place. \n\nAlas, I haven't found anything that preserves the layout in the same way DCE does for Discord. Or at least, most options I've found don't make it clear how the exported tweets &amp; media are going to look once captured. Just having an excel spread sheet or TXT file containing all the tweets, for example, may be better than nothing, but being able to screenshot them in their original form would be more convincing for the sake of doc. Whether that's in a HTML file or some other visual format.\n\nAnd in case it doesn't go without saying, manually screenshotting their entire Twitter archives isn't an option, as there's about 200,000 tweets spread between their accounts. Never in my life has the term \"Terminally Online\" seemed more appropriate.\n\nIf you guys have any suggestions, I'm listening.", "author_fullname": "t2_79nrqzf5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a Twitter Equivalent of DiscordChatExporter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h5iws", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684048876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Long story short, I&amp;#39;ve had some crazy people come after me recently &amp;amp; try to assassinate my character with a bunch of false accusations. I don&amp;#39;t think anything will come of it &amp;amp; I&amp;#39;m just ignoring them for now, but I&amp;#39;m also collecting receipts just to be on the safe side. If they somehow gain traction &amp;amp; I need to write up a doc addressing their claims, I don&amp;#39;t want to be caught with my pants down.&lt;/p&gt;\n\n&lt;p&gt;As the title suggests, the main two sources of receipts are Discord &amp;amp; Twitter.&lt;/p&gt;\n\n&lt;p&gt;On the Discord side of things, I&amp;#39;ve been using DiscordChatExporter to grab all the relevant conversations on that platform, which has been super convenient, because it preserves in a HTML file that duplicates how they would look on Discord. This means I can save everything locally, and screenshot later at my leisure.&lt;/p&gt;\n\n&lt;p&gt;Twitter, meanwhile, is proving to be more of a headache. Their tweets are a vast ocean of madness, and having them all archived would be more than enough evidence to defend myself, if I could get them all in one place. &lt;/p&gt;\n\n&lt;p&gt;Alas, I haven&amp;#39;t found anything that preserves the layout in the same way DCE does for Discord. Or at least, most options I&amp;#39;ve found don&amp;#39;t make it clear how the exported tweets &amp;amp; media are going to look once captured. Just having an excel spread sheet or TXT file containing all the tweets, for example, may be better than nothing, but being able to screenshot them in their original form would be more convincing for the sake of doc. Whether that&amp;#39;s in a HTML file or some other visual format.&lt;/p&gt;\n\n&lt;p&gt;And in case it doesn&amp;#39;t go without saying, manually screenshotting their entire Twitter archives isn&amp;#39;t an option, as there&amp;#39;s about 200,000 tweets spread between their accounts. Never in my life has the term &amp;quot;Terminally Online&amp;quot; seemed more appropriate.&lt;/p&gt;\n\n&lt;p&gt;If you guys have any suggestions, I&amp;#39;m listening.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h5iws", "is_robot_indexable": true, "report_reasons": null, "author": "Afraid_Alternative35", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13h5iws/looking_for_a_twitter_equivalent_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h5iws/looking_for_a_twitter_equivalent_of/", "subreddit_subscribers": 682393, "created_utc": 1684048876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Greetings all.\n\nShort version:\n\nPretty new to RAID arrays. Easily set up my array a few years ago, but now trying to move or recreate my two 18 TB WD Red RAID 1 array on my new computer (Gigabyte Aorus Master x670e motherboard). My efforts so far with AMD RAIDXpert2 have been unsuccessful, and I'm pretty certain I wiped one of the two drives (just deleted it by trying to initialize it). I hoped I could maybe wipe one of the two drives in RAIDXpert, set it up as a solo drive and copy the data from the \"legacy\" (other) drive then somehow make the RAID 1 array setup from that. I can't even get windows to do anything other than see the initialized drive as unallocated / can't format it or anything. Trying to do the array using SATA ports 5 and 6 on the motherboard. Currently no other SATA drives attached. My boot drive is a single NVME drive. Is there a simple third party software or (less than $100) hardware solution that could easily fix this / facilitates easily moving a 2 disk raid 1 from one system to a not alike second system?\n\nLong version:  \n\nHaven't been able to find many answers via Google, reddit, the RAIDXpert2 manual, or using LLM AI help on this issue, and I'm pretty inexperienced with managing RAID arrays to boot. A few years back, I set up a RAID array via the BIOS / Gigabyte's stuff on my old x99 motherboard and am trying to get rid of my older computers but want to move the RAID 1 to my new build (Gigabyte Aorus Master AMD x670e motherboard). I don't plan on buying any RAID hardware and I don't have any other 18TB drives sitting around (though I could potentially scrub some slightly smaller drives if I absolutely had to as temp backup, which makes me nervous / defeats the entire purpose of doing the RAID 1 in the first place) since I'm just doing a two disk RAID 1. Trying to do the array using SATA ports 5 and 6 on the motherboard. Currently no other SATA drives attached. My boot drive is a single NVME drive. I was annoyed to see that RAIDXpert2 doesn't seem to have any way to easily just convert the old RAID 1 over to it's \"flavor\" of RAID 1. It initially saw my two disks as two seperate arrays. Windows 11 saw both drives, but would only use one drive / the RAID wasn't functioning after enabling RAID in BIOS and manually installing the .inf file. I tried deleting one of the two arrays, and even tried initializing the disk. I was hoping I could delete one of the disks and have RAIDXpert2 be okay with it being a single JBOD disk, write all the crap from the disk with data, then scrub that too and make a RAID 1, but that also is looking increasingly unlikely. I'm frankly surprised this isn't just a few button easy process in this day and age. I still have the \"good\" disk sitting in array 1 as \"legacy\" in RAIDXpert2, and would like to find a way to rebuild my raid 1 from that if able, but haven't been able to figure out how to do so in RAIDxpert2, and cursory research for alternative solutions hasn't yielded any results either. Any assistance or resources appreciated.", "author_fullname": "t2_5tmzlo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Problems moving RAID 1 from old computer to new AMD x670e / RAIDxpert2 system.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h2yjm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684040023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings all.&lt;/p&gt;\n\n&lt;p&gt;Short version:&lt;/p&gt;\n\n&lt;p&gt;Pretty new to RAID arrays. Easily set up my array a few years ago, but now trying to move or recreate my two 18 TB WD Red RAID 1 array on my new computer (Gigabyte Aorus Master x670e motherboard). My efforts so far with AMD RAIDXpert2 have been unsuccessful, and I&amp;#39;m pretty certain I wiped one of the two drives (just deleted it by trying to initialize it). I hoped I could maybe wipe one of the two drives in RAIDXpert, set it up as a solo drive and copy the data from the &amp;quot;legacy&amp;quot; (other) drive then somehow make the RAID 1 array setup from that. I can&amp;#39;t even get windows to do anything other than see the initialized drive as unallocated / can&amp;#39;t format it or anything. Trying to do the array using SATA ports 5 and 6 on the motherboard. Currently no other SATA drives attached. My boot drive is a single NVME drive. Is there a simple third party software or (less than $100) hardware solution that could easily fix this / facilitates easily moving a 2 disk raid 1 from one system to a not alike second system?&lt;/p&gt;\n\n&lt;p&gt;Long version:  &lt;/p&gt;\n\n&lt;p&gt;Haven&amp;#39;t been able to find many answers via Google, reddit, the RAIDXpert2 manual, or using LLM AI help on this issue, and I&amp;#39;m pretty inexperienced with managing RAID arrays to boot. A few years back, I set up a RAID array via the BIOS / Gigabyte&amp;#39;s stuff on my old x99 motherboard and am trying to get rid of my older computers but want to move the RAID 1 to my new build (Gigabyte Aorus Master AMD x670e motherboard). I don&amp;#39;t plan on buying any RAID hardware and I don&amp;#39;t have any other 18TB drives sitting around (though I could potentially scrub some slightly smaller drives if I absolutely had to as temp backup, which makes me nervous / defeats the entire purpose of doing the RAID 1 in the first place) since I&amp;#39;m just doing a two disk RAID 1. Trying to do the array using SATA ports 5 and 6 on the motherboard. Currently no other SATA drives attached. My boot drive is a single NVME drive. I was annoyed to see that RAIDXpert2 doesn&amp;#39;t seem to have any way to easily just convert the old RAID 1 over to it&amp;#39;s &amp;quot;flavor&amp;quot; of RAID 1. It initially saw my two disks as two seperate arrays. Windows 11 saw both drives, but would only use one drive / the RAID wasn&amp;#39;t functioning after enabling RAID in BIOS and manually installing the .inf file. I tried deleting one of the two arrays, and even tried initializing the disk. I was hoping I could delete one of the disks and have RAIDXpert2 be okay with it being a single JBOD disk, write all the crap from the disk with data, then scrub that too and make a RAID 1, but that also is looking increasingly unlikely. I&amp;#39;m frankly surprised this isn&amp;#39;t just a few button easy process in this day and age. I still have the &amp;quot;good&amp;quot; disk sitting in array 1 as &amp;quot;legacy&amp;quot; in RAIDXpert2, and would like to find a way to rebuild my raid 1 from that if able, but haven&amp;#39;t been able to figure out how to do so in RAIDxpert2, and cursory research for alternative solutions hasn&amp;#39;t yielded any results either. Any assistance or resources appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13h2yjm", "is_robot_indexable": true, "report_reasons": null, "author": "The_Internal_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13h2yjm/problems_moving_raid_1_from_old_computer_to_new/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13h2yjm/problems_moving_raid_1_from_old_computer_to_new/", "subreddit_subscribers": 682393, "created_utc": 1684040023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi\n\nPlease could someone help me? I have 3 drives connected on the network, Windows 11. I would like those drives to sync completely with an external USB drive that I have connected.\n\nIs that possible? I don't want it to just back up, I want it to sync e.g. if I delete a file from the network drive, it is deleted from the USB drive.\n\n&amp;#x200B;\n\n(seen these recommended on another post, would one of these work and is one better than the other?  \n\nRCLONE\n\nROBOCOPY\n\nFreeFileSync\n\nSyncthing)\n\n&amp;#x200B;\n\nThanks\n\nJames", "author_fullname": "t2_7g4uo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sync network share drives with USB hard drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gkanv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683991307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;Please could someone help me? I have 3 drives connected on the network, Windows 11. I would like those drives to sync completely with an external USB drive that I have connected.&lt;/p&gt;\n\n&lt;p&gt;Is that possible? I don&amp;#39;t want it to just back up, I want it to sync e.g. if I delete a file from the network drive, it is deleted from the USB drive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(seen these recommended on another post, would one of these work and is one better than the other?  &lt;/p&gt;\n\n&lt;p&gt;RCLONE&lt;/p&gt;\n\n&lt;p&gt;ROBOCOPY&lt;/p&gt;\n\n&lt;p&gt;FreeFileSync&lt;/p&gt;\n\n&lt;p&gt;Syncthing)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n\n&lt;p&gt;James&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gkanv", "is_robot_indexable": true, "report_reasons": null, "author": "skadseye", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gkanv/sync_network_share_drives_with_usb_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gkanv/sync_network_share_drives_with_usb_hard_drive/", "subreddit_subscribers": 682393, "created_utc": 1683991307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_723w1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why is my 10-year-old SSDH like this...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 16, "top_awarded_type": null, "hide_score": false, "name": "t3_13gvmju", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.3, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kHz4-H2p82VI30NDYSW-TeYrhxw6OPfd1CbIOXMzL1E.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684019205.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/k8dgt0m0koza1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/k8dgt0m0koza1.png?auto=webp&amp;v=enabled&amp;s=09f8784726c5e46b4454e2bbcd68afb987157725", "width": 1061, "height": 125}, "resolutions": [{"url": "https://preview.redd.it/k8dgt0m0koza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=803602ea400d703bb887c1796b8436feb728ec06", "width": 108, "height": 12}, {"url": "https://preview.redd.it/k8dgt0m0koza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d788fb8af172c48ddabfa1e7352f895d19ede1b5", "width": 216, "height": 25}, {"url": "https://preview.redd.it/k8dgt0m0koza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40720e75c8cad190924893b34a01369f2c51679e", "width": 320, "height": 37}, {"url": "https://preview.redd.it/k8dgt0m0koza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4bf52597c9b96092ef9379a4901e0c321521a192", "width": 640, "height": 75}, {"url": "https://preview.redd.it/k8dgt0m0koza1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79235c163ba191974416dd01511c72c2086c3c9b", "width": 960, "height": 113}], "variants": {}, "id": "TDsXeLNYkU5JttBQHb-CmZrXg5UNlAqOusgSdM8efuw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gvmju", "is_robot_indexable": true, "report_reasons": null, "author": "sephiroth_vg", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gvmju/why_is_my_10yearold_ssdh_like_this/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/k8dgt0m0koza1.png", "subreddit_subscribers": 682393, "created_utc": 1684019205.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey fellow datahoarders, as AI technology advances and becomes more prevalent in our lives, the question arises - will we still care about storing and archiving data? With the ability to generate an endless stream of personalized media content, including books, TV shows, movies, and video games, tailored to our individual preferences and tastes, it's easy to wonder if the need to save and store data will become obsolete. As someone who loves collecting and preserving data, I'm curious to hear your thoughts on the future of datahoarding in a world where AI-generated media reigns supreme.", "author_fullname": "t2_w8b9dhvc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Future of Datahoarding in the Age of AI-generated Media", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gkhvz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.3, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683991803.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey fellow datahoarders, as AI technology advances and becomes more prevalent in our lives, the question arises - will we still care about storing and archiving data? With the ability to generate an endless stream of personalized media content, including books, TV shows, movies, and video games, tailored to our individual preferences and tastes, it&amp;#39;s easy to wonder if the need to save and store data will become obsolete. As someone who loves collecting and preserving data, I&amp;#39;m curious to hear your thoughts on the future of datahoarding in a world where AI-generated media reigns supreme.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13gkhvz", "is_robot_indexable": true, "report_reasons": null, "author": "Possible_Being_3189", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gkhvz/the_future_of_datahoarding_in_the_age_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gkhvz/the_future_of_datahoarding_in_the_age_of/", "subreddit_subscribers": 682393, "created_utc": 1683991803.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm talkin an application I can upload my videos to, give them cover art and scroll through them like I would netflix. Think something like winamp/WACUP. Plex looks alright but im not entirely sure its what im looking for. Help appreciated.", "author_fullname": "t2_3bskimmc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a personal netflix out there somewhere?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gwo00", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.26, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684021945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m talkin an application I can upload my videos to, give them cover art and scroll through them like I would netflix. Think something like winamp/WACUP. Plex looks alright but im not entirely sure its what im looking for. Help appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13gwo00", "is_robot_indexable": true, "report_reasons": null, "author": "TheRealKuthooloo", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13gwo00/is_there_a_personal_netflix_out_there_somewhere/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13gwo00/is_there_a_personal_netflix_out_there_somewhere/", "subreddit_subscribers": 682393, "created_utc": 1684021945.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}