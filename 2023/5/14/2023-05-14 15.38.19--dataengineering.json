{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you're working on **Snowflake** as a Data Engineer, this article might be interesting:\n\n[Snowflake Top Tips for Data Engineers - Loading and Transforming Data](https://www.analytics.today/blog/top-14-snowflake-data-engineering-best-practices)\n\n**Quote:**  \n\n&gt;*If the only tool you have is a hammer - you tend to see every problem as a nail.*  Abraham Maslow.\n\n**In Summary**\n\n1. **Follow the standard ingestion pattern:**\u00a0\u00a0This involves the multi-stage process of landing the data files in cloud storage and loading them to a landing table before transforming the data.\u00a0\u00a0Breaking the overall process into predefined steps makes it easier to orchestrate and test.\u00a0\u00a0\n2. **Retain history of raw data:**\u00a0\u00a0Unless your data is sourced from a raw data lake, it makes sense to keep the raw data history which should ideally be stored using the\u00a0[VARIANT](https://docs.snowflake.com/en/sql-reference/data-types-semistructured.html#variant)\u00a0data type to benefit from automatic schema evolution.\u00a0\u00a0This means you can truncate and re-process data if bugs are found in the transformation pipeline and provide an excellent raw data source for Data Scientists.\u00a0\u00a0While you may not yet have any machine learning requirements yet, it's almost certain you will, if not now, then in the coming years.\u00a0\u00a0Remember that Snowflake data storage is remarkably cheap, unlike on-premises solutions.\u00a0\n3. **Use multiple data models:**\u00a0\u00a0\u00a0On-premises data storage was so expensive it was not feasible to store multiple copies of data, each using a different data model to match the need.\u00a0\u00a0However, using Snowflake, it makes sense to store raw data history in either structured or variant format, cleaned and conformed data in\u00a0[3rd Normal Form](https://dwbi1.wordpress.com/2011/03/28/storing-history-on-3rd-normal-form/)\u00a0or using a\u00a0[Data Vault](https://www.analytics.today/blog/when-should-i-use-data-vault)\u00a0model. Finally, data is ready for consumption in a\u00a0[Kimball Dimensional Data model](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/).\u00a0\u00a0Each data model has unique benefits, and storing the results of intermediate steps has huge architectural benefits, not least the ability to reload and reprocess the data in case of mistakes.\n4. **Use the right tool:**\u00a0\u00a0As the quote above implies, if you only know one tool, you'll use it inappropriately some times.\u00a0\u00a0The decision about which tool to use should be based upon a range of factors, including, the existing skill set in the team, whether you need rapid near real-time delivery, and whether you're doing a once-off data load or a regular repeating process.\u00a0\u00a0Be aware that Snowflake can natively handle a range of file formats, including Avro, Parquet, ORC, JSON and CSV. There is extensive guidance on\u00a0[loading data into Snowflake](https://docs.snowflake.com/en/user-guide-data-load.html#loading-data-into-snowflake)\u00a0on the online documentation.\n5. **Use COPY or SNOWPIPE to load data:**\u00a0\u00a0Around 80% of data loaded into a data warehouse is either ingested using a regular batch process or, increasingly, immediately after the data files arrive.\u00a0\u00a0By far, the fastest, most cost-efficient way to load data is using COPY and SNOWPIPE, so avoid the temptation to use other methods (for example, queries against external tables) for regular data loads.\u00a0\u00a0Effectively, this is another example of\u00a0*using the right tool*.\u00a0\u00a0\n6. **Avoid JDBC or ODBC for regular large data loads:**\u00a0\u00a0Another\u00a0*right tool*\u00a0recommendation.\u00a0\u00a0While a JDBC or ODBC interface may be fine to load a few megabytes of data, these interfaces will not scale to the massive throughput of COPY and SNOWPIPE.\u00a0\u00a0Use them by all means, but not for large regular data loads.\n7. **Avoid Scanning Files:** Using the COPY command to ingest data, use\u00a0[partitioned staged data](https://docs.snowflake.com/en/user-guide/data-load-considerations-manage.html#partitioning-staged-data-files) files.\u00a0\u00a0This reduces the effort of scanning large numbers of data files in cloud storage.\n8. **Choose a suitable Virtual Warehouse size:**\u00a0\u00a0Don\u2019t assume an X6-LARGE warehouse will load huge data files any faster than an X-SMALL.  Each physical file is loaded sequentially on a single CPU, and it is more sensible to load most loads on an X-SMALL warehouse.  Consider splitting massive data files into 100-250MB chunks and loading them on a larger (perhaps MEDIUM size) warehouse.\n9. **Ensure 3rd party tools push down:**\u00a0\u00a0ETL tools like Ab Initio, Talend and Informatica were originally designed to extract data from source systems into an ETL server, transform the data and write them to the warehouse.\u00a0\u00a0As Snowflake can draw upon massive on-demand compute resources and automatically scale out, it makes no sense to use and have data copied to an external server.\u00a0\u00a0Instead, use the ELT (Extract, Load and Transform) method, and ensure the tools generate and execute SQL statements on Snowflake to maximise throughput and reduce costs.  Excellent examples include [DBT](https://www.getdbt.com/) and [Matillion](https://www.matillion.com/).\n10. **Transform data in Steps:**\u00a0\u00a0A common mistake by inexperienced data engineers is to write huge SQL statements that join, summarise and process lots of tables in the mistaken belief this is an efficient way of working.\u00a0\u00a0In reality, the code becomes over-complex, difficult to maintain, and, worst still, often performs poorly.\u00a0\u00a0Instead, break the transformation pipeline into multiple steps and write results to intermediate tables.\u00a0\u00a0This makes it easier to test intermediate results, simplifies the code and often produces simple SQL code that runs faster.  \n11. **Use Transient tables for intermediate results:**\u00a0\u00a0During a complex ELT pipeline, write intermediate results to a\u00a0[transient table](https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#transient-tables)\u00a0which may be truncated before the next load.\u00a0\u00a0This reduces the time-travel storage to just one day and avoids an additional seven days of fail-safe storage.\u00a0\u00a0By all means, use\u00a0[temporary tables](https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#temporary-tables)\u00a0if sensible, but the option to check the results of intermediate steps in a complex ELT pipeline is often helpful.\n12. **Avoid row-by-row processing:**\u00a0\u00a0As described in the article on [Snowflake Query Tuning](https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics), Snowflake is designed to ingest, process and analyse billions of rows at amazing speed.\u00a0\u00a0This is often referred to as *set-at-a-time processing.*  However, people tend to think about *row-by-row processing*, which sometimes leads to programming loops that fetch and update rows one at a time.\u00a0\u00a0Be aware\u00a0that row-by-row processing is the biggest way to kill query performance.\u00a0\u00a0Use SQL statements to process all table entries simultaneously and avoid row-by-row processing at all costs.\n13. **Use Query Tags:**\u00a0\u00a0When you start any multi-step transformation task, set the\u00a0[session query tag using](https://docs.snowflake.com/en/sql-reference/sql/alter-session.html#alter-session):\u00a0\u00a0**ALTER SESSION SET QUERY\\_TAG = 'XXXXXX'** and **ALTER SESSION UNSET QUERY\\_TAG**.\u00a0\u00a0This stamps every SQL statement until reset with an identifier and is invaluable to System Administrators.\u00a0\u00a0As every SQL statement (and QUERY\\_TAG) is recorded in the\u00a0[QUERY\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history.html#query-history-view)\u00a0view, you can track the job performance over time.\u00a0\u00a0This can be used to quickly identify when a task change has resulted in poor performance, identify inefficient transformation jobs or indicate when a job would be better executed on a larger or smaller warehouse.\n14. **Keep it Simple:**\u00a0\u00a0Probably the best indicator of an experienced data engineer is the value they place on ***simplicity***.\u00a0\u00a0You can always make a job 10% faster, generic, or more elegant, and it\u00a0*may*\u00a0be beneficial, but it's\u00a0*always*\u00a0beneficial to simplify a solution.\u00a0\u00a0Simple solutions are easier to understand, easier to diagnose problems and are therefore easier to maintain.\u00a0\u00a0Around 50% of the performance challenges I face are difficult to resolve because the solution is a single, monolithic complex block of code.\u00a0\u00a0The first thing I do is break down the solution into steps and only then identify the root cause.\u00a0\u00a0\n\nWould you agree with the above?  What would you add?", "author_fullname": "t2_d3q0tuqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My top 14 tips for Snowflake Data Engineers. What would you add?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gln9a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 140, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 140, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683994507.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re working on &lt;strong&gt;Snowflake&lt;/strong&gt; as a Data Engineer, this article might be interesting:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.analytics.today/blog/top-14-snowflake-data-engineering-best-practices\"&gt;Snowflake Top Tips for Data Engineers - Loading and Transforming Data&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quote:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;If the only tool you have is a hammer - you tend to see every problem as a nail.&lt;/em&gt;  Abraham Maslow.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;In Summary&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Follow the standard ingestion pattern:&lt;/strong&gt;\u00a0\u00a0This involves the multi-stage process of landing the data files in cloud storage and loading them to a landing table before transforming the data.\u00a0\u00a0Breaking the overall process into predefined steps makes it easier to orchestrate and test.\u00a0\u00a0&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Retain history of raw data:&lt;/strong&gt;\u00a0\u00a0Unless your data is sourced from a raw data lake, it makes sense to keep the raw data history which should ideally be stored using the\u00a0&lt;a href=\"https://docs.snowflake.com/en/sql-reference/data-types-semistructured.html#variant\"&gt;VARIANT&lt;/a&gt;\u00a0data type to benefit from automatic schema evolution.\u00a0\u00a0This means you can truncate and re-process data if bugs are found in the transformation pipeline and provide an excellent raw data source for Data Scientists.\u00a0\u00a0While you may not yet have any machine learning requirements yet, it&amp;#39;s almost certain you will, if not now, then in the coming years.\u00a0\u00a0Remember that Snowflake data storage is remarkably cheap, unlike on-premises solutions.\u00a0&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use multiple data models:&lt;/strong&gt;\u00a0\u00a0\u00a0On-premises data storage was so expensive it was not feasible to store multiple copies of data, each using a different data model to match the need.\u00a0\u00a0However, using Snowflake, it makes sense to store raw data history in either structured or variant format, cleaned and conformed data in\u00a0&lt;a href=\"https://dwbi1.wordpress.com/2011/03/28/storing-history-on-3rd-normal-form/\"&gt;3rd Normal Form&lt;/a&gt;\u00a0or using a\u00a0&lt;a href=\"https://www.analytics.today/blog/when-should-i-use-data-vault\"&gt;Data Vault&lt;/a&gt;\u00a0model. Finally, data is ready for consumption in a\u00a0&lt;a href=\"https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/\"&gt;Kimball Dimensional Data model&lt;/a&gt;.\u00a0\u00a0Each data model has unique benefits, and storing the results of intermediate steps has huge architectural benefits, not least the ability to reload and reprocess the data in case of mistakes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use the right tool:&lt;/strong&gt;\u00a0\u00a0As the quote above implies, if you only know one tool, you&amp;#39;ll use it inappropriately some times.\u00a0\u00a0The decision about which tool to use should be based upon a range of factors, including, the existing skill set in the team, whether you need rapid near real-time delivery, and whether you&amp;#39;re doing a once-off data load or a regular repeating process.\u00a0\u00a0Be aware that Snowflake can natively handle a range of file formats, including Avro, Parquet, ORC, JSON and CSV. There is extensive guidance on\u00a0&lt;a href=\"https://docs.snowflake.com/en/user-guide-data-load.html#loading-data-into-snowflake\"&gt;loading data into Snowflake&lt;/a&gt;\u00a0on the online documentation.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use COPY or SNOWPIPE to load data:&lt;/strong&gt;\u00a0\u00a0Around 80% of data loaded into a data warehouse is either ingested using a regular batch process or, increasingly, immediately after the data files arrive.\u00a0\u00a0By far, the fastest, most cost-efficient way to load data is using COPY and SNOWPIPE, so avoid the temptation to use other methods (for example, queries against external tables) for regular data loads.\u00a0\u00a0Effectively, this is another example of\u00a0&lt;em&gt;using the right tool&lt;/em&gt;.\u00a0\u00a0&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Avoid JDBC or ODBC for regular large data loads:&lt;/strong&gt;\u00a0\u00a0Another\u00a0&lt;em&gt;right tool&lt;/em&gt;\u00a0recommendation.\u00a0\u00a0While a JDBC or ODBC interface may be fine to load a few megabytes of data, these interfaces will not scale to the massive throughput of COPY and SNOWPIPE.\u00a0\u00a0Use them by all means, but not for large regular data loads.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Avoid Scanning Files:&lt;/strong&gt; Using the COPY command to ingest data, use\u00a0&lt;a href=\"https://docs.snowflake.com/en/user-guide/data-load-considerations-manage.html#partitioning-staged-data-files\"&gt;partitioned staged data&lt;/a&gt; files.\u00a0\u00a0This reduces the effort of scanning large numbers of data files in cloud storage.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Choose a suitable Virtual Warehouse size:&lt;/strong&gt;\u00a0\u00a0Don\u2019t assume an X6-LARGE warehouse will load huge data files any faster than an X-SMALL.  Each physical file is loaded sequentially on a single CPU, and it is more sensible to load most loads on an X-SMALL warehouse.  Consider splitting massive data files into 100-250MB chunks and loading them on a larger (perhaps MEDIUM size) warehouse.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Ensure 3rd party tools push down:&lt;/strong&gt;\u00a0\u00a0ETL tools like Ab Initio, Talend and Informatica were originally designed to extract data from source systems into an ETL server, transform the data and write them to the warehouse.\u00a0\u00a0As Snowflake can draw upon massive on-demand compute resources and automatically scale out, it makes no sense to use and have data copied to an external server.\u00a0\u00a0Instead, use the ELT (Extract, Load and Transform) method, and ensure the tools generate and execute SQL statements on Snowflake to maximise throughput and reduce costs.  Excellent examples include &lt;a href=\"https://www.getdbt.com/\"&gt;DBT&lt;/a&gt; and &lt;a href=\"https://www.matillion.com/\"&gt;Matillion&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Transform data in Steps:&lt;/strong&gt;\u00a0\u00a0A common mistake by inexperienced data engineers is to write huge SQL statements that join, summarise and process lots of tables in the mistaken belief this is an efficient way of working.\u00a0\u00a0In reality, the code becomes over-complex, difficult to maintain, and, worst still, often performs poorly.\u00a0\u00a0Instead, break the transformation pipeline into multiple steps and write results to intermediate tables.\u00a0\u00a0This makes it easier to test intermediate results, simplifies the code and often produces simple SQL code that runs faster.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use Transient tables for intermediate results:&lt;/strong&gt;\u00a0\u00a0During a complex ELT pipeline, write intermediate results to a\u00a0&lt;a href=\"https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#transient-tables\"&gt;transient table&lt;/a&gt;\u00a0which may be truncated before the next load.\u00a0\u00a0This reduces the time-travel storage to just one day and avoids an additional seven days of fail-safe storage.\u00a0\u00a0By all means, use\u00a0&lt;a href=\"https://docs.snowflake.com/en/user-guide/tables-temp-transient.html#temporary-tables\"&gt;temporary tables&lt;/a&gt;\u00a0if sensible, but the option to check the results of intermediate steps in a complex ELT pipeline is often helpful.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Avoid row-by-row processing:&lt;/strong&gt;\u00a0\u00a0As described in the article on &lt;a href=\"https://www.analytics.today/blog/top-3-snowflake-performance-tuning-tactics\"&gt;Snowflake Query Tuning&lt;/a&gt;, Snowflake is designed to ingest, process and analyse billions of rows at amazing speed.\u00a0\u00a0This is often referred to as &lt;em&gt;set-at-a-time processing.&lt;/em&gt;  However, people tend to think about &lt;em&gt;row-by-row processing&lt;/em&gt;, which sometimes leads to programming loops that fetch and update rows one at a time.\u00a0\u00a0Be aware\u00a0that row-by-row processing is the biggest way to kill query performance.\u00a0\u00a0Use SQL statements to process all table entries simultaneously and avoid row-by-row processing at all costs.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use Query Tags:&lt;/strong&gt;\u00a0\u00a0When you start any multi-step transformation task, set the\u00a0&lt;a href=\"https://docs.snowflake.com/en/sql-reference/sql/alter-session.html#alter-session\"&gt;session query tag using&lt;/a&gt;:\u00a0\u00a0&lt;strong&gt;ALTER SESSION SET QUERY_TAG = &amp;#39;XXXXXX&amp;#39;&lt;/strong&gt; and &lt;strong&gt;ALTER SESSION UNSET QUERY_TAG&lt;/strong&gt;.\u00a0\u00a0This stamps every SQL statement until reset with an identifier and is invaluable to System Administrators.\u00a0\u00a0As every SQL statement (and QUERY_TAG) is recorded in the\u00a0&lt;a href=\"https://docs.snowflake.com/en/sql-reference/account-usage/query_history.html#query-history-view\"&gt;QUERY_HISTORY&lt;/a&gt;\u00a0view, you can track the job performance over time.\u00a0\u00a0This can be used to quickly identify when a task change has resulted in poor performance, identify inefficient transformation jobs or indicate when a job would be better executed on a larger or smaller warehouse.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Keep it Simple:&lt;/strong&gt;\u00a0\u00a0Probably the best indicator of an experienced data engineer is the value they place on &lt;strong&gt;&lt;em&gt;simplicity&lt;/em&gt;&lt;/strong&gt;.\u00a0\u00a0You can always make a job 10% faster, generic, or more elegant, and it\u00a0&lt;em&gt;may&lt;/em&gt;\u00a0be beneficial, but it&amp;#39;s\u00a0&lt;em&gt;always&lt;/em&gt;\u00a0beneficial to simplify a solution.\u00a0\u00a0Simple solutions are easier to understand, easier to diagnose problems and are therefore easier to maintain.\u00a0\u00a0Around 50% of the performance challenges I face are difficult to resolve because the solution is a single, monolithic complex block of code.\u00a0\u00a0The first thing I do is break down the solution into steps and only then identify the root cause.\u00a0\u00a0&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Would you agree with the above?  What would you add?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?auto=webp&amp;v=enabled&amp;s=a38b09a47652c95dd2ea013d37781738e066f079", "width": 1280, "height": 854}, "resolutions": [{"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ffc85d52e5204333576281e4b4299ff55a69391", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09ed7088a5d8327d16e2b3e89aa32025389bd720", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1e032cf9c74ff92cdf3da0b64f4a616ca063d17", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be1880cbd36ce241224b6a45ddbf15fcd88554e8", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=28e656615a48f259f90f30509ab31d6dd54841be", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/BJ2c_0pxMjNUwpCqzQOFeNZ92Vj0VzzTS9GEuCFhz-A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cea027d69027985a8cbfe8daffc22ee98f25e7a", "width": 1080, "height": 720}], "variants": {}, "id": "isjiY2upUCL-ZGo8WHNj3lFtMwczeNb8SBerBsdDFA8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13gln9a", "is_robot_indexable": true, "report_reasons": null, "author": "JohnAnthonyRyan", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gln9a/my_top_14_tips_for_snowflake_data_engineers_what/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gln9a/my_top_14_tips_for_snowflake_data_engineers_what/", "subreddit_subscribers": 105471, "created_utc": 1683994507.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just got an offer for a big company that uses SSIS. Salary and benefits are really good. \n\nI started my career with SSIS. I found it be a little painful and 90% of the I would write C# code in the script component just get around missing functionality. \n\nSince then I decided I wanted to write more code on my career and transitioned to python and airflow for all my jobs in the past few years. \n\nBut the offer sounds good and honestly as long as I keep my skills up in python I\u2019m not sure I really care that o won\u2019t be coding as much. Obviously I don\u2019t want pigeonhole my self to no code tools but I think my past experience makes up for it, where as the beginning of my career it would be a death sentence. \n\nSo it SSIS a dealbreaker or should I just take the good offer?", "author_fullname": "t2_1kset4fg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Bad is SSIS These Days?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gm5ul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683995727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got an offer for a big company that uses SSIS. Salary and benefits are really good. &lt;/p&gt;\n\n&lt;p&gt;I started my career with SSIS. I found it be a little painful and 90% of the I would write C# code in the script component just get around missing functionality. &lt;/p&gt;\n\n&lt;p&gt;Since then I decided I wanted to write more code on my career and transitioned to python and airflow for all my jobs in the past few years. &lt;/p&gt;\n\n&lt;p&gt;But the offer sounds good and honestly as long as I keep my skills up in python I\u2019m not sure I really care that o won\u2019t be coding as much. Obviously I don\u2019t want pigeonhole my self to no code tools but I think my past experience makes up for it, where as the beginning of my career it would be a death sentence. &lt;/p&gt;\n\n&lt;p&gt;So it SSIS a dealbreaker or should I just take the good offer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13gm5ul", "is_robot_indexable": true, "report_reasons": null, "author": "shittyfuckdick", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gm5ul/how_bad_is_ssis_these_days/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gm5ul/how_bad_is_ssis_these_days/", "subreddit_subscribers": 105471, "created_utc": 1683995727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some uses cases that require a noSQL database like MongoDB? I have been a data engineer for over eight years and never had to use one.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NoSQL database use cases", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h30py", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684040213.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some uses cases that require a noSQL database like MongoDB? I have been a data engineer for over eight years and never had to use one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13h30py", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13h30py/nosql_database_use_cases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13h30py/nosql_database_use_cases/", "subreddit_subscribers": 105471, "created_utc": 1684040213.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am on the beginning of my journey:\n\n*  I'm starting to understand the fundamentals\n* Went on 16h course\n* I'm midway through the 'Learning spark' book\n* Already bought 'Spark: definitive guide'\n* Done almost all of zillacode spark exercises\n* Considering preparation for Databricks Spark Associate\n\nNow, I would like to become good at Spark (pyspark specifically, as I don't know scala or java). How do I go about that?\n\nAt my current workplace there isn't anyone that's good at it and when I take initiative to do something in spark it bites me in the ass - I get stuck on some bugs that no one can help me with and I fail/take ages to deliver.\n\nI am already sending out applications but  \na) it's going to take a while to find something  \nb) in order to find some position focused on spark, I do need to have some skill and convince the employer that I'm a good bet.\n\nTo the people who become solid, go-to guys in Spark, can you give some hints on how to make progress? \n\nWith SQL /pandas I could practice strata a lot, with spark there isn't a lot of resources like that, I used zilla out of desperation but it doesn't even come close to stratascratch.\n\nThanks", "author_fullname": "t2_gfx5s46h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to become good at spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13gjzt1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683990551.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am on the beginning of my journey:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; I&amp;#39;m starting to understand the fundamentals&lt;/li&gt;\n&lt;li&gt;Went on 16h course&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m midway through the &amp;#39;Learning spark&amp;#39; book&lt;/li&gt;\n&lt;li&gt;Already bought &amp;#39;Spark: definitive guide&amp;#39;&lt;/li&gt;\n&lt;li&gt;Done almost all of zillacode spark exercises&lt;/li&gt;\n&lt;li&gt;Considering preparation for Databricks Spark Associate&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now, I would like to become good at Spark (pyspark specifically, as I don&amp;#39;t know scala or java). How do I go about that?&lt;/p&gt;\n\n&lt;p&gt;At my current workplace there isn&amp;#39;t anyone that&amp;#39;s good at it and when I take initiative to do something in spark it bites me in the ass - I get stuck on some bugs that no one can help me with and I fail/take ages to deliver.&lt;/p&gt;\n\n&lt;p&gt;I am already sending out applications but&lt;br/&gt;\na) it&amp;#39;s going to take a while to find something&lt;br/&gt;\nb) in order to find some position focused on spark, I do need to have some skill and convince the employer that I&amp;#39;m a good bet.&lt;/p&gt;\n\n&lt;p&gt;To the people who become solid, go-to guys in Spark, can you give some hints on how to make progress? &lt;/p&gt;\n\n&lt;p&gt;With SQL /pandas I could practice strata a lot, with spark there isn&amp;#39;t a lot of resources like that, I used zilla out of desperation but it doesn&amp;#39;t even come close to stratascratch.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13gjzt1", "is_robot_indexable": true, "report_reasons": null, "author": "LeftHelicopter5297", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gjzt1/how_to_become_good_at_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13gjzt1/how_to_become_good_at_spark/", "subreddit_subscribers": 105471, "created_utc": 1683990551.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My data engineering project, that scrapes front page news headline data and outputs it to google data studio.\n\n* [GitHub link](https://github.com/dhruvi-9/news-headlines)\n* At the top are filters for the dashboard, this also cross filters in most of the charts (not the word cloud). Word filtering capability limited to one word.\n\n* [This line](https://github.com/dhruvi-9/news-headlines/blob/main/transform_word_count.py#L10) has a list of values that are excluding from the word table, as they don't add value to the meaning of the title.\n\n[google data studio](https://lookerstudio.google.com/u/0/reporting/78afc346-af31-485e-b6c9-e88ddebdfe8b/page/qA9CD) Visual preview:\n\nhttps://preview.redd.it/qcxpr1wxaqza1.png?width=1484&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9d52f54d07661f4976ca3609e3e53e229f218ca3\n\nTech Stack:\n\n* AWS Lambda \\[compute\\]\n* AWS Step Functions \\[orchestrate\\]\n* AWS RDS for Postgres \\[storage\\]\n\nFor the future, I'd like to migrate off of orchestration via AWS step functions, they are pretty annoying to work with. I'd like to scale this and simulate a real-world application where there's a larger amount of data to work with. \n\nWould like to hear any constructive feedback on the code, GitHub file set up, dashboard, libraries / tools used, future state, anything really.\n\nMy goal with this was implementing things that I wanted to learn more about.\n\nThanks for reading.\n\nEdit 1: words have to be lowercase for the filter.", "author_fullname": "t2_f2plcmjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Front page news headline scraping data engineering project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 136, "top_awarded_type": null, "hide_score": false, "media_metadata": {"qcxpr1wxaqza1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 105, "x": 108, "u": "https://preview.redd.it/qcxpr1wxaqza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14baee46a04f4933114e25ac436b1f252fc4d454"}, {"y": 210, "x": 216, "u": "https://preview.redd.it/qcxpr1wxaqza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=659fe9a8473d367c4c07aa6892c02a20e841abfc"}, {"y": 312, "x": 320, "u": "https://preview.redd.it/qcxpr1wxaqza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed8559a57dde834fe2928ef3a1851831de84c3e0"}, {"y": 624, "x": 640, "u": "https://preview.redd.it/qcxpr1wxaqza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be77350c08fc14d34fddf53ddd96737d403caf58"}, {"y": 936, "x": 960, "u": "https://preview.redd.it/qcxpr1wxaqza1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=259d8068cff9c39bf480d3090390656ef60eec89"}, {"y": 1053, "x": 1080, "u": "https://preview.redd.it/qcxpr1wxaqza1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50a2c39dbf924b8f05ee83f81996ad8c5ff242b9"}], "s": {"y": 1448, "x": 1484, "u": "https://preview.redd.it/qcxpr1wxaqza1.png?width=1484&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9d52f54d07661f4976ca3609e3e53e229f218ca3"}, "id": "qcxpr1wxaqza1"}}, "name": "t3_13h3hek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bIpYjDIqP11Zfqd-i6m5bJ8X6xmryCgGqdliuphBBRA.jpg", "edited": 1684046994.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1684041756.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My data engineering project, that scrapes front page news headline data and outputs it to google data studio.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/dhruvi-9/news-headlines\"&gt;GitHub link&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;At the top are filters for the dashboard, this also cross filters in most of the charts (not the word cloud). Word filtering capability limited to one word.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;a href=\"https://github.com/dhruvi-9/news-headlines/blob/main/transform_word_count.py#L10\"&gt;This line&lt;/a&gt; has a list of values that are excluding from the word table, as they don&amp;#39;t add value to the meaning of the title.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://lookerstudio.google.com/u/0/reporting/78afc346-af31-485e-b6c9-e88ddebdfe8b/page/qA9CD\"&gt;google data studio&lt;/a&gt; Visual preview:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qcxpr1wxaqza1.png?width=1484&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9d52f54d07661f4976ca3609e3e53e229f218ca3\"&gt;https://preview.redd.it/qcxpr1wxaqza1.png?width=1484&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=9d52f54d07661f4976ca3609e3e53e229f218ca3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tech Stack:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AWS Lambda [compute]&lt;/li&gt;\n&lt;li&gt;AWS Step Functions [orchestrate]&lt;/li&gt;\n&lt;li&gt;AWS RDS for Postgres [storage]&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For the future, I&amp;#39;d like to migrate off of orchestration via AWS step functions, they are pretty annoying to work with. I&amp;#39;d like to scale this and simulate a real-world application where there&amp;#39;s a larger amount of data to work with. &lt;/p&gt;\n\n&lt;p&gt;Would like to hear any constructive feedback on the code, GitHub file set up, dashboard, libraries / tools used, future state, anything really.&lt;/p&gt;\n\n&lt;p&gt;My goal with this was implementing things that I wanted to learn more about.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading.&lt;/p&gt;\n\n&lt;p&gt;Edit 1: words have to be lowercase for the filter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q2x48eiKavXfpc68ZQo262ZlIBpZ7dhQAZG-tNG0bGg.png?auto=webp&amp;v=enabled&amp;s=2199daa31a1ca7c83b386666b6757c0208767055", "width": 1484, "height": 1448}, "resolutions": [{"url": "https://external-preview.redd.it/q2x48eiKavXfpc68ZQo262ZlIBpZ7dhQAZG-tNG0bGg.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cadf98c74dd4d682c04fab38066442006fda086", "width": 108, "height": 105}, {"url": "https://external-preview.redd.it/q2x48eiKavXfpc68ZQo262ZlIBpZ7dhQAZG-tNG0bGg.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30275e23682a5aa7614c2ca01064dc1583fed49f", "width": 216, "height": 210}, {"url": "https://external-preview.redd.it/q2x48eiKavXfpc68ZQo262ZlIBpZ7dhQAZG-tNG0bGg.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=248d0fc20d1547f662b6d7bc2e8980e616e91572", "width": 320, "height": 312}, {"url": "https://external-preview.redd.it/q2x48eiKavXfpc68ZQo262ZlIBpZ7dhQAZG-tNG0bGg.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca04966c9737001a904a4f002539a0f1ef8ce878", "width": 640, "height": 624}, {"url": "https://external-preview.redd.it/q2x48eiKavXfpc68ZQo262ZlIBpZ7dhQAZG-tNG0bGg.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff9ab81d77f02c69991ec47bbe0e4b311cc0cf51", "width": 960, "height": 936}, {"url": "https://external-preview.redd.it/q2x48eiKavXfpc68ZQo262ZlIBpZ7dhQAZG-tNG0bGg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba69af1657bb975a2817dea6604ec8abe4241751", "width": 1080, "height": 1053}], "variants": {}, "id": "cBExlX-Ng09krk5B24hL7GQEmdokAUIJJBlSw5LliIg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "13h3hek", "is_robot_indexable": true, "report_reasons": null, "author": "Tough_Bag_458", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13h3hek/front_page_news_headline_scraping_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13h3hek/front_page_news_headline_scraping_data/", "subreddit_subscribers": 105471, "created_utc": 1684041756.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im currently a DE at a tech company working in the Marketing Optimization domain. I honestly like my company and we use all the modern data stack including a CDP and a rETL tool. Seems to be the best place to learn since that\u2019s were all the money is. BUT i found this marketing jargon and stuff really boring lol. The only other \u201cdomain\u201d in my company for DEs is in manufacturing and supply chain which seems even more boring imo lol.\n\nAnyone here working in some interesting companies or domains? I honestly prefer financial data but refuse to work in a bank (again lol). Mainly looking for tech companies. \n\nAny info or experience would be highly appreciated!", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any fun domains in tech to work in as DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hbvf6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684069860.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im currently a DE at a tech company working in the Marketing Optimization domain. I honestly like my company and we use all the modern data stack including a CDP and a rETL tool. Seems to be the best place to learn since that\u2019s were all the money is. BUT i found this marketing jargon and stuff really boring lol. The only other \u201cdomain\u201d in my company for DEs is in manufacturing and supply chain which seems even more boring imo lol.&lt;/p&gt;\n\n&lt;p&gt;Anyone here working in some interesting companies or domains? I honestly prefer financial data but refuse to work in a bank (again lol). Mainly looking for tech companies. &lt;/p&gt;\n\n&lt;p&gt;Any info or experience would be highly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13hbvf6", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13hbvf6/any_fun_domains_in_tech_to_work_in_as_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13hbvf6/any_fun_domains_in_tech_to_work_in_as_de/", "subreddit_subscribers": 105471, "created_utc": 1684069860.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nNot sure if there is an easy way to do this. I have a SELECT statement. I want to use it to either append the results to a table in Snowflake, or to create the table using the defined fields / field types and then insert the data into the table.\n\nIs there an easy way to do this?\n\nKinda 'CREATE TABLE IF NOT EXISTS OR INSERT INTO...'\n\n&amp;#x200B;\n\nEdit: I was thinking about a \n\nCREATE TABLE IF NOT EXISTS some\\_table AS SELECT a, b, c FROM xyz LIMIT 0;\n\nINSERT INTO some\\_table SELECT a, b, c FROM xyz;", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake / Python : Create table if not exists from SELECT, else append with same SELECT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13h6kks", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684053296.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684052635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Not sure if there is an easy way to do this. I have a SELECT statement. I want to use it to either append the results to a table in Snowflake, or to create the table using the defined fields / field types and then insert the data into the table.&lt;/p&gt;\n\n&lt;p&gt;Is there an easy way to do this?&lt;/p&gt;\n\n&lt;p&gt;Kinda &amp;#39;CREATE TABLE IF NOT EXISTS OR INSERT INTO...&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: I was thinking about a &lt;/p&gt;\n\n&lt;p&gt;CREATE TABLE IF NOT EXISTS some_table AS SELECT a, b, c FROM xyz LIMIT 0;&lt;/p&gt;\n\n&lt;p&gt;INSERT INTO some_table SELECT a, b, c FROM xyz;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13h6kks", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13h6kks/snowflake_python_create_table_if_not_exists_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13h6kks/snowflake_python_create_table_if_not_exists_from/", "subreddit_subscribers": 105471, "created_utc": 1684052635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_76fvluuq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE's when a new job uses a different cloud platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_13hebz5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/aXF_s_0TbGZ_b6GiRUD8cuCIYiGvR0sGT-VeW5mxcJU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684076149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/szesjdra9tza1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/szesjdra9tza1.png?auto=webp&amp;v=enabled&amp;s=97a7a234a1fb89096fc8b25829f3a83b825ce669", "width": 800, "height": 600}, "resolutions": [{"url": "https://preview.redd.it/szesjdra9tza1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=706b0e986cc6ad4706b30636de07d80edf600751", "width": 108, "height": 81}, {"url": "https://preview.redd.it/szesjdra9tza1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=654ea07ba1763899ef2239f3cbcda2f983851d92", "width": 216, "height": 162}, {"url": "https://preview.redd.it/szesjdra9tza1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db01d98c882be03433ebbd756bc728e45daa50e8", "width": 320, "height": 240}, {"url": "https://preview.redd.it/szesjdra9tza1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ece7d6375cca563383ec102d0abefdbacaded33a", "width": 640, "height": 480}], "variants": {}, "id": "U5cr1AKQ2F8uMX5OSqGr7IVYFhLoHIsY4mCjdRMX-mc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "13hebz5", "is_robot_indexable": true, "report_reasons": null, "author": "notGaruda1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13hebz5/des_when_a_new_job_uses_a_different_cloud_platform/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/szesjdra9tza1.png", "subreddit_subscribers": 105471, "created_utc": 1684076149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello! \n\nOur data is mostly in BigQuery and is around 1TB give or take. What cloud based tools other than dbt can be used for transformation and aggregation with all the features of dbt ?\n\nThanks!", "author_fullname": "t2_dtq1w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data transformation tools other than DBT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13he7yf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684075869.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! &lt;/p&gt;\n\n&lt;p&gt;Our data is mostly in BigQuery and is around 1TB give or take. What cloud based tools other than dbt can be used for transformation and aggregation with all the features of dbt ?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13he7yf", "is_robot_indexable": true, "report_reasons": null, "author": "perpetually_phi", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13he7yf/data_transformation_tools_other_than_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13he7yf/data_transformation_tools_other_than_dbt/", "subreddit_subscribers": 105471, "created_utc": 1684075869.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3297c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLM meets SQL; developing ChatGPT Plugins in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_13he4lp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ca75Tly8_h5SHlp6vS7ptT3OHJJFcBlWmy83D76FG_w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684075630.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "jinjat.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://jinjat.com/blog/develop-chatgpt-plugins-with-sql-using-jinjat", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mC24g1tcbPK5MydR89jjfsn5-_iKOuq7PUZt9IqFPO8.jpg?auto=webp&amp;v=enabled&amp;s=cf9e4548f859727950a5f6ffad2997b1d0597253", "width": 1588, "height": 892}, "resolutions": [{"url": "https://external-preview.redd.it/mC24g1tcbPK5MydR89jjfsn5-_iKOuq7PUZt9IqFPO8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74b6722e13ec5d19192a1f87515b400ccb1912ce", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/mC24g1tcbPK5MydR89jjfsn5-_iKOuq7PUZt9IqFPO8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d63bda156b7fa7d4c0f32642a7ad077bc982b13", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/mC24g1tcbPK5MydR89jjfsn5-_iKOuq7PUZt9IqFPO8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78565b589eabc1f2151242f2a1c66718c0b26436", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/mC24g1tcbPK5MydR89jjfsn5-_iKOuq7PUZt9IqFPO8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99bd980985d0b124f5131c2b1900996ef411b757", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/mC24g1tcbPK5MydR89jjfsn5-_iKOuq7PUZt9IqFPO8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9edcb8cf841d94d5dc038458fe812898b05f1ac", "width": 960, "height": 539}, {"url": "https://external-preview.redd.it/mC24g1tcbPK5MydR89jjfsn5-_iKOuq7PUZt9IqFPO8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da85c8bf9646bebe4926a03048bad0ba8f2fd050", "width": 1080, "height": 606}], "variants": {}, "id": "7FJ9CGvCl-NAcgjBp5EGRnVa08pU1OwkWtzhIEUbMJ4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13he4lp", "is_robot_indexable": true, "report_reasons": null, "author": "Buremba", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13he4lp/llm_meets_sql_developing_chatgpt_plugins_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://jinjat.com/blog/develop-chatgpt-plugins-with-sql-using-jinjat", "subreddit_subscribers": 105471, "created_utc": 1684075630.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "GCP bigquery recently announced the cdc feature(via stream write API). see [Real-time CDC replication into BigQuery | Google Cloud Blog](https://cloud.google.com/blog/products/data-analytics/real-time-cdc-replication-bigquery)\nMy question is, with this feature, can we view BigQuery as a lakehouse and apply [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture) on it?", "author_fullname": "t2_22f1tgt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can we apply Medallion Architecture on BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13hdayd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684073572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;GCP bigquery recently announced the cdc feature(via stream write API). see &lt;a href=\"https://cloud.google.com/blog/products/data-analytics/real-time-cdc-replication-bigquery\"&gt;Real-time CDC replication into BigQuery | Google Cloud Blog&lt;/a&gt;\nMy question is, with this feature, can we view BigQuery as a lakehouse and apply &lt;a href=\"https://www.databricks.com/glossary/medallion-architecture\"&gt;Medallion Architecture&lt;/a&gt; on it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?auto=webp&amp;v=enabled&amp;s=2bb423580e49d6864e3715f6654632d558a7d49c", "width": 2500, "height": 1232}, "resolutions": [{"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48579d8f753ce69b5f1b8ae0af753906892694f5", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1386d91cb1dd393b161dbfe57a97c27025c38f1", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c6c56995d10243a604293adda8ea07608cc4e59", "width": 320, "height": 157}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf7551884fe67c86f8908209089a05249b1c74ee", "width": 640, "height": 315}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07368c03b392acb2c54be0f5f3abda104f59aff0", "width": 960, "height": 473}, {"url": "https://external-preview.redd.it/AGcjPNSZNVF2pFc4CUhjCAKemCb21B_K6EBD1q4MueY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d94a8dee7f54cbfd6516eef96347cfa5a61ec285", "width": 1080, "height": 532}], "variants": {}, "id": "wcoegTgls8vDT7sU2cRo3smh6WlMonA1_OjrS3a-3OQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13hdayd", "is_robot_indexable": true, "report_reasons": null, "author": "kk17forever", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13hdayd/can_we_apply_medallion_architecture_on_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13hdayd/can_we_apply_medallion_architecture_on_bigquery/", "subreddit_subscribers": 105471, "created_utc": 1684073572.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello engineers,\n\nI work for a start-up style ecommerce company with a large amount of traffic and revenue. We're aiming to become more data-oriented and start taking CRO seriously. Our data is severly lacking - we have issues with our GA properties, often the data we see on analytics is wrong (eg a product will say it has no revenue, when we know on our CRM it does), integrations seem not to work properly, and feels impossible to discern information from. We had someone who was working on it to both fix our broken data issues and build a usable dashboard on Looker studio, but the task seems beyond them. Our company is scaling and we are implementing A/B testing, heatmaps analysis, etc, and simply I'd like to be able to analyse our user activity. It seems like it should be simple, but we've faced roadblock after roadblock!\n\nSo, we are considering starting from fresh. I've been tasked with hiring someone to fix up our Data, and I barely know where to start. I've gathered I need a Data engineer, and that what we are looking for is concerning ETL and big data, but beyond that, I'm lost. Can anyone advise me on what I actually need, and whether it can all be done in GA4? I'm tired of going in circles!\n\nYours,\n\nA confused but determined creative.", "author_fullname": "t2_adgm2h29", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please advise this confused marketing manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13hc7ti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684071088.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684070779.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello engineers,&lt;/p&gt;\n\n&lt;p&gt;I work for a start-up style ecommerce company with a large amount of traffic and revenue. We&amp;#39;re aiming to become more data-oriented and start taking CRO seriously. Our data is severly lacking - we have issues with our GA properties, often the data we see on analytics is wrong (eg a product will say it has no revenue, when we know on our CRM it does), integrations seem not to work properly, and feels impossible to discern information from. We had someone who was working on it to both fix our broken data issues and build a usable dashboard on Looker studio, but the task seems beyond them. Our company is scaling and we are implementing A/B testing, heatmaps analysis, etc, and simply I&amp;#39;d like to be able to analyse our user activity. It seems like it should be simple, but we&amp;#39;ve faced roadblock after roadblock!&lt;/p&gt;\n\n&lt;p&gt;So, we are considering starting from fresh. I&amp;#39;ve been tasked with hiring someone to fix up our Data, and I barely know where to start. I&amp;#39;ve gathered I need a Data engineer, and that what we are looking for is concerning ETL and big data, but beyond that, I&amp;#39;m lost. Can anyone advise me on what I actually need, and whether it can all be done in GA4? I&amp;#39;m tired of going in circles!&lt;/p&gt;\n\n&lt;p&gt;Yours,&lt;/p&gt;\n\n&lt;p&gt;A confused but determined creative.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13hc7ti", "is_robot_indexable": true, "report_reasons": null, "author": "raelelel", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13hc7ti/please_advise_this_confused_marketing_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13hc7ti/please_advise_this_confused_marketing_manager/", "subreddit_subscribers": 105471, "created_utc": 1684070779.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r1ahxly5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating Healthcare Data Pipelines using Ascend platform", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13h9l5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/729wPsWjklI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Automating Healthcare Data Pipelines with Sarwat Fatima\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Automating Healthcare Data Pipelines with Sarwat Fatima", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/729wPsWjklI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Automating Healthcare Data Pipelines with Sarwat Fatima\"&gt;&lt;/iframe&gt;", "author_name": "Ascend", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/729wPsWjklI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Ascend-io-data"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/729wPsWjklI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Automating Healthcare Data Pipelines with Sarwat Fatima\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13h9l5n", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1bZOWKF6smtZVzCtLLEuXqnmxTYEcPGA6tRP4055qVI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684063068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/729wPsWjklI", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZKR8c-wtdt6gkNFjx_7XgVreUlMLuMGGXZ6nm3qWuaU.jpg?auto=webp&amp;v=enabled&amp;s=2061757c685072cd2e471ebd5ee542b19a44d753", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/ZKR8c-wtdt6gkNFjx_7XgVreUlMLuMGGXZ6nm3qWuaU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8dc421295c405a576c024558297abae813eba104", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/ZKR8c-wtdt6gkNFjx_7XgVreUlMLuMGGXZ6nm3qWuaU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a019fd5164f1e1525ee423807e1cab095763aaa8", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/ZKR8c-wtdt6gkNFjx_7XgVreUlMLuMGGXZ6nm3qWuaU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5cc6864538aacab57df98b25960dce32b27650a7", "width": 320, "height": 240}], "variants": {}, "id": "T2L67Uvg1lkw-OLAA5nu8urIdqMffjIcGgpsCo99cYY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13h9l5n", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Meringue6544", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13h9l5n/automating_healthcare_data_pipelines_using_ascend/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/729wPsWjklI", "subreddit_subscribers": 105471, "created_utc": 1684063068.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Automating Healthcare Data Pipelines with Sarwat Fatima", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/729wPsWjklI?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Automating Healthcare Data Pipelines with Sarwat Fatima\"&gt;&lt;/iframe&gt;", "author_name": "Ascend", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/729wPsWjklI/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Ascend-io-data"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_o0puly65", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the top tools for Data Engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13gkq41", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zm5gzbKNTMmZ8E0pTjVsMlmo27PB02vQADXELI4o8mw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683992337.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thenewstack.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://thenewstack.io/top-10-tools-for-data-engineers/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?auto=webp&amp;v=enabled&amp;s=8ca1be5fc4c37599118a8495576087d7636a2714", "width": 1280, "height": 854}, "resolutions": [{"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0c4b4af62ed47e543c74bca266afca68f2b875f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=065d16d7945642ca299bd1e58a0ee902c56a5df1", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50d7c338bfc52a2b12b3e3212aca34371bb4f846", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a304969b8dec7788bd7941f37596dd7b6dc158b", "width": 640, "height": 427}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dca53b02e6b99e538a964279ca4794f682bab314", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/iNENGD0m_kkxjzSRYZamkj0OMsTsGmSG3OLiZIPhj8M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c15ee3ab394e6c3dfcad081e5e7cfac0612409d", "width": 1080, "height": 720}], "variants": {}, "id": "NOjHhdkiTy-5NXVCSGae5PHkRgAYZj5-JzhPYPtWvMs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13gkq41", "is_robot_indexable": true, "report_reasons": null, "author": "Jumpy_Name_827", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13gkq41/what_are_the_top_tools_for_data_engineers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://thenewstack.io/top-10-tools-for-data-engineers/", "subreddit_subscribers": 105471, "created_utc": 1683992337.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}