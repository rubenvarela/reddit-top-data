{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is you boss someone who can help you with technical stuff or do they just manage you and you team from above? I'm just asking because my boss is completely non technical and sometimes its not an issue, but other times it would be nice to have someone who understands what his team is doing. It prevents hours wasted trying to explain something rather than a quick call or maybe an email. More importantly, it helps when you're trying to explain \"why\" something can't be done or why something might take longer than expected when you're dealing with someone who understands certain technical aspects of your job.", "author_fullname": "t2_aobif6bsf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is your boss technical in that he/she can help you with difficult problems or non technical?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v6dku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 74, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 74, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685393936.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is you boss someone who can help you with technical stuff or do they just manage you and you team from above? I&amp;#39;m just asking because my boss is completely non technical and sometimes its not an issue, but other times it would be nice to have someone who understands what his team is doing. It prevents hours wasted trying to explain something rather than a quick call or maybe an email. More importantly, it helps when you&amp;#39;re trying to explain &amp;quot;why&amp;quot; something can&amp;#39;t be done or why something might take longer than expected when you&amp;#39;re dealing with someone who understands certain technical aspects of your job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13v6dku", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Flow5900", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v6dku/is_your_boss_technical_in_that_heshe_can_help_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v6dku/is_your_boss_technical_in_that_heshe_can_help_you/", "subreddit_subscribers": 108005, "created_utc": 1685393936.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jx0q8m4g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Email addresses are not primary user identities", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vj2ub", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685430143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ntietz.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ntietz.com/blog/email-address-not-identifier/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vj2ub", "is_robot_indexable": true, "report_reasons": null, "author": "ageam54", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vj2ub/email_addresses_are_not_primary_user_identities/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ntietz.com/blog/email-address-not-identifier/", "subreddit_subscribers": 108005, "created_utc": 1685430143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is inverted index, and how we made log analysis 10 times more cost-effective with it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_13vo4mw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6sHewnuNxqUUcp1rGXcVZYcr_aY93d91Ay6MadPRVOI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685447453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/6afc6cc81d20", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?auto=webp&amp;v=enabled&amp;s=85d15c6f2e44360b37ad945b2d7a176af391a63b", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a31eb2219d52e1e6ce76d5470baa480b91ee96", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea69238f22243a5dbf0acc3150375db7d73bd5d8", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a729d4c6f9b056f40e8a38400c8c4300f58c99b4", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26ba87daf0b4018fa4c7dcc0947739253a6986fb", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfdf25e3d76da7c9d452b90ca7cfa55bc45e1c7e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=200617b9d58e70e4cfb8da5af77fb36cb5770236", "width": 1080, "height": 720}], "variants": {}, "id": "RaCe7N3GeU-J2bdRI-tTmDBSLGqF5H1FeMi9MIqYHIQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vo4mw", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vo4mw/what_is_inverted_index_and_how_we_made_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/6afc6cc81d20", "subreddit_subscribers": 108005, "created_utc": 1685447453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand why it takes forever to copy 500,000 small files from one partition to another. However if I put those 500,000 files into a zip and copy that over, then unzip it, it only takes about 10 seconds.\n\nWhat is a zip/unzip process is able to do with the filesystem that makes it quick that a copy process is not?", "author_fullname": "t2_5aiux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A question about filesystems and quantity of files in relation to speed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vdmhk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685412925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand why it takes forever to copy 500,000 small files from one partition to another. However if I put those 500,000 files into a zip and copy that over, then unzip it, it only takes about 10 seconds.&lt;/p&gt;\n\n&lt;p&gt;What is a zip/unzip process is able to do with the filesystem that makes it quick that a copy process is not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vdmhk", "is_robot_indexable": true, "report_reasons": null, "author": "Eisenstein", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vdmhk/a_question_about_filesystems_and_quantity_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vdmhk/a_question_about_filesystems_and_quantity_of/", "subreddit_subscribers": 108005, "created_utc": 1685412925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have several tables stored in Delta format, having any number of columns from 5-500. I wish to scan each table, going through each column and automatically flagging Email/Phone Number fields.\n\nI searched for open source frameworks and came across Great Expectations, Piperider and such but these seem more for data quality and putting thresholds/conditions on particular fields. I want to scan through the entire column list for any such PII field.\n\nA last-resort option is to have an ML model trained to identify email addresses and phone numbers run through the top 100 or so non-null records and try to check for these conditions over each of the hundred columns on a best-effort basis.\n\nP.S. The dataframes can have millions of records, so scanning through the entire df seems pointless.\n\nWould appreciate any help/pointer in the right direction. Thanks!", "author_fullname": "t2_xx7bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting PII columns in a Dataframe/Delta Format", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vm8zs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685441564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several tables stored in Delta format, having any number of columns from 5-500. I wish to scan each table, going through each column and automatically flagging Email/Phone Number fields.&lt;/p&gt;\n\n&lt;p&gt;I searched for open source frameworks and came across Great Expectations, Piperider and such but these seem more for data quality and putting thresholds/conditions on particular fields. I want to scan through the entire column list for any such PII field.&lt;/p&gt;\n\n&lt;p&gt;A last-resort option is to have an ML model trained to identify email addresses and phone numbers run through the top 100 or so non-null records and try to check for these conditions over each of the hundred columns on a best-effort basis.&lt;/p&gt;\n\n&lt;p&gt;P.S. The dataframes can have millions of records, so scanning through the entire df seems pointless.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any help/pointer in the right direction. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vm8zs", "is_robot_indexable": true, "report_reasons": null, "author": "sArThAk882", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vm8zs/detecting_pii_columns_in_a_dataframedelta_format/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vm8zs/detecting_pii_columns_in_a_dataframedelta_format/", "subreddit_subscribers": 108005, "created_utc": 1685441564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone.\nWe are currently ingesting structured data daily, with incremental and full loads, on AWS Glue Spark jobs.\nWe wanted to increase frequency but there is a lot of cost involved.\nWe were advised to attempt a migration of workloads to AWS fargate to make it more cost effective.\nIs there anyone who has had experience with either one of them or such a migration that can speak to the cost and performance. Any guidance would be profoundly appreciated.", "author_fullname": "t2_8ixxnnf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Fargate (EKS) vs AWS Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vhyyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685426113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone.\nWe are currently ingesting structured data daily, with incremental and full loads, on AWS Glue Spark jobs.\nWe wanted to increase frequency but there is a lot of cost involved.\nWe were advised to attempt a migration of workloads to AWS fargate to make it more cost effective.\nIs there anyone who has had experience with either one of them or such a migration that can speak to the cost and performance. Any guidance would be profoundly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vhyyd", "is_robot_indexable": true, "report_reasons": null, "author": "mozakaak", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vhyyd/aws_fargate_eks_vs_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vhyyd/aws_fargate_eks_vs_aws_glue/", "subreddit_subscribers": 108005, "created_utc": 1685426113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any resource you recommend to learn how to optimize spark applications?", "author_fullname": "t2_1o26bmw3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Parameter Optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v56mt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685391137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any resource you recommend to learn how to optimize spark applications?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13v56mt", "is_robot_indexable": true, "report_reasons": null, "author": "da_chosen1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v56mt/spark_parameter_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v56mt/spark_parameter_optimization/", "subreddit_subscribers": 108005, "created_utc": 1685391137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello good people! \n\nI am on a journey of transition from DS to DE. Basically I am advanced now in SQL, Python, Spark for batch, Docker, Airflow. I know Kubernetes architecture on a high level, am decent with my understanding of networking, Linux, a bit of cloud. \n\nI was thinking next I should focus on streaming? Would that be a good next stop? \n\nAnd if so, should I go for Spark streaming since I know PySpark very well already? Or should I just go for Kafka? And do you know any good resources to learn besides the official documentation? \n\nThanks in advance!", "author_fullname": "t2_uv1rtgfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Diving into Streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uz8fa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685376998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello good people! &lt;/p&gt;\n\n&lt;p&gt;I am on a journey of transition from DS to DE. Basically I am advanced now in SQL, Python, Spark for batch, Docker, Airflow. I know Kubernetes architecture on a high level, am decent with my understanding of networking, Linux, a bit of cloud. &lt;/p&gt;\n\n&lt;p&gt;I was thinking next I should focus on streaming? Would that be a good next stop? &lt;/p&gt;\n\n&lt;p&gt;And if so, should I go for Spark streaming since I know PySpark very well already? Or should I just go for Kafka? And do you know any good resources to learn besides the official documentation? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13uz8fa", "is_robot_indexable": true, "report_reasons": null, "author": "4eyes1soul", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uz8fa/diving_into_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uz8fa/diving_into_streaming/", "subreddit_subscribers": 108005, "created_utc": 1685376998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can someone explain what these two are in a simple language without the consultant-speak bullshit? Anything I\u2019ve seen online just sounds like fluffy gibberish.", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master data and reference data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uyixi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685375393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone explain what these two are in a simple language without the consultant-speak bullshit? Anything I\u2019ve seen online just sounds like fluffy gibberish.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13uyixi", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uyixi/master_data_and_reference_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uyixi/master_data_and_reference_data/", "subreddit_subscribers": 108005, "created_utc": 1685375393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Write-Audit-Publish (WAP) is a pattern in data engineering that gives teams greater control over data quality.** It was popularized by Netflix back in 2017 in [a talk](https://www.youtube.com/watch?v=fXHdeBnpXrg) by Michelle Winters at the DataWorks Summit called \u201c*Whoops the Numbers are wrong! Scaling Data Quality @ Netflix*.\u201d\n\nThe name is fairly self-descriptive:\n\nhttps://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc\n\n## What is WAP not (in this context)?\n\n&amp;#x200B;\n\n[Am I showing my age? ;-D](https://preview.redd.it/cmxhd90xjz2b1.png?width=169&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=924223e919c8df079e1f21f6f80c668298480289)\n\n## Why is WAP useful for data engineers?\n\nThe data engineering world has always lagged behind its software engineering brethren.\n\nConcepts like source control were well established in software engineering for a decade or more before data engineers realised that there might just be something in the idea of not emailing around files called `DIM_DATE_V1_FINAL_REVISED_v2_PROD.sql`. (In fairness, it took a shift away from the old mindset of the established vendors too, in parallel with the emergence of the modern data stack for things to really click).\n\nWrite-Audit-Publish is very similar\u2014or perhaps the same, if you squint\u2014to the idea of Blue-Green deployments in the software engineering world. As data engineers we can learn a lot from established and proven patterns, and the Blue-Green one is a good example of this.\n\nWhy *wouldn\u2019t* we want to adopt this, perhaps other than inertia and fear of something new? WAP is a perfect fit for both regular data pipelines as well as one-off data processing jobs.\n\n## Wanna Read More?\n\n\ud83d\udc49\ud83c\udffbCheck out the full article that I wrote: [Data Engineering Patterns: Write-Audit-Publish (WAP)](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/?utm_campaign=Social%20media%20activity&amp;utm_source=reddit&amp;utm_medium=social&amp;utm_content=blog_rm-wap1)\n\n\\---\n\n*Full disclosure: I work for Treeverse, the company behind lakeFS.* ", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Write-Audit-Publish - and why is it useful for data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2w9ow4kehz2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 131, "x": 108, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91bad6163ad6ec681770fb026532a20892676e6d"}, {"y": 262, "x": 216, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=955e951592ac3466153560e1490921e6eea30d00"}, {"y": 388, "x": 320, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53f594be0151b869d1ef65bdec6020ea1fa179fa"}, {"y": 776, "x": 640, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43c5675342b4528484b0272706ce61711ada0bf8"}], "s": {"y": 971, "x": 800, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc"}, "id": "2w9ow4kehz2b1"}, "cmxhd90xjz2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 138, "x": 108, "u": "https://preview.redd.it/cmxhd90xjz2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c747c004507167e1cc6cc87e5f0f8ced5eedd6c"}], "s": {"y": 217, "x": 169, "u": "https://preview.redd.it/cmxhd90xjz2b1.png?width=169&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=924223e919c8df079e1f21f6f80c668298480289"}, "id": "cmxhd90xjz2b1"}}, "name": "t3_13vmdj6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fXHdeBnpXrg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fXHdeBnpXrg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX\"&gt;&lt;/iframe&gt;", "author_name": "DataWorks Summit", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fXHdeBnpXrg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataWorksSummit"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fXHdeBnpXrg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13vmdj6", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/B3xUFVYX71jE12eiUsZItYUOvl1DFXOYOXm0aYgFrtM.jpg", "edited": 1685452641.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1685441989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Write-Audit-Publish (WAP) is a pattern in data engineering that gives teams greater control over data quality.&lt;/strong&gt; It was popularized by Netflix back in 2017 in &lt;a href=\"https://www.youtube.com/watch?v=fXHdeBnpXrg\"&gt;a talk&lt;/a&gt; by Michelle Winters at the DataWorks Summit called \u201c&lt;em&gt;Whoops the Numbers are wrong! Scaling Data Quality @ Netflix&lt;/em&gt;.\u201d&lt;/p&gt;\n\n&lt;p&gt;The name is fairly self-descriptive:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc\"&gt;https://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;What is WAP not (in this context)?&lt;/h2&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cmxhd90xjz2b1.png?width=169&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=924223e919c8df079e1f21f6f80c668298480289\"&gt;Am I showing my age? ;-D&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Why is WAP useful for data engineers?&lt;/h2&gt;\n\n&lt;p&gt;The data engineering world has always lagged behind its software engineering brethren.&lt;/p&gt;\n\n&lt;p&gt;Concepts like source control were well established in software engineering for a decade or more before data engineers realised that there might just be something in the idea of not emailing around files called &lt;code&gt;DIM_DATE_V1_FINAL_REVISED_v2_PROD.sql&lt;/code&gt;. (In fairness, it took a shift away from the old mindset of the established vendors too, in parallel with the emergence of the modern data stack for things to really click).&lt;/p&gt;\n\n&lt;p&gt;Write-Audit-Publish is very similar\u2014or perhaps the same, if you squint\u2014to the idea of Blue-Green deployments in the software engineering world. As data engineers we can learn a lot from established and proven patterns, and the Blue-Green one is a good example of this.&lt;/p&gt;\n\n&lt;p&gt;Why &lt;em&gt;wouldn\u2019t&lt;/em&gt; we want to adopt this, perhaps other than inertia and fear of something new? WAP is a perfect fit for both regular data pipelines as well as one-off data processing jobs.&lt;/p&gt;\n\n&lt;h2&gt;Wanna Read More?&lt;/h2&gt;\n\n&lt;p&gt;\ud83d\udc49\ud83c\udffbCheck out the full article that I wrote: &lt;a href=\"https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/?utm_campaign=Social%20media%20activity&amp;amp;utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_content=blog_rm-wap1\"&gt;Data Engineering Patterns: Write-Audit-Publish (WAP)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Full disclosure: I work for Treeverse, the company behind lakeFS.&lt;/em&gt; &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?auto=webp&amp;v=enabled&amp;s=e636ce7d35a6ffe63bd8d2bcefd07cf7d55c6392", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1257c3e138f6335e24a1a4cee0d488d27364532d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=078eeabf7b25c2fe02e02714669f3b6cda833417", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5282a32789acaac7bd0c2567442756e2aeb3bcc0", "width": 320, "height": 240}], "variants": {}, "id": "YzD2yhQk3wTSSmdUqerRI6LFu8Dk_iCDwBZvhHo8aek"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vmdj6", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vmdj6/what_is_writeauditpublish_and_why_is_it_useful/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vmdj6/what_is_writeauditpublish_and_why_is_it_useful/", "subreddit_subscribers": 108005, "created_utc": 1685441989.0, "num_crossposts": 1, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/fXHdeBnpXrg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"WHOOPS, THE NUMBERS ARE WRONG! SCALING DATA QUALITY @ NETFLIX\"&gt;&lt;/iframe&gt;", "author_name": "DataWorks Summit", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/fXHdeBnpXrg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@DataWorksSummit"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vle5v8ic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is DataOps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13v7of3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What is DataOps?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "author_name": "Polyseam", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/HNgpk9IUfK4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@polyseam"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13v7of3", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZV9tDN9N8Oy3pnxB22xIP11rQZvrVyX4a3uRkht-ZQU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685397040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/HNgpk9IUfK4", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?auto=webp&amp;v=enabled&amp;s=558ed727ae34d44e61e800d5c68b6d4e3b06c9b5", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de8987cee6ecdb05960e5deea7cec8c9c38efe0c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c5ce5990d7fd1f1cf4b4389b41bc453fc017fdb", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2be9fc510038f6f9ddb00aac3996a20d0ea37dc", "width": 320, "height": 240}], "variants": {}, "id": "YzEWw94GU1pBBr-CsbqyKbtLUvvhgd5ZFCQ5vQvUDYI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13v7of3", "is_robot_indexable": true, "report_reasons": null, "author": "Polyseam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v7of3/what_is_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/HNgpk9IUfK4", "subreddit_subscribers": 108005, "created_utc": 1685397040.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What is DataOps?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "author_name": "Polyseam", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/HNgpk9IUfK4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@polyseam"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI work as a shitty mid DE, with airflow, spark on prem as well as cloud. I sometimes need to connect to some servers using a different port or using a different protocol and I know nothing about these things.\n\nCould you recommend some book or other resource (I like books) that explains these things in layman terms? I know basically nothing and don't know where to start, it seems very overwhelming.\n\nThank you", "author_fullname": "t2_fadc9ofm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn basics of networking and how much do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vlg83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685438773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I work as a shitty mid DE, with airflow, spark on prem as well as cloud. I sometimes need to connect to some servers using a different port or using a different protocol and I know nothing about these things.&lt;/p&gt;\n\n&lt;p&gt;Could you recommend some book or other resource (I like books) that explains these things in layman terms? I know basically nothing and don&amp;#39;t know where to start, it seems very overwhelming.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vlg83", "is_robot_indexable": true, "report_reasons": null, "author": "signacaste", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vlg83/how_to_learn_basics_of_networking_and_how_much_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vlg83/how_to_learn_basics_of_networking_and_how_much_do/", "subreddit_subscribers": 108005, "created_utc": 1685438773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am given several tables from Google analytics, Google ads and SAP. The client wants me to feed them all to a generative AI engine that will generate \"useful insights\" from them automatically. It seems like  Tableau GPT/Pulse could help me with that but it's still not available for commercial use. Do you know of any other tools that might fit to the job?", "author_fullname": "t2_t6tuznl6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generative AI tools generating automated insights on data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vgqpb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685421863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am given several tables from Google analytics, Google ads and SAP. The client wants me to feed them all to a generative AI engine that will generate &amp;quot;useful insights&amp;quot; from them automatically. It seems like  Tableau GPT/Pulse could help me with that but it&amp;#39;s still not available for commercial use. Do you know of any other tools that might fit to the job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vgqpb", "is_robot_indexable": true, "report_reasons": null, "author": "Inevitable-Sea-658", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vgqpb/generative_ai_tools_generating_automated_insights/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vgqpb/generative_ai_tools_generating_automated_insights/", "subreddit_subscribers": 108005, "created_utc": 1685421863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering,\n\nsome time ago I shared about our OSS project [Dozer](https://github.com/getdozer/dozer) with the group.  We just did a presentation and demo at the Python Meetup group here in Singapore and just wanted to share the full video [here](https://www.youtube.com/watch?v=brAY4VO4uO4). \n\nMatteo", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a data app with Dozer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uw1kt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685369522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;some time ago I shared about our OSS project &lt;a href=\"https://github.com/getdozer/dozer\"&gt;Dozer&lt;/a&gt; with the group.  We just did a presentation and demo at the Python Meetup group here in Singapore and just wanted to share the full video &lt;a href=\"https://www.youtube.com/watch?v=brAY4VO4uO4\"&gt;here&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Matteo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?auto=webp&amp;v=enabled&amp;s=21d67c92c68884db35a7082df4ddc21216307a24", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdbc3d07f31a859ee2a8d25d15cae583fc18c5f4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7a2e0c784ada39f866c3d45fab42c88108638d9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f59ace580c62456dda56d89067b0dd9507902a5c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c76b28ed0cb8af8e5e3b01175825351c378809b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfb7b7d94301a88ffa5f55977ec84217eaa5854b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9b190e8f7ec9163ad9d669b9e6fed4162602a0e", "width": 1080, "height": 540}], "variants": {}, "id": "TNj7Ap_pyNxfOqQaVa0jsttZokHMgZvWhJSpFA4MTBM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13uw1kt", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uw1kt/building_a_data_app_with_dozer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uw1kt/building_a_data_app_with_dozer/", "subreddit_subscribers": 108005, "created_utc": 1685369522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to figure out the best way to do ETL through AWS.  \n\n\nHere's our current use case and setup.   \n\n\nWe pull from **sources** such as REST APIs and RDBMSs that exist both on-prem or in other VPCs.  \n\n\nThe **targets** will be RDBMs in on prem or other VPCs (snowflake in AWS). i.e. the *target will never be the same VPC that's running the glue job.*  \n\n\nThe **transformations** are fairly complex and are currently using either R or Python. These transformations are all done in memory. So there is no writing to a file, processing said file etc.  \n\n\nI thought about AWS Glue, but one thing that gave me pause is that it all seems based on Spark, which based on my VERY limited understanding is designed around file processing (csv, parquet, etc.) and as such seems to require a pitstop in S3? There's also appears to be a requirement of storing schema information within the service, which to me overcomplicates the process and makes the solution less portable. Again, I'm not enough of an expert to judge the service, these are just layman observations.  \n\n\nThere's also the possibility of using EC2 or EKS. We're familiar with developing for containers, but I wasn't sure how widely accepted this solution is.  \n\n\nThis is a long way of saying I don't know what the heck I'm doing so any feedback on best practices etc. would be greatly appreciated!", "author_fullname": "t2_ry4ze", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS tools for ETL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13vp8y3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685450541.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out the best way to do ETL through AWS.  &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s our current use case and setup.   &lt;/p&gt;\n\n&lt;p&gt;We pull from &lt;strong&gt;sources&lt;/strong&gt; such as REST APIs and RDBMSs that exist both on-prem or in other VPCs.  &lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;targets&lt;/strong&gt; will be RDBMs in on prem or other VPCs (snowflake in AWS). i.e. the &lt;em&gt;target will never be the same VPC that&amp;#39;s running the glue job.&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;The &lt;strong&gt;transformations&lt;/strong&gt; are fairly complex and are currently using either R or Python. These transformations are all done in memory. So there is no writing to a file, processing said file etc.  &lt;/p&gt;\n\n&lt;p&gt;I thought about AWS Glue, but one thing that gave me pause is that it all seems based on Spark, which based on my VERY limited understanding is designed around file processing (csv, parquet, etc.) and as such seems to require a pitstop in S3? There&amp;#39;s also appears to be a requirement of storing schema information within the service, which to me overcomplicates the process and makes the solution less portable. Again, I&amp;#39;m not enough of an expert to judge the service, these are just layman observations.  &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also the possibility of using EC2 or EKS. We&amp;#39;re familiar with developing for containers, but I wasn&amp;#39;t sure how widely accepted this solution is.  &lt;/p&gt;\n\n&lt;p&gt;This is a long way of saying I don&amp;#39;t know what the heck I&amp;#39;m doing so any feedback on best practices etc. would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vp8y3", "is_robot_indexable": true, "report_reasons": null, "author": "vincentx99", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vp8y3/aws_tools_for_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vp8y3/aws_tools_for_etl/", "subreddit_subscribers": 108005, "created_utc": 1685450541.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have this situation that I can't solve so I figured I might ask here.\n\nI have a large parquet file (10s of GB) that is accesible via URL. I need to process it and either store it in postgres or S3.\n\nThe thing is I can't find a way to stream it (load it in small chunks, processing it and therefore avoiding pulling it in memory all at once).\n\nI tried doing it with python with requests and pyarrow, by doing something like:\n\nresponse = requests.get(URL, stream=True)\n\nAnd then doing iter_lines(), iter_content() or iter_batches(), but pyarrow fails to parse chunks, usually throwing an error that says it's either a broken file or not a valid parquet format.\n\nI'm guessing it can be done with spark, so I wanted to ask you what would you suggest as a potential solution here.\n\nI have to choice between python or scala, I can use sagemaker potentially for resources or maybe even glue (I'm.not sure glue is applicable but maybe it is).\n\nAny idea would be much appreciated.\nThanks", "author_fullname": "t2_4bdifwrk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Process large parquet file accessible via URL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v42ey", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685388475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have this situation that I can&amp;#39;t solve so I figured I might ask here.&lt;/p&gt;\n\n&lt;p&gt;I have a large parquet file (10s of GB) that is accesible via URL. I need to process it and either store it in postgres or S3.&lt;/p&gt;\n\n&lt;p&gt;The thing is I can&amp;#39;t find a way to stream it (load it in small chunks, processing it and therefore avoiding pulling it in memory all at once).&lt;/p&gt;\n\n&lt;p&gt;I tried doing it with python with requests and pyarrow, by doing something like:&lt;/p&gt;\n\n&lt;p&gt;response = requests.get(URL, stream=True)&lt;/p&gt;\n\n&lt;p&gt;And then doing iter_lines(), iter_content() or iter_batches(), but pyarrow fails to parse chunks, usually throwing an error that says it&amp;#39;s either a broken file or not a valid parquet format.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m guessing it can be done with spark, so I wanted to ask you what would you suggest as a potential solution here.&lt;/p&gt;\n\n&lt;p&gt;I have to choice between python or scala, I can use sagemaker potentially for resources or maybe even glue (I&amp;#39;m.not sure glue is applicable but maybe it is).&lt;/p&gt;\n\n&lt;p&gt;Any idea would be much appreciated.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13v42ey", "is_robot_indexable": true, "report_reasons": null, "author": "skippy_nk", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v42ey/process_large_parquet_file_accessible_via_url/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v42ey/process_large_parquet_file_accessible_via_url/", "subreddit_subscribers": 108005, "created_utc": 1685388475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If I have large dataset to process but setup spark locally, would it be as same as running pandas on my pc locally?\n\nSo far I have done everything with pandas but with large data it does get slow. Hence I thought i should learn pyspark and add something useful to my portfolio. Other is airflow, but haven't had a use for it yet.\n\nGetting IT to setup a cluster on Azure/AWS is a long bureaucratic process (my colleague still waiting for months on his web app to be approved). \n\nI was told off for not getting it approved when I used databricks, so alternative is to run locally but don't know if it as effective as running it on databricks.", "author_fullname": "t2_9sli62zz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does spark run like pandas if run locally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13vpc7h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685450776.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I have large dataset to process but setup spark locally, would it be as same as running pandas on my pc locally?&lt;/p&gt;\n\n&lt;p&gt;So far I have done everything with pandas but with large data it does get slow. Hence I thought i should learn pyspark and add something useful to my portfolio. Other is airflow, but haven&amp;#39;t had a use for it yet.&lt;/p&gt;\n\n&lt;p&gt;Getting IT to setup a cluster on Azure/AWS is a long bureaucratic process (my colleague still waiting for months on his web app to be approved). &lt;/p&gt;\n\n&lt;p&gt;I was told off for not getting it approved when I used databricks, so alternative is to run locally but don&amp;#39;t know if it as effective as running it on databricks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vpc7h", "is_robot_indexable": true, "report_reasons": null, "author": "Chemical-Pollution59", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vpc7h/does_spark_run_like_pandas_if_run_locally/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vpc7h/does_spark_run_like_pandas_if_run_locally/", "subreddit_subscribers": 108005, "created_utc": 1685450776.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI have to transfer data from RDS to salesforce on daily basis. For that I am planning to use Lambda service to write a function that would get the data from RDS and transfer it to Salesforce via salesforce bulk api. Now my company cloud architect has suggested me to use Appflow in between to securely transfer data to salesforce. Now I am confuse what are the drawbacks from security perspective If we don\u2019t use Appflow.\n\nFeedback would be appreciated.\n\nThanks!", "author_fullname": "t2_udvutrbv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS RDS to Salesforce", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13vo840", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685447739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I have to transfer data from RDS to salesforce on daily basis. For that I am planning to use Lambda service to write a function that would get the data from RDS and transfer it to Salesforce via salesforce bulk api. Now my company cloud architect has suggested me to use Appflow in between to securely transfer data to salesforce. Now I am confuse what are the drawbacks from security perspective If we don\u2019t use Appflow.&lt;/p&gt;\n\n&lt;p&gt;Feedback would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vo840", "is_robot_indexable": true, "report_reasons": null, "author": "Usamacheema12345", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vo840/aws_rds_to_salesforce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vo840/aws_rds_to_salesforce/", "subreddit_subscribers": 108005, "created_utc": 1685447739.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nThis must be a pretty common problem but one I can\u2019t think of how to solve. Currently we are getting a flat file of sales that is rolling 36 months, except we need to capture and keep 48 months (ideally 60). How can I split this file to achieve is?\n\nSql server, Python at the disposable, anything to get the right solution. \n\nThe data ends up in tableau. \n\nThank you", "author_fullname": "t2_a7nec5bo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "36 months of rolling data, solution to storing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13vnwr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685446772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;This must be a pretty common problem but one I can\u2019t think of how to solve. Currently we are getting a flat file of sales that is rolling 36 months, except we need to capture and keep 48 months (ideally 60). How can I split this file to achieve is?&lt;/p&gt;\n\n&lt;p&gt;Sql server, Python at the disposable, anything to get the right solution. &lt;/p&gt;\n\n&lt;p&gt;The data ends up in tableau. &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vnwr6", "is_robot_indexable": true, "report_reasons": null, "author": "TheCumCopter", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vnwr6/36_months_of_rolling_data_solution_to_storing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vnwr6/36_months_of_rolling_data_solution_to_storing/", "subreddit_subscribers": 108005, "created_utc": 1685446772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nI'm a mathematician/process/statistical modeller working in agricultural/environmental science. Our company has invested in Snowflake for data storage and R for data analysis. However I am finding that the volumes of data are becoming a bit more than can be comfortably handled in R on a single PC (we're in Windows 10). I am looking for options for data visualisation, extraction, cleaning, statistical modelling that don't require downloading the data and/or having it in memory. I don't really understand the IT side of data science very well, but two options look like Spark(lyr) and Snowpark.\n\nAny suggestions or advice or experience you can share?\n\nThanks!", "author_fullname": "t2_221frg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools for modelling (e.g. lm, gam) high res time series data in Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v4ghy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685389422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a mathematician/process/statistical modeller working in agricultural/environmental science. Our company has invested in Snowflake for data storage and R for data analysis. However I am finding that the volumes of data are becoming a bit more than can be comfortably handled in R on a single PC (we&amp;#39;re in Windows 10). I am looking for options for data visualisation, extraction, cleaning, statistical modelling that don&amp;#39;t require downloading the data and/or having it in memory. I don&amp;#39;t really understand the IT side of data science very well, but two options look like Spark(lyr) and Snowpark.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or advice or experience you can share?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13v4ghy", "is_robot_indexable": true, "report_reasons": null, "author": "si_wo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v4ghy/best_tools_for_modelling_eg_lm_gam_high_res_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v4ghy/best_tools_for_modelling_eg_lm_gam_high_res_time/", "subreddit_subscribers": 108005, "created_utc": 1685389422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data mostly exist in the form of text (CSV) and Parquet files. I'm looking for an open source software to be able to manage all these files and their metadata. So when accessing a file, I want to know its content (e.g., what timespan it covers, what columns there are, ...) anx also find a file based on such criteria. Importing them into a database of some sort is not the answer, because we dont want to go through the trouble of managing the database (the files were initially exported from a DB).\n\nAre there any open source software for this?", "author_fullname": "t2_sl4ecm1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage files &amp; metadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uw7ez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685369940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data mostly exist in the form of text (CSV) and Parquet files. I&amp;#39;m looking for an open source software to be able to manage all these files and their metadata. So when accessing a file, I want to know its content (e.g., what timespan it covers, what columns there are, ...) anx also find a file based on such criteria. Importing them into a database of some sort is not the answer, because we dont want to go through the trouble of managing the database (the files were initially exported from a DB).&lt;/p&gt;\n\n&lt;p&gt;Are there any open source software for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13uw7ez", "is_robot_indexable": true, "report_reasons": null, "author": "UnlikelyNorth2048", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uw7ez/manage_files_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uw7ez/manage_files_metadata/", "subreddit_subscribers": 108005, "created_utc": 1685369940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI created [https://nodb.sh](https://nodb.sh) to help store and fetch JSON data through HTTPS. No dependency is needed, like \"npm install some-database\", or such. It's for serverless applications like cloud functions where you don't want too many dependencies.\n\nI'd like to see someone using it on a side project and give positive or negative feedback please.\n\nThe API is protected by access token located in the query params, but I'll make it so that it can also be in the headers. The idea was that you can share the link to your data and use it in some other project, same data.\n\nEndpoints have the following construct: \\`/{appName}/{envName}/your-model/:id/your-sub-model/:id/...?token={accessToken}\\`. This way you can split your data between environments like \"dev\" or \"prod\". Only apps and environments have to be created in the dashboard behind bearer token, but your JSON models are protected by access token, so you can call HTTPS requests easily from your code.\n\nI will very much appreciate it if you have any questions and decide to use it somewhere. :)", "author_fullname": "t2_32bje2j7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made nodb, a JSON API to remove the burden of installing databases in my projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vln9e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685439456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I created &lt;a href=\"https://nodb.sh\"&gt;https://nodb.sh&lt;/a&gt; to help store and fetch JSON data through HTTPS. No dependency is needed, like &amp;quot;npm install some-database&amp;quot;, or such. It&amp;#39;s for serverless applications like cloud functions where you don&amp;#39;t want too many dependencies.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to see someone using it on a side project and give positive or negative feedback please.&lt;/p&gt;\n\n&lt;p&gt;The API is protected by access token located in the query params, but I&amp;#39;ll make it so that it can also be in the headers. The idea was that you can share the link to your data and use it in some other project, same data.&lt;/p&gt;\n\n&lt;p&gt;Endpoints have the following construct: `/{appName}/{envName}/your-model/:id/your-sub-model/:id/...?token={accessToken}`. This way you can split your data between environments like &amp;quot;dev&amp;quot; or &amp;quot;prod&amp;quot;. Only apps and environments have to be created in the dashboard behind bearer token, but your JSON models are protected by access token, so you can call HTTPS requests easily from your code.&lt;/p&gt;\n\n&lt;p&gt;I will very much appreciate it if you have any questions and decide to use it somewhere. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?auto=webp&amp;v=enabled&amp;s=254888a2b0c653e6155a8f6eadaf2e4f708c472b", "width": 1200, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae4dd7831acb59ed46f6f91075803ffe3136d59c", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e31063f8e5251a16392a25eeae1870255ad8cd2", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a743ad32846c3580465511afd7b959e51a7c2a7a", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c0c35cc5a170fba29705628ecf845dff8a5a282", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b10a23710cbbc3808bd180059eb4d2ed5b586e0e", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8d0e8556fc82096284df11dced30d8a5d34803d", "width": 1080, "height": 648}], "variants": {}, "id": "MD4fxjHEg1HUTmQl1rr8SSVLbVLLFhnYHQr8eiiH0es"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vln9e", "is_robot_indexable": true, "report_reasons": null, "author": "mk0y8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vln9e/i_made_nodb_a_json_api_to_remove_the_burden_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vln9e/i_made_nodb_a_json_api_to_remove_the_burden_of/", "subreddit_subscribers": 108005, "created_utc": 1685439456.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}