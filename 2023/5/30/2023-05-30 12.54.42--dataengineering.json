{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is you boss someone who can help you with technical stuff or do they just manage you and you team from above? I'm just asking because my boss is completely non technical and sometimes its not an issue, but other times it would be nice to have someone who understands what his team is doing. It prevents hours wasted trying to explain something rather than a quick call or maybe an email. More importantly, it helps when you're trying to explain \"why\" something can't be done or why something might take longer than expected when you're dealing with someone who understands certain technical aspects of your job.", "author_fullname": "t2_aobif6bsf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is your boss technical in that he/she can help you with difficult problems or non technical?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v6dku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 72, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 72, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685393936.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is you boss someone who can help you with technical stuff or do they just manage you and you team from above? I&amp;#39;m just asking because my boss is completely non technical and sometimes its not an issue, but other times it would be nice to have someone who understands what his team is doing. It prevents hours wasted trying to explain something rather than a quick call or maybe an email. More importantly, it helps when you&amp;#39;re trying to explain &amp;quot;why&amp;quot; something can&amp;#39;t be done or why something might take longer than expected when you&amp;#39;re dealing with someone who understands certain technical aspects of your job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13v6dku", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Flow5900", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v6dku/is_your_boss_technical_in_that_heshe_can_help_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v6dku/is_your_boss_technical_in_that_heshe_can_help_you/", "subreddit_subscribers": 108004, "created_utc": 1685393936.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "MS in Data Science or CS doesn\u2019t matter, my department president is saying that those degrees can help get me more prepared for the job market, but I\u2019m not sure if this is simply because it\u2019s not easy to find a job right now. I\u2019d be a masters student with YOE as a data analyst trying to be a DE. \n\nNot sure if there\u2019s a big overall benefit to having a masters for Data Engineering\n\n(I am getting a BS in Computer Science with a Computational Data Science specialization, as requested)\n\nEdit: I feel it\u2019s necessary to add that looking at a lot of data engineering jobs on sites like Indeed and LinkedIn directly state \u201cMasters degree preferred\u201d or reference that a masters is acceptable. It seems like many responses here aren\u2019t aware of that.", "author_fullname": "t2_55fytx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Masters Degrees beneficial in this field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uu7qh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685380809.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685365111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MS in Data Science or CS doesn\u2019t matter, my department president is saying that those degrees can help get me more prepared for the job market, but I\u2019m not sure if this is simply because it\u2019s not easy to find a job right now. I\u2019d be a masters student with YOE as a data analyst trying to be a DE. &lt;/p&gt;\n\n&lt;p&gt;Not sure if there\u2019s a big overall benefit to having a masters for Data Engineering&lt;/p&gt;\n\n&lt;p&gt;(I am getting a BS in Computer Science with a Computational Data Science specialization, as requested)&lt;/p&gt;\n\n&lt;p&gt;Edit: I feel it\u2019s necessary to add that looking at a lot of data engineering jobs on sites like Indeed and LinkedIn directly state \u201cMasters degree preferred\u201d or reference that a masters is acceptable. It seems like many responses here aren\u2019t aware of that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13uu7qh", "is_robot_indexable": true, "report_reasons": null, "author": "ToothPickLegs", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13uu7qh/are_masters_degrees_beneficial_in_this_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uu7qh/are_masters_degrees_beneficial_in_this_field/", "subreddit_subscribers": 108004, "created_utc": 1685365111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello colleagues ;)\n\nI have 800\u20ac available for my personal development budget and I wanna get ideas to use them\u2026\nWhat would you do if you have such amount of money?", "author_fullname": "t2_1b9hdy2v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you share some Ideas to expend 800\u20ac personal development budget?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13utouy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685363759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello colleagues ;)&lt;/p&gt;\n\n&lt;p&gt;I have 800\u20ac available for my personal development budget and I wanna get ideas to use them\u2026\nWhat would you do if you have such amount of money?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13utouy", "is_robot_indexable": true, "report_reasons": null, "author": "DrGiacometto", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13utouy/would_you_share_some_ideas_to_expend_800_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13utouy/would_you_share_some_ideas_to_expend_800_personal/", "subreddit_subscribers": 108004, "created_utc": 1685363759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand why it takes forever to copy 500,000 small files from one partition to another. However if I put those 500,000 files into a zip and copy that over, then unzip it, it only takes about 10 seconds.\n\nWhat is a zip/unzip process is able to do with the filesystem that makes it quick that a copy process is not?", "author_fullname": "t2_5aiux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A question about filesystems and quantity of files in relation to speed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vdmhk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685412925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand why it takes forever to copy 500,000 small files from one partition to another. However if I put those 500,000 files into a zip and copy that over, then unzip it, it only takes about 10 seconds.&lt;/p&gt;\n\n&lt;p&gt;What is a zip/unzip process is able to do with the filesystem that makes it quick that a copy process is not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vdmhk", "is_robot_indexable": true, "report_reasons": null, "author": "Eisenstein", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vdmhk/a_question_about_filesystems_and_quantity_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vdmhk/a_question_about_filesystems_and_quantity_of/", "subreddit_subscribers": 108004, "created_utc": 1685412925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jx0q8m4g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Email addresses are not primary user identities", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vj2ub", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1685430143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ntietz.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ntietz.com/blog/email-address-not-identifier/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vj2ub", "is_robot_indexable": true, "report_reasons": null, "author": "ageam54", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vj2ub/email_addresses_are_not_primary_user_identities/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ntietz.com/blog/email-address-not-identifier/", "subreddit_subscribers": 108004, "created_utc": 1685430143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have several tables stored in Delta format, having any number of columns from 5-500. I wish to scan each table, going through each column and automatically flagging Email/Phone Number fields.\n\nI searched for open source frameworks and came across Great Expectations, Piperider and such but these seem more for data quality and putting thresholds/conditions on particular fields. I want to scan through the entire column list for any such PII field.\n\nA last-resort option is to have an ML model trained to identify email addresses and phone numbers run through the top 100 or so non-null records and try to check for these conditions over each of the hundred columns on a best-effort basis.\n\nP.S. The dataframes can have millions of records, so scanning through the entire df seems pointless.\n\nWould appreciate any help/pointer in the right direction. Thanks!", "author_fullname": "t2_xx7bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Detecting PII columns in a Dataframe/Delta Format", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vm8zs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685441564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several tables stored in Delta format, having any number of columns from 5-500. I wish to scan each table, going through each column and automatically flagging Email/Phone Number fields.&lt;/p&gt;\n\n&lt;p&gt;I searched for open source frameworks and came across Great Expectations, Piperider and such but these seem more for data quality and putting thresholds/conditions on particular fields. I want to scan through the entire column list for any such PII field.&lt;/p&gt;\n\n&lt;p&gt;A last-resort option is to have an ML model trained to identify email addresses and phone numbers run through the top 100 or so non-null records and try to check for these conditions over each of the hundred columns on a best-effort basis.&lt;/p&gt;\n\n&lt;p&gt;P.S. The dataframes can have millions of records, so scanning through the entire df seems pointless.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any help/pointer in the right direction. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vm8zs", "is_robot_indexable": true, "report_reasons": null, "author": "sArThAk882", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vm8zs/detecting_pii_columns_in_a_dataframedelta_format/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vm8zs/detecting_pii_columns_in_a_dataframedelta_format/", "subreddit_subscribers": 108004, "created_utc": 1685441564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone.\nWe are currently ingesting structured data daily, with incremental and full loads, on AWS Glue Spark jobs.\nWe wanted to increase frequency but there is a lot of cost involved.\nWe were advised to attempt a migration of workloads to AWS fargate to make it more cost effective.\nIs there anyone who has had experience with either one of them or such a migration that can speak to the cost and performance. Any guidance would be profoundly appreciated.", "author_fullname": "t2_8ixxnnf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Fargate (EKS) vs AWS Glue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vhyyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685426113.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone.\nWe are currently ingesting structured data daily, with incremental and full loads, on AWS Glue Spark jobs.\nWe wanted to increase frequency but there is a lot of cost involved.\nWe were advised to attempt a migration of workloads to AWS fargate to make it more cost effective.\nIs there anyone who has had experience with either one of them or such a migration that can speak to the cost and performance. Any guidance would be profoundly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vhyyd", "is_robot_indexable": true, "report_reasons": null, "author": "mozakaak", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vhyyd/aws_fargate_eks_vs_aws_glue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vhyyd/aws_fargate_eks_vs_aws_glue/", "subreddit_subscribers": 108004, "created_utc": 1685426113.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any resource you recommend to learn how to optimize spark applications?", "author_fullname": "t2_1o26bmw3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Parameter Optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v56mt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685391137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any resource you recommend to learn how to optimize spark applications?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13v56mt", "is_robot_indexable": true, "report_reasons": null, "author": "da_chosen1", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v56mt/spark_parameter_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v56mt/spark_parameter_optimization/", "subreddit_subscribers": 108004, "created_utc": 1685391137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello good people! \n\nI am on a journey of transition from DS to DE. Basically I am advanced now in SQL, Python, Spark for batch, Docker, Airflow. I know Kubernetes architecture on a high level, am decent with my understanding of networking, Linux, a bit of cloud. \n\nI was thinking next I should focus on streaming? Would that be a good next stop? \n\nAnd if so, should I go for Spark streaming since I know PySpark very well already? Or should I just go for Kafka? And do you know any good resources to learn besides the official documentation? \n\nThanks in advance!", "author_fullname": "t2_uv1rtgfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Diving into Streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uz8fa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685376998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello good people! &lt;/p&gt;\n\n&lt;p&gt;I am on a journey of transition from DS to DE. Basically I am advanced now in SQL, Python, Spark for batch, Docker, Airflow. I know Kubernetes architecture on a high level, am decent with my understanding of networking, Linux, a bit of cloud. &lt;/p&gt;\n\n&lt;p&gt;I was thinking next I should focus on streaming? Would that be a good next stop? &lt;/p&gt;\n\n&lt;p&gt;And if so, should I go for Spark streaming since I know PySpark very well already? Or should I just go for Kafka? And do you know any good resources to learn besides the official documentation? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13uz8fa", "is_robot_indexable": true, "report_reasons": null, "author": "4eyes1soul", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uz8fa/diving_into_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uz8fa/diving_into_streaming/", "subreddit_subscribers": 108004, "created_utc": 1685376998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can someone explain what these two are in a simple language without the consultant-speak bullshit? Anything I\u2019ve seen online just sounds like fluffy gibberish.", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master data and reference data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uyixi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685375393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone explain what these two are in a simple language without the consultant-speak bullshit? Anything I\u2019ve seen online just sounds like fluffy gibberish.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13uyixi", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uyixi/master_data_and_reference_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uyixi/master_data_and_reference_data/", "subreddit_subscribers": 108004, "created_utc": 1685375393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vle5v8ic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is DataOps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13v7of3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What is DataOps?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "author_name": "Polyseam", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/HNgpk9IUfK4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@polyseam"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13v7of3", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZV9tDN9N8Oy3pnxB22xIP11rQZvrVyX4a3uRkht-ZQU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685397040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/HNgpk9IUfK4", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?auto=webp&amp;v=enabled&amp;s=558ed727ae34d44e61e800d5c68b6d4e3b06c9b5", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de8987cee6ecdb05960e5deea7cec8c9c38efe0c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c5ce5990d7fd1f1cf4b4389b41bc453fc017fdb", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2be9fc510038f6f9ddb00aac3996a20d0ea37dc", "width": 320, "height": 240}], "variants": {}, "id": "YzEWw94GU1pBBr-CsbqyKbtLUvvhgd5ZFCQ5vQvUDYI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13v7of3", "is_robot_indexable": true, "report_reasons": null, "author": "Polyseam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v7of3/what_is_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/HNgpk9IUfK4", "subreddit_subscribers": 108004, "created_utc": 1685397040.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What is DataOps?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "author_name": "Polyseam", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/HNgpk9IUfK4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@polyseam"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is inverted index, and how we made log analysis 10 times more cost-effective with it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": true, "name": "t3_13vo4mw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6sHewnuNxqUUcp1rGXcVZYcr_aY93d91Ay6MadPRVOI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685447453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/6afc6cc81d20", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?auto=webp&amp;v=enabled&amp;s=85d15c6f2e44360b37ad945b2d7a176af391a63b", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60a31eb2219d52e1e6ce76d5470baa480b91ee96", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea69238f22243a5dbf0acc3150375db7d73bd5d8", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a729d4c6f9b056f40e8a38400c8c4300f58c99b4", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26ba87daf0b4018fa4c7dcc0947739253a6986fb", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfdf25e3d76da7c9d452b90ca7cfa55bc45e1c7e", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/bfqXMdiX1WO2CT-BfkgHkhuhl-oPEDcpnwGQyapvvms.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=200617b9d58e70e4cfb8da5af77fb36cb5770236", "width": 1080, "height": 720}], "variants": {}, "id": "RaCe7N3GeU-J2bdRI-tTmDBSLGqF5H1FeMi9MIqYHIQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vo4mw", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vo4mw/what_is_inverted_index_and_how_we_made_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/6afc6cc81d20", "subreddit_subscribers": 108004, "created_utc": 1685447453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Write-Audit-Publish (WAP) is a pattern in data engineering that gives teams greater control over data quality.** It was popularized by Netflix back in 2017 in [a talk](https://www.youtube.com/watch?v=fXHdeBnpXrg) by Michelle Winters at the DataWorks Summit called \u201c*Whoops the Numbers are wrong! Scaling Data Quality @ Netflix*.\u201d\n\nThe name is fairly self-descriptive:\n\nhttps://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc\n\n## What is WAP not (in this context)? \n\n&amp;#x200B;\n\n[Am I showing my age? ;-D](https://preview.redd.it/cmxhd90xjz2b1.png?width=169&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=924223e919c8df079e1f21f6f80c668298480289)\n\n## Why is WAP useful for data engineers?\n\nThe data engineering world has always lagged behind its software engineering brethren.\n\nConcepts like source control were well established in software engineering for a decade or more before data engineers realised that there might just be something in the idea of not emailing around files called `DIM_DATE_V1_FINAL_REVISED_v2_PROD.sql`. (In fairness, it took a shift away from the old mindset of the established vendors too, in parallel with the emergence of the modern data stack for things to really click).\n\nWrite-Audit-Publish is very similar\u2014or perhaps the same, if you squint\u2014to the idea of Blue-Green deployments in the software engineering world. As data engineers we can learn a lot from established and proven patterns, and the Blue-Green one is a good example of this.\n\nWhy *wouldn\u2019t* we want to adopt this, perhaps other than inertia and fear of something new? WAP is a perfect fit for both regular data pipelines as well as one-off data processing jobs.\n\n## Wanna Read More? \n\n\ud83d\udc49\ud83c\udffbCheck out the full article that I wrote: [Data Engineering Patterns: Write-Audit-Publish (WAP)](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/?utm_campaign=Social%20media%20activity&amp;utm_source=reddit&amp;utm_medium=social&amp;utm_content=blog_rm-wap1)", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Write-Audit-Publish - and why is it useful for data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"2w9ow4kehz2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 131, "x": 108, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91bad6163ad6ec681770fb026532a20892676e6d"}, {"y": 262, "x": 216, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=955e951592ac3466153560e1490921e6eea30d00"}, {"y": 388, "x": 320, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53f594be0151b869d1ef65bdec6020ea1fa179fa"}, {"y": 776, "x": 640, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43c5675342b4528484b0272706ce61711ada0bf8"}], "s": {"y": 971, "x": 800, "u": "https://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc"}, "id": "2w9ow4kehz2b1"}, "cmxhd90xjz2b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 138, "x": 108, "u": "https://preview.redd.it/cmxhd90xjz2b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c747c004507167e1cc6cc87e5f0f8ced5eedd6c"}], "s": {"y": 217, "x": 169, "u": "https://preview.redd.it/cmxhd90xjz2b1.png?width=169&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=924223e919c8df079e1f21f6f80c668298480289"}, "id": "cmxhd90xjz2b1"}}, "name": "t3_13vmdj6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mUJ84NP8jHsCkve8pO46C5HvfZ-6A6xSShkaiqVgLVs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685441989.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Write-Audit-Publish (WAP) is a pattern in data engineering that gives teams greater control over data quality.&lt;/strong&gt; It was popularized by Netflix back in 2017 in &lt;a href=\"https://www.youtube.com/watch?v=fXHdeBnpXrg\"&gt;a talk&lt;/a&gt; by Michelle Winters at the DataWorks Summit called \u201c&lt;em&gt;Whoops the Numbers are wrong! Scaling Data Quality @ Netflix&lt;/em&gt;.\u201d&lt;/p&gt;\n\n&lt;p&gt;The name is fairly self-descriptive:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc\"&gt;https://preview.redd.it/2w9ow4kehz2b1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e6100eb44951dd37ae916c33ac7b8b461d45b2fc&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;What is WAP not (in this context)?&lt;/h2&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cmxhd90xjz2b1.png?width=169&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=924223e919c8df079e1f21f6f80c668298480289\"&gt;Am I showing my age? ;-D&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Why is WAP useful for data engineers?&lt;/h2&gt;\n\n&lt;p&gt;The data engineering world has always lagged behind its software engineering brethren.&lt;/p&gt;\n\n&lt;p&gt;Concepts like source control were well established in software engineering for a decade or more before data engineers realised that there might just be something in the idea of not emailing around files called &lt;code&gt;DIM_DATE_V1_FINAL_REVISED_v2_PROD.sql&lt;/code&gt;. (In fairness, it took a shift away from the old mindset of the established vendors too, in parallel with the emergence of the modern data stack for things to really click).&lt;/p&gt;\n\n&lt;p&gt;Write-Audit-Publish is very similar\u2014or perhaps the same, if you squint\u2014to the idea of Blue-Green deployments in the software engineering world. As data engineers we can learn a lot from established and proven patterns, and the Blue-Green one is a good example of this.&lt;/p&gt;\n\n&lt;p&gt;Why &lt;em&gt;wouldn\u2019t&lt;/em&gt; we want to adopt this, perhaps other than inertia and fear of something new? WAP is a perfect fit for both regular data pipelines as well as one-off data processing jobs.&lt;/p&gt;\n\n&lt;h2&gt;Wanna Read More?&lt;/h2&gt;\n\n&lt;p&gt;\ud83d\udc49\ud83c\udffbCheck out the full article that I wrote: &lt;a href=\"https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/?utm_campaign=Social%20media%20activity&amp;amp;utm_source=reddit&amp;amp;utm_medium=social&amp;amp;utm_content=blog_rm-wap1\"&gt;Data Engineering Patterns: Write-Audit-Publish (WAP)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vmdj6", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vmdj6/what_is_writeauditpublish_and_why_is_it_useful/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vmdj6/what_is_writeauditpublish_and_why_is_it_useful/", "subreddit_subscribers": 108004, "created_utc": 1685441989.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering,\n\nsome time ago I shared about our OSS project [Dozer](https://github.com/getdozer/dozer) with the group.  We just did a presentation and demo at the Python Meetup group here in Singapore and just wanted to share the full video [here](https://www.youtube.com/watch?v=brAY4VO4uO4). \n\nMatteo", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a data app with Dozer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uw1kt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685369522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;some time ago I shared about our OSS project &lt;a href=\"https://github.com/getdozer/dozer\"&gt;Dozer&lt;/a&gt; with the group.  We just did a presentation and demo at the Python Meetup group here in Singapore and just wanted to share the full video &lt;a href=\"https://www.youtube.com/watch?v=brAY4VO4uO4\"&gt;here&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Matteo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?auto=webp&amp;v=enabled&amp;s=21d67c92c68884db35a7082df4ddc21216307a24", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdbc3d07f31a859ee2a8d25d15cae583fc18c5f4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7a2e0c784ada39f866c3d45fab42c88108638d9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f59ace580c62456dda56d89067b0dd9507902a5c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c76b28ed0cb8af8e5e3b01175825351c378809b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfb7b7d94301a88ffa5f55977ec84217eaa5854b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9b190e8f7ec9163ad9d669b9e6fed4162602a0e", "width": 1080, "height": 540}], "variants": {}, "id": "TNj7Ap_pyNxfOqQaVa0jsttZokHMgZvWhJSpFA4MTBM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13uw1kt", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uw1kt/building_a_data_app_with_dozer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uw1kt/building_a_data_app_with_dozer/", "subreddit_subscribers": 108004, "created_utc": 1685369522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI work as a shitty mid DE, with airflow, spark on prem as well as cloud. I sometimes need to connect to some servers using a different port or using a different protocol and I know nothing about these things.\n\nCould you recommend some book or other resource (I like books) that explains these things in layman terms? I know basically nothing and don't know where to start, it seems very overwhelming.\n\nThank you", "author_fullname": "t2_fadc9ofm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to learn basics of networking and how much do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vlg83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685438773.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I work as a shitty mid DE, with airflow, spark on prem as well as cloud. I sometimes need to connect to some servers using a different port or using a different protocol and I know nothing about these things.&lt;/p&gt;\n\n&lt;p&gt;Could you recommend some book or other resource (I like books) that explains these things in layman terms? I know basically nothing and don&amp;#39;t know where to start, it seems very overwhelming.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vlg83", "is_robot_indexable": true, "report_reasons": null, "author": "signacaste", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vlg83/how_to_learn_basics_of_networking_and_how_much_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vlg83/how_to_learn_basics_of_networking_and_how_much_do/", "subreddit_subscribers": 108004, "created_utc": 1685438773.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am given several tables from Google analytics, Google ads and SAP. The client wants me to feed them all to a generative AI engine that will generate \"useful insights\" from them automatically. It seems like  Tableau GPT/Pulse could help me with that but it's still not available for commercial use. Do you know of any other tools that might fit to the job?", "author_fullname": "t2_t6tuznl6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generative AI tools generating automated insights on data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vgqpb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685421863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am given several tables from Google analytics, Google ads and SAP. The client wants me to feed them all to a generative AI engine that will generate &amp;quot;useful insights&amp;quot; from them automatically. It seems like  Tableau GPT/Pulse could help me with that but it&amp;#39;s still not available for commercial use. Do you know of any other tools that might fit to the job?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vgqpb", "is_robot_indexable": true, "report_reasons": null, "author": "Inevitable-Sea-658", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vgqpb/generative_ai_tools_generating_automated_insights/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vgqpb/generative_ai_tools_generating_automated_insights/", "subreddit_subscribers": 108004, "created_utc": 1685421863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have this situation that I can't solve so I figured I might ask here.\n\nI have a large parquet file (10s of GB) that is accesible via URL. I need to process it and either store it in postgres or S3.\n\nThe thing is I can't find a way to stream it (load it in small chunks, processing it and therefore avoiding pulling it in memory all at once).\n\nI tried doing it with python with requests and pyarrow, by doing something like:\n\nresponse = requests.get(URL, stream=True)\n\nAnd then doing iter_lines(), iter_content() or iter_batches(), but pyarrow fails to parse chunks, usually throwing an error that says it's either a broken file or not a valid parquet format.\n\nI'm guessing it can be done with spark, so I wanted to ask you what would you suggest as a potential solution here.\n\nI have to choice between python or scala, I can use sagemaker potentially for resources or maybe even glue (I'm.not sure glue is applicable but maybe it is).\n\nAny idea would be much appreciated.\nThanks", "author_fullname": "t2_4bdifwrk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Process large parquet file accessible via URL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v42ey", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685388475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have this situation that I can&amp;#39;t solve so I figured I might ask here.&lt;/p&gt;\n\n&lt;p&gt;I have a large parquet file (10s of GB) that is accesible via URL. I need to process it and either store it in postgres or S3.&lt;/p&gt;\n\n&lt;p&gt;The thing is I can&amp;#39;t find a way to stream it (load it in small chunks, processing it and therefore avoiding pulling it in memory all at once).&lt;/p&gt;\n\n&lt;p&gt;I tried doing it with python with requests and pyarrow, by doing something like:&lt;/p&gt;\n\n&lt;p&gt;response = requests.get(URL, stream=True)&lt;/p&gt;\n\n&lt;p&gt;And then doing iter_lines(), iter_content() or iter_batches(), but pyarrow fails to parse chunks, usually throwing an error that says it&amp;#39;s either a broken file or not a valid parquet format.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m guessing it can be done with spark, so I wanted to ask you what would you suggest as a potential solution here.&lt;/p&gt;\n\n&lt;p&gt;I have to choice between python or scala, I can use sagemaker potentially for resources or maybe even glue (I&amp;#39;m.not sure glue is applicable but maybe it is).&lt;/p&gt;\n\n&lt;p&gt;Any idea would be much appreciated.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13v42ey", "is_robot_indexable": true, "report_reasons": null, "author": "skippy_nk", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v42ey/process_large_parquet_file_accessible_via_url/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v42ey/process_large_parquet_file_accessible_via_url/", "subreddit_subscribers": 108004, "created_utc": 1685388475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learn about Vector Database | Concepts &amp; examples", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_13ut0ly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/nd4oe5eMRrSvnNkpHW3ekKjca0R5HlgXFTbMWTtXH24.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685361941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://substack.com/inbox/post/124553209", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?auto=webp&amp;v=enabled&amp;s=1d7d08abffc62cc98f3e8018f515d8dea23e3b6b", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05e51dda9acfe695b7ccf72cb4f5f142d1fdf048", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb9ee222ba18e966d454ebabccc3ba09c514a54c", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21976c5c4c11165ec7cc1809f52b0b47299ca932", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c725dc8ecbd936297c4e2524d25b73dc46ac37cb", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6bf58cd784a891c018a053682f7dbef57ada392", "width": 960, "height": 576}], "variants": {}, "id": "uJRu0sqkpaL2K3XaVf87sDx1OTexdsKjkTC7aTw5Ogs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13ut0ly", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ut0ly/learn_about_vector_database_concepts_examples/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://substack.com/inbox/post/124553209", "subreddit_subscribers": 108004, "created_utc": 1685361941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nThis must be a pretty common problem but one I can\u2019t think of how to solve. Currently we are getting a flat file of sales that is rolling 36 months, except we need to capture and keep 48 months (ideally 60). How can I split this file to achieve is?\n\nSql server, Python at the disposable, anything to get the right solution. \n\nThe data ends up in tableau. \n\nThank you", "author_fullname": "t2_a7nec5bo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "36 months of rolling data, solution to storing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13vnwr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685446772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;This must be a pretty common problem but one I can\u2019t think of how to solve. Currently we are getting a flat file of sales that is rolling 36 months, except we need to capture and keep 48 months (ideally 60). How can I split this file to achieve is?&lt;/p&gt;\n\n&lt;p&gt;Sql server, Python at the disposable, anything to get the right solution. &lt;/p&gt;\n\n&lt;p&gt;The data ends up in tableau. &lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13vnwr6", "is_robot_indexable": true, "report_reasons": null, "author": "TheCumCopter", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vnwr6/36_months_of_rolling_data_solution_to_storing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vnwr6/36_months_of_rolling_data_solution_to_storing/", "subreddit_subscribers": 108004, "created_utc": 1685446772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nI'm a mathematician/process/statistical modeller working in agricultural/environmental science. Our company has invested in Snowflake for data storage and R for data analysis. However I am finding that the volumes of data are becoming a bit more than can be comfortably handled in R on a single PC (we're in Windows 10). I am looking for options for data visualisation, extraction, cleaning, statistical modelling that don't require downloading the data and/or having it in memory. I don't really understand the IT side of data science very well, but two options look like Spark(lyr) and Snowpark.\n\nAny suggestions or advice or experience you can share?\n\nThanks!", "author_fullname": "t2_221frg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools for modelling (e.g. lm, gam) high res time series data in Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v4ghy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685389422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a mathematician/process/statistical modeller working in agricultural/environmental science. Our company has invested in Snowflake for data storage and R for data analysis. However I am finding that the volumes of data are becoming a bit more than can be comfortably handled in R on a single PC (we&amp;#39;re in Windows 10). I am looking for options for data visualisation, extraction, cleaning, statistical modelling that don&amp;#39;t require downloading the data and/or having it in memory. I don&amp;#39;t really understand the IT side of data science very well, but two options look like Spark(lyr) and Snowpark.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or advice or experience you can share?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13v4ghy", "is_robot_indexable": true, "report_reasons": null, "author": "si_wo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v4ghy/best_tools_for_modelling_eg_lm_gam_high_res_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v4ghy/best_tools_for_modelling_eg_lm_gam_high_res_time/", "subreddit_subscribers": 108004, "created_utc": 1685389422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data mostly exist in the form of text (CSV) and Parquet files. I'm looking for an open source software to be able to manage all these files and their metadata. So when accessing a file, I want to know its content (e.g., what timespan it covers, what columns there are, ...) anx also find a file based on such criteria. Importing them into a database of some sort is not the answer, because we dont want to go through the trouble of managing the database (the files were initially exported from a DB).\n\nAre there any open source software for this?", "author_fullname": "t2_sl4ecm1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage files &amp; metadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uw7ez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685369940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data mostly exist in the form of text (CSV) and Parquet files. I&amp;#39;m looking for an open source software to be able to manage all these files and their metadata. So when accessing a file, I want to know its content (e.g., what timespan it covers, what columns there are, ...) anx also find a file based on such criteria. Importing them into a database of some sort is not the answer, because we dont want to go through the trouble of managing the database (the files were initially exported from a DB).&lt;/p&gt;\n\n&lt;p&gt;Are there any open source software for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13uw7ez", "is_robot_indexable": true, "report_reasons": null, "author": "UnlikelyNorth2048", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uw7ez/manage_files_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uw7ez/manage_files_metadata/", "subreddit_subscribers": 108004, "created_utc": 1685369940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nCurrently, we have 90 Google Cloud servers that regularly transfer raw data to a Google BigQuery \"bucket\" every few minutes.\n\nThe problem arises when attempting to work with this raw data, as several parameters are stored as numerical values. For instance, the user account status may be represented as follows:\n\n* *02 = Active*\n* *05 = Suspended*\n* *16 = Verification needed,.. and so on.*\n\nTo address this, we aim to map this data to corresponding words or text once and for all. However, our engineers have informed us that modifying or editing data within BigQuery is challenging because it operates as a one-way synchronization, and any alterations would significantly increase the cost of the data warehouse.\n\nTo overcome this issue, our plan is to implement a Metabase layer on top of the Google BigQuery data warehouse. Therefore, my question is: Is it feasible to perform the data mapping within Metabase so that analytics can be conducted using more user-friendly values?", "author_fullname": "t2_jger5m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Metabase be used to edit/modify data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ut4rb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685362261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently, we have 90 Google Cloud servers that regularly transfer raw data to a Google BigQuery &amp;quot;bucket&amp;quot; every few minutes.&lt;/p&gt;\n\n&lt;p&gt;The problem arises when attempting to work with this raw data, as several parameters are stored as numerical values. For instance, the user account status may be represented as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;02 = Active&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;05 = Suspended&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;16 = Verification needed,.. and so on.&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To address this, we aim to map this data to corresponding words or text once and for all. However, our engineers have informed us that modifying or editing data within BigQuery is challenging because it operates as a one-way synchronization, and any alterations would significantly increase the cost of the data warehouse.&lt;/p&gt;\n\n&lt;p&gt;To overcome this issue, our plan is to implement a Metabase layer on top of the Google BigQuery data warehouse. Therefore, my question is: Is it feasible to perform the data mapping within Metabase so that analytics can be conducted using more user-friendly values?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ut4rb", "is_robot_indexable": true, "report_reasons": null, "author": "MysteriousBrilliant", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ut4rb/can_metabase_be_used_to_editmodify_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ut4rb/can_metabase_be_used_to_editmodify_data/", "subreddit_subscribers": 108004, "created_utc": 1685362261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI created [https://nodb.sh](https://nodb.sh) to help store and fetch JSON data through HTTPS. No dependency is needed, like \"npm install some-database\", or such. It's for serverless applications like cloud functions where you don't want too many dependencies.\n\nI'd like to see someone using it on a side project and give positive or negative feedback please.\n\nThe API is protected by access token located in the query params, but I'll make it so that it can also be in the headers. The idea was that you can share the link to your data and use it in some other project, same data.\n\nEndpoints have the following construct: \\`/{appName}/{envName}/your-model/:id/your-sub-model/:id/...?token={accessToken}\\`. This way you can split your data between environments like \"dev\" or \"prod\". Only apps and environments have to be created in the dashboard behind bearer token, but your JSON models are protected by access token, so you can call HTTPS requests easily from your code.\n\nI will very much appreciate it if you have any questions and decide to use it somewhere. :)", "author_fullname": "t2_32bje2j7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made nodb, a JSON API to remove the burden of installing databases in my projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13vln9e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685439456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I created &lt;a href=\"https://nodb.sh\"&gt;https://nodb.sh&lt;/a&gt; to help store and fetch JSON data through HTTPS. No dependency is needed, like &amp;quot;npm install some-database&amp;quot;, or such. It&amp;#39;s for serverless applications like cloud functions where you don&amp;#39;t want too many dependencies.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to see someone using it on a side project and give positive or negative feedback please.&lt;/p&gt;\n\n&lt;p&gt;The API is protected by access token located in the query params, but I&amp;#39;ll make it so that it can also be in the headers. The idea was that you can share the link to your data and use it in some other project, same data.&lt;/p&gt;\n\n&lt;p&gt;Endpoints have the following construct: `/{appName}/{envName}/your-model/:id/your-sub-model/:id/...?token={accessToken}`. This way you can split your data between environments like &amp;quot;dev&amp;quot; or &amp;quot;prod&amp;quot;. Only apps and environments have to be created in the dashboard behind bearer token, but your JSON models are protected by access token, so you can call HTTPS requests easily from your code.&lt;/p&gt;\n\n&lt;p&gt;I will very much appreciate it if you have any questions and decide to use it somewhere. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?auto=webp&amp;v=enabled&amp;s=254888a2b0c653e6155a8f6eadaf2e4f708c472b", "width": 1200, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae4dd7831acb59ed46f6f91075803ffe3136d59c", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e31063f8e5251a16392a25eeae1870255ad8cd2", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a743ad32846c3580465511afd7b959e51a7c2a7a", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c0c35cc5a170fba29705628ecf845dff8a5a282", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b10a23710cbbc3808bd180059eb4d2ed5b586e0e", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/VIRr3lSu-zIkC2tI0fWEoGYQMh-lmfNSYNlDkZiEdPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8d0e8556fc82096284df11dced30d8a5d34803d", "width": 1080, "height": 648}], "variants": {}, "id": "MD4fxjHEg1HUTmQl1rr8SSVLbVLLFhnYHQr8eiiH0es"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13vln9e", "is_robot_indexable": true, "report_reasons": null, "author": "mk0y8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vln9e/i_made_nodb_a_json_api_to_remove_the_burden_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vln9e/i_made_nodb_a_json_api_to_remove_the_burden_of/", "subreddit_subscribers": 108004, "created_utc": 1685439456.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}