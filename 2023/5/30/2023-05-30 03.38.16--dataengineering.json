{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A few years ago, Microsoft's reference Lakehouse architecture used Azure Storage, Data Factory and Databricks.  All cool.  As a (new) Microsoft shop, we bought into all.\n\nThen a year or two ago, they removed references to Databricks and started pimping Synapse.  Even last week, I had consultants telling me we should ditch Databricks in favour of Synapse... and I have a CIO (swayed by the consultants) asking me questions about why we aren't using a Microsoft product.  I started referring to Databricks as *Azure* Databricks as a little reminder that DB is still part of the Microsoft stack.  And we are still using Data Factory (despite my dislike for it).\n\nAnd then Microsoft announces Fabric.\n\nWhat happens to Data Factory?  No idea.  What happens to Synapse?  No idea.\n\nI'd like to ignore it all... but I know that within the next few months, I'm going to have consultants telling me (and my CIO) that we *really* should be looking at Fabric.  And in maybe 3-6 months, when I hit the job market, I'm going to get recruiters asking me what my experience is with Fabric.  They won't have a clue what they are talking about, mind you... but they know how to use buzz words, so you can be it will start appearing on job ads.\n\nI feel like Danny Glover in Lethal Weapon... I'm getting too old for this shit.", "author_fullname": "t2_5txrt2ap", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "So I watched a few videos about Fabric, and started to cry a little...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13umeek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 231, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 231, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685339896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few years ago, Microsoft&amp;#39;s reference Lakehouse architecture used Azure Storage, Data Factory and Databricks.  All cool.  As a (new) Microsoft shop, we bought into all.&lt;/p&gt;\n\n&lt;p&gt;Then a year or two ago, they removed references to Databricks and started pimping Synapse.  Even last week, I had consultants telling me we should ditch Databricks in favour of Synapse... and I have a CIO (swayed by the consultants) asking me questions about why we aren&amp;#39;t using a Microsoft product.  I started referring to Databricks as &lt;em&gt;Azure&lt;/em&gt; Databricks as a little reminder that DB is still part of the Microsoft stack.  And we are still using Data Factory (despite my dislike for it).&lt;/p&gt;\n\n&lt;p&gt;And then Microsoft announces Fabric.&lt;/p&gt;\n\n&lt;p&gt;What happens to Data Factory?  No idea.  What happens to Synapse?  No idea.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to ignore it all... but I know that within the next few months, I&amp;#39;m going to have consultants telling me (and my CIO) that we &lt;em&gt;really&lt;/em&gt; should be looking at Fabric.  And in maybe 3-6 months, when I hit the job market, I&amp;#39;m going to get recruiters asking me what my experience is with Fabric.  They won&amp;#39;t have a clue what they are talking about, mind you... but they know how to use buzz words, so you can be it will start appearing on job ads.&lt;/p&gt;\n\n&lt;p&gt;I feel like Danny Glover in Lethal Weapon... I&amp;#39;m getting too old for this shit.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13umeek", "is_robot_indexable": true, "report_reasons": null, "author": "tarzanboy76", "discussion_type": null, "num_comments": 90, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13umeek/so_i_watched_a_few_videos_about_fabric_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13umeek/so_i_watched_a_few_videos_about_fabric_and/", "subreddit_subscribers": 107951, "created_utc": 1685339896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is you boss someone who can help you with technical stuff or do they just manage you and you team from above? I'm just asking because my boss is completely non technical and sometimes its not an issue, but other times it would be nice to have someone who understands what his team is doing. It prevents hours wasted trying to explain something rather than a quick call or maybe an email. More importantly, it helps when you're trying to explain \"why\" something can't be done or why something might take longer than expected when you're dealing with someone who understands certain technical aspects of your job.", "author_fullname": "t2_aobif6bsf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is your boss technical in that he/she can help you with difficult problems or non technical?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v6dku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685393936.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is you boss someone who can help you with technical stuff or do they just manage you and you team from above? I&amp;#39;m just asking because my boss is completely non technical and sometimes its not an issue, but other times it would be nice to have someone who understands what his team is doing. It prevents hours wasted trying to explain something rather than a quick call or maybe an email. More importantly, it helps when you&amp;#39;re trying to explain &amp;quot;why&amp;quot; something can&amp;#39;t be done or why something might take longer than expected when you&amp;#39;re dealing with someone who understands certain technical aspects of your job.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13v6dku", "is_robot_indexable": true, "report_reasons": null, "author": "Significant-Flow5900", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v6dku/is_your_boss_technical_in_that_heshe_can_help_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v6dku/is_your_boss_technical_in_that_heshe_can_help_you/", "subreddit_subscribers": 107951, "created_utc": 1685393936.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "MS in Data Science or CS doesn\u2019t matter, my department president is saying that those degrees can help get me more prepared for the job market, but I\u2019m not sure if this is simply because it\u2019s not easy to find a job right now. I\u2019d be a masters student with YOE as a data analyst trying to be a DE. \n\nNot sure if there\u2019s a big overall benefit to having a masters for Data Engineering\n\n(I am getting a BS in Computer Science with a Computational Data Science specialization, as requested)\n\nEdit: I feel it\u2019s necessary to add that looking at a lot of data engineering jobs on sites like Indeed and LinkedIn directly state \u201cMasters degree preferred\u201d or reference that a masters is acceptable. It seems like many responses here aren\u2019t aware of that.", "author_fullname": "t2_55fytx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Masters Degrees beneficial in this field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uu7qh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685380809.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685365111.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MS in Data Science or CS doesn\u2019t matter, my department president is saying that those degrees can help get me more prepared for the job market, but I\u2019m not sure if this is simply because it\u2019s not easy to find a job right now. I\u2019d be a masters student with YOE as a data analyst trying to be a DE. &lt;/p&gt;\n\n&lt;p&gt;Not sure if there\u2019s a big overall benefit to having a masters for Data Engineering&lt;/p&gt;\n\n&lt;p&gt;(I am getting a BS in Computer Science with a Computational Data Science specialization, as requested)&lt;/p&gt;\n\n&lt;p&gt;Edit: I feel it\u2019s necessary to add that looking at a lot of data engineering jobs on sites like Indeed and LinkedIn directly state \u201cMasters degree preferred\u201d or reference that a masters is acceptable. It seems like many responses here aren\u2019t aware of that.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13uu7qh", "is_robot_indexable": true, "report_reasons": null, "author": "ToothPickLegs", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13uu7qh/are_masters_degrees_beneficial_in_this_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uu7qh/are_masters_degrees_beneficial_in_this_field/", "subreddit_subscribers": 107951, "created_utc": 1685365111.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello colleagues ;)\n\nI have 800\u20ac available for my personal development budget and I wanna get ideas to use them\u2026\nWhat would you do if you have such amount of money?", "author_fullname": "t2_1b9hdy2v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you share some Ideas to expend 800\u20ac personal development budget?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13utouy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685363759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello colleagues ;)&lt;/p&gt;\n\n&lt;p&gt;I have 800\u20ac available for my personal development budget and I wanna get ideas to use them\u2026\nWhat would you do if you have such amount of money?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13utouy", "is_robot_indexable": true, "report_reasons": null, "author": "DrGiacometto", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13utouy/would_you_share_some_ideas_to_expend_800_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13utouy/would_you_share_some_ideas_to_expend_800_personal/", "subreddit_subscribers": 107951, "created_utc": 1685363759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are there any resource you recommend to learn how to optimize spark applications?", "author_fullname": "t2_1o26bmw3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Parameter Optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v56mt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685391137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any resource you recommend to learn how to optimize spark applications?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13v56mt", "is_robot_indexable": true, "report_reasons": null, "author": "da_chosen1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v56mt/spark_parameter_optimization/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v56mt/spark_parameter_optimization/", "subreddit_subscribers": 107951, "created_utc": 1685391137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello good people! \n\nI am on a journey of transition from DS to DE. Basically I am advanced now in SQL, Python, Spark for batch, Docker, Airflow. I know Kubernetes architecture on a high level, am decent with my understanding of networking, Linux, a bit of cloud. \n\nI was thinking next I should focus on streaming? Would that be a good next stop? \n\nAnd if so, should I go for Spark streaming since I know PySpark very well already? Or should I just go for Kafka? And do you know any good resources to learn besides the official documentation? \n\nThanks in advance!", "author_fullname": "t2_uv1rtgfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Diving into Streaming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uz8fa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685376998.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello good people! &lt;/p&gt;\n\n&lt;p&gt;I am on a journey of transition from DS to DE. Basically I am advanced now in SQL, Python, Spark for batch, Docker, Airflow. I know Kubernetes architecture on a high level, am decent with my understanding of networking, Linux, a bit of cloud. &lt;/p&gt;\n\n&lt;p&gt;I was thinking next I should focus on streaming? Would that be a good next stop? &lt;/p&gt;\n\n&lt;p&gt;And if so, should I go for Spark streaming since I know PySpark very well already? Or should I just go for Kafka? And do you know any good resources to learn besides the official documentation? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13uz8fa", "is_robot_indexable": true, "report_reasons": null, "author": "4eyes1soul", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uz8fa/diving_into_streaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uz8fa/diving_into_streaming/", "subreddit_subscribers": 107951, "created_utc": 1685376998.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm starting a data science project with a friend. The big issue is my friend is trying to learn data science and I'm trying to improve my data engineering ability so figuring out how to think of the project\n\nBasic premise is that once a week our data source is updated. I'll have a web scraper that grabs the data, processes it through an intermediary API, and adds it to a document data base. The key part of the data for analysis as we're concerned is date.\n\nWhat is the best way to think about data like this to identify good endpoints to make my friends side easier? I know being able to select a date range is good but I'm not sure if there is another way to view or think of the use of the API that would present a work item at the outset of the project.\n\n\nThanks in advance for any thoughts and discussion!", "author_fullname": "t2_a1v3e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Brainstorming API endpoints?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ul4bv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685335613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting a data science project with a friend. The big issue is my friend is trying to learn data science and I&amp;#39;m trying to improve my data engineering ability so figuring out how to think of the project&lt;/p&gt;\n\n&lt;p&gt;Basic premise is that once a week our data source is updated. I&amp;#39;ll have a web scraper that grabs the data, processes it through an intermediary API, and adds it to a document data base. The key part of the data for analysis as we&amp;#39;re concerned is date.&lt;/p&gt;\n\n&lt;p&gt;What is the best way to think about data like this to identify good endpoints to make my friends side easier? I know being able to select a date range is good but I&amp;#39;m not sure if there is another way to view or think of the use of the API that would present a work item at the outset of the project.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any thoughts and discussion!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ul4bv", "is_robot_indexable": true, "report_reasons": null, "author": "AnAceOfBlades", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ul4bv/brainstorming_api_endpoints/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ul4bv/brainstorming_api_endpoints/", "subreddit_subscribers": 107951, "created_utc": 1685335613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can someone explain what these two are in a simple language without the consultant-speak bullshit? Anything I\u2019ve seen online just sounds like fluffy gibberish.", "author_fullname": "t2_hhn5ois5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master data and reference data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uyixi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685375393.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone explain what these two are in a simple language without the consultant-speak bullshit? Anything I\u2019ve seen online just sounds like fluffy gibberish.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13uyixi", "is_robot_indexable": true, "report_reasons": null, "author": "datarbeiter", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uyixi/master_data_and_reference_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uyixi/master_data_and_reference_data/", "subreddit_subscribers": 107951, "created_utc": 1685375393.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI recently joined a BI team that maintains a data model featuring a [One True Lookup Table](https://oracle-base.com/articles/misc/one-true-lookup-tables-otlt) (OTLT) \\[one big lookup table for the entire database, which is not a practice I agree with, but can't do much about it for now\\]. They are asking me to build an ER diagram using ERWIN to represent the data model, including the OTLT. \n\nI haven't worked with ERWIN or with OTLT's before, and I am having trouble building the ER diagram. Say I have a CONTRACT table, with a column for CONTRACT\\_TYPE. CONTRACT\\_TYPE is a number from 1 to 4, and should link to the OTLT. \n\nThe OTLT has an ID (primary key), a LOOKUP\\_KEY, a LOOKUP\\_VALUE, and a LOOKUP\\_TYPE. I want to link the OTLT to the CONTRACT table, so that for LOOKUP\\_KEY = 1 and LOOKUP\\_TYPE = 'Contract\\_type', I get the LOOKUP\\_VALUE = 'Basic contract', and so on for values 2, 3 and 4.\n\nI would really appreciate your help with any of these issues:\n\n* More general (software agnostic?) ideas about how to represent OTLT's in ER diagrams for logical/physical data models, given that you need to join on at least two non-key fields, and that you can end up having over 100 joins to instances of the OTLT, many originating in the same table.\n* Specific ideas about how to do this in ERWIN. I can't figure out how to do any of the things below:\n\n1. Draw a one-to-one link between CONTRACT and the OTLT. When I do so, the primary key from contract (CONTRACT\\_ID) migrates in the OTLT. Obviously, I don't want to JOIN on the primary key, but on CONTRACT\\_TYPE, which should link to the LOOKUP\\_KEY column in the OTLT. Can't I link two tables in ERWIN on a column that is not a primary key? I have this option with other software.\n2. Specify somewhere (where?) that I need to link CONTRACT\\_TYPE with LOOKUP\\_KEY only where LOOKUP\\_TYPE = 'Contract\\_type'\n3. Deal with over 100 JOINS to the OTLT. I need to JOIN the CONTRACT table alone with 12 instances of the OTLT to get the LOOKUP\\_VALUE for a LOOKUP\\_KEY + LOOKUP\\_TYPE.\n\nI hope the above makes sense, thank you so much for sharing your knowledge.\n\nMLC", "author_fullname": "t2_3dghdpvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Representing a One True Lookup Table in an ERD diagram (ERWIN)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13useeb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685360254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I recently joined a BI team that maintains a data model featuring a &lt;a href=\"https://oracle-base.com/articles/misc/one-true-lookup-tables-otlt\"&gt;One True Lookup Table&lt;/a&gt; (OTLT) [one big lookup table for the entire database, which is not a practice I agree with, but can&amp;#39;t do much about it for now]. They are asking me to build an ER diagram using ERWIN to represent the data model, including the OTLT. &lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t worked with ERWIN or with OTLT&amp;#39;s before, and I am having trouble building the ER diagram. Say I have a CONTRACT table, with a column for CONTRACT_TYPE. CONTRACT_TYPE is a number from 1 to 4, and should link to the OTLT. &lt;/p&gt;\n\n&lt;p&gt;The OTLT has an ID (primary key), a LOOKUP_KEY, a LOOKUP_VALUE, and a LOOKUP_TYPE. I want to link the OTLT to the CONTRACT table, so that for LOOKUP_KEY = 1 and LOOKUP_TYPE = &amp;#39;Contract_type&amp;#39;, I get the LOOKUP_VALUE = &amp;#39;Basic contract&amp;#39;, and so on for values 2, 3 and 4.&lt;/p&gt;\n\n&lt;p&gt;I would really appreciate your help with any of these issues:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;More general (software agnostic?) ideas about how to represent OTLT&amp;#39;s in ER diagrams for logical/physical data models, given that you need to join on at least two non-key fields, and that you can end up having over 100 joins to instances of the OTLT, many originating in the same table.&lt;/li&gt;\n&lt;li&gt;Specific ideas about how to do this in ERWIN. I can&amp;#39;t figure out how to do any of the things below:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Draw a one-to-one link between CONTRACT and the OTLT. When I do so, the primary key from contract (CONTRACT_ID) migrates in the OTLT. Obviously, I don&amp;#39;t want to JOIN on the primary key, but on CONTRACT_TYPE, which should link to the LOOKUP_KEY column in the OTLT. Can&amp;#39;t I link two tables in ERWIN on a column that is not a primary key? I have this option with other software.&lt;/li&gt;\n&lt;li&gt;Specify somewhere (where?) that I need to link CONTRACT_TYPE with LOOKUP_KEY only where LOOKUP_TYPE = &amp;#39;Contract_type&amp;#39;&lt;/li&gt;\n&lt;li&gt;Deal with over 100 JOINS to the OTLT. I need to JOIN the CONTRACT table alone with 12 instances of the OTLT to get the LOOKUP_VALUE for a LOOKUP_KEY + LOOKUP_TYPE.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I hope the above makes sense, thank you so much for sharing your knowledge.&lt;/p&gt;\n\n&lt;p&gt;MLC&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w7MgnlMFhajn1cDt9qrXL5XsJOsp5u0iOT2kS1AbZZM.jpg?auto=webp&amp;v=enabled&amp;s=113f840c3115461fd9d0d30a49dc2eadb36d170c", "width": 1754, "height": 945}, "resolutions": [{"url": "https://external-preview.redd.it/w7MgnlMFhajn1cDt9qrXL5XsJOsp5u0iOT2kS1AbZZM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0154914a4eeb79ab3541d39635bc0a1df42c7f9", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/w7MgnlMFhajn1cDt9qrXL5XsJOsp5u0iOT2kS1AbZZM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a646522c73d265ffbc5cb1377c8d70934d0b05d9", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/w7MgnlMFhajn1cDt9qrXL5XsJOsp5u0iOT2kS1AbZZM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9b7c20fce0196be6fc6d17bd88c87af2d91274a", "width": 320, "height": 172}, {"url": "https://external-preview.redd.it/w7MgnlMFhajn1cDt9qrXL5XsJOsp5u0iOT2kS1AbZZM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16d8ef3eb8ab6b2374d53dee2c9abccdb3135d77", "width": 640, "height": 344}, {"url": "https://external-preview.redd.it/w7MgnlMFhajn1cDt9qrXL5XsJOsp5u0iOT2kS1AbZZM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=750d72f65c8676c91136d05d3cb0d6d464b1bc2d", "width": 960, "height": 517}, {"url": "https://external-preview.redd.it/w7MgnlMFhajn1cDt9qrXL5XsJOsp5u0iOT2kS1AbZZM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b97b1fc1171c0ceabafef3c9da069d81a03c0d90", "width": 1080, "height": 581}], "variants": {}, "id": "l2lbKwfEPVnksnzGxuWK8sXrouDPylsD74pYhxDAe5Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13useeb", "is_robot_indexable": true, "report_reasons": null, "author": "MaLinChao", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13useeb/representing_a_one_true_lookup_table_in_an_erd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13useeb/representing_a_one_true_lookup_table_in_an_erd/", "subreddit_subscribers": 107951, "created_utc": 1685360254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I understand why it takes forever to copy 500,000 small files from one partition to another. However if I put those 500,000 files into a zip and copy that over, then unzip it, it only takes about 10 seconds.\n\nWhat is a zip/unzip process is able to do with the filesystem that makes it quick that a copy process is not?", "author_fullname": "t2_5aiux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A question about filesystems and quantity of files in relation to speed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13vdmhk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685412925.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I understand why it takes forever to copy 500,000 small files from one partition to another. However if I put those 500,000 files into a zip and copy that over, then unzip it, it only takes about 10 seconds.&lt;/p&gt;\n\n&lt;p&gt;What is a zip/unzip process is able to do with the filesystem that makes it quick that a copy process is not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13vdmhk", "is_robot_indexable": true, "report_reasons": null, "author": "Eisenstein", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13vdmhk/a_question_about_filesystems_and_quantity_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13vdmhk/a_question_about_filesystems_and_quantity_of/", "subreddit_subscribers": 107951, "created_utc": 1685412925.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have this situation that I can't solve so I figured I might ask here.\n\nI have a large parquet file (10s of GB) that is accesible via URL. I need to process it and either store it in postgres or S3.\n\nThe thing is I can't find a way to stream it (load it in small chunks, processing it and therefore avoiding pulling it in memory all at once).\n\nI tried doing it with python with requests and pyarrow, by doing something like:\n\nresponse = requests.get(URL, stream=True)\n\nAnd then doing iter_lines(), iter_content() or iter_batches(), but pyarrow fails to parse chunks, usually throwing an error that says it's either a broken file or not a valid parquet format.\n\nI'm guessing it can be done with spark, so I wanted to ask you what would you suggest as a potential solution here.\n\nI have to choice between python or scala, I can use sagemaker potentially for resources or maybe even glue (I'm.not sure glue is applicable but maybe it is).\n\nAny idea would be much appreciated.\nThanks", "author_fullname": "t2_4bdifwrk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Process large parquet file accessible via URL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v42ey", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685388475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have this situation that I can&amp;#39;t solve so I figured I might ask here.&lt;/p&gt;\n\n&lt;p&gt;I have a large parquet file (10s of GB) that is accesible via URL. I need to process it and either store it in postgres or S3.&lt;/p&gt;\n\n&lt;p&gt;The thing is I can&amp;#39;t find a way to stream it (load it in small chunks, processing it and therefore avoiding pulling it in memory all at once).&lt;/p&gt;\n\n&lt;p&gt;I tried doing it with python with requests and pyarrow, by doing something like:&lt;/p&gt;\n\n&lt;p&gt;response = requests.get(URL, stream=True)&lt;/p&gt;\n\n&lt;p&gt;And then doing iter_lines(), iter_content() or iter_batches(), but pyarrow fails to parse chunks, usually throwing an error that says it&amp;#39;s either a broken file or not a valid parquet format.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m guessing it can be done with spark, so I wanted to ask you what would you suggest as a potential solution here.&lt;/p&gt;\n\n&lt;p&gt;I have to choice between python or scala, I can use sagemaker potentially for resources or maybe even glue (I&amp;#39;m.not sure glue is applicable but maybe it is).&lt;/p&gt;\n\n&lt;p&gt;Any idea would be much appreciated.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13v42ey", "is_robot_indexable": true, "report_reasons": null, "author": "skippy_nk", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v42ey/process_large_parquet_file_accessible_via_url/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v42ey/process_large_parquet_file_accessible_via_url/", "subreddit_subscribers": 107951, "created_utc": 1685388475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering,\n\nsome time ago I shared about our OSS project [Dozer](https://github.com/getdozer/dozer) with the group.  We just did a presentation and demo at the Python Meetup group here in Singapore and just wanted to share the full video [here](https://www.youtube.com/watch?v=brAY4VO4uO4). \n\nMatteo", "author_fullname": "t2_5efs1s7d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a data app with Dozer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uw1kt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1685369522.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;some time ago I shared about our OSS project &lt;a href=\"https://github.com/getdozer/dozer\"&gt;Dozer&lt;/a&gt; with the group.  We just did a presentation and demo at the Python Meetup group here in Singapore and just wanted to share the full video &lt;a href=\"https://www.youtube.com/watch?v=brAY4VO4uO4\"&gt;here&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Matteo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?auto=webp&amp;v=enabled&amp;s=21d67c92c68884db35a7082df4ddc21216307a24", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdbc3d07f31a859ee2a8d25d15cae583fc18c5f4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7a2e0c784ada39f866c3d45fab42c88108638d9", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f59ace580c62456dda56d89067b0dd9507902a5c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c76b28ed0cb8af8e5e3b01175825351c378809b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfb7b7d94301a88ffa5f55977ec84217eaa5854b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/06mOiCPhGuq2NZED5CA5WFMmhzbxo7MpiaRUTdYHOSA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f9b190e8f7ec9163ad9d669b9e6fed4162602a0e", "width": 1080, "height": 540}], "variants": {}, "id": "TNj7Ap_pyNxfOqQaVa0jsttZokHMgZvWhJSpFA4MTBM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13uw1kt", "is_robot_indexable": true, "report_reasons": null, "author": "matteopelati76", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uw1kt/building_a_data_app_with_dozer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uw1kt/building_a_data_app_with_dozer/", "subreddit_subscribers": 107951, "created_utc": 1685369522.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5b3y9jqyc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learn about Vector Database | Concepts &amp; examples", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_13ut0ly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/nd4oe5eMRrSvnNkpHW3ekKjca0R5HlgXFTbMWTtXH24.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685361941.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://substack.com/inbox/post/124553209", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?auto=webp&amp;v=enabled&amp;s=1d7d08abffc62cc98f3e8018f515d8dea23e3b6b", "width": 1000, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05e51dda9acfe695b7ccf72cb4f5f142d1fdf048", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb9ee222ba18e966d454ebabccc3ba09c514a54c", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=21976c5c4c11165ec7cc1809f52b0b47299ca932", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c725dc8ecbd936297c4e2524d25b73dc46ac37cb", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/hj-SGOZUpYVxilFMH7kycNCYbr0OUkpEduuJyaZ_Ws4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6bf58cd784a891c018a053682f7dbef57ada392", "width": 960, "height": 576}], "variants": {}, "id": "uJRu0sqkpaL2K3XaVf87sDx1OTexdsKjkTC7aTw5Ogs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13ut0ly", "is_robot_indexable": true, "report_reasons": null, "author": "de4all", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ut0ly/learn_about_vector_database_concepts_examples/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://substack.com/inbox/post/124553209", "subreddit_subscribers": 107951, "created_utc": 1685361941.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_kgkprqpn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Warehouse Architecture and Design: A Reflective Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13urp2z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/a9vtDAWJGJ0Zgkmralyw2tk3YfddHptfFrwzFEJI1xY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685358202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dasca.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.dasca.org/world-of-big-data/article/data-warehouse-architecture-and-design-a-reflective-guide", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2tmTBA7Cy6T5OJmm6Vl6Iy6UX_My7wMGS4Ni150b0RU.jpg?auto=webp&amp;v=enabled&amp;s=157d112950fe27f4ad501f440cbf75497580ffac", "width": 800, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/2tmTBA7Cy6T5OJmm6Vl6Iy6UX_My7wMGS4Ni150b0RU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5530b4536d80051b224487d09644634f6a792406", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/2tmTBA7Cy6T5OJmm6Vl6Iy6UX_My7wMGS4Ni150b0RU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45388d797418efa4838383728e51da3b64d01c82", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/2tmTBA7Cy6T5OJmm6Vl6Iy6UX_My7wMGS4Ni150b0RU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b076ff8f3b0bd02677d541977f7479b7b3b0c045", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/2tmTBA7Cy6T5OJmm6Vl6Iy6UX_My7wMGS4Ni150b0RU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d0fd08d5a3db0d977883f51690d5b65c4f80358", "width": 640, "height": 336}], "variants": {}, "id": "VgWmliN686rtckPJmZChzmQJgAxoA5mbZRXDS0t1loo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13urp2z", "is_robot_indexable": true, "report_reasons": null, "author": "Emily-joe", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13urp2z/data_warehouse_architecture_and_design_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.dasca.org/world-of-big-data/article/data-warehouse-architecture-and-design-a-reflective-guide", "subreddit_subscribers": 107951, "created_utc": 1685358202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi There.   \n\n\nI'm modelling a DW to a retail store - at this time I just have one `Fact` table covering the `orders` and `measures` from it.   \nBut now I need to work with data from the `payments` table which has the `amount` field (which represents the value of the payment). In that application, one order can have multiple payments.   \nWith that scnery - can you guys help me how to define if it's the case to use multiple Fact tables or one single Fact covering the `Orders` and the `Payments`?", "author_fullname": "t2_iweexjfw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help to create Fact tables - DW", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uroaz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685358145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi There.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m modelling a DW to a retail store - at this time I just have one &lt;code&gt;Fact&lt;/code&gt; table covering the &lt;code&gt;orders&lt;/code&gt; and &lt;code&gt;measures&lt;/code&gt; from it.&lt;br/&gt;\nBut now I need to work with data from the &lt;code&gt;payments&lt;/code&gt; table which has the &lt;code&gt;amount&lt;/code&gt; field (which represents the value of the payment). In that application, one order can have multiple payments.&lt;br/&gt;\nWith that scnery - can you guys help me how to define if it&amp;#39;s the case to use multiple Fact tables or one single Fact covering the &lt;code&gt;Orders&lt;/code&gt; and the &lt;code&gt;Payments&lt;/code&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13uroaz", "is_robot_indexable": true, "report_reasons": null, "author": "yeager_doug", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uroaz/help_to_create_fact_tables_dw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uroaz/help_to_create_fact_tables_dw/", "subreddit_subscribers": 107951, "created_utc": 1685358145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to read NY data set which is stored &amp; publically available here, I extracted the underlying location of the parquet file for the 2022 as \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet\". Now I was trying to read data form this URL and used the read_parquet method to do it quite easily. But I am not able to figure out on how to read this data if the data size is too big and which might cause memory overload. Unlike read_csv does, read_parquet does not have stream option &amp; converting into pyarrow.parquet.parquetfile to use its iter_batches functionality does not seem to be an option since it cannot read from URL", "author_fullname": "t2_6ys5mu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to read parquet file from URL in chunks to avoid Memory issues?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uosio", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1685349207.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685348270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to read NY data set which is stored &amp;amp; publically available here, I extracted the underlying location of the parquet file for the 2022 as &amp;quot;&lt;a href=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet\"&gt;https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2019-01.parquet&lt;/a&gt;&amp;quot;. Now I was trying to read data form this URL and used the read_parquet method to do it quite easily. But I am not able to figure out on how to read this data if the data size is too big and which might cause memory overload. Unlike read_csv does, read_parquet does not have stream option &amp;amp; converting into pyarrow.parquet.parquetfile to use its iter_batches functionality does not seem to be an option since it cannot read from URL&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13uosio", "is_robot_indexable": true, "report_reasons": null, "author": "user19911506", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uosio/how_to_read_parquet_file_from_url_in_chunks_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uosio/how_to_read_parquet_file_from_url_in_chunks_to/", "subreddit_subscribers": 107951, "created_utc": 1685348270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vle5v8ic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is DataOps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13v7of3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What is DataOps?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "author_name": "Polyseam", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/HNgpk9IUfK4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@polyseam"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13v7of3", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZV9tDN9N8Oy3pnxB22xIP11rQZvrVyX4a3uRkht-ZQU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1685397040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/HNgpk9IUfK4", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?auto=webp&amp;v=enabled&amp;s=558ed727ae34d44e61e800d5c68b6d4e3b06c9b5", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de8987cee6ecdb05960e5deea7cec8c9c38efe0c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c5ce5990d7fd1f1cf4b4389b41bc453fc017fdb", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/W1qM9D_6cWiZbGrXpiuQqPmrErnOBlTyAJPpGfrgRtw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d2be9fc510038f6f9ddb00aac3996a20d0ea37dc", "width": 320, "height": 240}], "variants": {}, "id": "YzEWw94GU1pBBr-CsbqyKbtLUvvhgd5ZFCQ5vQvUDYI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13v7of3", "is_robot_indexable": true, "report_reasons": null, "author": "Polyseam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v7of3/what_is_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/HNgpk9IUfK4", "subreddit_subscribers": 107951, "created_utc": 1685397040.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What is DataOps?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/HNgpk9IUfK4?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What is DataOps?\"&gt;&lt;/iframe&gt;", "author_name": "Polyseam", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/HNgpk9IUfK4/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@polyseam"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all\n\nI'm a mathematician/process/statistical modeller working in agricultural/environmental science. Our company has invested in Snowflake for data storage and R for data analysis. However I am finding that the volumes of data are becoming a bit more than can be comfortably handled in R on a single PC (we're in Windows 10). I am looking for options for data visualisation, extraction, cleaning, statistical modelling that don't require downloading the data and/or having it in memory. I don't really understand the IT side of data science very well, but two options look like Spark(lyr) and Snowpark.\n\nAny suggestions or advice or experience you can share?\n\nThanks!", "author_fullname": "t2_221frg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools for modelling (e.g. lm, gam) high res time series data in Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13v4ghy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685389422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a mathematician/process/statistical modeller working in agricultural/environmental science. Our company has invested in Snowflake for data storage and R for data analysis. However I am finding that the volumes of data are becoming a bit more than can be comfortably handled in R on a single PC (we&amp;#39;re in Windows 10). I am looking for options for data visualisation, extraction, cleaning, statistical modelling that don&amp;#39;t require downloading the data and/or having it in memory. I don&amp;#39;t really understand the IT side of data science very well, but two options look like Spark(lyr) and Snowpark.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or advice or experience you can share?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13v4ghy", "is_robot_indexable": true, "report_reasons": null, "author": "si_wo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13v4ghy/best_tools_for_modelling_eg_lm_gam_high_res_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13v4ghy/best_tools_for_modelling_eg_lm_gam_high_res_time/", "subreddit_subscribers": 107951, "created_utc": 1685389422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our data mostly exist in the form of text (CSV) and Parquet files. I'm looking for an open source software to be able to manage all these files and their metadata. So when accessing a file, I want to know its content (e.g., what timespan it covers, what columns there are, ...) anx also find a file based on such criteria. Importing them into a database of some sort is not the answer, because we dont want to go through the trouble of managing the database (the files were initially exported from a DB).\n\nAre there any open source software for this?", "author_fullname": "t2_sl4ecm1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage files &amp; metadata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13uw7ez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685369940.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our data mostly exist in the form of text (CSV) and Parquet files. I&amp;#39;m looking for an open source software to be able to manage all these files and their metadata. So when accessing a file, I want to know its content (e.g., what timespan it covers, what columns there are, ...) anx also find a file based on such criteria. Importing them into a database of some sort is not the answer, because we dont want to go through the trouble of managing the database (the files were initially exported from a DB).&lt;/p&gt;\n\n&lt;p&gt;Are there any open source software for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13uw7ez", "is_robot_indexable": true, "report_reasons": null, "author": "UnlikelyNorth2048", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13uw7ez/manage_files_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13uw7ez/manage_files_metadata/", "subreddit_subscribers": 107951, "created_utc": 1685369940.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nCurrently, we have 90 Google Cloud servers that regularly transfer raw data to a Google BigQuery \"bucket\" every few minutes.\n\nThe problem arises when attempting to work with this raw data, as several parameters are stored as numerical values. For instance, the user account status may be represented as follows:\n\n* *02 = Active*\n* *05 = Suspended*\n* *16 = Verification needed,.. and so on.*\n\nTo address this, we aim to map this data to corresponding words or text once and for all. However, our engineers have informed us that modifying or editing data within BigQuery is challenging because it operates as a one-way synchronization, and any alterations would significantly increase the cost of the data warehouse.\n\nTo overcome this issue, our plan is to implement a Metabase layer on top of the Google BigQuery data warehouse. Therefore, my question is: Is it feasible to perform the data mapping within Metabase so that analytics can be conducted using more user-friendly values?", "author_fullname": "t2_jger5m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Metabase be used to edit/modify data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ut4rb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685362261.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently, we have 90 Google Cloud servers that regularly transfer raw data to a Google BigQuery &amp;quot;bucket&amp;quot; every few minutes.&lt;/p&gt;\n\n&lt;p&gt;The problem arises when attempting to work with this raw data, as several parameters are stored as numerical values. For instance, the user account status may be represented as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;02 = Active&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;05 = Suspended&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;16 = Verification needed,.. and so on.&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To address this, we aim to map this data to corresponding words or text once and for all. However, our engineers have informed us that modifying or editing data within BigQuery is challenging because it operates as a one-way synchronization, and any alterations would significantly increase the cost of the data warehouse.&lt;/p&gt;\n\n&lt;p&gt;To overcome this issue, our plan is to implement a Metabase layer on top of the Google BigQuery data warehouse. Therefore, my question is: Is it feasible to perform the data mapping within Metabase so that analytics can be conducted using more user-friendly values?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ut4rb", "is_robot_indexable": true, "report_reasons": null, "author": "MysteriousBrilliant", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ut4rb/can_metabase_be_used_to_editmodify_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ut4rb/can_metabase_be_used_to_editmodify_data/", "subreddit_subscribers": 107951, "created_utc": 1685362261.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have never worked in Scala. I just have a simple question .\n\nIn Scala:\n\n`val var1 = \"hello\" `\n\noutput:\n\n`var1: String = hello`\n\nIn Python:\n\n`var1 = \"hello\" `\n\nThis doesn't generate any output by default, so only way is to print the variable.\n\nIs there any way I can get similar output in python at least the variable value if getting data type is not possible, w/o explicitly using any extra statement as it can be really useful for debugging later or developing.", "author_fullname": "t2_wkq4zhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scala like feature in Python Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13urslv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685358479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have never worked in Scala. I just have a simple question .&lt;/p&gt;\n\n&lt;p&gt;In Scala:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;val var1 = &amp;quot;hello&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;output:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;var1: String = hello&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;In Python:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;var1 = &amp;quot;hello&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This doesn&amp;#39;t generate any output by default, so only way is to print the variable.&lt;/p&gt;\n\n&lt;p&gt;Is there any way I can get similar output in python at least the variable value if getting data type is not possible, w/o explicitly using any extra statement as it can be really useful for debugging later or developing.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13urslv", "is_robot_indexable": true, "report_reasons": null, "author": "the_aris", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13urslv/scala_like_feature_in_python_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13urslv/scala_like_feature_in_python_databricks/", "subreddit_subscribers": 107951, "created_utc": 1685358479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Per title. \n\nI'm working in a smallish company. They started the introduction of a dataops person. One of the things they did was remove access to resources. \n\nI was curious if permission setting and control is part of the role for dataops. For most of the resources, it defined it as optimizing and ensuring the pipeline is efficient and error free.", "author_fullname": "t2_6o6sl8n7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is dataops responsible for permission access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13umqfy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685340967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Per title. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working in a smallish company. They started the introduction of a dataops person. One of the things they did was remove access to resources. &lt;/p&gt;\n\n&lt;p&gt;I was curious if permission setting and control is part of the role for dataops. For most of the resources, it defined it as optimizing and ensuring the pipeline is efficient and error free.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13umqfy", "is_robot_indexable": true, "report_reasons": null, "author": "Tasty_Fold3012", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13umqfy/is_dataops_responsible_for_permission_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13umqfy/is_dataops_responsible_for_permission_access/", "subreddit_subscribers": 107951, "created_utc": 1685340967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to somehow use airflow variable (environment variables or custom backend variables) in dbt profiles.yml file?", "author_fullname": "t2_1d0xp3eg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Airflow variables in dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ureb8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1685357325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to somehow use airflow variable (environment variables or custom backend variables) in dbt profiles.yml file?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ureb8", "is_robot_indexable": true, "report_reasons": null, "author": "pigminster", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ureb8/using_airflow_variables_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ureb8/using_airflow_variables_in_dbt/", "subreddit_subscribers": 107951, "created_utc": 1685357325.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}