{"kind": "Listing", "data": {"after": "t3_13lmvxa", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_tlrlz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Disney to remove dozens of series from Disney+ on May 26th", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13lwuhp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 342, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 342, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9x9vB_01tDatW6AH2D_iGzlpUWi08ghTVEv7i17i96w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684506056.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "deadline.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://deadline.com/2023/05/disney-remove-series-streaming-disney-plus-hulu-big-shot-willow-y-dollface-turner-hooch-pistol-1235372512/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qN7hX6w0Qel7k3lRUitz0ePcHVyI4a4-2hS9Ac1rCTk.jpg?auto=webp&amp;v=enabled&amp;s=8bd3dec33564ba41f489e139bd0dd59ca8068d13", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/qN7hX6w0Qel7k3lRUitz0ePcHVyI4a4-2hS9Ac1rCTk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=849ead53f79c4a2b9e9e29bbcd3f04313d8505f6", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/qN7hX6w0Qel7k3lRUitz0ePcHVyI4a4-2hS9Ac1rCTk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f08429d88dba537ac56ef233565f0369e35bcf4f", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/qN7hX6w0Qel7k3lRUitz0ePcHVyI4a4-2hS9Ac1rCTk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93f5ff36051c36eeb12968b63c50f9b6c1addac5", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/qN7hX6w0Qel7k3lRUitz0ePcHVyI4a4-2hS9Ac1rCTk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f0bbdb7e2a5a2db61b1e3dfc7e74e6227e4a61c", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/qN7hX6w0Qel7k3lRUitz0ePcHVyI4a4-2hS9Ac1rCTk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f51f3ba4ef4c2de3e2d6d21543ef5d1a60282ba", "width": 960, "height": 540}], "variants": {}, "id": "UfbpEpwCeLiC9ncuaD79bAGbBex4hN2nERy3nDyEoXU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lwuhp", "is_robot_indexable": true, "report_reasons": null, "author": "Loosel", "discussion_type": null, "num_comments": 159, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lwuhp/disney_to_remove_dozens_of_series_from_disney_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://deadline.com/2023/05/disney-remove-series-streaming-disney-plus-hulu-big-shot-willow-y-dollface-turner-hooch-pistol-1235372512/", "subreddit_subscribers": 683508, "created_utc": 1684506056.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_edp6wvb2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The r/WorldFunnies subreddit got privated, all the videos are in a mega archive now, just putting this out there before it gets forgotten forever, that sub had really excellent posts, the bests i've ever seen!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lkl8c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "r/worldfunnies", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1684469506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ibradude.net", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ibradude.net/content/misc/worldfunnies", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 250, "id": "award_31260000-2f4a-4b40-ad20-f5aa46a577bf", "penny_donate": null, "award_sub_type": "APPRECIATION", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/Timeless_512.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/Timeless_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/Timeless_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/Timeless_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/Timeless_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/Timeless_128.png", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Beauty that's forever. Gives %{coin_symbol}100 Coins each to the author and the community.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 100, "count": 1, "static_icon_height": 2048, "name": "Timeless Beauty", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/08ps702w9l581_Timeless.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=b0fb7c1afe2b3c12435485c105672a7db5316c95", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/08ps702w9l581_Timeless.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=939f9fb48c84066df1b3ea21ab9c8d42982d1102", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/08ps702w9l581_Timeless.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=65beacd9ffa2df68fc986fa6ad54801a4366b20a", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/08ps702w9l581_Timeless.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=5ace6dca05d1fc5b13f98f39648af43ec1aa7523", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/08ps702w9l581_Timeless.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=9b5ba3e734ae8b5aee727a2d8aca612be7701e6b", "width": 128, "height": 128}], "icon_format": "APNG", "icon_height": 2048, "penny_price": null, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/08ps702w9l581_Timeless.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13lkl8c", "is_robot_indexable": true, "report_reasons": null, "author": "nameistakenmate", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lkl8c/the_rworldfunnies_subreddit_got_privated_all_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ibradude.net/content/misc/worldfunnies", "subreddit_subscribers": 683508, "created_utc": 1684469506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got this nas off Facebook marketplace. It won\u2019t power on. The motherboard has a known issue with qnap I\u2019ve contacted qnap and without a receipt I can\u2019t get them to intervene. (Was hoping to get lucky) now it\u2019s essentially an itx computer inside. \n\nThe main difference is the cables coming from the backplane. Is there a way I can utilize this? \n\n\nIt\u2019s not going to be simple as most motherboards won\u2019t match up with the io panel on the back but with a dremel and some cando attitude I might be able to salvage this compact build into my server rack that\u2019s very shallow. \n\nHas anyone else tried this? \n\nAny ideas which cable connectors I would need? \n\nOr if this is completely stupid and I should stop building computers today. \n\nThanks for your time.", "author_fullname": "t2_ho9oc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old nas to new itx nas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": false, "name": "t3_13ljd79", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nLGfNckUienP3UQP0tDUU8EzeQVm9SRvJN22zLJZNXE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684465870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got this nas off Facebook marketplace. It won\u2019t power on. The motherboard has a known issue with qnap I\u2019ve contacted qnap and without a receipt I can\u2019t get them to intervene. (Was hoping to get lucky) now it\u2019s essentially an itx computer inside. &lt;/p&gt;\n\n&lt;p&gt;The main difference is the cables coming from the backplane. Is there a way I can utilize this? &lt;/p&gt;\n\n&lt;p&gt;It\u2019s not going to be simple as most motherboards won\u2019t match up with the io panel on the back but with a dremel and some cando attitude I might be able to salvage this compact build into my server rack that\u2019s very shallow. &lt;/p&gt;\n\n&lt;p&gt;Has anyone else tried this? &lt;/p&gt;\n\n&lt;p&gt;Any ideas which cable connectors I would need? &lt;/p&gt;\n\n&lt;p&gt;Or if this is completely stupid and I should stop building computers today. &lt;/p&gt;\n\n&lt;p&gt;Thanks for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zf9x927vxq0b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?auto=webp&amp;v=enabled&amp;s=f9d83ab50b8a262cb51bd38bc936305aa3bfddf1", "width": 1290, "height": 1230}, "resolutions": [{"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04be140884caa479f1015a06f1bb76272b6a114c", "width": 108, "height": 102}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a01f7498663850e09b61eaefc40c06800aca207", "width": 216, "height": 205}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20cce8ba25e7e9d8ab6122525ae95911df89e26f", "width": 320, "height": 305}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a431e021243496bc6574f36785a80f4afbd790e", "width": 640, "height": 610}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da0c5a73f1badb6f11a269f48d78bf1ab18067f4", "width": 960, "height": 915}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e147e190e898220c65354e4947516afbcf674dae", "width": 1080, "height": 1029}], "variants": {}, "id": "XdZTyG1fBjnJs_3wVjHjBw_jI_japMGT5varghi_LLU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ljd79", "is_robot_indexable": true, "report_reasons": null, "author": "Cor4eyh", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ljd79/old_nas_to_new_itx_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zf9x927vxq0b1.jpg", "subreddit_subscribers": 683508, "created_utc": 1684465870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My cousin took her personal laptop at work and the IT guy from her work added an additional sata ssd. Her system was still on during lunch, she went to some other room and voila all her personal data is deleted from the primary m.2 nvme ssd which has the windows OS. She kept all her marriage, first child's photos and videos in a folder on the desktop (yeah big mistake) and there was no backup. One of her colleagues who is supposedly jealous of her did this. Anyways her husband bought recuva software and tried to recover the data. It showed almost all of the file names but most say cannot be recovered. The one's which showed green icon are recovered and most show like half of the photo and just black for the rest.\n\nThe primary m.2 nvme ssd is like 500gb in capacity and almost all of it is empty. I have told them to disconnect it from the internet and not to copy or install anything. They have kept the laptop powered on. They are from a small town and no professional data recovery centers are there. They contacted one center which is in other state but he is asking for $800 usd which will be non refundable in case nothing is recovered. Plus he said that he will need the laptop for atleast 10 days.\n\nIs there any software or any other way through which we can recover some of the data if not all?\n\nHope you guys don't mind me asking, but what are the names of the data recovery softwares used by professionals? Are they sold to retail customers?\n\nWhen these data recovery softwares are run, do they hurt a ssd's health?", "author_fullname": "t2_6zywxuin", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need guidance on data recovery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lwg7l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684505224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My cousin took her personal laptop at work and the IT guy from her work added an additional sata ssd. Her system was still on during lunch, she went to some other room and voila all her personal data is deleted from the primary m.2 nvme ssd which has the windows OS. She kept all her marriage, first child&amp;#39;s photos and videos in a folder on the desktop (yeah big mistake) and there was no backup. One of her colleagues who is supposedly jealous of her did this. Anyways her husband bought recuva software and tried to recover the data. It showed almost all of the file names but most say cannot be recovered. The one&amp;#39;s which showed green icon are recovered and most show like half of the photo and just black for the rest.&lt;/p&gt;\n\n&lt;p&gt;The primary m.2 nvme ssd is like 500gb in capacity and almost all of it is empty. I have told them to disconnect it from the internet and not to copy or install anything. They have kept the laptop powered on. They are from a small town and no professional data recovery centers are there. They contacted one center which is in other state but he is asking for $800 usd which will be non refundable in case nothing is recovered. Plus he said that he will need the laptop for atleast 10 days.&lt;/p&gt;\n\n&lt;p&gt;Is there any software or any other way through which we can recover some of the data if not all?&lt;/p&gt;\n\n&lt;p&gt;Hope you guys don&amp;#39;t mind me asking, but what are the names of the data recovery softwares used by professionals? Are they sold to retail customers?&lt;/p&gt;\n\n&lt;p&gt;When these data recovery softwares are run, do they hurt a ssd&amp;#39;s health?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 100, "id": "award_81cf5c92-8500-498c-9c94-3e4034cece0a", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=d480674cac39cfd00fb7f2bcb2dca08bde3ea20a", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=26a606f2cd07ae19f81e02351c19ca516c19c156", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=49ac7349e652d9dd96fb12949271de1d88eb4aa7", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=5ccffd26141775968bbac3b8fb7508b3104d5967", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=92d87db9692a365022eb58c15fc727bb44e2b86d", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Staring into the abyss and it's staring right back", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Dread", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=d480674cac39cfd00fb7f2bcb2dca08bde3ea20a", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=26a606f2cd07ae19f81e02351c19ca516c19c156", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=49ac7349e652d9dd96fb12949271de1d88eb4aa7", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=5ccffd26141775968bbac3b8fb7508b3104d5967", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=92d87db9692a365022eb58c15fc727bb44e2b86d", "width": 128, "height": 128}], "icon_format": "PNG", "icon_height": 2048, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_22cerq/nvfe4gyawnf51_Dread.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13lwg7l", "is_robot_indexable": true, "report_reasons": null, "author": "somuchtolearn007", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lwg7l/need_guidance_on_data_recovery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lwg7l/need_guidance_on_data_recovery/", "subreddit_subscribers": 683508, "created_utc": 1684505224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys,\n\nI like to save Youtube Channels from Youtubers I really like. Because of that, i built a quite big library.  \nI would really like to give other people access to my files and get access to theirs.\n\nIs there a software, where i can share my youtube channels and make them accessible via some kind of torrent software? But youtube video focused?", "author_fullname": "t2_11morj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a P2P Plattform, for archived youtube channels/youtube videos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lcn7b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684448181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I like to save Youtube Channels from Youtubers I really like. Because of that, i built a quite big library.&lt;br/&gt;\nI would really like to give other people access to my files and get access to theirs.&lt;/p&gt;\n\n&lt;p&gt;Is there a software, where i can share my youtube channels and make them accessible via some kind of torrent software? But youtube video focused?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lcn7b", "is_robot_indexable": true, "report_reasons": null, "author": "BurningPixels", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lcn7b/is_there_a_p2p_plattform_for_archived_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lcn7b/is_there_a_p2p_plattform_for_archived_youtube/", "subreddit_subscribers": 683508, "created_utc": 1684448181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all!\n\nI wanted to make sure I had a backup of my personal GitHub account with all my repositories, but didn't want to self-host Gitea and set up a script for syncing my account just for the sake of backup.\n\nSo I created a simple program that runs in a container, that will back up all your repositories to a compressed tar file and quickly (multi-processed). I already added it to my nightly backup job :)\n\nYou can pass multiple parameters such as whether you want to include forks you made of other repos, as well as an exclusion list for repos you don't want to back up.\n\nLet me know if you have any feedback!\n\n[https://github.com/orellazri/github-backup/](https://github.com/orellazri/github-backup/)", "author_fullname": "t2_i36uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a tool to back up your GitHub account repos to a compressed archive file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lyp4o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684509905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;I wanted to make sure I had a backup of my personal GitHub account with all my repositories, but didn&amp;#39;t want to self-host Gitea and set up a script for syncing my account just for the sake of backup.&lt;/p&gt;\n\n&lt;p&gt;So I created a simple program that runs in a container, that will back up all your repositories to a compressed tar file and quickly (multi-processed). I already added it to my nightly backup job :)&lt;/p&gt;\n\n&lt;p&gt;You can pass multiple parameters such as whether you want to include forks you made of other repos, as well as an exclusion list for repos you don&amp;#39;t want to back up.&lt;/p&gt;\n\n&lt;p&gt;Let me know if you have any feedback!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/orellazri/github-backup/\"&gt;https://github.com/orellazri/github-backup/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/leoe90qxtKS2SGAIP_qhTcOQXIIfYITMYQLfSpB9kPM.jpg?auto=webp&amp;v=enabled&amp;s=8dd7ffc2b26eb6d55853645f98b9b133f1c022b4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/leoe90qxtKS2SGAIP_qhTcOQXIIfYITMYQLfSpB9kPM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6112edbcf2130b527286b0f97aac1111a3ad0ed8", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/leoe90qxtKS2SGAIP_qhTcOQXIIfYITMYQLfSpB9kPM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fa5534df2efc48e04a0bca8bcf54df32238ccd6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/leoe90qxtKS2SGAIP_qhTcOQXIIfYITMYQLfSpB9kPM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2cadc4044b13c48b3411bb522566ddbb98cb4399", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/leoe90qxtKS2SGAIP_qhTcOQXIIfYITMYQLfSpB9kPM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09b0c41667c6db8b378927549608bb719d5c0cd9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/leoe90qxtKS2SGAIP_qhTcOQXIIfYITMYQLfSpB9kPM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2647be131dd86b2c0da3225ae818823e81b3acb1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/leoe90qxtKS2SGAIP_qhTcOQXIIfYITMYQLfSpB9kPM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32c8b207845249cacdec8578109553d12e4958b7", "width": 1080, "height": 540}], "variants": {}, "id": "em3kyWvOCjQv9d2uqUj51tggATwJRTEwKw2pctaO5Hg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lyp4o", "is_robot_indexable": true, "report_reasons": null, "author": "Ryiseld", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lyp4o/i_created_a_tool_to_back_up_your_github_account/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lyp4o/i_created_a_tool_to_back_up_your_github_account/", "subreddit_subscribers": 683508, "created_utc": 1684509905.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been exploiting the unenforced drive limits on Google Drive for the past few years to do cloud backups.  With them cracking down on the limits now I want to just get rid of everything in my drive and look for an alternative.  Problem is, my backup software has uploaded what is probably millions of small files and there doesn't seem to be any quick or easy way to delete all of them.  I've gone into \"Manage Apps\" and removed the backup software and chose the option to delete all associated files but that hasn't changed the amount of data Gdrive is reporting as used.  To clarify, there is nothing in \"My Drive\", it all appears on the Storage tab of the Gdrive web interface.\n\nIs there any quick and easy way to wipe out all that data or am I stuck going through page after page deleting the files one page at a time?", "author_fullname": "t2_aclgt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I wipe my Google Drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lvu4d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684503767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been exploiting the unenforced drive limits on Google Drive for the past few years to do cloud backups.  With them cracking down on the limits now I want to just get rid of everything in my drive and look for an alternative.  Problem is, my backup software has uploaded what is probably millions of small files and there doesn&amp;#39;t seem to be any quick or easy way to delete all of them.  I&amp;#39;ve gone into &amp;quot;Manage Apps&amp;quot; and removed the backup software and chose the option to delete all associated files but that hasn&amp;#39;t changed the amount of data Gdrive is reporting as used.  To clarify, there is nothing in &amp;quot;My Drive&amp;quot;, it all appears on the Storage tab of the Gdrive web interface.&lt;/p&gt;\n\n&lt;p&gt;Is there any quick and easy way to wipe out all that data or am I stuck going through page after page deleting the files one page at a time?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lvu4d", "is_robot_indexable": true, "report_reasons": null, "author": "Davel23", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lvu4d/how_do_i_wipe_my_google_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lvu4d/how_do_i_wipe_my_google_drive/", "subreddit_subscribers": 683508, "created_utc": 1684503767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, I recently bought an 8TB WD Easystore external drive off of FB marketplace. I plugged it into my computer (HP Pavilion gaming laptop) and it recognizes it....but says its capacity is 500mb, which makes no sense. Am I doing something wrong? Also, it won't allow me to look for a driver, maybe that's the issue? I know nothing about this stuff, I just wanted an external drive to move stuff over to that I could access while using my laptop because my storage is full. Is this fixable or did I buya bunk drive/the wrong thing? thanks", "author_fullname": "t2_d4k4hzfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Easystore 8TB problems with Windows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lf3js", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684454330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I recently bought an 8TB WD Easystore external drive off of FB marketplace. I plugged it into my computer (HP Pavilion gaming laptop) and it recognizes it....but says its capacity is 500mb, which makes no sense. Am I doing something wrong? Also, it won&amp;#39;t allow me to look for a driver, maybe that&amp;#39;s the issue? I know nothing about this stuff, I just wanted an external drive to move stuff over to that I could access while using my laptop because my storage is full. Is this fixable or did I buya bunk drive/the wrong thing? thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lf3js", "is_robot_indexable": true, "report_reasons": null, "author": "77177717", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lf3js/wd_easystore_8tb_problems_with_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lf3js/wd_easystore_8tb_problems_with_windows/", "subreddit_subscribers": 683508, "created_utc": 1684454330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_gc8ph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any idea where to find the longer drive rails on the left? Older Areca/Proware 64-bay DAS.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "name": "t3_13m2usq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/6gAacV9HltJw0nDor6qhm56pyMshykwmgKZsuxcQUr0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684519103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/d2mxpzweut0b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/d2mxpzweut0b1.jpg?auto=webp&amp;v=enabled&amp;s=4cf4f0ed92e20e843ea5474c72ec88f331b6e06e", "width": 3645, "height": 1886}, "resolutions": [{"url": "https://preview.redd.it/d2mxpzweut0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0a80e917723f255150efe2503852f5062238f1a", "width": 108, "height": 55}, {"url": "https://preview.redd.it/d2mxpzweut0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fabaa9b89d78da4d7e95d2f61cf9d1c0871dc2d0", "width": 216, "height": 111}, {"url": "https://preview.redd.it/d2mxpzweut0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=34e2643f068b32824cb52c08d038a5e60a630afc", "width": 320, "height": 165}, {"url": "https://preview.redd.it/d2mxpzweut0b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5b959c884170f9d358c0192db57326b20b7e2d8", "width": 640, "height": 331}, {"url": "https://preview.redd.it/d2mxpzweut0b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ab8dd0ccb6279f67070ba9e808ae0002aaca28e", "width": 960, "height": 496}, {"url": "https://preview.redd.it/d2mxpzweut0b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c5813216b95f7b3543b71c0fa802fcf89425bdc", "width": 1080, "height": 558}], "variants": {}, "id": "FL5239R-BFvS1AR0gk0XNvIuwJfsGBO2JkEvCDKay1E"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13m2usq", "is_robot_indexable": true, "report_reasons": null, "author": "ripsfo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13m2usq/any_idea_where_to_find_the_longer_drive_rails_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/d2mxpzweut0b1.jpg", "subreddit_subscribers": 683508, "created_utc": 1684519103.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI have a DS1821+ with x8 16TB drives plus x2 (mismatched) SSD drives for caching which are a Samsung Evo 970 500GB and a Sandisk WDS500G2B0C 500GB.\n\nIf i upgraded the SSD's for bigger ones and matching ones (1TB or 2TB), will this make any difference in performance?\n\nFor reference, the files on the drive are mainly .mp4 files that are used for video editing direct from the NAS via 10GB ethernet plus some photos that I use direct from NAS again Photoshop etc.\n\nQuite happy with the performance now but just wondered if an SSD drive would make things even better.", "author_fullname": "t2_8zytwi0n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bigger SSD cache drives make a difference on big NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ltda4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684497449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have a DS1821+ with x8 16TB drives plus x2 (mismatched) SSD drives for caching which are a Samsung Evo 970 500GB and a Sandisk WDS500G2B0C 500GB.&lt;/p&gt;\n\n&lt;p&gt;If i upgraded the SSD&amp;#39;s for bigger ones and matching ones (1TB or 2TB), will this make any difference in performance?&lt;/p&gt;\n\n&lt;p&gt;For reference, the files on the drive are mainly .mp4 files that are used for video editing direct from the NAS via 10GB ethernet plus some photos that I use direct from NAS again Photoshop etc.&lt;/p&gt;\n\n&lt;p&gt;Quite happy with the performance now but just wondered if an SSD drive would make things even better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ltda4", "is_robot_indexable": true, "report_reasons": null, "author": "Monk_Significant", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ltda4/bigger_ssd_cache_drives_make_a_difference_on_big/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ltda4/bigger_ssd_cache_drives_make_a_difference_on_big/", "subreddit_subscribers": 683508, "created_utc": 1684497449.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Apologies if this isn't the right forum, but it is free post Friday sooooo..\n\nTo get to it, I run a specialized archive website of digital historical documents focused specifically on telecommunications. Over the years I've scanned a few hundred thousand pages. However, I've been lucky enough to have access to the large scale scanners with automatic document feeders with ledger (11x17\") sized support at my job.\n\nUnfortunately I was laid off a couple weeks ago and no longer have this access. It's likely that whatever I do next will have much less access to an office that would have a high capacity scanner.\n\nSo I'm thinking about acquiring something for home use. I have literally hundreds of thousands of pages of documents in the long term queue. Specifications would be:\n\n* Supports up to 11x17\" ledger pages\n* Double sided scanning\n* Color, grey scale, and black and white\n* 400 dpi but 600 dpi as an option is preferable\n* Scan to PDF\n* Both automatic document feeder and platen scanning (some documents are close to 100 years old and sheet feeding them is a bad idea)\n* Preferably scan to something like Google Drive but scanning to e-mail is perfectly fine\n\nSearching around, a lot of these things seem to be pretty standard. The big sticking point for me seems to be the ledger sized support. A lot of these documents actually have fold outs that are larger than that, so I sometimes have to merge images, but having to do that with just letter or legal sized images is not fun.\n\nFor price range I'm willing to get into the low thousands. I've come across a few potential options but haven't had much luck finding reliable reviews. So any hands on personal experience that can be shared is valuable. Thanks!", "author_fullname": "t2_5xfq7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Free-Post Friday!] Recommendations for high volume document scanners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13m5u6i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684525792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this isn&amp;#39;t the right forum, but it is free post Friday sooooo..&lt;/p&gt;\n\n&lt;p&gt;To get to it, I run a specialized archive website of digital historical documents focused specifically on telecommunications. Over the years I&amp;#39;ve scanned a few hundred thousand pages. However, I&amp;#39;ve been lucky enough to have access to the large scale scanners with automatic document feeders with ledger (11x17&amp;quot;) sized support at my job.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately I was laid off a couple weeks ago and no longer have this access. It&amp;#39;s likely that whatever I do next will have much less access to an office that would have a high capacity scanner.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m thinking about acquiring something for home use. I have literally hundreds of thousands of pages of documents in the long term queue. Specifications would be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Supports up to 11x17&amp;quot; ledger pages&lt;/li&gt;\n&lt;li&gt;Double sided scanning&lt;/li&gt;\n&lt;li&gt;Color, grey scale, and black and white&lt;/li&gt;\n&lt;li&gt;400 dpi but 600 dpi as an option is preferable&lt;/li&gt;\n&lt;li&gt;Scan to PDF&lt;/li&gt;\n&lt;li&gt;Both automatic document feeder and platen scanning (some documents are close to 100 years old and sheet feeding them is a bad idea)&lt;/li&gt;\n&lt;li&gt;Preferably scan to something like Google Drive but scanning to e-mail is perfectly fine&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Searching around, a lot of these things seem to be pretty standard. The big sticking point for me seems to be the ledger sized support. A lot of these documents actually have fold outs that are larger than that, so I sometimes have to merge images, but having to do that with just letter or legal sized images is not fun.&lt;/p&gt;\n\n&lt;p&gt;For price range I&amp;#39;m willing to get into the low thousands. I&amp;#39;ve come across a few potential options but haven&amp;#39;t had much luck finding reliable reviews. So any hands on personal experience that can be shared is valuable. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13m5u6i", "is_robot_indexable": true, "report_reasons": null, "author": "bg-j38", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13m5u6i/freepost_friday_recommendations_for_high_volume/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13m5u6i/freepost_friday_recommendations_for_high_volume/", "subreddit_subscribers": 683508, "created_utc": 1684525792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_frbquaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SanDisk Extreme 2TB on sale for $120 right now on Amazon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_13m4e9k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JrQx06kcLib4HtRRBjo2XbKnpQimcDWc71r3t9cfaMA.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684522549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/kniimxaemv0b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/kniimxaemv0b1.jpg?auto=webp&amp;v=enabled&amp;s=141e2399c67e76af8d4dbf814abec3148c6e7f61", "width": 674, "height": 1165}, "resolutions": [{"url": "https://preview.redd.it/kniimxaemv0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5d553e8139724ff2477740ab750ab9fa548c7fbc", "width": 108, "height": 186}, {"url": "https://preview.redd.it/kniimxaemv0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6105ad04b40be672945301925eec0caeaedb05cd", "width": 216, "height": 373}, {"url": "https://preview.redd.it/kniimxaemv0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0bb080a6de796e0496faa6e1827844aeb368b513", "width": 320, "height": 553}, {"url": "https://preview.redd.it/kniimxaemv0b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72222d3d45983060c749262ed4b06b45157186da", "width": 640, "height": 1106}], "variants": {}, "id": "c5aX_VQ9iu0aENMyeVudEWlXEPhM50BZ7MToyyii7Yo"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "8TB RAID NAS", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "13m4e9k", "is_robot_indexable": true, "report_reasons": null, "author": "BroiledBoatmanship", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13m4e9k/sandisk_extreme_2tb_on_sale_for_120_right_now_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/kniimxaemv0b1.jpg", "subreddit_subscribers": 683508, "created_utc": 1684522549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have 53K Unmapped tracks out of 330K, so I would like to move them out of Lidarr without deleting them. The list shows all the paths, so it's a matter of minutes to make a move script, but I cannot just save/copy/export the entire list, only the visible portion.\n\nI have tried Data Miner and No Coding Data Scraper extensions, but the first doesn't allow to set a mouse scroll page (Lidarr has no Next/Previous page button, only slider) and the second has actually  extracted all the data, but costs a lot to allow export for this single task, and anyway crashes as soon as I click Yes to add credits (if I understand correctly, this would cost me 103$).\n\nAlternatively, is there any expert that can tell me how to extract Unmapped track paths from lidarr.db? I have opened it with DB Browser (SQL Lite), but I have no idea how to use it.\n\n\\[for those unaware, Lidarr is a selfhosted application to automate music albums download\\]\n\nEDIT I have asked already on /lidarr but got no useful answer", "author_fullname": "t2_fv3kf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any way to scrape Lidarr data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13m35l3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684520306.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684519742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 53K Unmapped tracks out of 330K, so I would like to move them out of Lidarr without deleting them. The list shows all the paths, so it&amp;#39;s a matter of minutes to make a move script, but I cannot just save/copy/export the entire list, only the visible portion.&lt;/p&gt;\n\n&lt;p&gt;I have tried Data Miner and No Coding Data Scraper extensions, but the first doesn&amp;#39;t allow to set a mouse scroll page (Lidarr has no Next/Previous page button, only slider) and the second has actually  extracted all the data, but costs a lot to allow export for this single task, and anyway crashes as soon as I click Yes to add credits (if I understand correctly, this would cost me 103$).&lt;/p&gt;\n\n&lt;p&gt;Alternatively, is there any expert that can tell me how to extract Unmapped track paths from lidarr.db? I have opened it with DB Browser (SQL Lite), but I have no idea how to use it.&lt;/p&gt;\n\n&lt;p&gt;[for those unaware, Lidarr is a selfhosted application to automate music albums download]&lt;/p&gt;\n\n&lt;p&gt;EDIT I have asked already on /lidarr but got no useful answer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13m35l3", "is_robot_indexable": true, "report_reasons": null, "author": "janaxhell", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13m35l3/any_way_to_scrape_lidarr_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13m35l3/any_way_to_scrape_lidarr_data/", "subreddit_subscribers": 683508, "created_utc": 1684519742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "How well does WD direct ship these drives?  I've been reading they just throw the electrostatic bag into a big shipping box that can be thrown/move around during shipping?  If I buy the drive in a retail store in comes in a nice secure retail small box/package", "author_fullname": "t2_80s831t1d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Direct Red Plus - Shipping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lzk2m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684511775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How well does WD direct ship these drives?  I&amp;#39;ve been reading they just throw the electrostatic bag into a big shipping box that can be thrown/move around during shipping?  If I buy the drive in a retail store in comes in a nice secure retail small box/package&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lzk2m", "is_robot_indexable": true, "report_reasons": null, "author": "onkyouser777", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lzk2m/wd_direct_red_plus_shipping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lzk2m/wd_direct_red_plus_shipping/", "subreddit_subscribers": 683508, "created_utc": 1684511775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently purchased two 2TB 990 Pros for my laptops so I can just hoard all my games rather than redownloading them all the time. I queued up 1.2TB would of downloads for the first drive and it started to download at 140MBps. After about an hour, I noticed the speeds dropped down to 45MBps. And then about an hour after that, down to 25-30MBps. The drive temps as reported by Samsung and HWmonitor64 showed 58c.\n\nI ended the test and ran the Samsung benchmark. after 30seconds the first half finished and the Reads looked normal, but once the writes finished it showed 1400MB/s for the sequential write vs the 6500 it showed when I first connected the drive. \n\nWhen I switch my downloads to the 2nd drive, the same exact thing happened. Everything was good for about an hour and then the same exact results. Now I tried my desktop that has a 1TB 980Pro and queued up 400GB worth of downloads and didn't see this issue. I am not sure if it did just not run long enough but I feel the 990 would have shown the issue already. I am not sure if these 990 Pros may have an issue or what could be going on. Does anyone have any ideas?", "author_fullname": "t2_4dx9vii", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Possible 990 Pro issue?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lze97", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684511428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently purchased two 2TB 990 Pros for my laptops so I can just hoard all my games rather than redownloading them all the time. I queued up 1.2TB would of downloads for the first drive and it started to download at 140MBps. After about an hour, I noticed the speeds dropped down to 45MBps. And then about an hour after that, down to 25-30MBps. The drive temps as reported by Samsung and HWmonitor64 showed 58c.&lt;/p&gt;\n\n&lt;p&gt;I ended the test and ran the Samsung benchmark. after 30seconds the first half finished and the Reads looked normal, but once the writes finished it showed 1400MB/s for the sequential write vs the 6500 it showed when I first connected the drive. &lt;/p&gt;\n\n&lt;p&gt;When I switch my downloads to the 2nd drive, the same exact thing happened. Everything was good for about an hour and then the same exact results. Now I tried my desktop that has a 1TB 980Pro and queued up 400GB worth of downloads and didn&amp;#39;t see this issue. I am not sure if it did just not run long enough but I feel the 990 would have shown the issue already. I am not sure if these 990 Pros may have an issue or what could be going on. Does anyone have any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lze97", "is_robot_indexable": true, "report_reasons": null, "author": "PersonSuitTV", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lze97/possible_990_pro_issue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lze97/possible_990_pro_issue/", "subreddit_subscribers": 683508, "created_utc": 1684511428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "As title says, I have 4x8TB drives in my NAS right now, and I'm kinda getting full. I considered just swapping out one or two of the drives with larger ones, rebuilding the RAID, but then I'd just have spare 8TB drives laying around.\n\nI think a better option would be for me to just get a new NAS box with more bays and just add more drives that way. I've been kinda wanting to get rid of this NAS anyway since the CPU in it is a bit slow on transcoding videos.\n\nMy main concern is, should I even go with upgrading to a box with more bays or should I just swap out the drives I do have and just keep my current NAS?\n\nSecond concern and probably a really newbie question: What kind of issues might I run into if I do get a new NAS box to put these current drives into, since they are in RAID 5 already? Would a new NAS just see the RAID like normal or do I have to do a particular setup before I take the drives out of the old one? I'm not sure if I'm going to stick with an Asustor brand NAS if I do upgrade the box or if I should go with something else. I'm trying to get the best bang for my buck too so its not like I have unlimited budget.\n\nAny help would be appreciated!", "author_fullname": "t2_iv30x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on upgrading NAS, from Asustor 4-bay with 4x8TB drives in RAID 5", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ls273", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684493680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As title says, I have 4x8TB drives in my NAS right now, and I&amp;#39;m kinda getting full. I considered just swapping out one or two of the drives with larger ones, rebuilding the RAID, but then I&amp;#39;d just have spare 8TB drives laying around.&lt;/p&gt;\n\n&lt;p&gt;I think a better option would be for me to just get a new NAS box with more bays and just add more drives that way. I&amp;#39;ve been kinda wanting to get rid of this NAS anyway since the CPU in it is a bit slow on transcoding videos.&lt;/p&gt;\n\n&lt;p&gt;My main concern is, should I even go with upgrading to a box with more bays or should I just swap out the drives I do have and just keep my current NAS?&lt;/p&gt;\n\n&lt;p&gt;Second concern and probably a really newbie question: What kind of issues might I run into if I do get a new NAS box to put these current drives into, since they are in RAID 5 already? Would a new NAS just see the RAID like normal or do I have to do a particular setup before I take the drives out of the old one? I&amp;#39;m not sure if I&amp;#39;m going to stick with an Asustor brand NAS if I do upgrade the box or if I should go with something else. I&amp;#39;m trying to get the best bang for my buck too so its not like I have unlimited budget.&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ls273", "is_robot_indexable": true, "report_reasons": null, "author": "ryohazuki224", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ls273/advice_on_upgrading_nas_from_asustor_4bay_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ls273/advice_on_upgrading_nas_from_asustor_4bay_with/", "subreddit_subscribers": 683508, "created_utc": 1684493680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Mine is a small accounting firm of around 8 staff. The data is shared from the following PC.  \nAMD Ryzen 5 3400G  \nCrucial 16GB DDR4 2133MHz  \nSilicon Power 512GB M.2 PCIe SSD (SP512GBP34A80)  \nD: (350GB, Shared through network)  \n3 Other PCs and 5 Laptops uses the shared data from this D drive by LAN and Wi-Fi.  The Drive used for the past 3 years is 70GB only. But expecting 100GB per year in the next few years. Need a backup plan for next 3 years after which will buy a NAS instead of file sharing. No single file will exceed a maximum of 5MB. Some pdf's may be around 10MB. But almost 80% of the files are less than 1MB. Most of the data are excel, word, pdf, jpg, Tally. Some data (For eg: Tally, an accounting software) are used by more than 1 staff at a time. They simultaneously read and write on the same file. The backed up files should be accessible file by file at any time.  \n1. External Drive  \n2. Online  \nFor External Drive, whether to use SSD or HDD? I hope 1TB SSD is more than enough for our requirement. How do I back up those files in that drive twice a week?   \nI don't have any idea about online backup though I read the wiki of this subreddit. Based on the above info can anyone suggest me a backup plan?", "author_fullname": "t2_4p34arxg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup setup for a small office.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lm72d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684474609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mine is a small accounting firm of around 8 staff. The data is shared from the following PC.&lt;br/&gt;\nAMD Ryzen 5 3400G&lt;br/&gt;\nCrucial 16GB DDR4 2133MHz&lt;br/&gt;\nSilicon Power 512GB M.2 PCIe SSD (SP512GBP34A80)&lt;br/&gt;\nD: (350GB, Shared through network)&lt;br/&gt;\n3 Other PCs and 5 Laptops uses the shared data from this D drive by LAN and Wi-Fi.  The Drive used for the past 3 years is 70GB only. But expecting 100GB per year in the next few years. Need a backup plan for next 3 years after which will buy a NAS instead of file sharing. No single file will exceed a maximum of 5MB. Some pdf&amp;#39;s may be around 10MB. But almost 80% of the files are less than 1MB. Most of the data are excel, word, pdf, jpg, Tally. Some data (For eg: Tally, an accounting software) are used by more than 1 staff at a time. They simultaneously read and write on the same file. The backed up files should be accessible file by file at any time.&lt;br/&gt;\n1. External Drive&lt;br/&gt;\n2. Online&lt;br/&gt;\nFor External Drive, whether to use SSD or HDD? I hope 1TB SSD is more than enough for our requirement. How do I back up those files in that drive twice a week?&lt;br/&gt;\nI don&amp;#39;t have any idea about online backup though I read the wiki of this subreddit. Based on the above info can anyone suggest me a backup plan?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lm72d", "is_robot_indexable": true, "report_reasons": null, "author": "MohanKumar2010", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lm72d/backup_setup_for_a_small_office/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lm72d/backup_setup_for_a_small_office/", "subreddit_subscribers": 683508, "created_utc": 1684474609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've had a Qnap NAS for a few years to hold my backups (via Acronis,) and I've recently gotten a new one. I want to still use the old one for another layer of backup, but maybe this is a trivial question but does it make more sense to have a separate Acronis backup routine set up to use the old NAS, perhaps less often and only more critical stuff; or should I be using the new NAS' Snapshots to backup my backups?", "author_fullname": "t2_3mkur2v7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategy for second-level backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13li570", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684462416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve had a Qnap NAS for a few years to hold my backups (via Acronis,) and I&amp;#39;ve recently gotten a new one. I want to still use the old one for another layer of backup, but maybe this is a trivial question but does it make more sense to have a separate Acronis backup routine set up to use the old NAS, perhaps less often and only more critical stuff; or should I be using the new NAS&amp;#39; Snapshots to backup my backups?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13li570", "is_robot_indexable": true, "report_reasons": null, "author": "DeliciousPool5", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13li570/strategy_for_secondlevel_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13li570/strategy_for_secondlevel_backup/", "subreddit_subscribers": 683508, "created_utc": 1684462416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux Multi-Volume LTO4 Tape Backup Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lexiv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_rvam4", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "homelab", "selftext": "Hi there! Long time lurker, first time poster...\n\nIn my homelab environment, I have a few VMs running on an ESXi hypervisor that serve Samba shares for various services on different subnets. All of the data is stored on a RAID array, and unfortunately, I've had two 4TB drives fail on me in the past year. This has prompted me to start backing up to LTO4 tapes as I have a drive kicking around because I'm starting to seriously doubt that the disks will hold up in the long term, and I worry about a failed rebuild after rebuilding multiple times.\n\nFor some background, the way that I'm performing this tape backup is over the network. So, I have the tape drive connected to my physical Linux workstation, and I'm mounting the Samba shares in the /mnt directory (so, /mnt/smb-share1 for example).\n\nNow,  in the past I've never had issues creating tape backups that are to a single tape. I'm trying to back up one of these Samba shares, and this one in particular is about 1.2TB in size, which exceeds the 800GB uncompressed limitation of the LTO4 standard. I also do not want to compress the data going to the tapes. As a result, I need to create multi-volume tar backups.\n\nWhen I tried creating the multi-volume backup, I was able to write all 1.2TB spanning two tapes, but when I \"mock\" a restore using \"tar -tvf\", the second tape fails with the error \"tar: \u2018./example/example.zip\u2019 is not continued on this volume\".\n\nSo, for my question... how should I be creating these backups? I'm not sure if mbuffer is the issue here, but I really would prefer to continue using it to prevent buffer underruns, which isn't good for the tape or the tape drive. Here's the two commands that I'm using...\n\nWriting to the tapes:\n\n    # cd /mnt/smb-share1\n    # tar -b 4096 --directory=\"/mnt/smb-share1\" --multi-volume --one-file-system --xattrs -cf - ./ | mbuffer -m 2G -L -P 95 -f -o /dev/st0\n\n\"Restoring\" (just reading each file) from the tapes:\n\n    # tar -b 4096 --multi-volume -tvf /dev/st0 | tee /home/cjms/Documents/TAPE-BACKUP-CONTENTS.TXT\n    drwxr-xr-x cjms/cjms            0 2019-08-14 14:56 ./dir0/\n    ...\n    -rwxr-xr-x cjms/cjms   8999733302 2021-06-24 05:06 ./dir9/someFile.7z\n    Prepare volume #2 for \u2018/dev/st0\u2019 and hit return: [return]\n    tar: \u2018./dir9/someFile.7z\u2019 is not continued on this volume\n    Prepare volume #2 for \u2018/dev/st0\u2019 and hit return: \n\nAnyone have any suggestions? The operating system I'm performing this on is Rocky Linux 9.1. I do not want to use a proprietary/paid solution for this... tar is the way!\n\nThanks!", "author_fullname": "t2_rvam4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux Multi-Volume LTO4 Tape Backup Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/homelab", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13leuir", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684453673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there! Long time lurker, first time poster...&lt;/p&gt;\n\n&lt;p&gt;In my homelab environment, I have a few VMs running on an ESXi hypervisor that serve Samba shares for various services on different subnets. All of the data is stored on a RAID array, and unfortunately, I&amp;#39;ve had two 4TB drives fail on me in the past year. This has prompted me to start backing up to LTO4 tapes as I have a drive kicking around because I&amp;#39;m starting to seriously doubt that the disks will hold up in the long term, and I worry about a failed rebuild after rebuilding multiple times.&lt;/p&gt;\n\n&lt;p&gt;For some background, the way that I&amp;#39;m performing this tape backup is over the network. So, I have the tape drive connected to my physical Linux workstation, and I&amp;#39;m mounting the Samba shares in the /mnt directory (so, /mnt/smb-share1 for example).&lt;/p&gt;\n\n&lt;p&gt;Now,  in the past I&amp;#39;ve never had issues creating tape backups that are to a single tape. I&amp;#39;m trying to back up one of these Samba shares, and this one in particular is about 1.2TB in size, which exceeds the 800GB uncompressed limitation of the LTO4 standard. I also do not want to compress the data going to the tapes. As a result, I need to create multi-volume tar backups.&lt;/p&gt;\n\n&lt;p&gt;When I tried creating the multi-volume backup, I was able to write all 1.2TB spanning two tapes, but when I &amp;quot;mock&amp;quot; a restore using &amp;quot;tar -tvf&amp;quot;, the second tape fails with the error &amp;quot;tar: \u2018./example/example.zip\u2019 is not continued on this volume&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;So, for my question... how should I be creating these backups? I&amp;#39;m not sure if mbuffer is the issue here, but I really would prefer to continue using it to prevent buffer underruns, which isn&amp;#39;t good for the tape or the tape drive. Here&amp;#39;s the two commands that I&amp;#39;m using...&lt;/p&gt;\n\n&lt;p&gt;Writing to the tapes:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# cd /mnt/smb-share1\n# tar -b 4096 --directory=&amp;quot;/mnt/smb-share1&amp;quot; --multi-volume --one-file-system --xattrs -cf - ./ | mbuffer -m 2G -L -P 95 -f -o /dev/st0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;quot;Restoring&amp;quot; (just reading each file) from the tapes:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# tar -b 4096 --multi-volume -tvf /dev/st0 | tee /home/cjms/Documents/TAPE-BACKUP-CONTENTS.TXT\ndrwxr-xr-x cjms/cjms            0 2019-08-14 14:56 ./dir0/\n...\n-rwxr-xr-x cjms/cjms   8999733302 2021-06-24 05:06 ./dir9/someFile.7z\nPrepare volume #2 for \u2018/dev/st0\u2019 and hit return: [return]\ntar: \u2018./dir9/someFile.7z\u2019 is not continued on this volume\nPrepare volume #2 for \u2018/dev/st0\u2019 and hit return: \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Anyone have any suggestions? The operating system I&amp;#39;m performing this on is Rocky Linux 9.1. I do not want to use a proprietary/paid solution for this... tar is the way!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "664a26e4-322a-11e6-80ae-0e0378709321", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2ubz7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff6347", "id": "13leuir", "is_robot_indexable": true, "report_reasons": null, "author": "cjmspartans96", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/homelab/comments/13leuir/linux_multivolume_lto4_tape_backup_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/homelab/comments/13leuir/linux_multivolume_lto4_tape_backup_question/", "subreddit_subscribers": 572239, "created_utc": 1684453673.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1684453887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/homelab/comments/13leuir/linux_multivolume_lto4_tape_backup_question/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lexiv", "is_robot_indexable": true, "report_reasons": null, "author": "cjmspartans96", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13leuir", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lexiv/linux_multivolume_lto4_tape_backup_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/homelab/comments/13leuir/linux_multivolume_lto4_tape_backup_question/", "subreddit_subscribers": 683508, "created_utc": 1684453887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nRecently had a hard drive die on me that contained lots of projects and data (unfortunately did not back this data up, my negligence..). So to avoid this issue ever happening again, I'm looking into purchasing a HDD docking station that supports up to 2 drives to back up my data on a routinely basis. SSD or HD, either or.\n\nI use a Macbook Pro M1 for my daily driver, music projects, some editing/motion graphics. I also have a PC for the heavy lifting of 3D work and more intense motion graphic projects (due to the dedicated video card).\n\nMy question is, if I were to purchase a docking bay that accepts 2 drives or more, can I use the docking station on both my Mac and PC? I do realize you can format the HD to whatever OS I am connecting it to, however, can I plug the docking station into both my computers separately? For instance, I would set one HD dedicated for PC backups and the other HD dedicated for my Macbook backups (One drive format to NTFS for Windows and the other drive format to MacOS extended for Time Machine).\n\nI have searched online and cannot find the specific answer to this question, so anything would be helpful!", "author_fullname": "t2_128wmf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD Docking Bay Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13le06t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684451514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Recently had a hard drive die on me that contained lots of projects and data (unfortunately did not back this data up, my negligence..). So to avoid this issue ever happening again, I&amp;#39;m looking into purchasing a HDD docking station that supports up to 2 drives to back up my data on a routinely basis. SSD or HD, either or.&lt;/p&gt;\n\n&lt;p&gt;I use a Macbook Pro M1 for my daily driver, music projects, some editing/motion graphics. I also have a PC for the heavy lifting of 3D work and more intense motion graphic projects (due to the dedicated video card).&lt;/p&gt;\n\n&lt;p&gt;My question is, if I were to purchase a docking bay that accepts 2 drives or more, can I use the docking station on both my Mac and PC? I do realize you can format the HD to whatever OS I am connecting it to, however, can I plug the docking station into both my computers separately? For instance, I would set one HD dedicated for PC backups and the other HD dedicated for my Macbook backups (One drive format to NTFS for Windows and the other drive format to MacOS extended for Time Machine).&lt;/p&gt;\n\n&lt;p&gt;I have searched online and cannot find the specific answer to this question, so anything would be helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13le06t", "is_robot_indexable": true, "report_reasons": null, "author": "anduroox", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13le06t/hdd_docking_bay_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13le06t/hdd_docking_bay_question/", "subreddit_subscribers": 683508, "created_utc": 1684451514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am looking into S3-compatible storage solutions to use for a private file host and I have my eye on a couple but I thought I would ask here to see if there are any better alternatives. The ones that I am currently looking at are Cloudflare R2 and Backblaze B2 which both seem like great options but I am not sure which would be best to use for a file host. \n\nThey both provide 10GB of free storage which is nice and Backblaze's storage is only $0.005/GB/Month compared to Cloudflare's $0.015/GB/Month however Cloudflare has no egress fee whereas Backblaze has a fee of $0.01/GB with the first 1GB being free each day. \n\nThey are both solid solutions but I am not sure which would be cheaper for my use case (or if there is a better alternative).", "author_fullname": "t2_qb1ud93i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best S3-compatible storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ldn8q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684450642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am looking into S3-compatible storage solutions to use for a private file host and I have my eye on a couple but I thought I would ask here to see if there are any better alternatives. The ones that I am currently looking at are Cloudflare R2 and Backblaze B2 which both seem like great options but I am not sure which would be best to use for a file host. &lt;/p&gt;\n\n&lt;p&gt;They both provide 10GB of free storage which is nice and Backblaze&amp;#39;s storage is only $0.005/GB/Month compared to Cloudflare&amp;#39;s $0.015/GB/Month however Cloudflare has no egress fee whereas Backblaze has a fee of $0.01/GB with the first 1GB being free each day. &lt;/p&gt;\n\n&lt;p&gt;They are both solid solutions but I am not sure which would be cheaper for my use case (or if there is a better alternative).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ldn8q", "is_robot_indexable": true, "report_reasons": null, "author": "wraithdotcat", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ldn8q/best_s3compatible_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ldn8q/best_s3compatible_storage/", "subreddit_subscribers": 683508, "created_utc": 1684450642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, anyone have any experience with using this board in a NAS? I'm looking to upgrade my aging Unraid server to this hardware I found on Aliexpress.\n\nHowever, I've seen some issues on forums with the intel i225V networking card. Not sure if the B3 revision fixed all the issues though. There's a version with the upgraded i226 though but that costs a bit more money.\n\n[https://www.aliexpress.us/item/3256804692292280.html?gatewayAdapt=glo2usa](https://www.aliexpress.us/item/3256804692292280.html?gatewayAdapt=glo2usa)\n\nThanks!", "author_fullname": "t2_mw23q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Celeron N5105 and intel i225V issues?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lb1p1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684444355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, anyone have any experience with using this board in a NAS? I&amp;#39;m looking to upgrade my aging Unraid server to this hardware I found on Aliexpress.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;ve seen some issues on forums with the intel i225V networking card. Not sure if the B3 revision fixed all the issues though. There&amp;#39;s a version with the upgraded i226 though but that costs a bit more money.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.aliexpress.us/item/3256804692292280.html?gatewayAdapt=glo2usa\"&gt;https://www.aliexpress.us/item/3256804692292280.html?gatewayAdapt=glo2usa&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?auto=webp&amp;v=enabled&amp;s=882bc7a4cdfe98ed5d2e92707c4349f1cff944e3", "width": 1000, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8688778458e5b0bd843a30cb9b898d307151def5", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74f8e008599638040fbda8db23f0ec3753bb026c", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a029381962923f25329fb565ae8f48c2cc5fb18c", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f79f8bbcd7e3494cff477f04e153871a852cc950", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=969a85586b8c509bd2eac9dee4868ef7eb7be8f5", "width": 960, "height": 960}], "variants": {}, "id": "B7qADUhmpPLVxwZKQ4Xno3NxYIVvrGoWisHxswCSnWY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lb1p1", "is_robot_indexable": true, "report_reasons": null, "author": "errr_mah_gawsh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lb1p1/celeron_n5105_and_intel_i225v_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lb1p1/celeron_n5105_and_intel_i225v_issues/", "subreddit_subscribers": 683508, "created_utc": 1684444355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My organization recently switched from Google Workspace to Sharepoint, and it's been a major pain in the ass ever since, as trying to use OD on the mac has been a disaster.\n\nThe OD app for mac is just... bad, especially compared with GDrive. On two different devices, it goes rogue consistently, even after a fresh install and setup - frequent beachballs in the menu bar, and it just disregards the fact I don't want it to occupy all of my disk space. I did use all of the (very little) options to avoid disk space use, and I did use the \"free space\" context menu command, which does nothing anyway. It just decides to occupy 10-80 GBs erratically, even if every single file and folder has the little cloud thingy meaning the should not be residing locally.\n\nI would use OD for the following:\n\n1) sync a few folders (one-way only) when needed, while working on local files;\n\n2) access these folders on different devices when needed;\n\n3) store some very large files that I don't need to access often I know OD is not meant to be used this way, and should be more of a sync, but I still have 5TB available and I need a remote backup.\n\nI could use rclone for all of the above, but 1) it's noticeably slower than rsync, which I have used successfully to date; 2) some kind of integration with the Mac finder (navigating dirs, searching files) would be nice.\n\nIs Mountain Duck worth using in my case? Or should I try mounting the OD filesystem with rclone and tinker some more?", "author_fullname": "t2_4u2z7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some guidance for OneDrive on Mac", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lajnv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684443197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My organization recently switched from Google Workspace to Sharepoint, and it&amp;#39;s been a major pain in the ass ever since, as trying to use OD on the mac has been a disaster.&lt;/p&gt;\n\n&lt;p&gt;The OD app for mac is just... bad, especially compared with GDrive. On two different devices, it goes rogue consistently, even after a fresh install and setup - frequent beachballs in the menu bar, and it just disregards the fact I don&amp;#39;t want it to occupy all of my disk space. I did use all of the (very little) options to avoid disk space use, and I did use the &amp;quot;free space&amp;quot; context menu command, which does nothing anyway. It just decides to occupy 10-80 GBs erratically, even if every single file and folder has the little cloud thingy meaning the should not be residing locally.&lt;/p&gt;\n\n&lt;p&gt;I would use OD for the following:&lt;/p&gt;\n\n&lt;p&gt;1) sync a few folders (one-way only) when needed, while working on local files;&lt;/p&gt;\n\n&lt;p&gt;2) access these folders on different devices when needed;&lt;/p&gt;\n\n&lt;p&gt;3) store some very large files that I don&amp;#39;t need to access often I know OD is not meant to be used this way, and should be more of a sync, but I still have 5TB available and I need a remote backup.&lt;/p&gt;\n\n&lt;p&gt;I could use rclone for all of the above, but 1) it&amp;#39;s noticeably slower than rsync, which I have used successfully to date; 2) some kind of integration with the Mac finder (navigating dirs, searching files) would be nice.&lt;/p&gt;\n\n&lt;p&gt;Is Mountain Duck worth using in my case? Or should I try mounting the OD filesystem with rclone and tinker some more?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lajnv", "is_robot_indexable": true, "report_reasons": null, "author": "zen_arcade", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lajnv/need_some_guidance_for_onedrive_on_mac/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lajnv/need_some_guidance_for_onedrive_on_mac/", "subreddit_subscribers": 683508, "created_utc": 1684443197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Bottom text.", "author_fullname": "t2_3ta0ift9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roblox blocks third-party apps these days. Is it still not possible again to rip assets like music and 3D models from Roblox games?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13m5o64", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684525398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bottom text.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13m5o64", "is_robot_indexable": true, "report_reasons": null, "author": "throwagayaccount93", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13m5o64/roblox_blocks_thirdparty_apps_these_days_is_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13m5o64/roblox_blocks_thirdparty_apps_these_days_is_it/", "subreddit_subscribers": 683508, "created_utc": 1684525398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI am looking to get an empty 2-drive USB RAID enclosure.\n\nIn Australia, my options are perhaps more limited - either by availability or pricing - than those in other countries such that my current options are:\n\n&amp;#x200B;\n\n* **ICY BOX IB-RD3621U3** ($135AUD)\n* **Orico NS200-3U** ($150AUD)\n* **QNAP TR-002** ($280AUD)\n\n&amp;#x200B;\n\nThe main questions I have are:\n\n&amp;#x200B;\n\n1. Is there anything meaningful to choose between the ICY BOX and the Orico?\n2. Is there any real benefit in spending twice as much for the QNAP?\n\n&amp;#x200B;\n\nTo expand, I already have 2 HDDs to use with this setup so this is a way to relatively cheaply add storage for my single PC.\n\n&amp;#x200B;\n\nI will be using this in RAID-1 configuration and my primary criterion is that the unit not in any way do anything funny with the drives that might jeopardise my ability to pull one out and (for example,) connect it to a computer, independent of the enclosure.\n\n&amp;#x200B;\n\nIdeally, I would also like something *quiet* as it will be 2ft away from me.\n\n&amp;#x200B;\n\nIn order to streamline and head off a few questions:\n\n&amp;#x200B;\n\n* I do NOT need or want a NAS.\n* I understand RAID, as a technology. If there is some specific implementation differences with the above units, that would certainly be relevant.\n* I want USB, specifically.\n\n&amp;#x200B;\n\nDoes anyone have any experience with any of these units?\n\n&amp;#x200B;\n\nMany thanks.", "author_fullname": "t2_blbae9w0b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USB RAID enclosures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lmvxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684493620.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684476880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am looking to get an empty 2-drive USB RAID enclosure.&lt;/p&gt;\n\n&lt;p&gt;In Australia, my options are perhaps more limited - either by availability or pricing - than those in other countries such that my current options are:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;ICY BOX IB-RD3621U3&lt;/strong&gt; ($135AUD)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Orico NS200-3U&lt;/strong&gt; ($150AUD)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;QNAP TR-002&lt;/strong&gt; ($280AUD)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The main questions I have are:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is there anything meaningful to choose between the ICY BOX and the Orico?&lt;/li&gt;\n&lt;li&gt;Is there any real benefit in spending twice as much for the QNAP?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;To expand, I already have 2 HDDs to use with this setup so this is a way to relatively cheaply add storage for my single PC.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I will be using this in RAID-1 configuration and my primary criterion is that the unit not in any way do anything funny with the drives that might jeopardise my ability to pull one out and (for example,) connect it to a computer, independent of the enclosure.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Ideally, I would also like something &lt;em&gt;quiet&lt;/em&gt; as it will be 2ft away from me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In order to streamline and head off a few questions:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I do NOT need or want a NAS.&lt;/li&gt;\n&lt;li&gt;I understand RAID, as a technology. If there is some specific implementation differences with the above units, that would certainly be relevant.&lt;/li&gt;\n&lt;li&gt;I want USB, specifically.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any experience with any of these units?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Many thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lmvxa", "is_robot_indexable": true, "report_reasons": null, "author": "CaptDanReddy", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lmvxa/usb_raid_enclosures/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lmvxa/usb_raid_enclosures/", "subreddit_subscribers": 683508, "created_utc": 1684476880.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}