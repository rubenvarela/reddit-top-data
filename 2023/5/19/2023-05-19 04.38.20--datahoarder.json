{"kind": "Listing", "data": {"after": "t3_13kyl8c", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_cvmj7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "pre2010 - I'm not the only one am I? (before I understood how to backup ISO's.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13l8kk0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 367, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 367, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/FUUVdIT4kHh1_pS1wEi5LWCct--NzMAOZ4Kg6Krvkfk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684438594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/wagfcg1w6n0b1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?auto=webp&amp;v=enabled&amp;s=8cfaa8f207685b75b8505c74b2e941f1d0bba858", "width": 3264, "height": 1840}, "resolutions": [{"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe038705a925708deae688b5143f366792d422b2", "width": 108, "height": 60}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bae8109df559c09243d1422980bd7db1fef284e", "width": 216, "height": 121}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fb101af111a49f1a9782f99fb4fbab14534ca82d", "width": 320, "height": 180}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec4a4e03151c5d2be8d447b7d06c1bdd3df23bac", "width": 640, "height": 360}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0399cbec679a9779135707ff3722d1e11edf395e", "width": 960, "height": 541}, {"url": "https://preview.redd.it/wagfcg1w6n0b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a332c548fc01cbef9d77e42c7400da4d2a632d45", "width": 1080, "height": 608}], "variants": {}, "id": "a5qT0VYDfmbh3wXd7lE4_LDRW7CqKoIEhARxyIkUod8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13l8kk0", "is_robot_indexable": true, "report_reasons": null, "author": "m3arls", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l8kk0/pre2010_im_not_the_only_one_am_i_before_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/wagfcg1w6n0b1.jpg", "subreddit_subscribers": 683385, "created_utc": 1684438594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_57o7t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dropbox: After four years of SMR storage, here's what we love\u2014and what comes next", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_13kqy64", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 298, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 298, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3bdhj-XDo3ZX3GZvoJoeB0S1SiC8J4QHjV6TfmP5d7k.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684392618.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dropbox.tech", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dropbox.tech/infrastructure/four-years-of-smr-storage-what-we-love-and-whats-next", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?auto=webp&amp;v=enabled&amp;s=b21c2418753b5628f18962d82d3d9a626e76eebc", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85709443795963dfeab8995e54970230fd8eefb1", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=93c9d1523dd8f7432ed9eafd3c450696a44216db", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ba255c083eef77e0397f347c9d995542f19a05a", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23dcfc3cbf1fb0264f6085b55da28d3de3139329", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f7754ae0c9bc1e9d66cc67b1545e633656f1c66", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/w9o049_q2hhPwD3Ivxgom2MmZy0DHK4rEqwwqhZoXb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b541b658216c00c466fe1af6ffcb5a89dc44cbf3", "width": 1080, "height": 565}], "variants": {}, "id": "Fu1Qer5s91j2J3UBkhCf3H59eCu9Ane8MkOnvbEvgxs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "152 TB ZFS", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13kqy64", "is_robot_indexable": true, "report_reasons": null, "author": "callcifer", "discussion_type": null, "num_comments": 69, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13kqy64/dropbox_after_four_years_of_smr_storage_heres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dropbox.tech/infrastructure/four-years-of-smr-storage-what-we-love-and-whats-next", "subreddit_subscribers": 683385, "created_utc": 1684392618.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Well was about time until last week it would show unlimited in drive. That was even after the forced migrations and everything \n\n&amp;#x200B;\n\n1 user - no limit would be shown in GDrive.\n\n&amp;#x200B;\n\nJust recieved an email that I was over quota - being limited to 2TB now if I check my GDrive.\n\n&amp;#x200B;\n\n(had 3TB)\n\n&amp;#x200B;\n\nWell, good bye Google!", "author_fullname": "t2_5gi1u304", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google enforcing storage limit on old GSuite unlimited Accounts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kr75h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 63, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 63, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684393430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Well was about time until last week it would show unlimited in drive. That was even after the forced migrations and everything &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1 user - no limit would be shown in GDrive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Just recieved an email that I was over quota - being limited to 2TB now if I check my GDrive.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;(had 3TB)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Well, good bye Google!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kr75h", "is_robot_indexable": true, "report_reasons": null, "author": "Goose-Difficult", "discussion_type": null, "num_comments": 87, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kr75h/google_enforcing_storage_limit_on_old_gsuite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kr75h/google_enforcing_storage_limit_on_old_gsuite/", "subreddit_subscribers": 683385, "created_utc": 1684393430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Imgur had a rule in place that pornographic content would be removed. At the beginning of May, I could no longer post pornographic content on the site. An error message was displayed.\n\nBut since a few days, I can post porn again and my nsfw content has not been deleted. Could someone tell me why?", "author_fullname": "t2_uobphu1e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Imgur allows nsfw again? I don't understand.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l5ahd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684431196.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Imgur had a rule in place that pornographic content would be removed. At the beginning of May, I could no longer post pornographic content on the site. An error message was displayed.&lt;/p&gt;\n\n&lt;p&gt;But since a few days, I can post porn again and my nsfw content has not been deleted. Could someone tell me why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l5ahd", "is_robot_indexable": true, "report_reasons": null, "author": "Eatomi", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l5ahd/imgur_allows_nsfw_again_i_dont_understand/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l5ahd/imgur_allows_nsfw_again_i_dont_understand/", "subreddit_subscribers": 683385, "created_utc": 1684431196.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nJust got the dreaded email for my Google Workspace Enterprise Standard account that I've been using for almost 3 years now with about 30TB in drive.\n\nI recently set up my NAS of 54TB so atleast I won't lose my data immediately and was using Google Drive as a cloud backup solution. I was paying $15/mo for this, incredible deal!\n\nI use rclone to encrypt my files and every week, it checks for any changes between my local &amp; backup and syncs the backup accordingly. \n\nMy question, is what do I move to now? Dropbox will cost about $72-75/mo and they can change their policy for unlimited storage similar to google at any time.  \nOr Hetzner server auction will cost me about $80/mo but I'll have more control over it, albeit still under Hetzner.\n\nBackblaze becomes too expensive at around $150/ mo ($5\\*30TB).  \nWhat are you switching too or already using for as a cloud backup that maybe i cheaper? Spending $80/mo on cloud storage for a backup just seems a lot :(", "author_fullname": "t2_cxhmpy2s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive alternative for cloud backup; 30TB +1TB/year", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ksggt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684397711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Just got the dreaded email for my Google Workspace Enterprise Standard account that I&amp;#39;ve been using for almost 3 years now with about 30TB in drive.&lt;/p&gt;\n\n&lt;p&gt;I recently set up my NAS of 54TB so atleast I won&amp;#39;t lose my data immediately and was using Google Drive as a cloud backup solution. I was paying $15/mo for this, incredible deal!&lt;/p&gt;\n\n&lt;p&gt;I use rclone to encrypt my files and every week, it checks for any changes between my local &amp;amp; backup and syncs the backup accordingly. &lt;/p&gt;\n\n&lt;p&gt;My question, is what do I move to now? Dropbox will cost about $72-75/mo and they can change their policy for unlimited storage similar to google at any time.&lt;br/&gt;\nOr Hetzner server auction will cost me about $80/mo but I&amp;#39;ll have more control over it, albeit still under Hetzner.&lt;/p&gt;\n\n&lt;p&gt;Backblaze becomes too expensive at around $150/ mo ($5*30TB).&lt;br/&gt;\nWhat are you switching too or already using for as a cloud backup that maybe i cheaper? Spending $80/mo on cloud storage for a backup just seems a lot :(&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ksggt", "is_robot_indexable": true, "report_reasons": null, "author": "seriouslyfun95", "discussion_type": null, "num_comments": 63, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ksggt/google_drive_alternative_for_cloud_backup_30tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ksggt/google_drive_alternative_for_cloud_backup_30tb/", "subreddit_subscribers": 683385, "created_utc": 1684397711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI just want to ask if this is a good setup for a 3-2-1 backup?\n\nMy PC:\n\n* 1TB Samsung 980 Pro = OS Drive\n* 2TB Teamgroup MP34 = Games/Software/Work Drive\n* 8TB WD Blue WD80EAZZ = First BackUp synced with backblaze personal\n* 8TB WD Blue WD80EAZZ = As Cold Storage via external hard drive, stored in the closet\n\nIs this a good starting point?\n\nI'm currently using Beyond Compare 4 to update the cold storage from time to time (probably once a week)\n\nWhat is the difference actually with FreeFileSync and Beyond Compare 4?\n\nEDIT: Follow up question, how do you backup a NAS?? Do you buy another NAS?", "author_fullname": "t2_solnjf6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this a good setup for a photographer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l5z5b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684463979.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684432866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I just want to ask if this is a good setup for a 3-2-1 backup?&lt;/p&gt;\n\n&lt;p&gt;My PC:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1TB Samsung 980 Pro = OS Drive&lt;/li&gt;\n&lt;li&gt;2TB Teamgroup MP34 = Games/Software/Work Drive&lt;/li&gt;\n&lt;li&gt;8TB WD Blue WD80EAZZ = First BackUp synced with backblaze personal&lt;/li&gt;\n&lt;li&gt;8TB WD Blue WD80EAZZ = As Cold Storage via external hard drive, stored in the closet&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Is this a good starting point?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently using Beyond Compare 4 to update the cold storage from time to time (probably once a week)&lt;/p&gt;\n\n&lt;p&gt;What is the difference actually with FreeFileSync and Beyond Compare 4?&lt;/p&gt;\n\n&lt;p&gt;EDIT: Follow up question, how do you backup a NAS?? Do you buy another NAS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l5z5b", "is_robot_indexable": true, "report_reasons": null, "author": "v4rmilo", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l5z5b/is_this_a_good_setup_for_a_photographer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l5z5b/is_this_a_good_setup_for_a_photographer/", "subreddit_subscribers": 683385, "created_utc": 1684432866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "For example when you add 5 Mb file to 50 Gb rar archive, there will be 50 Gb written to disk again. It's very slow and inefficient. Does all archive formats has this limitation?", "author_fullname": "t2_pkh7uvi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to add file to an exisiting archive without fully overwriting it on disk?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kv4n8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684406227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example when you add 5 Mb file to 50 Gb rar archive, there will be 50 Gb written to disk again. It&amp;#39;s very slow and inefficient. Does all archive formats has this limitation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kv4n8", "is_robot_indexable": true, "report_reasons": null, "author": "Animus_777", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kv4n8/is_it_possible_to_add_file_to_an_exisiting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kv4n8/is_it_possible_to_add_file_to_an_exisiting/", "subreddit_subscribers": 683385, "created_utc": 1684406227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, I recently bought an 8TB WD Easystore external drive off of FB marketplace. I plugged it into my computer (HP Pavilion gaming laptop) and it recognizes it....but says its capacity is 500mb, which makes no sense. Am I doing something wrong? Also, it won't allow me to look for a driver, maybe that's the issue? I know nothing about this stuff, I just wanted an external drive to move stuff over to that I could access while using my laptop because my storage is full. Is this fixable or did I buya bunk drive/the wrong thing? thanks", "author_fullname": "t2_d4k4hzfz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Easystore 8TB problems with Windows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lf3js", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684454330.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I recently bought an 8TB WD Easystore external drive off of FB marketplace. I plugged it into my computer (HP Pavilion gaming laptop) and it recognizes it....but says its capacity is 500mb, which makes no sense. Am I doing something wrong? Also, it won&amp;#39;t allow me to look for a driver, maybe that&amp;#39;s the issue? I know nothing about this stuff, I just wanted an external drive to move stuff over to that I could access while using my laptop because my storage is full. Is this fixable or did I buya bunk drive/the wrong thing? thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lf3js", "is_robot_indexable": true, "report_reasons": null, "author": "77177717", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lf3js/wd_easystore_8tb_problems_with_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lf3js/wd_easystore_8tb_problems_with_windows/", "subreddit_subscribers": 683385, "created_utc": 1684454330.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_tstzz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drive test script for disks over 16TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_13l2bqo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uuB18woVoMVXSPd84I453BFuXTS9R3KsH9DFrSUWZ9c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684424152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/ZizzyDizzyMC/drivetest64", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?auto=webp&amp;v=enabled&amp;s=137a278fab660ba9df8b2e9b97bbf7cbe5cdb34b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4dfa8026aa71144cd3d91e64daa19708a0b76f54", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a5b1bf7db23ce1ef516eb13c6ef713c3dc5547f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fb0ab1ced76671b07765f0dcb766ce9213207e", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82a11aef9c4e12f5abe5b20e9977853ffe46e22e", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=08470a9850da31681015c47dfa5dd12c8007ced2", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/TrU2eu1uZw-4mlcJ_9eIw7WBp6x3GEbYopcYZd5faXw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f44809ca37184bb9cb4b1d03c2c5330004e786f3", "width": 1080, "height": 540}], "variants": {}, "id": "VF4jXnKq6B--DogNN7JCPNNkO3ZM-AnWo-y-QBvZgYw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l2bqo", "is_robot_indexable": true, "report_reasons": null, "author": "ZizzyDizzyMC42", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l2bqo/drive_test_script_for_disks_over_16tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/ZizzyDizzyMC/drivetest64", "subreddit_subscribers": 683385, "created_utc": 1684424152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " Greetings, cartoon hoarder here. As the title says. I need some help downloading from [france.tv](https://france.tv) and [npo3.nl](https://npo3.nl). I've tried it with yt-dlp but to no success, and also tried with IDM but it keeps giving me a http/1.1 403 forbidden error. The shows I'm trying to get are [this](https://www.france.tv/france-5/ernest-et-celestine) and [this](https://www.npo3.nl/woezel-pip/KN_1658875). Trying to find help for non-English sites is hard, so I hope I can finally get some help here. Thanks.", "author_fullname": "t2_16625pqj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help downloading from france.tv and npo3.nl", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l1fwv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684422064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, cartoon hoarder here. As the title says. I need some help downloading from &lt;a href=\"https://france.tv\"&gt;france.tv&lt;/a&gt; and &lt;a href=\"https://npo3.nl\"&gt;npo3.nl&lt;/a&gt;. I&amp;#39;ve tried it with yt-dlp but to no success, and also tried with IDM but it keeps giving me a http/1.1 403 forbidden error. The shows I&amp;#39;m trying to get are &lt;a href=\"https://www.france.tv/france-5/ernest-et-celestine\"&gt;this&lt;/a&gt; and &lt;a href=\"https://www.npo3.nl/woezel-pip/KN_1658875\"&gt;this&lt;/a&gt;. Trying to find help for non-English sites is hard, so I hope I can finally get some help here. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?auto=webp&amp;v=enabled&amp;s=a61f1ce1969e23eaff4d2db816bf0bbe9f66663e", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a6de21343bb6e87eb3ea9f4971b8bf996001f52", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f4d89eb39d9635e6ee46190109ff9a8b0915a52", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=80cb56d02a0dc28fddb84307eea2313e6bac25d1", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c45113cb2bfaac1982d6a29b14b713ea72dd7d3", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=204cbee0c5e27b081eb5bbd480f63c9ff3f357a3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/g7kQbQgpmsnZ4p2Ih9ioEHOldr7e3ctcjawSTbMZiXM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1690f03a7345709c9382a1d4f1848cabf9df50d2", "width": 1080, "height": 567}], "variants": {}, "id": "to0TvSPMreVZ6awlrK8K4uHG6qFp3UG19hyOlIEk02Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l1fwv", "is_robot_indexable": true, "report_reasons": null, "author": "aussiecuno", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l1fwv/help_downloading_from_francetv_and_npo3nl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l1fwv/help_downloading_from_francetv_and_npo3nl/", "subreddit_subscribers": 683385, "created_utc": 1684422064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NASA uses laser system to perform fastest data transfer ever in space | The laser-based system transferred 3.6 terabytes in six minutes, which is roughly equivalent to one million songs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13ksfb4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_hswvjukr", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CIuoQpDQsr5ZjtjdzJEKz_ZeqDXCUL_Tqeh1PMcom8Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "space", "selftext": "", "author_fullname": "t2_2uwit82z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NASA uses laser system to perform fastest data transfer ever in space | The laser-based system transferred 3.6 terabytes in six minutes, which is roughly equivalent to one million songs.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/space", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13k1s6j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3422, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 3422, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/CIuoQpDQsr5ZjtjdzJEKz_ZeqDXCUL_Tqeh1PMcom8Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1684330122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "interestingengineering.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://interestingengineering.com/innovation/nasa-uses-laser-system-to-perform-fastest-data-transfer-ever-in-space", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?auto=webp&amp;v=enabled&amp;s=bff6baa2b9f3ce4cceaa41256683bd4e4253cdcc", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e141770b1ca0ec3ca770aa281150a118e53d7901", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a114f17437c89f1d521dfefa32fe2f4ebb27730", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=987df446a16feeb11f9612e9cdb22f3371d0f49b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e378b795597acad9f159c60b22b417b93a5ef362", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bba123626971a4f51c1b06e6fb1dd340820c5c70", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2eca261c8360e6bed317905f69affdbe9cb28c0b", "width": 1080, "height": 607}], "variants": {}, "id": "jMXXyhaYDbbbiGPVPyAAEPe0cxfRHo_QQSbNgD5xTNk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qh87", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13k1s6j", "is_robot_indexable": true, "report_reasons": null, "author": "chrisdh79", "discussion_type": null, "num_comments": 303, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/space/comments/13k1s6j/nasa_uses_laser_system_to_perform_fastest_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://interestingengineering.com/innovation/nasa-uses-laser-system-to-perform-fastest-data-transfer-ever-in-space", "subreddit_subscribers": 23280081, "created_utc": 1684330122.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1684397594.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "interestingengineering.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://interestingengineering.com/innovation/nasa-uses-laser-system-to-perform-fastest-data-transfer-ever-in-space", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?auto=webp&amp;v=enabled&amp;s=bff6baa2b9f3ce4cceaa41256683bd4e4253cdcc", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e141770b1ca0ec3ca770aa281150a118e53d7901", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a114f17437c89f1d521dfefa32fe2f4ebb27730", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=987df446a16feeb11f9612e9cdb22f3371d0f49b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e378b795597acad9f159c60b22b417b93a5ef362", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bba123626971a4f51c1b06e6fb1dd340820c5c70", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/VksGQ8_Ldg4eeJSDhi3RAhlN6EgcfEHxyOWGb2xKbGM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2eca261c8360e6bed317905f69affdbe9cb28c0b", "width": 1080, "height": 607}], "variants": {}, "id": "jMXXyhaYDbbbiGPVPyAAEPe0cxfRHo_QQSbNgD5xTNk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ksfb4", "is_robot_indexable": true, "report_reasons": null, "author": "tjwalkr3", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13k1s6j", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ksfb4/nasa_uses_laser_system_to_perform_fastest_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://interestingengineering.com/innovation/nasa-uses-laser-system-to-perform-fastest-data-transfer-ever-in-space", "subreddit_subscribers": 683385, "created_utc": 1684397594.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm thinking of using rclone alongside IDrive e2, and was wondering if anyone here had any experience with it?\n\n**Question:**  Are you able to view (obviously non encrypted) media from the IDrive e2 website? For example, when you press on the bucket, then can you see the images and videos with their thumbnails? Or do things just appear as their file name with no picture. I want to be able to browse photos and stuff using the rclone browser GUI.\n\nAny thoughts/recommendations/warnings about IDrive e2? I was thinking of choosing it because of the cheap price and no egress charge.\n\nThanks!", "author_fullname": "t2_heaw12g7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you think of IDrive e2?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l0yr4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684420866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m thinking of using rclone alongside IDrive e2, and was wondering if anyone here had any experience with it?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt;  Are you able to view (obviously non encrypted) media from the IDrive e2 website? For example, when you press on the bucket, then can you see the images and videos with their thumbnails? Or do things just appear as their file name with no picture. I want to be able to browse photos and stuff using the rclone browser GUI.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts/recommendations/warnings about IDrive e2? I was thinking of choosing it because of the cheap price and no egress charge.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l0yr4", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway52075", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l0yr4/what_do_you_think_of_idrive_e2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l0yr4/what_do_you_think_of_idrive_e2/", "subreddit_subscribers": 683385, "created_utc": 1684420866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am looking into S3-compatible storage solutions to use for a private file host and I have my eye on a couple but I thought I would ask here to see if there are any better alternatives. The ones that I am currently looking at are Cloudflare R2 and Backblaze B2 which both seem like great options but I am not sure which would be best to use for a file host. \n\nThey both provide 10GB of free storage which is nice and Backblaze's storage is only $0.005/GB/Month compared to Cloudflare's $0.015/GB/Month however Cloudflare has no egress fee whereas Backblaze has a fee of $0.01/GB with the first 1GB being free each day. \n\nThey are both solid solutions but I am not sure which would be cheaper for my use case (or if there is a better alternative).", "author_fullname": "t2_qb1ud93i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best S3-compatible storage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ldn8q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684450642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am looking into S3-compatible storage solutions to use for a private file host and I have my eye on a couple but I thought I would ask here to see if there are any better alternatives. The ones that I am currently looking at are Cloudflare R2 and Backblaze B2 which both seem like great options but I am not sure which would be best to use for a file host. &lt;/p&gt;\n\n&lt;p&gt;They both provide 10GB of free storage which is nice and Backblaze&amp;#39;s storage is only $0.005/GB/Month compared to Cloudflare&amp;#39;s $0.015/GB/Month however Cloudflare has no egress fee whereas Backblaze has a fee of $0.01/GB with the first 1GB being free each day. &lt;/p&gt;\n\n&lt;p&gt;They are both solid solutions but I am not sure which would be cheaper for my use case (or if there is a better alternative).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ldn8q", "is_robot_indexable": true, "report_reasons": null, "author": "wraithdotcat", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ldn8q/best_s3compatible_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ldn8q/best_s3compatible_storage/", "subreddit_subscribers": 683385, "created_utc": 1684450642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I know people are on fire about archiving Imgur now, so I'm sorry to spring this one on y'all.\n\nFound the CocoCut chrome extension able to DL twitter nsfw stuff, which other online DL tools can't do after the API change in mid april.\n\nJust wondering if that extension is safe to use? And also their DL manager / P2P client CocoFetch.\n\n'preciate the help guys.", "author_fullname": "t2_du9df", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is CocoCut/CocoFetch safe?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ldh97", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684450220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know people are on fire about archiving Imgur now, so I&amp;#39;m sorry to spring this one on y&amp;#39;all.&lt;/p&gt;\n\n&lt;p&gt;Found the CocoCut chrome extension able to DL twitter nsfw stuff, which other online DL tools can&amp;#39;t do after the API change in mid april.&lt;/p&gt;\n\n&lt;p&gt;Just wondering if that extension is safe to use? And also their DL manager / P2P client CocoFetch.&lt;/p&gt;\n\n&lt;p&gt;&amp;#39;preciate the help guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ldh97", "is_robot_indexable": true, "report_reasons": null, "author": "prismstein", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ldh97/is_cococutcocofetch_safe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ldh97/is_cococutcocofetch_safe/", "subreddit_subscribers": 683385, "created_utc": 1684450220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since DVDs/CDs are WORM storage, why is the standard way of writing to it is not by filling the whole empty space by redundancy as much as possible? Why would I want even the slightest amount of free space when I could just fill it with more and more redundancy? Yes, I can fill the space by par2 files but I\u2019m wondering why this isn\u2019t a standard (of course not \u201cpar2\u201d, the redundancy). Like, after writing the whole CDFS and data, a sector by sector (if that is how its called) redundancy could be added to the remaining free space, but they just decided \u201cNah, lets waste that extra space\u201d?\n\nOr I\u2019m misinformed. Correct me if thats the standard. I just looked at the back of my disk and thought it was free space by comparing with the written space, not scratching a disk to prove my theory and I think the hardware handles sector by sector data which has redundancy info, so no software can show me raw sector data or I don\u2019t know\u2026\n\n(Of course redundancy could also far surpass the actual data size, let\u2019s say a 5 MB file on a CD with 695 MB of redundancy.)", "author_fullname": "t2_yj6p9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are DVD/CDs not completely written to until they fill all the free space with redundancy? (Or is it done already?)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l6phf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684434266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since DVDs/CDs are WORM storage, why is the standard way of writing to it is not by filling the whole empty space by redundancy as much as possible? Why would I want even the slightest amount of free space when I could just fill it with more and more redundancy? Yes, I can fill the space by par2 files but I\u2019m wondering why this isn\u2019t a standard (of course not \u201cpar2\u201d, the redundancy). Like, after writing the whole CDFS and data, a sector by sector (if that is how its called) redundancy could be added to the remaining free space, but they just decided \u201cNah, lets waste that extra space\u201d?&lt;/p&gt;\n\n&lt;p&gt;Or I\u2019m misinformed. Correct me if thats the standard. I just looked at the back of my disk and thought it was free space by comparing with the written space, not scratching a disk to prove my theory and I think the hardware handles sector by sector data which has redundancy info, so no software can show me raw sector data or I don\u2019t know\u2026&lt;/p&gt;\n\n&lt;p&gt;(Of course redundancy could also far surpass the actual data size, let\u2019s say a 5 MB file on a CD with 695 MB of redundancy.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l6phf", "is_robot_indexable": true, "report_reasons": null, "author": "LAMGE2", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l6phf/why_are_dvdcds_not_completely_written_to_until/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l6phf/why_are_dvdcds_not_completely_written_to_until/", "subreddit_subscribers": 683385, "created_utc": 1684434266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have been sticking with wget for downloading webpages and I find a lot of pages look broken when I view them, and its extremely difficult to organize. How do you download and save webpages without encountering this issue as often.", "author_fullname": "t2_ln1lseds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need a way of saving websites", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l55hv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684430868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been sticking with wget for downloading webpages and I find a lot of pages look broken when I view them, and its extremely difficult to organize. How do you download and save webpages without encountering this issue as often.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l55hv", "is_robot_indexable": true, "report_reasons": null, "author": "EncryptionIsFreedom", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l55hv/need_a_way_of_saving_websites/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l55hv/need_a_way_of_saving_websites/", "subreddit_subscribers": 683385, "created_utc": 1684430868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there!For the longest time, I have wanted to get myself a NAS to be able to work more reliably with large amounts of media material on multiple devices (my main Windows workstation and my Macbook M1 pro 14\") and to archive larger projects for future use. I was originally planning on getting myself a Synology NAS, but after reading some concerns of the extensive proprietary hardware and limitations, and just the overall cost, I opted to build myself a NAS computer instead. Because of this, I also began considering making it quite powerful for multiple use-cases and to be reliable for the long-term.\n\nI have added different notes of use-cases, and would like to hear some opinions if this is optimal, if there are specs I should change, or if a pre-built NAS is a safer bet overall. I am prepared for the maintenance such a NAS would require, but believe that the benefits will outweigh it overall, and cause I love to troubleshoot a lot.\n\n**Main use-case:**\n\n* To archive and work with large amounts media material on different devices (Windows and MacOS). (Certain major projects will be backed up on external drives for safekeeping just in case)\n* Able to access remotely in case of remote work.\n* Able to share files and request files from clients.\n\nSecondary use-cases (Not necessary):\n\n* Host simple game servers (Minecraft, Project Zomboid, etc.)\n* Use as an office hub (Nextcloud, OnlyOffice, etc.)\n* Potential rendering station(?)\n\n&amp;#x200B;\n\n**These are the specs:**\n\n* Fractal Node 304\n* 2x Noctua NF-A9x14 HS-PWM\n* Noctua NF-A14 PWM\n* Noctua NH-D15S (?) or alternative from Noctua.\n* Gigabyte B550I AORUS PRO AX\n* Ryzen 7 5700G\n* 2x 32 GB DDR4-3600 ram\n* Corsair SFX SF750 PSU\n* TP-Link TX401 10 Gigabit PCI Express Network Adapter\n* Samsung 980 PRO SSD, 1TB\n* 4x Seagate Ironwolf 12 TB (for a total of 48 TB of storage without RAID)\n\nReasoning and wishes with these specs:\n\n* The Fractal Node 304 is a small case that would fit in my tiny office space and would fit in on of the Kallax shelf slots next to my desk.\n* The Ryzen 7 5700G offers an alright inbuilt GPU which will suffice for most productivity task, as I won't be doing anything graphically intensive, and can avoid the purchase of an separate GPU.\n* 64 GB ram might be overkill, but I would like to be on the safe side for the long-term and for it to be able to handle multiple tasks (such as constant file transferring and game server hosting).\n* Samsung 980 Pro SSD for the TrueNAS Core OS.\n* I decided to opt for the Ironwolf non-Pro disks as I've read that the Pro tend to be noisier and because the cost would be lower and more economical.\n* To setup a RAID6 configuration with the HDD's for redundancy.\n\nEdit: Simple corrections, formatting fixes and added the adapter for faster GBit networking.", "author_fullname": "t2_1rl7h47h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Planning my first NAS for media production, is this a good build?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13l23mx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684425039.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684423639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!For the longest time, I have wanted to get myself a NAS to be able to work more reliably with large amounts of media material on multiple devices (my main Windows workstation and my Macbook M1 pro 14&amp;quot;) and to archive larger projects for future use. I was originally planning on getting myself a Synology NAS, but after reading some concerns of the extensive proprietary hardware and limitations, and just the overall cost, I opted to build myself a NAS computer instead. Because of this, I also began considering making it quite powerful for multiple use-cases and to be reliable for the long-term.&lt;/p&gt;\n\n&lt;p&gt;I have added different notes of use-cases, and would like to hear some opinions if this is optimal, if there are specs I should change, or if a pre-built NAS is a safer bet overall. I am prepared for the maintenance such a NAS would require, but believe that the benefits will outweigh it overall, and cause I love to troubleshoot a lot.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Main use-case:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;To archive and work with large amounts media material on different devices (Windows and MacOS). (Certain major projects will be backed up on external drives for safekeeping just in case)&lt;/li&gt;\n&lt;li&gt;Able to access remotely in case of remote work.&lt;/li&gt;\n&lt;li&gt;Able to share files and request files from clients.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Secondary use-cases (Not necessary):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Host simple game servers (Minecraft, Project Zomboid, etc.)&lt;/li&gt;\n&lt;li&gt;Use as an office hub (Nextcloud, OnlyOffice, etc.)&lt;/li&gt;\n&lt;li&gt;Potential rendering station(?)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;These are the specs:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fractal Node 304&lt;/li&gt;\n&lt;li&gt;2x Noctua NF-A9x14 HS-PWM&lt;/li&gt;\n&lt;li&gt;Noctua NF-A14 PWM&lt;/li&gt;\n&lt;li&gt;Noctua NH-D15S (?) or alternative from Noctua.&lt;/li&gt;\n&lt;li&gt;Gigabyte B550I AORUS PRO AX&lt;/li&gt;\n&lt;li&gt;Ryzen 7 5700G&lt;/li&gt;\n&lt;li&gt;2x 32 GB DDR4-3600 ram&lt;/li&gt;\n&lt;li&gt;Corsair SFX SF750 PSU&lt;/li&gt;\n&lt;li&gt;TP-Link TX401 10 Gigabit PCI Express Network Adapter&lt;/li&gt;\n&lt;li&gt;Samsung 980 PRO SSD, 1TB&lt;/li&gt;\n&lt;li&gt;4x Seagate Ironwolf 12 TB (for a total of 48 TB of storage without RAID)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Reasoning and wishes with these specs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The Fractal Node 304 is a small case that would fit in my tiny office space and would fit in on of the Kallax shelf slots next to my desk.&lt;/li&gt;\n&lt;li&gt;The Ryzen 7 5700G offers an alright inbuilt GPU which will suffice for most productivity task, as I won&amp;#39;t be doing anything graphically intensive, and can avoid the purchase of an separate GPU.&lt;/li&gt;\n&lt;li&gt;64 GB ram might be overkill, but I would like to be on the safe side for the long-term and for it to be able to handle multiple tasks (such as constant file transferring and game server hosting).&lt;/li&gt;\n&lt;li&gt;Samsung 980 Pro SSD for the TrueNAS Core OS.&lt;/li&gt;\n&lt;li&gt;I decided to opt for the Ironwolf non-Pro disks as I&amp;#39;ve read that the Pro tend to be noisier and because the cost would be lower and more economical.&lt;/li&gt;\n&lt;li&gt;To setup a RAID6 configuration with the HDD&amp;#39;s for redundancy.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Edit: Simple corrections, formatting fixes and added the adapter for faster GBit networking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13l23mx", "is_robot_indexable": true, "report_reasons": null, "author": "restalot", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13l23mx/planning_my_first_nas_for_media_production_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13l23mx/planning_my_first_nas_for_media_production_is/", "subreddit_subscribers": 683385, "created_utc": 1684423639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Got this nas off Facebook marketplace. It won\u2019t power on. The motherboard has a known issue with qnap I\u2019ve contacted qnap and without a receipt I can\u2019t get them to intervene. (Was hoping to get lucky) now it\u2019s essentially an itx computer inside. \n\nThe main difference is the cables coming from the backplane. Is there a way I can utilize this? \n\n\nIt\u2019s not going to be simple as most motherboards won\u2019t match up with the io panel on the back but with a dremel and some cando attitude I might be able to salvage this compact build into my server rack that\u2019s very shallow. \n\nHas anyone else tried this? \n\nAny ideas which cable connectors I would need? \n\nOr if this is completely stupid and I should stop building computers today. \n\nThanks for your time.", "author_fullname": "t2_ho9oc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old nas to new itx nas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 133, "top_awarded_type": null, "hide_score": true, "name": "t3_13ljd79", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nLGfNckUienP3UQP0tDUU8EzeQVm9SRvJN22zLJZNXE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684465870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got this nas off Facebook marketplace. It won\u2019t power on. The motherboard has a known issue with qnap I\u2019ve contacted qnap and without a receipt I can\u2019t get them to intervene. (Was hoping to get lucky) now it\u2019s essentially an itx computer inside. &lt;/p&gt;\n\n&lt;p&gt;The main difference is the cables coming from the backplane. Is there a way I can utilize this? &lt;/p&gt;\n\n&lt;p&gt;It\u2019s not going to be simple as most motherboards won\u2019t match up with the io panel on the back but with a dremel and some cando attitude I might be able to salvage this compact build into my server rack that\u2019s very shallow. &lt;/p&gt;\n\n&lt;p&gt;Has anyone else tried this? &lt;/p&gt;\n\n&lt;p&gt;Any ideas which cable connectors I would need? &lt;/p&gt;\n\n&lt;p&gt;Or if this is completely stupid and I should stop building computers today. &lt;/p&gt;\n\n&lt;p&gt;Thanks for your time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/zf9x927vxq0b1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?auto=webp&amp;v=enabled&amp;s=f9d83ab50b8a262cb51bd38bc936305aa3bfddf1", "width": 1290, "height": 1230}, "resolutions": [{"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04be140884caa479f1015a06f1bb76272b6a114c", "width": 108, "height": 102}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6a01f7498663850e09b61eaefc40c06800aca207", "width": 216, "height": 205}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20cce8ba25e7e9d8ab6122525ae95911df89e26f", "width": 320, "height": 305}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a431e021243496bc6574f36785a80f4afbd790e", "width": 640, "height": 610}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=da0c5a73f1badb6f11a269f48d78bf1ab18067f4", "width": 960, "height": 915}, {"url": "https://preview.redd.it/zf9x927vxq0b1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e147e190e898220c65354e4947516afbcf674dae", "width": 1080, "height": 1029}], "variants": {}, "id": "XdZTyG1fBjnJs_3wVjHjBw_jI_japMGT5varghi_LLU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ljd79", "is_robot_indexable": true, "report_reasons": null, "author": "Cor4eyh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ljd79/old_nas_to_new_itx_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/zf9x927vxq0b1.jpg", "subreddit_subscribers": 683385, "created_utc": 1684465870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've had a Qnap NAS for a few years to hold my backups (via Acronis,) and I've recently gotten a new one. I want to still use the old one for another layer of backup, but maybe this is a trivial question but does it make more sense to have a separate Acronis backup routine set up to use the old NAS, perhaps less often and only more critical stuff; or should I be using the new NAS' Snapshots to backup my backups?", "author_fullname": "t2_3mkur2v7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategy for second-level backup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13li570", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684462416.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve had a Qnap NAS for a few years to hold my backups (via Acronis,) and I&amp;#39;ve recently gotten a new one. I want to still use the old one for another layer of backup, but maybe this is a trivial question but does it make more sense to have a separate Acronis backup routine set up to use the old NAS, perhaps less often and only more critical stuff; or should I be using the new NAS&amp;#39; Snapshots to backup my backups?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13li570", "is_robot_indexable": true, "report_reasons": null, "author": "DeliciousPool5", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13li570/strategy_for_secondlevel_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13li570/strategy_for_secondlevel_backup/", "subreddit_subscribers": 683385, "created_utc": 1684462416.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux Multi-Volume LTO4 Tape Backup Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lexiv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_rvam4", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "homelab", "selftext": "Hi there! Long time lurker, first time poster...\n\nIn my homelab environment, I have a few VMs running on an ESXi hypervisor that serve Samba shares for various services on different subnets. All of the data is stored on a RAID array, and unfortunately, I've had two 4TB drives fail on me in the past year. This has prompted me to start backing up to LTO4 tapes as I have a drive kicking around because I'm starting to seriously doubt that the disks will hold up in the long term, and I worry about a failed rebuild after rebuilding multiple times.\n\nFor some background, the way that I'm performing this tape backup is over the network. So, I have the tape drive connected to my physical Linux workstation, and I'm mounting the Samba shares in the /mnt directory (so, /mnt/smb-share1 for example).\n\nNow,  in the past I've never had issues creating tape backups that are to a single tape. I'm trying to back up one of these Samba shares, and this one in particular is about 1.2TB in size, which exceeds the 800GB uncompressed limitation of the LTO4 standard. I also do not want to compress the data going to the tapes. As a result, I need to create multi-volume tar backups.\n\nWhen I tried creating the multi-volume backup, I was able to write all 1.2TB spanning two tapes, but when I \"mock\" a restore using \"tar -tvf\", the second tape fails with the error \"tar: \u2018./example/example.zip\u2019 is not continued on this volume\".\n\nSo, for my question... how should I be creating these backups? I'm not sure if mbuffer is the issue here, but I really would prefer to continue using it to prevent buffer underruns, which isn't good for the tape or the tape drive. Here's the two commands that I'm using...\n\nWriting to the tapes:\n\n    # cd /mnt/smb-share1\n    # tar -b 4096 --directory=\"/mnt/smb-share1\" --multi-volume --one-file-system --xattrs -cf - ./ | mbuffer -m 2G -L -P 95 -f -o /dev/st0\n\n\"Restoring\" (just reading each file) from the tapes:\n\n    # tar -b 4096 --multi-volume -tvf /dev/st0 | tee /home/cjms/Documents/TAPE-BACKUP-CONTENTS.TXT\n    drwxr-xr-x cjms/cjms            0 2019-08-14 14:56 ./dir0/\n    ...\n    -rwxr-xr-x cjms/cjms   8999733302 2021-06-24 05:06 ./dir9/someFile.7z\n    Prepare volume #2 for \u2018/dev/st0\u2019 and hit return: [return]\n    tar: \u2018./dir9/someFile.7z\u2019 is not continued on this volume\n    Prepare volume #2 for \u2018/dev/st0\u2019 and hit return: \n\nAnyone have any suggestions? The operating system I'm performing this on is Rocky Linux 9.1. I do not want to use a proprietary/paid solution for this... tar is the way!\n\nThanks!", "author_fullname": "t2_rvam4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux Multi-Volume LTO4 Tape Backup Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/homelab", "hidden": false, "pwls": 6, "link_flair_css_class": "help", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13leuir", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684453673.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there! Long time lurker, first time poster...&lt;/p&gt;\n\n&lt;p&gt;In my homelab environment, I have a few VMs running on an ESXi hypervisor that serve Samba shares for various services on different subnets. All of the data is stored on a RAID array, and unfortunately, I&amp;#39;ve had two 4TB drives fail on me in the past year. This has prompted me to start backing up to LTO4 tapes as I have a drive kicking around because I&amp;#39;m starting to seriously doubt that the disks will hold up in the long term, and I worry about a failed rebuild after rebuilding multiple times.&lt;/p&gt;\n\n&lt;p&gt;For some background, the way that I&amp;#39;m performing this tape backup is over the network. So, I have the tape drive connected to my physical Linux workstation, and I&amp;#39;m mounting the Samba shares in the /mnt directory (so, /mnt/smb-share1 for example).&lt;/p&gt;\n\n&lt;p&gt;Now,  in the past I&amp;#39;ve never had issues creating tape backups that are to a single tape. I&amp;#39;m trying to back up one of these Samba shares, and this one in particular is about 1.2TB in size, which exceeds the 800GB uncompressed limitation of the LTO4 standard. I also do not want to compress the data going to the tapes. As a result, I need to create multi-volume tar backups.&lt;/p&gt;\n\n&lt;p&gt;When I tried creating the multi-volume backup, I was able to write all 1.2TB spanning two tapes, but when I &amp;quot;mock&amp;quot; a restore using &amp;quot;tar -tvf&amp;quot;, the second tape fails with the error &amp;quot;tar: \u2018./example/example.zip\u2019 is not continued on this volume&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;So, for my question... how should I be creating these backups? I&amp;#39;m not sure if mbuffer is the issue here, but I really would prefer to continue using it to prevent buffer underruns, which isn&amp;#39;t good for the tape or the tape drive. Here&amp;#39;s the two commands that I&amp;#39;m using...&lt;/p&gt;\n\n&lt;p&gt;Writing to the tapes:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# cd /mnt/smb-share1\n# tar -b 4096 --directory=&amp;quot;/mnt/smb-share1&amp;quot; --multi-volume --one-file-system --xattrs -cf - ./ | mbuffer -m 2G -L -P 95 -f -o /dev/st0\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;quot;Restoring&amp;quot; (just reading each file) from the tapes:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# tar -b 4096 --multi-volume -tvf /dev/st0 | tee /home/cjms/Documents/TAPE-BACKUP-CONTENTS.TXT\ndrwxr-xr-x cjms/cjms            0 2019-08-14 14:56 ./dir0/\n...\n-rwxr-xr-x cjms/cjms   8999733302 2021-06-24 05:06 ./dir9/someFile.7z\nPrepare volume #2 for \u2018/dev/st0\u2019 and hit return: [return]\ntar: \u2018./dir9/someFile.7z\u2019 is not continued on this volume\nPrepare volume #2 for \u2018/dev/st0\u2019 and hit return: \n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Anyone have any suggestions? The operating system I&amp;#39;m performing this on is Rocky Linux 9.1. I do not want to use a proprietary/paid solution for this... tar is the way!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "664a26e4-322a-11e6-80ae-0e0378709321", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2ubz7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff6347", "id": "13leuir", "is_robot_indexable": true, "report_reasons": null, "author": "cjmspartans96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/homelab/comments/13leuir/linux_multivolume_lto4_tape_backup_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/homelab/comments/13leuir/linux_multivolume_lto4_tape_backup_question/", "subreddit_subscribers": 572140, "created_utc": 1684453673.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1684453887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/homelab/comments/13leuir/linux_multivolume_lto4_tape_backup_question/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lexiv", "is_robot_indexable": true, "report_reasons": null, "author": "cjmspartans96", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_13leuir", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lexiv/linux_multivolume_lto4_tape_backup_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/homelab/comments/13leuir/linux_multivolume_lto4_tape_backup_question/", "subreddit_subscribers": 683385, "created_utc": 1684453887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nRecently had a hard drive die on me that contained lots of projects and data (unfortunately did not back this data up, my negligence..). So to avoid this issue ever happening again, I'm looking into purchasing a HDD docking station that supports up to 2 drives to back up my data on a routinely basis. SSD or HD, either or.\n\nI use a Macbook Pro M1 for my daily driver, music projects, some editing/motion graphics. I also have a PC for the heavy lifting of 3D work and more intense motion graphic projects (due to the dedicated video card).\n\nMy question is, if I were to purchase a docking bay that accepts 2 drives or more, can I use the docking station on both my Mac and PC? I do realize you can format the HD to whatever OS I am connecting it to, however, can I plug the docking station into both my computers separately? For instance, I would set one HD dedicated for PC backups and the other HD dedicated for my Macbook backups (One drive format to NTFS for Windows and the other drive format to MacOS extended for Time Machine).\n\nI have searched online and cannot find the specific answer to this question, so anything would be helpful!", "author_fullname": "t2_128wmf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD Docking Bay Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13le06t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684451514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Recently had a hard drive die on me that contained lots of projects and data (unfortunately did not back this data up, my negligence..). So to avoid this issue ever happening again, I&amp;#39;m looking into purchasing a HDD docking station that supports up to 2 drives to back up my data on a routinely basis. SSD or HD, either or.&lt;/p&gt;\n\n&lt;p&gt;I use a Macbook Pro M1 for my daily driver, music projects, some editing/motion graphics. I also have a PC for the heavy lifting of 3D work and more intense motion graphic projects (due to the dedicated video card).&lt;/p&gt;\n\n&lt;p&gt;My question is, if I were to purchase a docking bay that accepts 2 drives or more, can I use the docking station on both my Mac and PC? I do realize you can format the HD to whatever OS I am connecting it to, however, can I plug the docking station into both my computers separately? For instance, I would set one HD dedicated for PC backups and the other HD dedicated for my Macbook backups (One drive format to NTFS for Windows and the other drive format to MacOS extended for Time Machine).&lt;/p&gt;\n\n&lt;p&gt;I have searched online and cannot find the specific answer to this question, so anything would be helpful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13le06t", "is_robot_indexable": true, "report_reasons": null, "author": "anduroox", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13le06t/hdd_docking_bay_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13le06t/hdd_docking_bay_question/", "subreddit_subscribers": 683385, "created_utc": 1684451514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys,\n\nI like to save Youtube Channels from Youtubers I really like. Because of that, i built a quite big library.  \nI would really like to give other people access to my files and get access to theirs.\n\nIs there a software, where i can share my youtube channels and make them accessible via some kind of torrent software? But youtube video focused?", "author_fullname": "t2_11morj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a P2P Plattform, for archived youtube channels/youtube videos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lcn7b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684448181.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I like to save Youtube Channels from Youtubers I really like. Because of that, i built a quite big library.&lt;br/&gt;\nI would really like to give other people access to my files and get access to theirs.&lt;/p&gt;\n\n&lt;p&gt;Is there a software, where i can share my youtube channels and make them accessible via some kind of torrent software? But youtube video focused?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lcn7b", "is_robot_indexable": true, "report_reasons": null, "author": "BurningPixels", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lcn7b/is_there_a_p2p_plattform_for_archived_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lcn7b/is_there_a_p2p_plattform_for_archived_youtube/", "subreddit_subscribers": 683385, "created_utc": 1684448181.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, anyone have any experience with using this board in a NAS? I'm looking to upgrade my aging Unraid server to this hardware I found on Aliexpress.\n\nHowever, I've seen some issues on forums with the intel i225V networking card. Not sure if the B3 revision fixed all the issues though. There's a version with the upgraded i226 though but that costs a bit more money.\n\n[https://www.aliexpress.us/item/3256804692292280.html?gatewayAdapt=glo2usa](https://www.aliexpress.us/item/3256804692292280.html?gatewayAdapt=glo2usa)\n\nThanks!", "author_fullname": "t2_mw23q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Celeron N5105 and intel i225V issues?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lb1p1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684444355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, anyone have any experience with using this board in a NAS? I&amp;#39;m looking to upgrade my aging Unraid server to this hardware I found on Aliexpress.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;ve seen some issues on forums with the intel i225V networking card. Not sure if the B3 revision fixed all the issues though. There&amp;#39;s a version with the upgraded i226 though but that costs a bit more money.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.aliexpress.us/item/3256804692292280.html?gatewayAdapt=glo2usa\"&gt;https://www.aliexpress.us/item/3256804692292280.html?gatewayAdapt=glo2usa&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?auto=webp&amp;v=enabled&amp;s=882bc7a4cdfe98ed5d2e92707c4349f1cff944e3", "width": 1000, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8688778458e5b0bd843a30cb9b898d307151def5", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74f8e008599638040fbda8db23f0ec3753bb026c", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a029381962923f25329fb565ae8f48c2cc5fb18c", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f79f8bbcd7e3494cff477f04e153871a852cc950", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/vSPw7TdPTW0a2iZuqWa9j4lS8YV-uA88hzqfSyLGcos.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=969a85586b8c509bd2eac9dee4868ef7eb7be8f5", "width": 960, "height": 960}], "variants": {}, "id": "B7qADUhmpPLVxwZKQ4Xno3NxYIVvrGoWisHxswCSnWY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lb1p1", "is_robot_indexable": true, "report_reasons": null, "author": "errr_mah_gawsh", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lb1p1/celeron_n5105_and_intel_i225v_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lb1p1/celeron_n5105_and_intel_i225v_issues/", "subreddit_subscribers": 683385, "created_utc": 1684444355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My organization recently switched from Google Workspace to Sharepoint, and it's been a major pain in the ass ever since, as trying to use OD on the mac has been a disaster.\n\nThe OD app for mac is just... bad, especially compared with GDrive. On two different devices, it goes rogue consistently, even after a fresh install and setup - frequent beachballs in the menu bar, and it just disregards the fact I don't want it to occupy all of my disk space. I did use all of the (very little) options to avoid disk space use, and I did use the \"free space\" context menu command, which does nothing anyway. It just decides to occupy 10-80 GBs erratically, even if every single file and folder has the little cloud thingy meaning the should not be residing locally.\n\nI would use OD for the following:\n\n1) sync a few folders (one-way only) when needed, while working on local files;\n\n2) access these folders on different devices when needed;\n\n3) store some very large files that I don't need to access often I know OD is not meant to be used this way, and should be more of a sync, but I still have 5TB available and I need a remote backup.\n\nI could use rclone for all of the above, but 1) it's noticeably slower than rsync, which I have used successfully to date; 2) some kind of integration with the Mac finder (navigating dirs, searching files) would be nice.\n\nIs Mountain Duck worth using in my case? Or should I try mounting the OD filesystem with rclone and tinker some more?", "author_fullname": "t2_4u2z7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some guidance for OneDrive on Mac", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13lajnv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684443197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My organization recently switched from Google Workspace to Sharepoint, and it&amp;#39;s been a major pain in the ass ever since, as trying to use OD on the mac has been a disaster.&lt;/p&gt;\n\n&lt;p&gt;The OD app for mac is just... bad, especially compared with GDrive. On two different devices, it goes rogue consistently, even after a fresh install and setup - frequent beachballs in the menu bar, and it just disregards the fact I don&amp;#39;t want it to occupy all of my disk space. I did use all of the (very little) options to avoid disk space use, and I did use the &amp;quot;free space&amp;quot; context menu command, which does nothing anyway. It just decides to occupy 10-80 GBs erratically, even if every single file and folder has the little cloud thingy meaning the should not be residing locally.&lt;/p&gt;\n\n&lt;p&gt;I would use OD for the following:&lt;/p&gt;\n\n&lt;p&gt;1) sync a few folders (one-way only) when needed, while working on local files;&lt;/p&gt;\n\n&lt;p&gt;2) access these folders on different devices when needed;&lt;/p&gt;\n\n&lt;p&gt;3) store some very large files that I don&amp;#39;t need to access often I know OD is not meant to be used this way, and should be more of a sync, but I still have 5TB available and I need a remote backup.&lt;/p&gt;\n\n&lt;p&gt;I could use rclone for all of the above, but 1) it&amp;#39;s noticeably slower than rsync, which I have used successfully to date; 2) some kind of integration with the Mac finder (navigating dirs, searching files) would be nice.&lt;/p&gt;\n\n&lt;p&gt;Is Mountain Duck worth using in my case? Or should I try mounting the OD filesystem with rclone and tinker some more?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13lajnv", "is_robot_indexable": true, "report_reasons": null, "author": "zen_arcade", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13lajnv/need_some_guidance_for_onedrive_on_mac/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13lajnv/need_some_guidance_for_onedrive_on_mac/", "subreddit_subscribers": 683385, "created_utc": 1684443197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am the type of person to over analyze everything. I can't help it, so I embrace it. But I'm hoping for some outside thoughts/perspectives.\n\nI've used both rclone and Syncovery and love both. I've contributed $ to both. I hope both stay in development. Both have their pros/cons. \n\nI use rclone on all my servers. I was using Syncovery on my Windows 10 daily driver to backup my 2 TB of data to Backblaze B2 using ZIP encryption. It's all photos and important documents. \n\nI've moved my daily driver from Windows 10 to Linux (Debian Bookworm + KDE Plasma).\n\nI can keep using Syncovery but am contemplating if I should move to rclone for backing up the data from my daily driver. \n\nI use encryption with both. With Syncovery there is a GUI that I can use, and it has built in job scheduler and email notification. With rclone I use cron + script I wrote that does the backing up and emailing (using mail).\n\nI only have \\~2 TB of data and it grows very slowly. I'll probably add 1-2 gigs a year.\n\nI'm just curious what others think.\n\nI really like/want ease of use/configuring, which Syncovery wins at, but rclone is so popular and also does a very good job once you create/configure the scripts.\n\nI've spent too much time thinking about this. I just need someone to tell me what to do. Heh.", "author_fullname": "t2_5wpob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help me with my analysis paralysis for backing up my data (photos and documents): rclone + crypt vs. Syncovery + ZIP + encryption", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13kyl8c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684414888.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am the type of person to over analyze everything. I can&amp;#39;t help it, so I embrace it. But I&amp;#39;m hoping for some outside thoughts/perspectives.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used both rclone and Syncovery and love both. I&amp;#39;ve contributed $ to both. I hope both stay in development. Both have their pros/cons. &lt;/p&gt;\n\n&lt;p&gt;I use rclone on all my servers. I was using Syncovery on my Windows 10 daily driver to backup my 2 TB of data to Backblaze B2 using ZIP encryption. It&amp;#39;s all photos and important documents. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve moved my daily driver from Windows 10 to Linux (Debian Bookworm + KDE Plasma).&lt;/p&gt;\n\n&lt;p&gt;I can keep using Syncovery but am contemplating if I should move to rclone for backing up the data from my daily driver. &lt;/p&gt;\n\n&lt;p&gt;I use encryption with both. With Syncovery there is a GUI that I can use, and it has built in job scheduler and email notification. With rclone I use cron + script I wrote that does the backing up and emailing (using mail).&lt;/p&gt;\n\n&lt;p&gt;I only have ~2 TB of data and it grows very slowly. I&amp;#39;ll probably add 1-2 gigs a year.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m just curious what others think.&lt;/p&gt;\n\n&lt;p&gt;I really like/want ease of use/configuring, which Syncovery wins at, but rclone is so popular and also does a very good job once you create/configure the scripts.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve spent too much time thinking about this. I just need someone to tell me what to do. Heh.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13kyl8c", "is_robot_indexable": true, "report_reasons": null, "author": "imthenachoman", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13kyl8c/help_me_with_my_analysis_paralysis_for_backing_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13kyl8c/help_me_with_my_analysis_paralysis_for_backing_up/", "subreddit_subscribers": 683385, "created_utc": 1684414888.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}