{"kind": "Listing", "data": {"after": "t3_13d01l7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was looking at Mr. Beast\u2019s new post for his recent giveaway. The number of comments are staggeringly high. It has more comments than the likes for the post. \n\nI was wondering how these comments are stored or handled within IG? Are they using SQL tables for this? And every time, I go the comments section does it read the database every time? Do we see real time numbers? \n\nIf you have worked in such a system. Can you give your two cents please? Thank you.", "author_fullname": "t2_emzh9atv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Instagram handles data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cprs6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 120, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 120, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683638600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was looking at Mr. Beast\u2019s new post for his recent giveaway. The number of comments are staggeringly high. It has more comments than the likes for the post. &lt;/p&gt;\n\n&lt;p&gt;I was wondering how these comments are stored or handled within IG? Are they using SQL tables for this? And every time, I go the comments section does it read the database every time? Do we see real time numbers? &lt;/p&gt;\n\n&lt;p&gt;If you have worked in such a system. Can you give your two cents please? Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13cprs6", "is_robot_indexable": true, "report_reasons": null, "author": "MaintenanceSad6825", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13cprs6/how_instagram_handles_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13cprs6/how_instagram_handles_data/", "subreddit_subscribers": 104805, "created_utc": 1683638600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just got my first job as a DE, am super pumped!! I am trying to learn as much as I can before I start the job. Wanting to start this discussion:\n\nIf there are only one/two most important skills you could master at DE work, what would they be?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is that one skill you would wanna master at work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d8rlr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 51, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 51, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683672462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just got my first job as a DE, am super pumped!! I am trying to learn as much as I can before I start the job. Wanting to start this discussion:&lt;/p&gt;\n\n&lt;p&gt;If there are only one/two most important skills you could master at DE work, what would they be?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13d8rlr", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13d8rlr/what_is_that_one_skill_you_would_wanna_master_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13d8rlr/what_is_that_one_skill_you_would_wanna_master_at/", "subreddit_subscribers": 104805, "created_utc": 1683672462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been studying a lot about data modeling, but much of the information is specifically tailored towards data warehousing, and not so much towards modeling in data lakes or data lakehouses. \n\nFor those of you who manage a Data Lakehouse, I am interested in knowing how you approach data modeling in the various layers. Although a Lakehouse aims to merge Data Warehouse and Data Lake features by introducing ACID and CRUD functionalities on top of object storage, I feel that it is essential to prioritize appropriate data modeling practices, which are commonly utilized in data warehousing.\n\nLets say I have an ELT architecture that follows: Landing (ephemeral) -&gt; Bronze -&gt; Silver -&gt; Gold \n\nMy questions is: How would you (or do you) enforce proper data modelling in Bronze/Silver/Gold layers?\n\nBased on my research, I believe that Inmon-style modeling is the most suitable approach for a Lakehouse. In this scenario, both the Bronze and Silver layers would be source-oriented and maintain the normalized ER model precisely as the source. The Bronze data would then be upserted into the Silver layer, which would resemble the Data Warehouse layer seen in the Inmon Data Warehouse. \n\n Next, the Silver layer is utilized to generate or update the data marts in the Gold layer, in response to business requests. To achieve this, I would design Kimball-style star schemas, wherein the fact and dimension tables remain as Delta Lake tables. These star schemas would be unique to each project or use-case, and would not feature any conformed dimensions. Furthermore, Power BI or any other BI tool would perform queries on these star schemas using Serverless Compute. \n\nIs this a clear and standard way to approaching data modeling in the Lakehouse, or do you any of you do it differently?", "author_fullname": "t2_9uqlze0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modeling in the Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d4wid", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683663874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been studying a lot about data modeling, but much of the information is specifically tailored towards data warehousing, and not so much towards modeling in data lakes or data lakehouses. &lt;/p&gt;\n\n&lt;p&gt;For those of you who manage a Data Lakehouse, I am interested in knowing how you approach data modeling in the various layers. Although a Lakehouse aims to merge Data Warehouse and Data Lake features by introducing ACID and CRUD functionalities on top of object storage, I feel that it is essential to prioritize appropriate data modeling practices, which are commonly utilized in data warehousing.&lt;/p&gt;\n\n&lt;p&gt;Lets say I have an ELT architecture that follows: Landing (ephemeral) -&amp;gt; Bronze -&amp;gt; Silver -&amp;gt; Gold &lt;/p&gt;\n\n&lt;p&gt;My questions is: How would you (or do you) enforce proper data modelling in Bronze/Silver/Gold layers?&lt;/p&gt;\n\n&lt;p&gt;Based on my research, I believe that Inmon-style modeling is the most suitable approach for a Lakehouse. In this scenario, both the Bronze and Silver layers would be source-oriented and maintain the normalized ER model precisely as the source. The Bronze data would then be upserted into the Silver layer, which would resemble the Data Warehouse layer seen in the Inmon Data Warehouse. &lt;/p&gt;\n\n&lt;p&gt;Next, the Silver layer is utilized to generate or update the data marts in the Gold layer, in response to business requests. To achieve this, I would design Kimball-style star schemas, wherein the fact and dimension tables remain as Delta Lake tables. These star schemas would be unique to each project or use-case, and would not feature any conformed dimensions. Furthermore, Power BI or any other BI tool would perform queries on these star schemas using Serverless Compute. &lt;/p&gt;\n\n&lt;p&gt;Is this a clear and standard way to approaching data modeling in the Lakehouse, or do you any of you do it differently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13d4wid", "is_robot_indexable": true, "report_reasons": null, "author": "EarthEmbarrassed4301", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13d4wid/data_modeling_in_the_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13d4wid/data_modeling_in_the_lakehouse/", "subreddit_subscribers": 104805, "created_utc": 1683663874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " **Discord:**\n\nwe\u2019re going from running 177 Cassandra nodes to just 72 ScyllaDB nodes. Each ScyllaDB node has 9 TB of disk space, up from the average of 4 TB per Cassandra node.\n\nOur tail latencies have also improved drastically. For example, fetching historical messages had a p99 of between 40-125ms on Cassandra, with ScyllaDB having a nice and chill 15ms p99 latency, and message insert performance going from 5-70ms p99 on Cassandra, to a steady 5ms p99 on ScyllaDB. Thanks to the aforementioned performance improvements, we\u2019ve unlocked new product use cases now that we have confidence in our messages database...\n\n(tracking events in the World Cup)\n\n People all over the world are stressed watching this incredible match, but meanwhile, Discord and the messages database aren\u2019t breaking a sweat. We\u2019re way up on message sends and handling it perfectly. \u00a0With our Rust-based data services and ScyllaDB (shard-per-core), we\u2019re able to shoulder this traffic and provide a platform for our users to communicate. [https://discord.com/blog/how-discord-stores-trillions-of-messages](https://discord.com/blog/how-discord-stores-trillions-of-messages) \n\n**1.28 Trillion rows per second but the input was to run the same data 100 times, so...kinda sus?**\n\n[https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/](https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/)\n\n**TInyBird - 12 Trillion Rows during Black Friday** \n\n An Nginx load balancer\n\n* A Varnish behind Nginx\n* Our Tinybird backend. It runs the API endpoints, the load balancing for Clickhouse replicas and also the ingestion part. It\u2019s a Python application with small bits of C++ for critical request paths.\n* A Clickhouse cluster\n* A Zookeeper cluster for data replication within Clickhouse\n\n[https://www.tinybird.co/blog-posts/how-we-setup-real-time-analytics-service-to-process-12-trillion-rows-during-black-friday](https://www.tinybird.co/blog-posts/how-we-setup-real-time-analytics-service-to-process-12-trillion-rows-during-black-friday)\n\n**VertiPaq - probably outdated since this is from 2010**\n\n There were plenty of \"oohs\" and \"aahs\" from the audience at the PASS Summit during Ted Kummert's keynote address Tuesday as he demonstrated new capabilities of the next version of SQL Server, called \"Denali.\"\n\nOne notable demonstration was with Microsoft's Amir Netz, who ran a side-by-side comparison of a database query using some 2 billion records, with one side of the screen showing the query being run with regular technology and the other side showing the same query running many times faster with the [**VertiPaq technology**](http://visualstudiomagazine.com/blogs/data-driver/2010/06/bi-powerpivot-demo-100-m-rows.aspx) borrowed from PowerPivot. Netz said the blinding speed of VertiPaq would equate to a theoretical processing rate of 1 trillion rows per minute.\n\n **The SQL Server Trillion Row Table - took over an hour**\n\n[https://erikdarlingdata.com/the-trillion-row-table/](https://erikdarlingdata.com/the-trillion-row-table/)", "author_fullname": "t2_80vvyzaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trillion Rows Per Minute: How would you structure an env, arch, or pipeline that did this?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dcngm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683682542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Discord:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;we\u2019re going from running 177 Cassandra nodes to just 72 ScyllaDB nodes. Each ScyllaDB node has 9 TB of disk space, up from the average of 4 TB per Cassandra node.&lt;/p&gt;\n\n&lt;p&gt;Our tail latencies have also improved drastically. For example, fetching historical messages had a p99 of between 40-125ms on Cassandra, with ScyllaDB having a nice and chill 15ms p99 latency, and message insert performance going from 5-70ms p99 on Cassandra, to a steady 5ms p99 on ScyllaDB. Thanks to the aforementioned performance improvements, we\u2019ve unlocked new product use cases now that we have confidence in our messages database...&lt;/p&gt;\n\n&lt;p&gt;(tracking events in the World Cup)&lt;/p&gt;\n\n&lt;p&gt;People all over the world are stressed watching this incredible match, but meanwhile, Discord and the messages database aren\u2019t breaking a sweat. We\u2019re way up on message sends and handling it perfectly. \u00a0With our Rust-based data services and ScyllaDB (shard-per-core), we\u2019re able to shoulder this traffic and provide a platform for our users to communicate. &lt;a href=\"https://discord.com/blog/how-discord-stores-trillions-of-messages\"&gt;https://discord.com/blog/how-discord-stores-trillions-of-messages&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1.28 Trillion rows per second but the input was to run the same data 100 times, so...kinda sus?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/\"&gt;https://www.singlestore.com/blog/memsql-processing-shatters-trillion-rows-per-second-barrier/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TInyBird - 12 Trillion Rows during Black Friday&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;An Nginx load balancer&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A Varnish behind Nginx&lt;/li&gt;\n&lt;li&gt;Our Tinybird backend. It runs the API endpoints, the load balancing for Clickhouse replicas and also the ingestion part. It\u2019s a Python application with small bits of C++ for critical request paths.&lt;/li&gt;\n&lt;li&gt;A Clickhouse cluster&lt;/li&gt;\n&lt;li&gt;A Zookeeper cluster for data replication within Clickhouse&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.tinybird.co/blog-posts/how-we-setup-real-time-analytics-service-to-process-12-trillion-rows-during-black-friday\"&gt;https://www.tinybird.co/blog-posts/how-we-setup-real-time-analytics-service-to-process-12-trillion-rows-during-black-friday&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;VertiPaq - probably outdated since this is from 2010&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;There were plenty of &amp;quot;oohs&amp;quot; and &amp;quot;aahs&amp;quot; from the audience at the PASS Summit during Ted Kummert&amp;#39;s keynote address Tuesday as he demonstrated new capabilities of the next version of SQL Server, called &amp;quot;Denali.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;One notable demonstration was with Microsoft&amp;#39;s Amir Netz, who ran a side-by-side comparison of a database query using some 2 billion records, with one side of the screen showing the query being run with regular technology and the other side showing the same query running many times faster with the &lt;a href=\"http://visualstudiomagazine.com/blogs/data-driver/2010/06/bi-powerpivot-demo-100-m-rows.aspx\"&gt;&lt;strong&gt;VertiPaq technology&lt;/strong&gt;&lt;/a&gt; borrowed from PowerPivot. Netz said the blinding speed of VertiPaq would equate to a theoretical processing rate of 1 trillion rows per minute.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The SQL Server Trillion Row Table - took over an hour&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://erikdarlingdata.com/the-trillion-row-table/\"&gt;https://erikdarlingdata.com/the-trillion-row-table/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?auto=webp&amp;v=enabled&amp;s=b1fc9516d12abf7d5fa616af3ff87ade48a9bcd5", "width": 1800, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f34b671a3572c6eff7efbf99c11b0c2930d5595d", "width": 108, "height": 43}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bfd681691b142c52ae5548ab621e9eb4da4dfcb", "width": 216, "height": 86}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5c9cb59417ea4eed46919dc15bf294251c6dd4c", "width": 320, "height": 128}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=60d656f86b1a63ba5119e5c7daf3b96dd8a725f3", "width": 640, "height": 256}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75f56faf6057b97e7ca7fcdf037130a66d9aedfc", "width": 960, "height": 384}, {"url": "https://external-preview.redd.it/AF7WSv243HMaUh0sy8xLWca9OrZMZIKEVyD1RsEJ-kY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa538c60fc7d4c5bd81151e90fb222772908404f", "width": 1080, "height": 432}], "variants": {}, "id": "pyoGenD5-m13RONB-xGYn7K1gfpx0aUjaCSfI-OcLdE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13dcngm", "is_robot_indexable": true, "report_reasons": null, "author": "Objective-Patient-37", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dcngm/trillion_rows_per_minute_how_would_you_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dcngm/trillion_rows_per_minute_how_would_you_structure/", "subreddit_subscribers": 104805, "created_utc": 1683682542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n just wondering if anyone of you did directly access an on-prem data source like SQL Server or Oracle Server directly from Azure Databricks and pulled data? \n\nWhile technically possible I think, I have some doubts with this approach in production but eager to hear some feedback from you. \nJust to contrast, a more common approach I have seen is in that scenario with on-prem sources is to use ADF with IR and stage the source data first in a Data Lake and then use Databricks to further process it..", "author_fullname": "t2_9fr6if3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks for extraction from on-premises?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cxtpv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683648549.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,\n just wondering if anyone of you did directly access an on-prem data source like SQL Server or Oracle Server directly from Azure Databricks and pulled data? &lt;/p&gt;\n\n&lt;p&gt;While technically possible I think, I have some doubts with this approach in production but eager to hear some feedback from you. \nJust to contrast, a more common approach I have seen is in that scenario with on-prem sources is to use ADF with IR and stage the source data first in a Data Lake and then use Databricks to further process it..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13cxtpv", "is_robot_indexable": true, "report_reasons": null, "author": "aj_here_", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13cxtpv/databricks_for_extraction_from_onpremises/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13cxtpv/databricks_for_extraction_from_onpremises/", "subreddit_subscribers": 104805, "created_utc": 1683648549.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_983tug55s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow 2.6: A New Milestone in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13cpkay", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vkn_c4X0CKJz4abof6U77eXeB6knf3zEtFni-M8VE2Y.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683638144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/artificial-corner/airflow-2-6-a-new-milestone-in-data-engineering-tech-ai-e9821d8d926e", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0t4Jd6IItpAJjpdnXk13Iqdgf_hDDlt64mAcNQmPFpQ.jpg?auto=webp&amp;v=enabled&amp;s=a022ad858feb8444ae4fd37a3c71809a279faf05", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/0t4Jd6IItpAJjpdnXk13Iqdgf_hDDlt64mAcNQmPFpQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd54069425de2c4a283bfc8bd16251e6b017fbe2", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/0t4Jd6IItpAJjpdnXk13Iqdgf_hDDlt64mAcNQmPFpQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb5581c20bf08201f0af6d8d0bddf2ea1bdb572d", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/0t4Jd6IItpAJjpdnXk13Iqdgf_hDDlt64mAcNQmPFpQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b3c0e01f880769285041fbc6c2e811bfc5612b5", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/0t4Jd6IItpAJjpdnXk13Iqdgf_hDDlt64mAcNQmPFpQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=380a096a26ee7dbb9d2d8851eae757494731dbb8", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/0t4Jd6IItpAJjpdnXk13Iqdgf_hDDlt64mAcNQmPFpQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=23c63fe973be4b2db78565e4158fa430649f07cf", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/0t4Jd6IItpAJjpdnXk13Iqdgf_hDDlt64mAcNQmPFpQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9b44774a6cc6dfa115581fb603ee38d8b30dbbcf", "width": 1080, "height": 720}], "variants": {}, "id": "QDKUm2qtbUlc5q7pZyGpS8mMqZr2KvVkyh_FWhq9AhE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13cpkay", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum247", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13cpkay/airflow_26_a_new_milestone_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/artificial-corner/airflow-2-6-a-new-milestone-in-data-engineering-tech-ai-e9821d8d926e", "subreddit_subscribers": 104805, "created_utc": 1683638144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Can you guys share your experience/story about building ETL systems on top of or modify complex ETL pipelines from a very big non-tech company with little tech related docs and that all of the people who design this system/architecture are not here ?   \nHow to build in and on a systems with a lot of reused code and none the peoples working with them know deep enough of the architecture.\n\nI am currently a junior and I feel so lost right now and hope for some guidance.", "author_fullname": "t2_ikks074r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience when working with very old (by tech standard) and very complex systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cllb8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683627751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can you guys share your experience/story about building ETL systems on top of or modify complex ETL pipelines from a very big non-tech company with little tech related docs and that all of the people who design this system/architecture are not here ?&lt;br/&gt;\nHow to build in and on a systems with a lot of reused code and none the peoples working with them know deep enough of the architecture.&lt;/p&gt;\n\n&lt;p&gt;I am currently a junior and I feel so lost right now and hope for some guidance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13cllb8", "is_robot_indexable": true, "report_reasons": null, "author": "SwimmingHair2075", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13cllb8/experience_when_working_with_very_old_by_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13cllb8/experience_when_working_with_very_old_by_tech/", "subreddit_subscribers": 104805, "created_utc": 1683627751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In our data analytic shop. we got about 20 automations (DAGs, Airflow, Cloud processing) to scrape and consume some public data. But some problems arise and some automations get stuck at different stages.\n\nThe notifications have to be very carefully crafted, so that only the important ones arrive to our data engineer, while the non-critical ones wait until the engineer does his daily check on them.\n\nIs there a system of alerts or a monitoring system, or simply and Standard Operating Procedure to achieve this crafted notification system, with different color codes for severity, etc? \n\nTake into account that this ideal system has to work on different platforms such as Ducker, Airflow, Amazon S3, Google Scheduler, etc\n\nI suspect that the final solution must be finally hand-coded. But I would like to have some guidance on what we need to code", "author_fullname": "t2_xnkh1lx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any recommendations on how to set up a system of alerts on data automation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dcjzi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683682278.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In our data analytic shop. we got about 20 automations (DAGs, Airflow, Cloud processing) to scrape and consume some public data. But some problems arise and some automations get stuck at different stages.&lt;/p&gt;\n\n&lt;p&gt;The notifications have to be very carefully crafted, so that only the important ones arrive to our data engineer, while the non-critical ones wait until the engineer does his daily check on them.&lt;/p&gt;\n\n&lt;p&gt;Is there a system of alerts or a monitoring system, or simply and Standard Operating Procedure to achieve this crafted notification system, with different color codes for severity, etc? &lt;/p&gt;\n\n&lt;p&gt;Take into account that this ideal system has to work on different platforms such as Ducker, Airflow, Amazon S3, Google Scheduler, etc&lt;/p&gt;\n\n&lt;p&gt;I suspect that the final solution must be finally hand-coded. But I would like to have some guidance on what we need to code&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13dcjzi", "is_robot_indexable": true, "report_reasons": null, "author": "rlopez7", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dcjzi/any_recommendations_on_how_to_set_up_a_system_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dcjzi/any_recommendations_on_how_to_set_up_a_system_of/", "subreddit_subscribers": 104805, "created_utc": 1683682278.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I got laid off a while ago and during that tough period I found out that even if I have my main job, maybe I should spend a little bit more time on my second job. Because if I\u2019m going to get married and raise a family in the future and I lost my job all of sudden, it\u2019ll be really difficult to make balance end. If I have a more flexible side business, that\u2019ll ensure cash flow and definitely improve risk prevention abilities for me and my family.\n\n I have been looking for some side business on the internet but actually there\u2019s not really any projects I\u2019ll call \u201cstable\u201d. As a DE there\u2019s not much you can do because many data solutions in the Upwork are just \u2018using Excel to perform a lookup\u2019. Maybe you\u2019ll be lucky enough to find a project that wants a Python script, but even for this type of entry level work the applicants can be many. Maybe I should learn a few frontend techniques.\n\nI do have some of my colleagues that are doing side business online or offline . Some open shop online. Some record courses.   but none of them really succeed on those. Sometimes it takes too much time and energy and even affects you main job.\n\nMy new job as a DE is not as stressful as my last one thus I finally got my time to literally practice on freelancing maybe. Anybody shed thoughts, experience or suggestion on that?", "author_fullname": "t2_oorup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is any of you working as a data engineer also doing side business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cpb2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683637581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got laid off a while ago and during that tough period I found out that even if I have my main job, maybe I should spend a little bit more time on my second job. Because if I\u2019m going to get married and raise a family in the future and I lost my job all of sudden, it\u2019ll be really difficult to make balance end. If I have a more flexible side business, that\u2019ll ensure cash flow and definitely improve risk prevention abilities for me and my family.&lt;/p&gt;\n\n&lt;p&gt;I have been looking for some side business on the internet but actually there\u2019s not really any projects I\u2019ll call \u201cstable\u201d. As a DE there\u2019s not much you can do because many data solutions in the Upwork are just \u2018using Excel to perform a lookup\u2019. Maybe you\u2019ll be lucky enough to find a project that wants a Python script, but even for this type of entry level work the applicants can be many. Maybe I should learn a few frontend techniques.&lt;/p&gt;\n\n&lt;p&gt;I do have some of my colleagues that are doing side business online or offline . Some open shop online. Some record courses.   but none of them really succeed on those. Sometimes it takes too much time and energy and even affects you main job.&lt;/p&gt;\n\n&lt;p&gt;My new job as a DE is not as stressful as my last one thus I finally got my time to literally practice on freelancing maybe. Anybody shed thoughts, experience or suggestion on that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13cpb2m", "is_robot_indexable": true, "report_reasons": null, "author": "GeForceKawaiiyo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13cpb2m/is_any_of_you_working_as_a_data_engineer_also/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13cpb2m/is_any_of_you_working_as_a_data_engineer_also/", "subreddit_subscribers": 104805, "created_utc": 1683637581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team has setup spark on kubernetes. We realised that we dont really understand the spark-operator.\nCan anybody here please point to any blog/book that can help us understand it better? some practical guidance would really help. Thanks", "author_fullname": "t2_5gs3oh162", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running Spark on kubernetes. How to understand the Spark-operator?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cz7ka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683651433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team has setup spark on kubernetes. We realised that we dont really understand the spark-operator.\nCan anybody here please point to any blog/book that can help us understand it better? some practical guidance would really help. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13cz7ka", "is_robot_indexable": true, "report_reasons": null, "author": "mythic-proportions", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13cz7ka/running_spark_on_kubernetes_how_to_understand_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13cz7ka/running_spark_on_kubernetes_how_to_understand_the/", "subreddit_subscribers": 104805, "created_utc": 1683651433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nWe often need users to be able to populate tables with basic information. Often, Google Sheets is a great fit for that especially that it can easily be linked to BigQuery tables for example.\n\nBut I have a case where I need a bit more than that. For example some fields should be drop downs coming from another table as they need to be consistent. Also, I need a bit more validation than is simple to do in Google Sheets.\n\nWhat are your go-to solutions for that? Any cool open source package? Django apps? Do you use commercial app builders like App Sheets? Heavily scripted google-sheets? \n\nAll suggestions welcome!", "author_fullname": "t2_w4trypto", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple Insert-Delete-Update apps?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d5ny5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683665584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;We often need users to be able to populate tables with basic information. Often, Google Sheets is a great fit for that especially that it can easily be linked to BigQuery tables for example.&lt;/p&gt;\n\n&lt;p&gt;But I have a case where I need a bit more than that. For example some fields should be drop downs coming from another table as they need to be consistent. Also, I need a bit more validation than is simple to do in Google Sheets.&lt;/p&gt;\n\n&lt;p&gt;What are your go-to solutions for that? Any cool open source package? Django apps? Do you use commercial app builders like App Sheets? Heavily scripted google-sheets? &lt;/p&gt;\n\n&lt;p&gt;All suggestions welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13d5ny5", "is_robot_indexable": true, "report_reasons": null, "author": "StressSnooze", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13d5ny5/simple_insertdeleteupdate_apps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13d5ny5/simple_insertdeleteupdate_apps/", "subreddit_subscribers": 104805, "created_utc": 1683665584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2mhgth69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s new in Apache Airflow 2.6 \ud83c\udf89", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13d2apn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/3YKqU_2ta6Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.6?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What's new in Apache Airflow 2.6?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/3YKqU_2ta6Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.6?\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/3YKqU_2ta6Q/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/3YKqU_2ta6Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.6?\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13d2apn", "height": 200}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/i0i4WgQGSsxciBmhvWifdLuAZ1LzkmqVjeivNsbC1o0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683658107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/3YKqU_2ta6Q", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YKDJOPa8zAdSNaUGdVggvZqBPHzu19IKWt0LXbRzKwQ.jpg?auto=webp&amp;v=enabled&amp;s=cfd94fbb23e71f1f78d3c995247d14c8ff05d4c0", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/YKDJOPa8zAdSNaUGdVggvZqBPHzu19IKWt0LXbRzKwQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32d5342d7592463afbee4a404b7719806afd21c2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/YKDJOPa8zAdSNaUGdVggvZqBPHzu19IKWt0LXbRzKwQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=81a9d09a1ad96c01d9bbda49ebf4497d8b10a8b2", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/YKDJOPa8zAdSNaUGdVggvZqBPHzu19IKWt0LXbRzKwQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=185efcedb20287d26cbaf9fba306aef7b1efd48e", "width": 320, "height": 240}], "variants": {}, "id": "GSyYbOTtccW94ZusLpCxJ_xGUETwkV0a0PhoCbM0WAY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13d2apn", "is_robot_indexable": true, "report_reasons": null, "author": "marclamberti", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13d2apn/whats_new_in_apache_airflow_26/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/3YKqU_2ta6Q", "subreddit_subscribers": 104805, "created_utc": 1683658107.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "What's new in Apache Airflow 2.6?", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/3YKqU_2ta6Q?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"What&amp;#39;s new in Apache Airflow 2.6?\"&gt;&lt;/iframe&gt;", "author_name": "Data with Marc", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/3YKqU_2ta6Q/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@MarcLamberti"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am working on implementing data optimization techniques such a z-ordering and partitioning on large datasets. What are some measurable ways to track the difference in query speeds before and after these changes?", "author_fullname": "t2_5w63rf7s2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Measure Query Speed In Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ctpp1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683644200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on implementing data optimization techniques such a z-ordering and partitioning on large datasets. What are some measurable ways to track the difference in query speeds before and after these changes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ctpp1", "is_robot_indexable": true, "report_reasons": null, "author": "eastieLad", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ctpp1/measure_query_speed_in_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ctpp1/measure_query_speed_in_databricks/", "subreddit_subscribers": 104805, "created_utc": 1683644200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Moving jobs and new company uses GCP and I come from Azure. Any recommended resources to start learning up GCP that you'd recommend?", "author_fullname": "t2_szv0ygic", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure to GCP resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13coe89", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683635516.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Moving jobs and new company uses GCP and I come from Azure. Any recommended resources to start learning up GCP that you&amp;#39;d recommend?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13coe89", "is_robot_indexable": true, "report_reasons": null, "author": "Hippodick666420", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13coe89/azure_to_gcp_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13coe89/azure_to_gcp_resources/", "subreddit_subscribers": 104805, "created_utc": 1683635516.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I wanted to share my personal newsletter with you. Every week, I collect information from the data world and compile it into the newsletter. If you're interested, please feel free to check it out. :)  \n\n\n[https://patrikbraborec.substack.com/p/data-news-27](https://patrikbraborec.substack.com/p/data-news-27)", "author_fullname": "t2_kyoi486i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "data news #27", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13djdap", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683703701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I wanted to share my personal newsletter with you. Every week, I collect information from the data world and compile it into the newsletter. If you&amp;#39;re interested, please feel free to check it out. :)  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://patrikbraborec.substack.com/p/data-news-27\"&gt;https://patrikbraborec.substack.com/p/data-news-27&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-KuSuklYLOIUJ40RHK_DQc2-IH8sTlAYZKY1cDarKpg.jpg?auto=webp&amp;v=enabled&amp;s=143df7fc7ff5798ff95632259dae11b8bc0f0c29", "width": 728, "height": 410}, "resolutions": [{"url": "https://external-preview.redd.it/-KuSuklYLOIUJ40RHK_DQc2-IH8sTlAYZKY1cDarKpg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b50cf17a8ed2fbf9c4a355d1ed66c9225e82c8a", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/-KuSuklYLOIUJ40RHK_DQc2-IH8sTlAYZKY1cDarKpg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4e9e076ffe1c78007f21dd2c588a28e10c4ad4c", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/-KuSuklYLOIUJ40RHK_DQc2-IH8sTlAYZKY1cDarKpg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd7da6fa517f400d974b94525baf4a63e8799103", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/-KuSuklYLOIUJ40RHK_DQc2-IH8sTlAYZKY1cDarKpg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09146a89447ce279daebe5b0c4ee63a4f648aa04", "width": 640, "height": 360}], "variants": {}, "id": "GlZqOZJb1qiWcUEE2Bnv64NVkdfXdOIMXF4o5nd3rEI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13djdap", "is_robot_indexable": true, "report_reasons": null, "author": "AmphibianInfamous574", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13djdap/data_news_27/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13djdap/data_news_27/", "subreddit_subscribers": 104805, "created_utc": 1683703701.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI\u2018ve got an upcoming Interview for a Data Quality Manager position in a large governmental organisation which is responsible for providing statistics for economical data to internal and external stakeholders.\n\nWhile I don\u2018t know much about their architecture and technological setup, I find the position quite intruiging because I\u2018m used to see that such a position is normally a responsibility of a certain other position.\n\nI have experience in T-SQL, but more in a business capacity, e.g. verifying data and KPI outputs and some Data Science/ML from my studies. I\u2018m a PO for a billing engine which calculates KPI\u2018s. The application is basically a huge SQL server DB which incorporates various ETL processes.\n\nAre there any redditors here who focus specifically on data quality as a field? Can someone provide some insights on what to expect?\n\nMy responsibilities would be:\n- Analyze data in a quality context and provide expertise to requestors for said data\n- develop and maintain data quality tools\n\nRequired skills from job posting:\n- Business studies (Master) at a technical college or university ideally with a minor  in Data Science or a comparable education; (got a Bachelors degree in business IT with minor data science)\n- Experience in the areas of financial markets and accounting is an advantage (I also have, but limited in financial markets, no experience with more complex stuff, i.e. derivatives and futures, accounting is one of my main stakeholders in my current position, which is in the financial sector)\n- experienced in the use of IT applications and programming experience (Python, R and SQL experience)", "author_fullname": "t2_oqhjefry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job Interview for Data Quality Manager Position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13djbxg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683703560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I\u2018ve got an upcoming Interview for a Data Quality Manager position in a large governmental organisation which is responsible for providing statistics for economical data to internal and external stakeholders.&lt;/p&gt;\n\n&lt;p&gt;While I don\u2018t know much about their architecture and technological setup, I find the position quite intruiging because I\u2018m used to see that such a position is normally a responsibility of a certain other position.&lt;/p&gt;\n\n&lt;p&gt;I have experience in T-SQL, but more in a business capacity, e.g. verifying data and KPI outputs and some Data Science/ML from my studies. I\u2018m a PO for a billing engine which calculates KPI\u2018s. The application is basically a huge SQL server DB which incorporates various ETL processes.&lt;/p&gt;\n\n&lt;p&gt;Are there any redditors here who focus specifically on data quality as a field? Can someone provide some insights on what to expect?&lt;/p&gt;\n\n&lt;p&gt;My responsibilities would be:\n- Analyze data in a quality context and provide expertise to requestors for said data\n- develop and maintain data quality tools&lt;/p&gt;\n\n&lt;p&gt;Required skills from job posting:\n- Business studies (Master) at a technical college or university ideally with a minor  in Data Science or a comparable education; (got a Bachelors degree in business IT with minor data science)\n- Experience in the areas of financial markets and accounting is an advantage (I also have, but limited in financial markets, no experience with more complex stuff, i.e. derivatives and futures, accounting is one of my main stakeholders in my current position, which is in the financial sector)\n- experienced in the use of IT applications and programming experience (Python, R and SQL experience)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13djbxg", "is_robot_indexable": true, "report_reasons": null, "author": "MisterObviousClearly", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13djbxg/job_interview_for_data_quality_manager_position/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13djbxg/job_interview_for_data_quality_manager_position/", "subreddit_subscribers": 104805, "created_utc": 1683703560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We've got IoT data coming in on a data stream. We're currently using some home-grown batch processing to analyse the data nightly, but we want to now do this analysis in real-time. The analyses comprise of many small steps, which become complicated when taken all together. For example: \n1. Get component1 current, averaged over 5-minute windows\n2. Get component1 voltage, averaged over 5-minute windows\n3. Join current and voltage, multiply to get component1 power\n4-6. Same thing for component 2\n7. Calculate total power consumption between component 1+2\n...\n\nThere are probably around 30 distinct steps, and most if not all of the intermediate results need to be persisted as output data. \n\nI feel like doing this as one big spark streaming job will get unmanageable and I'd like to split up the steps as a result.  Mentally I think of this as a DAG of transformation steps. Are there any tools or frameworks which support this mental model (something like Airflow/Prefect but for streaming data)?", "author_fullname": "t2_3ng3g5vc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DAG orchestration for streaming data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13diz51", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683702284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve got IoT data coming in on a data stream. We&amp;#39;re currently using some home-grown batch processing to analyse the data nightly, but we want to now do this analysis in real-time. The analyses comprise of many small steps, which become complicated when taken all together. For example: \n1. Get component1 current, averaged over 5-minute windows\n2. Get component1 voltage, averaged over 5-minute windows\n3. Join current and voltage, multiply to get component1 power\n4-6. Same thing for component 2\n7. Calculate total power consumption between component 1+2\n...&lt;/p&gt;\n\n&lt;p&gt;There are probably around 30 distinct steps, and most if not all of the intermediate results need to be persisted as output data. &lt;/p&gt;\n\n&lt;p&gt;I feel like doing this as one big spark streaming job will get unmanageable and I&amp;#39;d like to split up the steps as a result.  Mentally I think of this as a DAG of transformation steps. Are there any tools or frameworks which support this mental model (something like Airflow/Prefect but for streaming data)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13diz51", "is_robot_indexable": true, "report_reasons": null, "author": "cjolittle", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13diz51/dag_orchestration_for_streaming_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13diz51/dag_orchestration_for_streaming_data/", "subreddit_subscribers": 104805, "created_utc": 1683702284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a requirement to get the data near real time (within 5 minutes) from an event hub to both cosmos and Delta table (in ADLS). These will need to be done from two different Databricks clusters (and two different Databricks workspaces as well). There are some restrictions on the consumer groups in the event hub . I was thinking of a set up where the delta table  is loaded from the event hub and then for the cosmos job, the streaming source will be the delta table, running parallel. Anyone here used Delta as the streaming sink and streaming source at the same time ? Any challenges I should be aware of ? Any help would be much appreciated \ud83d\ude42", "author_fullname": "t2_qshu8mn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question on Spark Structured Streaming to and from Delta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13diyif", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683702220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a requirement to get the data near real time (within 5 minutes) from an event hub to both cosmos and Delta table (in ADLS). These will need to be done from two different Databricks clusters (and two different Databricks workspaces as well). There are some restrictions on the consumer groups in the event hub . I was thinking of a set up where the delta table  is loaded from the event hub and then for the cosmos job, the streaming source will be the delta table, running parallel. Anyone here used Delta as the streaming sink and streaming source at the same time ? Any challenges I should be aware of ? Any help would be much appreciated \ud83d\ude42&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13diyif", "is_robot_indexable": true, "report_reasons": null, "author": "Global_Industry_6801", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13diyif/question_on_spark_structured_streaming_to_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13diyif/question_on_spark_structured_streaming_to_and/", "subreddit_subscribers": 104805, "created_utc": 1683702220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have data in Amazon S3 stored in structure like \n\ns3://&lt;base_path&gt;/table_name/year=YYYY/month=MM/day=DD/datafile.parquet\n\nAnd now i want to query the table in specific time range for example 20220420 -&gt; 20220505. And i write query like: \n\nSelect * from Table_Name where (Year = 2022 and month=04 and date in(\u2026)) or ( Year=2022 and month=05 and date in(\u2026))  \n\nOr\n\nSelect * from Table_Name where Year || Month || Day between \u201c20220420\u201d and \u201c20220505\u201d\n\nAnd number of data scanned in 2 quries is the same. i am surprised that the second query can still leverage the partition columns since i concatenated them together. Right now i only have tens of partitons but if i have lots of partitions i do not know whether second query could still be efficient ? Should i still do that because writing query like the the first approach will take time. It will be great to hear your options about it\nThank you very much !!!", "author_fullname": "t2_7fyrjq8ff", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Select from Athena with table is partition like \u2018year=YYYY/month=MM/day=DD\u2019", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13diut3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683702747.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683701889.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have data in Amazon S3 stored in structure like &lt;/p&gt;\n\n&lt;p&gt;s3://&amp;lt;base_path&amp;gt;/table_name/year=YYYY/month=MM/day=DD/datafile.parquet&lt;/p&gt;\n\n&lt;p&gt;And now i want to query the table in specific time range for example 20220420 -&amp;gt; 20220505. And i write query like: &lt;/p&gt;\n\n&lt;p&gt;Select * from Table_Name where (Year = 2022 and month=04 and date in(\u2026)) or ( Year=2022 and month=05 and date in(\u2026))  &lt;/p&gt;\n\n&lt;p&gt;Or&lt;/p&gt;\n\n&lt;p&gt;Select * from Table_Name where Year || Month || Day between \u201c20220420\u201d and \u201c20220505\u201d&lt;/p&gt;\n\n&lt;p&gt;And number of data scanned in 2 quries is the same. i am surprised that the second query can still leverage the partition columns since i concatenated them together. Right now i only have tens of partitons but if i have lots of partitions i do not know whether second query could still be efficient ? Should i still do that because writing query like the the first approach will take time. It will be great to hear your options about it\nThank you very much !!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13diut3", "is_robot_indexable": true, "report_reasons": null, "author": "random_name_362", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13diut3/select_from_athena_with_table_is_partition_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13diut3/select_from_athena_with_table_is_partition_like/", "subreddit_subscribers": 104805, "created_utc": 1683701889.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One thing that stood out to me was the well-organized directory structure. [Github repo](https://github.com/ljvillarrealm/latam-cooliving) took the time to properly categorize their project files and included detailed documentation on changing directories while running the project. This helped ensure that everything ran smoothly and without any issues.  \n\n\n\\\\#dezoomcamp \\\\#datatalks", "author_fullname": "t2_52y1it67", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Review of LatAm CooLiving", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dhu7v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683698434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One thing that stood out to me was the well-organized directory structure. &lt;a href=\"https://github.com/ljvillarrealm/latam-cooliving\"&gt;Github repo&lt;/a&gt; took the time to properly categorize their project files and included detailed documentation on changing directories while running the project. This helped ensure that everything ran smoothly and without any issues.  &lt;/p&gt;\n\n&lt;p&gt;\\#dezoomcamp \\#datatalks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/k31VSZItZBjdRflrHXXfDMZtDG5Lwz8d70FraLtA2KY.jpg?auto=webp&amp;v=enabled&amp;s=6074d6c35e3cb4fb66de1736396cb2978eda36f4", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/k31VSZItZBjdRflrHXXfDMZtDG5Lwz8d70FraLtA2KY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce467ad10814f6e1248fef50560e1aa12b341278", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/k31VSZItZBjdRflrHXXfDMZtDG5Lwz8d70FraLtA2KY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1818e22a9b742d9405fdb10b313ad75ba070d376", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/k31VSZItZBjdRflrHXXfDMZtDG5Lwz8d70FraLtA2KY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52973a3c22a3056ed3fa54063243bf6f49ba11fd", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/k31VSZItZBjdRflrHXXfDMZtDG5Lwz8d70FraLtA2KY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f036b55b49066f0016d76793ee8053d567356666", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/k31VSZItZBjdRflrHXXfDMZtDG5Lwz8d70FraLtA2KY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9751418123d6f340d7b021940cb9e15456015220", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/k31VSZItZBjdRflrHXXfDMZtDG5Lwz8d70FraLtA2KY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e88a3a6f29193628618b57aae1810f573d3ba56", "width": 1080, "height": 540}], "variants": {}, "id": "XignxDSee0KlIeoV_SswhscdCc2ISRjet-ixbX28EH8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13dhu7v", "is_robot_indexable": true, "report_reasons": null, "author": "Manny-97", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dhu7v/review_of_latam_cooliving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dhu7v/review_of_latam_cooliving/", "subreddit_subscribers": 104805, "created_utc": 1683698434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've got several SAS files that have anywhere from 3,000 to 18,000 columns. 7,000 rows at most. Each column has a specific code that determines the columns meaning. For instance, the first character would be B for banana, A for apple, O for orange. Second char would be U for USA, M for Mexico, etc. \n\nMy goal is to make this more easily queryable using SQL -  just allowing me to search for all Oranges from Mexico for example. \n\nMy initial thought is to create a script that breaks down the \"code\" into several columns. So each cell in the old data set will turn into a row with the row ID (original), the value, and then the dimension values from the code having their own column. This would exponentially grow the number of rows, but I think would allow for easier querying. \n\nHas anyone done this before? Does anyone have any thoughts or resources?", "author_fullname": "t2_1yn5o9p2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating from wide SAS data sets to star-schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13desok", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683688643.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got several SAS files that have anywhere from 3,000 to 18,000 columns. 7,000 rows at most. Each column has a specific code that determines the columns meaning. For instance, the first character would be B for banana, A for apple, O for orange. Second char would be U for USA, M for Mexico, etc. &lt;/p&gt;\n\n&lt;p&gt;My goal is to make this more easily queryable using SQL -  just allowing me to search for all Oranges from Mexico for example. &lt;/p&gt;\n\n&lt;p&gt;My initial thought is to create a script that breaks down the &amp;quot;code&amp;quot; into several columns. So each cell in the old data set will turn into a row with the row ID (original), the value, and then the dimension values from the code having their own column. This would exponentially grow the number of rows, but I think would allow for easier querying. &lt;/p&gt;\n\n&lt;p&gt;Has anyone done this before? Does anyone have any thoughts or resources?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13desok", "is_robot_indexable": true, "report_reasons": null, "author": "10002Hours", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13desok/migrating_from_wide_sas_data_sets_to_starschema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13desok/migrating_from_wide_sas_data_sets_to_starschema/", "subreddit_subscribers": 104805, "created_utc": 1683688643.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Speed of Polars DataFrame is amazing so I love testing on it using billion-row dataset [https://youtu.be/odDOlU9KNqY](https://youtu.be/odDOlU9KNqY) \n\nSince it does not have processing message, therefore I open the windows resource monitor for doing long job.\n\nAlso suggest Polars to offer processing message automatically similar to this tradition  [https://youtu.be/nyjMMPoLnUg](https://youtu.be/nyjMMPoLnUg)", "author_fullname": "t2_9k0sxzns1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Processing Message Does Matter", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dchsj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683682114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Speed of Polars DataFrame is amazing so I love testing on it using billion-row dataset &lt;a href=\"https://youtu.be/odDOlU9KNqY\"&gt;https://youtu.be/odDOlU9KNqY&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Since it does not have processing message, therefore I open the windows resource monitor for doing long job.&lt;/p&gt;\n\n&lt;p&gt;Also suggest Polars to offer processing message automatically similar to this tradition  &lt;a href=\"https://youtu.be/nyjMMPoLnUg\"&gt;https://youtu.be/nyjMMPoLnUg&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GkJAE1lfWg5H7fzNc3YdjWIDiMgcU9uQtldBQr85Atc.jpg?auto=webp&amp;v=enabled&amp;s=47a5ab0a7b7f8fa5b28b6340e50e81653194f8d1", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GkJAE1lfWg5H7fzNc3YdjWIDiMgcU9uQtldBQr85Atc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=700b8e10702f5448a3c32b6543a408e09d14dd27", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GkJAE1lfWg5H7fzNc3YdjWIDiMgcU9uQtldBQr85Atc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e145a15744e2908719f91ee0c90e8588ae4de2b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GkJAE1lfWg5H7fzNc3YdjWIDiMgcU9uQtldBQr85Atc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=88dc15131f0dbbceaf3a257c25768ffa41c59076", "width": 320, "height": 240}], "variants": {}, "id": "leyjdRwNOA5Tr-xVzn4Jo1TvMJ39ezO--GWVeG6nzX4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13dchsj", "is_robot_indexable": true, "report_reasons": null, "author": "100GB-CSV", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13dchsj/processing_message_does_matter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13dchsj/processing_message_does_matter/", "subreddit_subscribers": 104805, "created_utc": 1683682114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3yleu7rp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Rise (and fall?) of Data Debt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "name": "t3_13d0gmj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KVusGwCOnbvEtREk3J1nj2TO2nr0s8KA9z4nQ0rwmcA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683654130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/alvin-ai/the-rise-and-fall-of-data-debt-cb99694c600b", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3s3jxWa5lcySCzZTJmD5ObuwV31fmIVA5_dh-cW7VVI.jpg?auto=webp&amp;v=enabled&amp;s=139f23a2606b6a585ff34baa062decf080b1339a", "width": 1200, "height": 692}, "resolutions": [{"url": "https://external-preview.redd.it/3s3jxWa5lcySCzZTJmD5ObuwV31fmIVA5_dh-cW7VVI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ce4340a24a47b567af61450f5c04c4238f2614", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/3s3jxWa5lcySCzZTJmD5ObuwV31fmIVA5_dh-cW7VVI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8af5ea6bac29a4e6815e6050038970ac027a33da", "width": 216, "height": 124}, {"url": "https://external-preview.redd.it/3s3jxWa5lcySCzZTJmD5ObuwV31fmIVA5_dh-cW7VVI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b967b798dc572503dd889cfeb6a49ddd5ee99e4", "width": 320, "height": 184}, {"url": "https://external-preview.redd.it/3s3jxWa5lcySCzZTJmD5ObuwV31fmIVA5_dh-cW7VVI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=324feafeeca6fa7ad163c01ce38c161f040b9279", "width": 640, "height": 369}, {"url": "https://external-preview.redd.it/3s3jxWa5lcySCzZTJmD5ObuwV31fmIVA5_dh-cW7VVI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b2590fdf3d35325ba0115d95a9f88c331d57960", "width": 960, "height": 553}, {"url": "https://external-preview.redd.it/3s3jxWa5lcySCzZTJmD5ObuwV31fmIVA5_dh-cW7VVI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b22012e439f191941d54d7f46663ece1b5cf5e2", "width": 1080, "height": 622}], "variants": {}, "id": "nu5QebDIZgn0iWsaaWPFOUkuy1U-HtHxU9Cfw3E1Q00"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13d0gmj", "is_robot_indexable": true, "report_reasons": null, "author": "gabsferreiradev", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13d0gmj/the_rise_and_fall_of_data_debt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/alvin-ai/the-rise-and-fall-of-data-debt-cb99694c600b", "subreddit_subscribers": 104805, "created_utc": 1683654130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a product person I am curious about the notion of out of the box connectors (think Kafka connect, airbyte, fivetran) and integrations which are subject to an insane amount of support requests and issues.\n\nIs this part of getting started quickly? Or not having to start from scratch?\n\nIs a large number of connectors which may or may not work very well preferable over development kits and APIs?", "author_fullname": "t2_6pheknqy6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a large catalog of unstable connectors preferred over development kits?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13cnjpv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683633381.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a product person I am curious about the notion of out of the box connectors (think Kafka connect, airbyte, fivetran) and integrations which are subject to an insane amount of support requests and issues.&lt;/p&gt;\n\n&lt;p&gt;Is this part of getting started quickly? Or not having to start from scratch?&lt;/p&gt;\n\n&lt;p&gt;Is a large number of connectors which may or may not work very well preferable over development kits and APIs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13cnjpv", "is_robot_indexable": true, "report_reasons": null, "author": "drc1728", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13cnjpv/is_a_large_catalog_of_unstable_connectors/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13cnjpv/is_a_large_catalog_of_unstable_connectors/", "subreddit_subscribers": 104805, "created_utc": 1683633381.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ClickHouse analytics can be fast\u00a0and\u00a0really cheap if you do it right. This webinar digs into the cheap part showing tricks any dev can apply to save up to 90% on cost. Here are three of many we'll discuss. First, do free as-in-beer development using open source. Second, optimize compute, storage, and memory on ClickHouse itself. Third, move off AWS or GCP completely to cheap hosting at vendors like Hetzner.\n\nJoin us LIVE for free TOMORROW 10th May. RSVP now: [https://hubs.la/Q01KS44X0](https://hubs.la/Q01KS44X0)", "author_fullname": "t2_s3zu6zpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Run ClickHouse like a Cheapskate \u2013 6 Ways to Save Money While Delivering Real-Time Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d01l7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683653223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ClickHouse analytics can be fast\u00a0and\u00a0really cheap if you do it right. This webinar digs into the cheap part showing tricks any dev can apply to save up to 90% on cost. Here are three of many we&amp;#39;ll discuss. First, do free as-in-beer development using open source. Second, optimize compute, storage, and memory on ClickHouse itself. Third, move off AWS or GCP completely to cheap hosting at vendors like Hetzner.&lt;/p&gt;\n\n&lt;p&gt;Join us LIVE for free TOMORROW 10th May. RSVP now: &lt;a href=\"https://hubs.la/Q01KS44X0\"&gt;https://hubs.la/Q01KS44X0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mfdWfVAUV9847uVp9ppvs3aDmchN8FP0TyJ9YWC6gOw.jpg?auto=webp&amp;v=enabled&amp;s=7d4ac615f6a63967da0e89661443db5b89921397", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/mfdWfVAUV9847uVp9ppvs3aDmchN8FP0TyJ9YWC6gOw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f709e11b232c12bf0a01ed3c2cc9cd6fbe428a52", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/mfdWfVAUV9847uVp9ppvs3aDmchN8FP0TyJ9YWC6gOw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c972bb8069656e7f620601e01a8f548e62e8cfc1", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/mfdWfVAUV9847uVp9ppvs3aDmchN8FP0TyJ9YWC6gOw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=11fc20e853dcc716da7698e1a5d88e5d7992363d", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/mfdWfVAUV9847uVp9ppvs3aDmchN8FP0TyJ9YWC6gOw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dda84edf8697443faa54ead92c53e0c44020c5a6", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/mfdWfVAUV9847uVp9ppvs3aDmchN8FP0TyJ9YWC6gOw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=921bc715d2ddbcde84f4fc7290baa539fe91eab0", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/mfdWfVAUV9847uVp9ppvs3aDmchN8FP0TyJ9YWC6gOw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=feccbf3f348b51d2079ffcf3b343fd5fefc01c8f", "width": 1080, "height": 607}], "variants": {}, "id": "xS1GOkkWuP_kkj4W-cnhetzDWfaWg9AR-SDydYAANZY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13d01l7", "is_robot_indexable": true, "report_reasons": null, "author": "RyhanSunny_Altinity", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13d01l7/run_clickhouse_like_a_cheapskate_6_ways_to_save/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13d01l7/run_clickhouse_like_a_cheapskate_6_ways_to_save/", "subreddit_subscribers": 104805, "created_utc": 1683653223.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}