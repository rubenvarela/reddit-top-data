{"kind": "Listing", "data": {"after": "t3_13d41ik", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Cause over time, it's gonna dry up and start to disintegrate, causing CRC errors in your SMART logs.", "author_fullname": "t2_9eczu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PSA: Western Digital HDD shuckers, don't use masking tape to block the 3.3V pin", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d9o47", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 345, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 345, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683674660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cause over time, it&amp;#39;s gonna dry up and start to disintegrate, causing CRC errors in your SMART logs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13d9o47", "is_robot_indexable": true, "report_reasons": null, "author": "lerouemm", "discussion_type": null, "num_comments": 140, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13d9o47/psa_western_digital_hdd_shuckers_dont_use_masking/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13d9o47/psa_western_digital_hdd_shuckers_dont_use_masking/", "subreddit_subscribers": 681909, "created_utc": 1683674660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I assume that by now most of us have heard of [youtube-dl](https://github.com/ytdl-org/youtube-dl)/[yt-dlp](https://github.com/yt-dlp/yt-dlp), and [gallery-dl](https://github.com/mikf/gallery-dl). And sure those are really really good if you want to download from sites popular enough to warrant extractors. But what if what you want to archive can't be archived by these tools?\n\nI want to see tools most of us have never heard or even thought of. Extractors for obscure websites or websites we don't think of in terms of extractors [like megatools](https://megatools.megous.com/) or [Mediafire Bulk Downloader](https://github.com/NicKoehler/mediafire_bulk_downloader). \"Glue\" tools like [rclone's `serve` command (as well as the rest of rclone)](https://rclone.org/commands/rclone_serve/) and [lftp](https://lftp.yar.ru/) that makes stuff work together easier. Phone apps like [FE File Explorer Pro (sadly the free version seems to be gone)](https://apps.apple.com/ca/app/fe-file-explorer-pro/id499470113) that makes accessing a home FTP server so much easier (once you've set up OpenVPN of course)\n\nAnd *especially* stuff so few people need that you had to make your own tools. We've all made crappy \"# TODO: FIX\" python scripts at 2am. There's no judgment at all from me\n\nHell, even if you only got 20% through making a downloader it may be a very useful start for people willing and able to finish it", "author_fullname": "t2_yj3jz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's a really niche tool you use that you can't live without?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dcxh2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 264, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 264, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683683296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I assume that by now most of us have heard of &lt;a href=\"https://github.com/ytdl-org/youtube-dl\"&gt;youtube-dl&lt;/a&gt;/&lt;a href=\"https://github.com/yt-dlp/yt-dlp\"&gt;yt-dlp&lt;/a&gt;, and &lt;a href=\"https://github.com/mikf/gallery-dl\"&gt;gallery-dl&lt;/a&gt;. And sure those are really really good if you want to download from sites popular enough to warrant extractors. But what if what you want to archive can&amp;#39;t be archived by these tools?&lt;/p&gt;\n\n&lt;p&gt;I want to see tools most of us have never heard or even thought of. Extractors for obscure websites or websites we don&amp;#39;t think of in terms of extractors &lt;a href=\"https://megatools.megous.com/\"&gt;like megatools&lt;/a&gt; or &lt;a href=\"https://github.com/NicKoehler/mediafire_bulk_downloader\"&gt;Mediafire Bulk Downloader&lt;/a&gt;. &amp;quot;Glue&amp;quot; tools like &lt;a href=\"https://rclone.org/commands/rclone_serve/\"&gt;rclone&amp;#39;s &lt;code&gt;serve&lt;/code&gt; command (as well as the rest of rclone)&lt;/a&gt; and &lt;a href=\"https://lftp.yar.ru/\"&gt;lftp&lt;/a&gt; that makes stuff work together easier. Phone apps like &lt;a href=\"https://apps.apple.com/ca/app/fe-file-explorer-pro/id499470113\"&gt;FE File Explorer Pro (sadly the free version seems to be gone)&lt;/a&gt; that makes accessing a home FTP server so much easier (once you&amp;#39;ve set up OpenVPN of course)&lt;/p&gt;\n\n&lt;p&gt;And &lt;em&gt;especially&lt;/em&gt; stuff so few people need that you had to make your own tools. We&amp;#39;ve all made crappy &amp;quot;# TODO: FIX&amp;quot; python scripts at 2am. There&amp;#39;s no judgment at all from me&lt;/p&gt;\n\n&lt;p&gt;Hell, even if you only got 20% through making a downloader it may be a very useful start for people willing and able to finish it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9ESi-sGeI-oz2o8DjYJofRlh8qQxu7HLEA02V5jA81U.jpg?auto=webp&amp;v=enabled&amp;s=11fa2ce22c0c277a773be64ffbd163f8b87aba24", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/9ESi-sGeI-oz2o8DjYJofRlh8qQxu7HLEA02V5jA81U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3310e5d2b5ec98736d08bac31856bbe7ff3fc1eb", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/9ESi-sGeI-oz2o8DjYJofRlh8qQxu7HLEA02V5jA81U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a11d31bd31633dc9809d85f35a1c607d5706420f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/9ESi-sGeI-oz2o8DjYJofRlh8qQxu7HLEA02V5jA81U.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d47e27dc1446186631c093ea9344e121135fb437", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/9ESi-sGeI-oz2o8DjYJofRlh8qQxu7HLEA02V5jA81U.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=336634dd0b528a8ba26bbebc43e2e159d928d868", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/9ESi-sGeI-oz2o8DjYJofRlh8qQxu7HLEA02V5jA81U.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f1bcc9ed2688c10732c622fa470a60f3e41c357", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/9ESi-sGeI-oz2o8DjYJofRlh8qQxu7HLEA02V5jA81U.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ac65670017096cd0240446267a46d89f1a7ac4d", "width": 1080, "height": 540}], "variants": {}, "id": "iOkfw7fTCubXFKjzrDQNlZPSG2GLlas8Oq9HbiK2Tlw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "The sexiest data storage medium", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13dcxh2", "is_robot_indexable": true, "report_reasons": null, "author": "Scripter17", "discussion_type": null, "num_comments": 117, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13dcxh2/whats_a_really_niche_tool_you_use_that_you_cant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dcxh2/whats_a_really_niche_tool_you_use_that_you_cant/", "subreddit_subscribers": 681909, "created_utc": 1683683296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_3a4wg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital: Customer info stolen in March IT attack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13ddd0n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 107, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 107, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JtY45QQNHzoC9fJBSkJOv7hO_OkITtIm6QzqUuzc5_M.jpg", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683684453.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theregister.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theregister.com/2023/05/08/western_digital_customer_data/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Mvf0tcbte6pj7PnnHn-1CVxZIwPE6uHSCRN43LV_qwo.jpg?auto=webp&amp;v=enabled&amp;s=e329e9e0319a12b4dfebd454fb10da87cdef130a", "width": 1000, "height": 667}, "resolutions": [{"url": "https://external-preview.redd.it/Mvf0tcbte6pj7PnnHn-1CVxZIwPE6uHSCRN43LV_qwo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6242184437122114547aebe205773922f4591f22", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/Mvf0tcbte6pj7PnnHn-1CVxZIwPE6uHSCRN43LV_qwo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5afeaab3dda95340d2993c20fba08a8dae4b56a0", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/Mvf0tcbte6pj7PnnHn-1CVxZIwPE6uHSCRN43LV_qwo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46aacd6cf55b7d4e1d496eb9181c2ee1ef7b0508", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/Mvf0tcbte6pj7PnnHn-1CVxZIwPE6uHSCRN43LV_qwo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92f5bcc8964232b311af6220e621cbc2de63354b", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/Mvf0tcbte6pj7PnnHn-1CVxZIwPE6uHSCRN43LV_qwo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=781f43a5175ef3ae9c7c7b581ed40c630434a181", "width": 960, "height": 640}], "variants": {}, "id": "scfEFwMYcZ5Z-cKYYQWl4vq8b_i-y2GT6cE8LSiewe4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ddd0n", "is_robot_indexable": true, "report_reasons": null, "author": "wewewawa", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ddd0n/western_digital_customer_info_stolen_in_march_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theregister.com/2023/05/08/western_digital_customer_data/", "subreddit_subscribers": 681909, "created_utc": 1683684453.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looks like it's dead for real this time.  Comment/submission datadumps have all been taken down today and the API has not been working for a week.  RIP\n\nMight be worth taking a look at their hacker news/twitter/stackoverflow archive before that's taken down as well", "author_fullname": "t2_jxsk4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pushshift Reddit archive is dead", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dds95", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683685653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looks like it&amp;#39;s dead for real this time.  Comment/submission datadumps have all been taken down today and the API has not been working for a week.  RIP&lt;/p&gt;\n\n&lt;p&gt;Might be worth taking a look at their hacker news/twitter/stackoverflow archive before that&amp;#39;s taken down as well&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100 Zettabytes zfs", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dds95", "is_robot_indexable": true, "report_reasons": null, "author": "Yekab0f", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13dds95/pushshift_reddit_archive_is_dead/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dds95/pushshift_reddit_archive_is_dead/", "subreddit_subscribers": 681909, "created_utc": 1683685653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just posting to raise awareness of BlueMaxima's [Project Flashpoint,](https://bluemaxima.org/flashpoint/), a project that aims to conserve as many flash games and animations as possible from before December 31, 2020, the discontinuation of Adobe Flash.  To date there are over 150,000 Flash games and over 25,000 animations saved.  \n\nThe entire collection downloads at just under 1.5TB, uncompressed it is 1.7 TB. \n\nOr, you can download an on-demand player that downloads the games a la carte that is just around 3GB uncompressed to begin with.\n\n[Download page for both is here](https://bluemaxima.org/flashpoint/downloads/).", "author_fullname": "t2_mp1eq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BlueMaxima's Project Flashpoint has saved over 150,000 Flash Games and 25,000 Flash animations.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dtot0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 83, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 83, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683733547.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683731793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just posting to raise awareness of BlueMaxima&amp;#39;s &lt;a href=\"https://bluemaxima.org/flashpoint/\"&gt;Project Flashpoint,&lt;/a&gt;, a project that aims to conserve as many flash games and animations as possible from before December 31, 2020, the discontinuation of Adobe Flash.  To date there are over 150,000 Flash games and over 25,000 animations saved.  &lt;/p&gt;\n\n&lt;p&gt;The entire collection downloads at just under 1.5TB, uncompressed it is 1.7 TB. &lt;/p&gt;\n\n&lt;p&gt;Or, you can download an on-demand player that downloads the games a la carte that is just around 3GB uncompressed to begin with.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://bluemaxima.org/flashpoint/downloads/\"&gt;Download page for both is here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13dtot0", "is_robot_indexable": true, "report_reasons": null, "author": "HGMIV926", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dtot0/bluemaximas_project_flashpoint_has_saved_over/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dtot0/bluemaximas_project_flashpoint_has_saved_over/", "subreddit_subscribers": 681909, "created_utc": 1683731793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone found a way to digitize there DVD collection for there NAS/Plex Server?\n\nI was thinking about buying a DVD/CD Duplicator and modding it's 7 drives to connect to USB 3.0 hub and connecting it to my computer, and using software to automate it.\n\nAny thoughts?", "author_fullname": "t2_52dasc0u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easiest way to digitize multiple DVDs for plex server/NAS storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d5ij5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683665242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone found a way to digitize there DVD collection for there NAS/Plex Server?&lt;/p&gt;\n\n&lt;p&gt;I was thinking about buying a DVD/CD Duplicator and modding it&amp;#39;s 7 drives to connect to USB 3.0 hub and connecting it to my computer, and using software to automate it.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13d5ij5", "is_robot_indexable": true, "report_reasons": null, "author": "mediagenius", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13d5ij5/easiest_way_to_digitize_multiple_dvds_for_plex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13d5ij5/easiest_way_to_digitize_multiple_dvds_for_plex/", "subreddit_subscribers": 681909, "created_utc": 1683665242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hola,\n\nSo a bit of backstory, my friend and I are photographers and he recently lent me his Extreme for a trip when my Orico M\n2 enclosure crapped out on me. My workflow consists of having my Lightroom catalog and the past year's photos on a portable drive and have the rest on my PC. This lets me easily sort and edit photos on my PC and Macbook. I also have an external backup.\n\nI'm returning this drive to him and intend on buying my own in 2TB, but I don't know which.\n\nOn one hand, the Extreme is smaller, noticeably lighter, and I quite like the shorter cable.\n\nThe Pro is 2x as fast, however, that speed I'm not going to utilize until I upgrade my PC a few years from now. The Pro is $60CAD or $45USD more than the Extreme.\n\nSo I have 2 schools of thought at the moment:\n1. Futureproof myself now at a slightly higher price.\n2. Keep the cheaper version, and when I need to upgrade to a faster 4TB, I can use the Extreme as a PS5 external drive.\n\nThoughts?", "author_fullname": "t2_gq2n9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SanDisk Extreme Portable vs Extreme Pro Portable. Which one to buy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_13dllfx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dUCTeZzpEKCuAPvlFNucwf3Ompozy8P4oshwIZBWhWw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683711728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hola,&lt;/p&gt;\n\n&lt;p&gt;So a bit of backstory, my friend and I are photographers and he recently lent me his Extreme for a trip when my Orico M\n2 enclosure crapped out on me. My workflow consists of having my Lightroom catalog and the past year&amp;#39;s photos on a portable drive and have the rest on my PC. This lets me easily sort and edit photos on my PC and Macbook. I also have an external backup.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m returning this drive to him and intend on buying my own in 2TB, but I don&amp;#39;t know which.&lt;/p&gt;\n\n&lt;p&gt;On one hand, the Extreme is smaller, noticeably lighter, and I quite like the shorter cable.&lt;/p&gt;\n\n&lt;p&gt;The Pro is 2x as fast, however, that speed I&amp;#39;m not going to utilize until I upgrade my PC a few years from now. The Pro is $60CAD or $45USD more than the Extreme.&lt;/p&gt;\n\n&lt;p&gt;So I have 2 schools of thought at the moment:\n1. Futureproof myself now at a slightly higher price.\n2. Keep the cheaper version, and when I need to upgrade to a faster 4TB, I can use the Extreme as a PS5 external drive.&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lmrm5bqen0za1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lmrm5bqen0za1.jpg?auto=webp&amp;v=enabled&amp;s=e44e3e0252d727676ddbd12e11be6206c40713cd", "width": 3000, "height": 4000}, "resolutions": [{"url": "https://preview.redd.it/lmrm5bqen0za1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=52aaf53ab2c9304aea26be9537c7918bca71e471", "width": 108, "height": 144}, {"url": "https://preview.redd.it/lmrm5bqen0za1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c666408bfef4cedc9d73efb9d9e2ecca67380ac5", "width": 216, "height": 288}, {"url": "https://preview.redd.it/lmrm5bqen0za1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eee2928b496a0e19658deb9a10093fa51f552c6a", "width": 320, "height": 426}, {"url": "https://preview.redd.it/lmrm5bqen0za1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1897042d0915debd15f25512044d52b5696d3c5a", "width": 640, "height": 853}, {"url": "https://preview.redd.it/lmrm5bqen0za1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a13fca662c1bb5cf7378547042a03031c8d6ff31", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/lmrm5bqen0za1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95efa7e5a2806400bf4534f1f862e7e51fc9e6fe", "width": 1080, "height": 1440}], "variants": {}, "id": "9c8hBJ-Niq5FBjPN7Cdn7iXofl7M-B7Qd76hpLAAOPs"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dllfx", "is_robot_indexable": true, "report_reasons": null, "author": "NavXIII", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dllfx/sandisk_extreme_portable_vs_extreme_pro_portable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lmrm5bqen0za1.jpg", "subreddit_subscribers": 681909, "created_utc": 1683711728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking to maybe re-configure my system. I have 2 LSI IT mode x8 2 port cards, cant seem to find anything on x16 4 port cards that support 16 drives. \n\nDoes anyone have any good recommendations?", "author_fullname": "t2_45xca", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any good PCIE x16 16 port sas cards that work in IT mode?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d79iu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683669067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to maybe re-configure my system. I have 2 LSI IT mode x8 2 port cards, cant seem to find anything on x16 4 port cards that support 16 drives. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any good recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13d79iu", "is_robot_indexable": true, "report_reasons": null, "author": "spikerman", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13d79iu/any_good_pcie_x16_16_port_sas_cards_that_work_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13d79iu/any_good_pcie_x16_16_port_sas_cards_that_work_in/", "subreddit_subscribers": 681909, "created_utc": 1683669067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is it possible to save every scorecard, profile, series, etc. from the MyCricket website ([http://mycricket.cricket.com.au/?entityid=1900&amp;save=0](http://mycricket.cricket.com.au/?entityid=1900&amp;save=0))? The site operators have announced that it will be taken down from Aug 31, 2023 and they have no current plans to save all the historical stats. \n\nSo other than going page by page manually, is it possible to download a working copy of the website locally or bulk export the data somehow? I've tried searching around, but didn't know what I should be looking for.", "author_fullname": "t2_4na0kimf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I save content from the MyCricket website before Aug 31?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dhk52", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683697472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to save every scorecard, profile, series, etc. from the MyCricket website (&lt;a href=\"http://mycricket.cricket.com.au/?entityid=1900&amp;amp;save=0\"&gt;http://mycricket.cricket.com.au/?entityid=1900&amp;amp;save=0&lt;/a&gt;)? The site operators have announced that it will be taken down from Aug 31, 2023 and they have no current plans to save all the historical stats. &lt;/p&gt;\n\n&lt;p&gt;So other than going page by page manually, is it possible to download a working copy of the website locally or bulk export the data somehow? I&amp;#39;ve tried searching around, but didn&amp;#39;t know what I should be looking for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/v8F3DY9uCIf-PdDmFmV0D1QuQRFhTNCYGx48XbWG4HQ.jpg?auto=webp&amp;v=enabled&amp;s=b5d8bf455bf5174ab950581c453bcdc7192b1ecf", "width": 135, "height": 175}, "resolutions": [{"url": "https://external-preview.redd.it/v8F3DY9uCIf-PdDmFmV0D1QuQRFhTNCYGx48XbWG4HQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76f53bcc8c579c91868fbac9b4f50b9bc8a132c3", "width": 108, "height": 140}], "variants": {}, "id": "aEEE8wuNoSUz5gWgUZF74Wty48zcSncMeDDQRd3t13I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dhk52", "is_robot_indexable": true, "report_reasons": null, "author": "chainglitch", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dhk52/how_can_i_save_content_from_the_mycricket_website/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dhk52/how_can_i_save_content_from_the_mycricket_website/", "subreddit_subscribers": 681909, "created_utc": 1683697472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Howdy Hoarders!\n\nI'm helping a close friend to get her life in order.  \nShe's a scientist with a 40+ year career behind her.  \nThere are three types of media that she needs to digitize.  \n\n\n1. Rare recordings  \nSettled locally with some volunteers.\n\n2. Brittle newspapers (from the fall of the Soviet Union era)  \nThe university and archive here are interested but are slow and probably don't have funding.  \nMaybe it's best to send these off to the Internet Archive?\n\n3. Handwritten journals  \nNow for the journals, I hope to convince her to get rid of the originals and keep the digital version only, so we can just cut the binding and feed them into a scanner.  \nMy main question about those is: how would you store this? Make a PDF out of each journal? Just keep them as loose scanned files and a folder per journal?  \n\n\nBonus: I have a year or two worth of my own paper planner as well, so question 3 would be relevant to myself as well. I keep a daily personal diary in Notion and I was thinking to just stick a scan of each daily page into that diary too. But then again I'd like to be able to browse it in a more simple way, such as a PDF.", "author_fullname": "t2_hghp21c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "40 years worth of PAPER data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dtan9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683730966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy Hoarders!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m helping a close friend to get her life in order.&lt;br/&gt;\nShe&amp;#39;s a scientist with a 40+ year career behind her.&lt;br/&gt;\nThere are three types of media that she needs to digitize.  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Rare recordings&lt;br/&gt;\nSettled locally with some volunteers.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Brittle newspapers (from the fall of the Soviet Union era)&lt;br/&gt;\nThe university and archive here are interested but are slow and probably don&amp;#39;t have funding.&lt;br/&gt;\nMaybe it&amp;#39;s best to send these off to the Internet Archive?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Handwritten journals&lt;br/&gt;\nNow for the journals, I hope to convince her to get rid of the originals and keep the digital version only, so we can just cut the binding and feed them into a scanner.&lt;br/&gt;\nMy main question about those is: how would you store this? Make a PDF out of each journal? Just keep them as loose scanned files and a folder per journal?  &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Bonus: I have a year or two worth of my own paper planner as well, so question 3 would be relevant to myself as well. I keep a daily personal diary in Notion and I was thinking to just stick a scan of each daily page into that diary too. But then again I&amp;#39;d like to be able to browse it in a more simple way, such as a PDF.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dtan9", "is_robot_indexable": true, "report_reasons": null, "author": "SeventhBus", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dtan9/40_years_worth_of_paper_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dtan9/40_years_worth_of_paper_data/", "subreddit_subscribers": 681909, "created_utc": 1683730966.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nI've trying to find the means to batch download a user's full profile on TikTok without watermark. Most posts on this subreddit about this issue have outdated methods, that no longer work (for example yt-dlp). I've also tried JDownloader 2, but it doesn't work for me.\n\nI've found [this Chinese GitHub](https://github.com/Johnserf-Seed/TikTokDownload), but I'm not sure how to use it as I do not know the language.\n\nDoes someone know another method of doing it? Preferably a FOSS way, that's compatible with Windows, but Linux is also fine I guess.\n\nAnyways, thank you for reading my post! Have a nice life!", "author_fullname": "t2_2p9hjt1v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Downloading all videos on a TikTok profile", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d3esi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683661530.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683660572.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve trying to find the means to batch download a user&amp;#39;s full profile on TikTok without watermark. Most posts on this subreddit about this issue have outdated methods, that no longer work (for example yt-dlp). I&amp;#39;ve also tried JDownloader 2, but it doesn&amp;#39;t work for me.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve found &lt;a href=\"https://github.com/Johnserf-Seed/TikTokDownload\"&gt;this Chinese GitHub&lt;/a&gt;, but I&amp;#39;m not sure how to use it as I do not know the language.&lt;/p&gt;\n\n&lt;p&gt;Does someone know another method of doing it? Preferably a FOSS way, that&amp;#39;s compatible with Windows, but Linux is also fine I guess.&lt;/p&gt;\n\n&lt;p&gt;Anyways, thank you for reading my post! Have a nice life!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/mOZ7vmYSm5qOYp_ZscI9eareJCYNkAn9k1D_VywwwFQ.jpg?auto=webp&amp;v=enabled&amp;s=446640de3735a7474516b7f9f9ada91a50b31ab9", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/mOZ7vmYSm5qOYp_ZscI9eareJCYNkAn9k1D_VywwwFQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ba73a37a39626bf44f86cfb055a84930b1e4ab", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/mOZ7vmYSm5qOYp_ZscI9eareJCYNkAn9k1D_VywwwFQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=171313dd3f49691b1916e0ffb28dbce22546351b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/mOZ7vmYSm5qOYp_ZscI9eareJCYNkAn9k1D_VywwwFQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3260b6e4780114e648ad96696ab5c97059f45da", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/mOZ7vmYSm5qOYp_ZscI9eareJCYNkAn9k1D_VywwwFQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d021281b3da2a077ff5d4bb6b4121f4402b8599", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/mOZ7vmYSm5qOYp_ZscI9eareJCYNkAn9k1D_VywwwFQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94107dc7692bde0e6cf6b83c907b24072198f729", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/mOZ7vmYSm5qOYp_ZscI9eareJCYNkAn9k1D_VywwwFQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e626728fee30caaba2986178d0b2901dd8a78be1", "width": 1080, "height": 540}], "variants": {}, "id": "qJ18FxgfqBxpOtYBodythfd3v_cpt-sZtkflZ0zrH7I"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13d3esi", "is_robot_indexable": true, "report_reasons": null, "author": "InterMob", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13d3esi/downloading_all_videos_on_a_tiktok_profile/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13d3esi/downloading_all_videos_on_a_tiktok_profile/", "subreddit_subscribers": 681909, "created_utc": 1683660572.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I found this one [https://www.amazon.ca/VERBATIM-External-Blu-ray-Compatible-71097/dp/B09PY6SC9Y/](https://www.amazon.ca/VERBATIM-External-Blu-ray-Compatible-71097/dp/B09PY6SC9Y/) \n\nIts the best price I've been able to find for a good brand but Its still a bit expensive. I also heard that only specific ones can do 4k discs? I'm mostly just going to use MakeMKV with it.\n\nIt can be an internal one with an external shell if thats better, I'm not sure what is better in this case.", "author_fullname": "t2_dd6cqc46", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can anyone help me pick out a good BD Drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dhoa7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683697857.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found this one &lt;a href=\"https://www.amazon.ca/VERBATIM-External-Blu-ray-Compatible-71097/dp/B09PY6SC9Y/\"&gt;https://www.amazon.ca/VERBATIM-External-Blu-ray-Compatible-71097/dp/B09PY6SC9Y/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Its the best price I&amp;#39;ve been able to find for a good brand but Its still a bit expensive. I also heard that only specific ones can do 4k discs? I&amp;#39;m mostly just going to use MakeMKV with it.&lt;/p&gt;\n\n&lt;p&gt;It can be an internal one with an external shell if thats better, I&amp;#39;m not sure what is better in this case.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dhoa7", "is_robot_indexable": true, "report_reasons": null, "author": "Standard-Historian79", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dhoa7/can_anyone_help_me_pick_out_a_good_bd_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dhoa7/can_anyone_help_me_pick_out_a_good_bd_drive/", "subreddit_subscribers": 681909, "created_utc": 1683697857.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to download a video with yt-dlp with only one specific video and audio track. The formats available are \n\n    ID EXT RESOLUTION FPS \u2502   TBR PROTO \u2502 VCODEC          VBR ACODEC      \n    14 m4a audio only     \u2502  449k dash  \u2502 audio only          ac-3      \n    13 m4a audio only     \u2502  196k dash  \u2502 audio only          mp4a.40.2\n    11 m4a audio only     \u2502  196k dash  \u2502 audio only          mp4a.40.2 \n    15 m4a audio only     \u2502  197k dash  \u2502 audio only          mp4a.40.2\n    9  mp4 416x234     25 \u2502  606k dash  \u2502 avc1.42c01f    606k video only\n    4  mp4 416x234     25 \u2502  448k dash  \u2502 hvc1.2.4.L123  448k video only\n    8  mp4 640x360     25 \u2502 1510k dash  \u2502 avc1.4d401f   1510k video only\n    3  mp4 640x360     25 \u2502  856k dash  \u2502 hvc1.2.4.L123  856k video only\n    7  mp4 960x540     25 \u2502 2299k dash  \u2502 avc1.4d401f   2299k video only\n    2  mp4 960x540     25 \u2502 1580k dash  \u2502 hvc1.2.4.L123 1580k video only\n    6  mp4 1280x720    25 \u2502 3631k dash  \u2502 avc1.4d401f   3631k video only\n    1  mp4 1280x720    25 \u2502 2458k dash  \u2502 hvc1.2.4.L123 2458k video only\n    5  mp4 1920x1080   25 \u2502 5435k dash  \u2502 avc1.640029   5435k video only\n    0  mp4 1920x1080   25 \u2502 4486k dash  \u2502 hvc1.2.4.L123 4486k video only\n\nI can get a correct format by using `-f mergeall` but that gives me several accessibility audio tracks that I'm currently not interested in having. If I use `-f 0,11` for example it gives me separate output files (one mp4 file which is only the video, and a separate m4a file that is audio only). Is there a way to merge video and audio output without having to download all formats?", "author_fullname": "t2_bjlgrfr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using yt-dlp to merge video and audio from specific formats only?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dcmxv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683682502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to download a video with yt-dlp with only one specific video and audio track. The formats available are &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ID EXT RESOLUTION FPS \u2502   TBR PROTO \u2502 VCODEC          VBR ACODEC      \n14 m4a audio only     \u2502  449k dash  \u2502 audio only          ac-3      \n13 m4a audio only     \u2502  196k dash  \u2502 audio only          mp4a.40.2\n11 m4a audio only     \u2502  196k dash  \u2502 audio only          mp4a.40.2 \n15 m4a audio only     \u2502  197k dash  \u2502 audio only          mp4a.40.2\n9  mp4 416x234     25 \u2502  606k dash  \u2502 avc1.42c01f    606k video only\n4  mp4 416x234     25 \u2502  448k dash  \u2502 hvc1.2.4.L123  448k video only\n8  mp4 640x360     25 \u2502 1510k dash  \u2502 avc1.4d401f   1510k video only\n3  mp4 640x360     25 \u2502  856k dash  \u2502 hvc1.2.4.L123  856k video only\n7  mp4 960x540     25 \u2502 2299k dash  \u2502 avc1.4d401f   2299k video only\n2  mp4 960x540     25 \u2502 1580k dash  \u2502 hvc1.2.4.L123 1580k video only\n6  mp4 1280x720    25 \u2502 3631k dash  \u2502 avc1.4d401f   3631k video only\n1  mp4 1280x720    25 \u2502 2458k dash  \u2502 hvc1.2.4.L123 2458k video only\n5  mp4 1920x1080   25 \u2502 5435k dash  \u2502 avc1.640029   5435k video only\n0  mp4 1920x1080   25 \u2502 4486k dash  \u2502 hvc1.2.4.L123 4486k video only\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I can get a correct format by using &lt;code&gt;-f mergeall&lt;/code&gt; but that gives me several accessibility audio tracks that I&amp;#39;m currently not interested in having. If I use &lt;code&gt;-f 0,11&lt;/code&gt; for example it gives me separate output files (one mp4 file which is only the video, and a separate m4a file that is audio only). Is there a way to merge video and audio output without having to download all formats?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dcmxv", "is_robot_indexable": true, "report_reasons": null, "author": "ConstantConsumption", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dcmxv/using_ytdlp_to_merge_video_and_audio_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dcmxv/using_ytdlp_to_merge_video_and_audio_from/", "subreddit_subscribers": 681909, "created_utc": 1683682502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The reddit dumps are several TB in size but not an impossible amount to work with. Someone could potentially extract all the imgur links there and archive the content to the wayback machine. Is a similar effort happening or am I imagining things...", "author_fullname": "t2_14mh5t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm OOTL, is there an effort to archive Imgur links contained in the reddit dumps before the purge?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dtw12", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683732194.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The reddit dumps are several TB in size but not an impossible amount to work with. Someone could potentially extract all the imgur links there and archive the content to the wayback machine. Is a similar effort happening or am I imagining things...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dtw12", "is_robot_indexable": true, "report_reasons": null, "author": "HQuasar", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dtw12/im_ootl_is_there_an_effort_to_archive_imgur_links/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dtw12/im_ootl_is_there_an_effort_to_archive_imgur_links/", "subreddit_subscribers": 681909, "created_utc": 1683732194.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought a WD Red Plus WD40EFZX 4 TB 5400 rpm CMR cache 128 MB as an internal device (SATA) for my PC to store movies and other data. I am using it as a normal HDD without RAID or anything else.\n\nWhen I transfer files from my other volumes, the speed is good for a second (150 MB/s) and then it drops and gets stuck at 0 bytes/s. I searched up on the internet and everyone says that this problem is common in SMR HDD and I should move to a CMR HDD. **But I have a CMR HDD**.\n\nI know tt's not the origin volume because it's an NVMe.\n\nAlso, the movies I was able to transfer are not possibile to open directly from the volume because they won't load up. I know it's a 5400 rpm volume created for NAS, but I didn't think it would be that bad as an internal HDD.\n\nI'm definitely missing something. \n\nThank you in advance for your help, and sorry if I am a noob in the matter.", "author_fullname": "t2_psbwpiiy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CMR HDD transfer speed drops to 0 bytes/s ???", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d2euj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683658360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought a WD Red Plus WD40EFZX 4 TB 5400 rpm CMR cache 128 MB as an internal device (SATA) for my PC to store movies and other data. I am using it as a normal HDD without RAID or anything else.&lt;/p&gt;\n\n&lt;p&gt;When I transfer files from my other volumes, the speed is good for a second (150 MB/s) and then it drops and gets stuck at 0 bytes/s. I searched up on the internet and everyone says that this problem is common in SMR HDD and I should move to a CMR HDD. &lt;strong&gt;But I have a CMR HDD&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;I know tt&amp;#39;s not the origin volume because it&amp;#39;s an NVMe.&lt;/p&gt;\n\n&lt;p&gt;Also, the movies I was able to transfer are not possibile to open directly from the volume because they won&amp;#39;t load up. I know it&amp;#39;s a 5400 rpm volume created for NAS, but I didn&amp;#39;t think it would be that bad as an internal HDD.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m definitely missing something. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help, and sorry if I am a noob in the matter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13d2euj", "is_robot_indexable": true, "report_reasons": null, "author": "sambertan_", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13d2euj/cmr_hdd_transfer_speed_drops_to_0_bytess/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13d2euj/cmr_hdd_transfer_speed_drops_to_0_bytess/", "subreddit_subscribers": 681909, "created_utc": 1683658360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_wzjsa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finally found out what's taking up so much space on my NAS, Apparently 145GB worth of jpegs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 39, "top_awarded_type": null, "hide_score": true, "name": "t3_13dwik6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7vwkIBNHx2ZzVr0WWlufA1R_Drh6OyhK3mzv19An1fg.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683737614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lhn4keqw91za1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lhn4keqw91za1.png?auto=webp&amp;v=enabled&amp;s=ca090b529119dbb8ea1ebc89ad9d795784098f9a", "width": 460, "height": 129}, "resolutions": [{"url": "https://preview.redd.it/lhn4keqw91za1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b916b62e28ac3c5caab21f3b8a165b1be2fcce3e", "width": 108, "height": 30}, {"url": "https://preview.redd.it/lhn4keqw91za1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b946c5611fa77d76b935865cfa8e25169ba24cb", "width": 216, "height": 60}, {"url": "https://preview.redd.it/lhn4keqw91za1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3680358ddb593269a1d1ee0da1ae78fdd5244616", "width": 320, "height": 89}], "variants": {}, "id": "kF3l5NcaI7zDFOF1wyAVTrnE4tgEz2bY-m_G1r9QC_E"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2.5TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13dwik6", "is_robot_indexable": true, "report_reasons": null, "author": "jordonbc", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13dwik6/finally_found_out_whats_taking_up_so_much_space/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lhn4keqw91za1.png", "subreddit_subscribers": 681909, "created_utc": 1683737614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My struggle is that the auto filename is the name of the email subject so if there\u2019s a back and forth conversation, it\u2019ll ask if I want to overwrite because the filename matches another reply.", "author_fullname": "t2_837akngy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have to save thousands Of outlook emails to save without overwriting, any tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dnise", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683717487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My struggle is that the auto filename is the name of the email subject so if there\u2019s a back and forth conversation, it\u2019ll ask if I want to overwrite because the filename matches another reply.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dnise", "is_robot_indexable": true, "report_reasons": null, "author": "ZeBloodyStretchr", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dnise/i_have_to_save_thousands_of_outlook_emails_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dnise/i_have_to_save_thousands_of_outlook_emails_to/", "subreddit_subscribers": 681909, "created_utc": 1683717487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, \nLong time lurker first time poster...\nMy question is the best budget way to connect my drives..\nSo for context my setups is:\n- 4 bay WD My cloud ex4 (4x3TB) (RAID10 bcz worst purchase i ever did, run away from WD for NAS hardware z their HDDs are great though)\n- 1 enterprise 14TB Seagate for backup (of the nas in an external box hooked directly via USB)\n- 1x12TB wd easystore media hooked to PC (not shucked)\n- 1x8TB Seagate backup plus (not shucked)\n\nInternally in my PC:\n1x 3TB \n1x 4TB\n(Other SSDs for operation irrelevant here) so my PC can't fit more drives \nLastely 2 unplugged HDDs\n2TB Seagate barracuda\n3TB Seagate Barracuda \n\nNow I'm looking into mainly start using those 2 Seagates,(other than external usb enclosures) \n\nWhat is the best way to properly get on-line all my drives? Why all online bcz I'm lazy to store them offline with data.\n\nShucking the 8tb and 12tb is an option but i don't mind leaving these portable\n\nI looked into DAS solutions but felt they are quite expensive .\nCurrently I'm not looking into a new NAS cz of cost and I would like one day to build my own NAS \n\nAny advice for an amateur hoarder?\n\nThanks\n\nEditing chuck to shuck", "author_fullname": "t2_1wio452u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Managing multiple HDDs the cheap way", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dhif5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683710252.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683697313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, \nLong time lurker first time poster...\nMy question is the best budget way to connect my drives..\nSo for context my setups is:\n- 4 bay WD My cloud ex4 (4x3TB) (RAID10 bcz worst purchase i ever did, run away from WD for NAS hardware z their HDDs are great though)\n- 1 enterprise 14TB Seagate for backup (of the nas in an external box hooked directly via USB)\n- 1x12TB wd easystore media hooked to PC (not shucked)\n- 1x8TB Seagate backup plus (not shucked)&lt;/p&gt;\n\n&lt;p&gt;Internally in my PC:\n1x 3TB \n1x 4TB\n(Other SSDs for operation irrelevant here) so my PC can&amp;#39;t fit more drives \nLastely 2 unplugged HDDs\n2TB Seagate barracuda\n3TB Seagate Barracuda &lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m looking into mainly start using those 2 Seagates,(other than external usb enclosures) &lt;/p&gt;\n\n&lt;p&gt;What is the best way to properly get on-line all my drives? Why all online bcz I&amp;#39;m lazy to store them offline with data.&lt;/p&gt;\n\n&lt;p&gt;Shucking the 8tb and 12tb is an option but i don&amp;#39;t mind leaving these portable&lt;/p&gt;\n\n&lt;p&gt;I looked into DAS solutions but felt they are quite expensive .\nCurrently I&amp;#39;m not looking into a new NAS cz of cost and I would like one day to build my own NAS &lt;/p&gt;\n\n&lt;p&gt;Any advice for an amateur hoarder?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n\n&lt;p&gt;Editing chuck to shuck&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dhif5", "is_robot_indexable": true, "report_reasons": null, "author": "Elkhose", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dhif5/managing_multiple_hdds_the_cheap_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dhif5/managing_multiple_hdds_the_cheap_way/", "subreddit_subscribers": 681909, "created_utc": 1683697313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "im collecting prototypes and and rare game media for the past 2 years, i got a nice collection, BUT there are a few things on the betaArchive that id love to grab but it appears you need to upload and contribute unique media, and stupidly donating money doesnt give access either.\n\ndoes anyone know a way to downlaod on there without having account access", "author_fullname": "t2_1nrkmxag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "any way to download from BetaArchive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dba2b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683678825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;im collecting prototypes and and rare game media for the past 2 years, i got a nice collection, BUT there are a few things on the betaArchive that id love to grab but it appears you need to upload and contribute unique media, and stupidly donating money doesnt give access either.&lt;/p&gt;\n\n&lt;p&gt;does anyone know a way to downlaod on there without having account access&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13dba2b", "is_robot_indexable": true, "report_reasons": null, "author": "Suicidalbutbaked", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dba2b/any_way_to_download_from_betaarchive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dba2b/any_way_to_download_from_betaarchive/", "subreddit_subscribers": 681909, "created_utc": 1683678825.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, \n\nI have got a 300 Mbps connection from a local ISP (Kanpur, India). The problem is when I test my internet using speedtest I get proper speeds 300 Mbps Down and Up. I have tried different Speedtests Ookla, Cloudflare, Google. Tried different servers, In India and Outside India. \n\nBut whenever I try to upload to Youtube and GDrive, my Uplink speeds fluctuate b/w 1-3 MB/s (instead of 37 MB/s). I also get laggy streams when I livestream to youtube. When I use Onedrive, for first few seconds I get 10 MB/s but then it throttles to 2-3 MB/s. \n\nSo is there any way to check that? Or a setting that I can change?\n\nI use Fedora GNU/Linux on 5700G. I have got a router by my ISP. I will be using that router in Bridge mode to connect to TP Link AC1200 (I haven't bought that yet) so my fiber connection gets converted to RJ45.", "author_fullname": "t2_5ov8p822", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is my ISP throttling Upload speeds?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13dw0bw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683736568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, &lt;/p&gt;\n\n&lt;p&gt;I have got a 300 Mbps connection from a local ISP (Kanpur, India). The problem is when I test my internet using speedtest I get proper speeds 300 Mbps Down and Up. I have tried different Speedtests Ookla, Cloudflare, Google. Tried different servers, In India and Outside India. &lt;/p&gt;\n\n&lt;p&gt;But whenever I try to upload to Youtube and GDrive, my Uplink speeds fluctuate b/w 1-3 MB/s (instead of 37 MB/s). I also get laggy streams when I livestream to youtube. When I use Onedrive, for first few seconds I get 10 MB/s but then it throttles to 2-3 MB/s. &lt;/p&gt;\n\n&lt;p&gt;So is there any way to check that? Or a setting that I can change?&lt;/p&gt;\n\n&lt;p&gt;I use Fedora GNU/Linux on 5700G. I have got a router by my ISP. I will be using that router in Bridge mode to connect to TP Link AC1200 (I haven&amp;#39;t bought that yet) so my fiber connection gets converted to RJ45.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dw0bw", "is_robot_indexable": true, "report_reasons": null, "author": "Xyncronix", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dw0bw/is_my_isp_throttling_upload_speeds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dw0bw/is_my_isp_throttling_upload_speeds/", "subreddit_subscribers": 681909, "created_utc": 1683736568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI would need some guidance to build a kind of backup device from different (4 to 7) drives (spinning or solid) of different sizes (from 240GB to 4TB).\n\nI imagine some kind of compact enclosure for all these 2.5\" drives, not necessarily a NAS, but that would be a plus.\n\nI could format the drives, say in btrfs (I use Linux, that's apparently a good option to extend a partition across several drives, possibly with some redundancy (which RAID?) although I don't think it would be possible if one of the disks is larger than all the others.  \n\n\nWhat would you advise to do with these drives? Thanks!", "author_fullname": "t2_z9kvhg7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DIY backup device from a bunch of 2.5\" drives of different sizes and speed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13dupsm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683733906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I would need some guidance to build a kind of backup device from different (4 to 7) drives (spinning or solid) of different sizes (from 240GB to 4TB).&lt;/p&gt;\n\n&lt;p&gt;I imagine some kind of compact enclosure for all these 2.5&amp;quot; drives, not necessarily a NAS, but that would be a plus.&lt;/p&gt;\n\n&lt;p&gt;I could format the drives, say in btrfs (I use Linux, that&amp;#39;s apparently a good option to extend a partition across several drives, possibly with some redundancy (which RAID?) although I don&amp;#39;t think it would be possible if one of the disks is larger than all the others.  &lt;/p&gt;\n\n&lt;p&gt;What would you advise to do with these drives? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dupsm", "is_robot_indexable": true, "report_reasons": null, "author": "cbnbs", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dupsm/diy_backup_device_from_a_bunch_of_25_drives_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dupsm/diy_backup_device_from_a_bunch_of_25_drives_of/", "subreddit_subscribers": 681909, "created_utc": 1683733906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I'm looking to buy an lto drive and I've seen it in an ad and I know it's for the library but I think there should be no problem removing it and leaving it with the SAS connector to put it in a bay or an external case, right?\n\nhttps://preview.redd.it/cbcdnouto0za1.png?width=640&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2fd306f4f2b54760021da1b9d26a17efe00e592a", "author_fullname": "t2_2zzhuwdp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help LTO Drive library to normal LTO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cbcdnouto0za1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 143, "x": 108, "u": "https://preview.redd.it/cbcdnouto0za1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c97857ee4f99a48ec0f0fd87f51e8ec5af871aa7"}, {"y": 287, "x": 216, "u": "https://preview.redd.it/cbcdnouto0za1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a69b24ae58a6fb348bce827ef6c53e627ccb4191"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/cbcdnouto0za1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02ab79cf28946e0033ddea9cf962d2cc28220a77"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/cbcdnouto0za1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce11428ebeccc425e0ea44f643f722a86f8676cd"}], "s": {"y": 853, "x": 640, "u": "https://preview.redd.it/cbcdnouto0za1.png?width=640&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2fd306f4f2b54760021da1b9d26a17efe00e592a"}, "id": "cbcdnouto0za1"}}, "name": "t3_13dt09h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jP10smwCHHGu-osI_62KU7EhTlzxnYNS40hHMDTYz2k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683730355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m looking to buy an lto drive and I&amp;#39;ve seen it in an ad and I know it&amp;#39;s for the library but I think there should be no problem removing it and leaving it with the SAS connector to put it in a bay or an external case, right?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/cbcdnouto0za1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2fd306f4f2b54760021da1b9d26a17efe00e592a\"&gt;https://preview.redd.it/cbcdnouto0za1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2fd306f4f2b54760021da1b9d26a17efe00e592a&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dt09h", "is_robot_indexable": true, "report_reasons": null, "author": "joselcl1998", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dt09h/help_lto_drive_library_to_normal_lto/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dt09h/help_lto_drive_library_to_normal_lto/", "subreddit_subscribers": 681909, "created_utc": 1683730355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "In some future fantasy, I'd have everything I'm about to list in one Proxmox/Unraid machine with VMs being snapshotted to some ZFS array for backup and drive redundancy. Sadly, that's not my reality right now and I apologize in advance to everyone whose brain falls apart from reading this horror :); my setup is a hydra that looks like this:\n\n4 laptops - 3 linux, 1 Win 10\n\n1 Server 2019 with 6 SATA and 4 USB drives all of various sizes; the server runs 3 VMs plus Jellyfin and contains my media\n\nMy backup strategy at this point:\n\nThe 3 linux machines use Veeam Agent to backup nightly to one of the server USB drives; one of them is my Nextcloud server and I use Freefilesync to backup those files (though I'm currently testing Nextcloud beta built-in backup); I'm doing file backup to Windows because I find command line mounting and restoring a huge pain vs opening a GUI and just a restoring a file.\n\nThe Windows laptop uses Veeam to backup to a second NVME card; while I wanted the Veeam snapshots to also copy to the server, they're just too huge, especially when I travel; thus, I use Syncthing to constantly copy file changes to the server, my idea being that I'll have both file and image backup in two separate places\n\nMy server use Veeam to backup the system drives and the two VM SSDs; my Jellyfin media (2 USB drives) is backed up using FreeFileSync with the recycle bin on a 3rd drive. Freefilesync also backs up the remaining drives on that system.\n\nThe reason I don't use Veeam for everything is because I've gotten hosed with Veeam having a corrupted file in the incremental chain and that was the end of the entire backup. I've used Duplicati, Urbackup, and Drivebender in the past for file backup and drive pooling but wound up ditching them all; Duplicati was too difficult for restoring without the GUI (all those zip files rather than actual files) and Urbackup required something like Drivebender because it can only point to one backup directory and, with my scattered drives of various sizes, Urbackup to Drivebender just wound up being a bad solution for me. Plus, as I learned the hard way with Drivebender, Veeam bare metal restore is a nightmare with backups saved to a pooled drives because all the incrementals are scattered everywhere.\n\nSo my question: while I have backup, I don't have drive redundancy because I don't have a RAID array or the like. I was contemplating get a huge drive and using it as a parity drive for my server since everything, except my Win10 laptop Veeam snapshots, backs up to that in some form, but I don't think Snapraid is the right solution for me because I definitely add and delete files and my understanding is that's not a great use case for Snapraid. I know people have issues with Storage Spaces but since it's built into my server, maybe it would be good for a parity-only drive a la Snapraid? Or maybe a parity drive is the wrong idea. I'm not sure, which is why I'm seeking help.\n\nBelieve me, I'd love to consolidate my whole system but the best I can do at this point is add some protection against drive failure which is the one thing that's missing, so any suggestions about how to tack that onto this preexisting mess would be greatly appreciated - thanks!", "author_fullname": "t2_5olrypq0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup strategy for awful setup (but I'm stuck with it)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13dn3so", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683716358.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In some future fantasy, I&amp;#39;d have everything I&amp;#39;m about to list in one Proxmox/Unraid machine with VMs being snapshotted to some ZFS array for backup and drive redundancy. Sadly, that&amp;#39;s not my reality right now and I apologize in advance to everyone whose brain falls apart from reading this horror :); my setup is a hydra that looks like this:&lt;/p&gt;\n\n&lt;p&gt;4 laptops - 3 linux, 1 Win 10&lt;/p&gt;\n\n&lt;p&gt;1 Server 2019 with 6 SATA and 4 USB drives all of various sizes; the server runs 3 VMs plus Jellyfin and contains my media&lt;/p&gt;\n\n&lt;p&gt;My backup strategy at this point:&lt;/p&gt;\n\n&lt;p&gt;The 3 linux machines use Veeam Agent to backup nightly to one of the server USB drives; one of them is my Nextcloud server and I use Freefilesync to backup those files (though I&amp;#39;m currently testing Nextcloud beta built-in backup); I&amp;#39;m doing file backup to Windows because I find command line mounting and restoring a huge pain vs opening a GUI and just a restoring a file.&lt;/p&gt;\n\n&lt;p&gt;The Windows laptop uses Veeam to backup to a second NVME card; while I wanted the Veeam snapshots to also copy to the server, they&amp;#39;re just too huge, especially when I travel; thus, I use Syncthing to constantly copy file changes to the server, my idea being that I&amp;#39;ll have both file and image backup in two separate places&lt;/p&gt;\n\n&lt;p&gt;My server use Veeam to backup the system drives and the two VM SSDs; my Jellyfin media (2 USB drives) is backed up using FreeFileSync with the recycle bin on a 3rd drive. Freefilesync also backs up the remaining drives on that system.&lt;/p&gt;\n\n&lt;p&gt;The reason I don&amp;#39;t use Veeam for everything is because I&amp;#39;ve gotten hosed with Veeam having a corrupted file in the incremental chain and that was the end of the entire backup. I&amp;#39;ve used Duplicati, Urbackup, and Drivebender in the past for file backup and drive pooling but wound up ditching them all; Duplicati was too difficult for restoring without the GUI (all those zip files rather than actual files) and Urbackup required something like Drivebender because it can only point to one backup directory and, with my scattered drives of various sizes, Urbackup to Drivebender just wound up being a bad solution for me. Plus, as I learned the hard way with Drivebender, Veeam bare metal restore is a nightmare with backups saved to a pooled drives because all the incrementals are scattered everywhere.&lt;/p&gt;\n\n&lt;p&gt;So my question: while I have backup, I don&amp;#39;t have drive redundancy because I don&amp;#39;t have a RAID array or the like. I was contemplating get a huge drive and using it as a parity drive for my server since everything, except my Win10 laptop Veeam snapshots, backs up to that in some form, but I don&amp;#39;t think Snapraid is the right solution for me because I definitely add and delete files and my understanding is that&amp;#39;s not a great use case for Snapraid. I know people have issues with Storage Spaces but since it&amp;#39;s built into my server, maybe it would be good for a parity-only drive a la Snapraid? Or maybe a parity drive is the wrong idea. I&amp;#39;m not sure, which is why I&amp;#39;m seeking help.&lt;/p&gt;\n\n&lt;p&gt;Believe me, I&amp;#39;d love to consolidate my whole system but the best I can do at this point is add some protection against drive failure which is the one thing that&amp;#39;s missing, so any suggestions about how to tack that onto this preexisting mess would be greatly appreciated - thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13dn3so", "is_robot_indexable": true, "report_reasons": null, "author": "Effect_Proud", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13dn3so/backup_strategy_for_awful_setup_but_im_stuck_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13dn3so/backup_strategy_for_awful_setup_but_im_stuck_with/", "subreddit_subscribers": 681909, "created_utc": 1683716358.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "After a lot of research. The choice paralysis is real. \n\n  \nI want to save a copy of 2 drives on my windows 10 pc to respective folders on a larger Hard Drive on the same system. As a backup of sorts. Id like for the software to make sure all the files are the same and not corrupted (if possible).   \n\n\nIt would be preferred if the software was open source.   \n\n\nThank you Reddit, You are my only hope.", "author_fullname": "t2_4lrsy7em", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need Help choosing a sync/backup software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13df7nf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683689919.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After a lot of research. The choice paralysis is real. &lt;/p&gt;\n\n&lt;p&gt;I want to save a copy of 2 drives on my windows 10 pc to respective folders on a larger Hard Drive on the same system. As a backup of sorts. Id like for the software to make sure all the files are the same and not corrupted (if possible).   &lt;/p&gt;\n\n&lt;p&gt;It would be preferred if the software was open source.   &lt;/p&gt;\n\n&lt;p&gt;Thank you Reddit, You are my only hope.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13df7nf", "is_robot_indexable": true, "report_reasons": null, "author": "DeconThe1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13df7nf/need_help_choosing_a_syncbackup_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13df7nf/need_help_choosing_a_syncbackup_software/", "subreddit_subscribers": 681909, "created_utc": 1683689919.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I'm trying to figure out what would be the most lossless way to record video from a PC to a VCR/VHS tape. Would it be to go from Hdmi output on my video card and use some adapter to go to S-video input to the VCR? Trying to maintain picture quality as much as possible (even though the idea is to get vhs quality picture in the end). Does anyone know the most lossless way to go about this? Thanks!", "author_fullname": "t2_8kv0d6cx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to record video from PC (HDMI) to VCR?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13d41ik", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683661942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m trying to figure out what would be the most lossless way to record video from a PC to a VCR/VHS tape. Would it be to go from Hdmi output on my video card and use some adapter to go to S-video input to the VCR? Trying to maintain picture quality as much as possible (even though the idea is to get vhs quality picture in the end). Does anyone know the most lossless way to go about this? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13d41ik", "is_robot_indexable": true, "report_reasons": null, "author": "Dead_wet_flesh_jets", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13d41ik/whats_the_best_way_to_record_video_from_pc_hdmi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13d41ik/whats_the_best_way_to_record_video_from_pc_hdmi/", "subreddit_subscribers": 681909, "created_utc": 1683661942.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}