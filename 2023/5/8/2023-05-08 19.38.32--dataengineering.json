{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an economics undergrad and 2 years of experience as a reporting analyst. I basically extract data from our data warehouse (Snowflake) using SQL (joins, aggregates, filters, etc.) into Tableau where I build dashboards. \nI\u2019ve realized I really like SQL and programming more than making dashboards, so now I\u2019m looking to become a data engineer. Would it make sense to continue at this job full-time while doing an online BSCS? I\u2019ve mainly been looking at WGU since I could afford it without loans.", "author_fullname": "t2_hyamjn58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Second bachelor\u2019s worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b3gr1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683494880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an economics undergrad and 2 years of experience as a reporting analyst. I basically extract data from our data warehouse (Snowflake) using SQL (joins, aggregates, filters, etc.) into Tableau where I build dashboards. \nI\u2019ve realized I really like SQL and programming more than making dashboards, so now I\u2019m looking to become a data engineer. Would it make sense to continue at this job full-time while doing an online BSCS? I\u2019ve mainly been looking at WGU since I could afford it without loans.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13b3gr1", "is_robot_indexable": true, "report_reasons": null, "author": "ggmmz", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b3gr1/second_bachelors_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b3gr1/second_bachelors_worth_it/", "subreddit_subscribers": 104603, "created_utc": 1683494880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_xamdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Leading Data Organizations Achieve Success: Prioritize People, Process, and Product", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 51, "top_awarded_type": null, "hide_score": false, "name": "t3_13bpeqx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/X8c2cXmQm9IBzo5IoWBm16vpQDiLrpMqX_H1pvvDGnc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683553148.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/datamindedbe/how-leading-data-organizations-achieve-success-prioritize-people-process-and-product-472cc56fd095", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/X7NO6r7Tf9XB1BFXyFF8JAfY1X8AoFyecR7NmWezGRA.jpg?auto=webp&amp;v=enabled&amp;s=923756c137c857853ea1576c20e5275623484a83", "width": 1200, "height": 442}, "resolutions": [{"url": "https://external-preview.redd.it/X7NO6r7Tf9XB1BFXyFF8JAfY1X8AoFyecR7NmWezGRA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53bdaaa47126ffa0215f19f29d39a3116624e834", "width": 108, "height": 39}, {"url": "https://external-preview.redd.it/X7NO6r7Tf9XB1BFXyFF8JAfY1X8AoFyecR7NmWezGRA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7c0ac092528cdc1be4904ddc2331702dbb52269", "width": 216, "height": 79}, {"url": "https://external-preview.redd.it/X7NO6r7Tf9XB1BFXyFF8JAfY1X8AoFyecR7NmWezGRA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e374d1d97c7166f04bf5994d675471e69eb9b83", "width": 320, "height": 117}, {"url": "https://external-preview.redd.it/X7NO6r7Tf9XB1BFXyFF8JAfY1X8AoFyecR7NmWezGRA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0302413e183a6dd17e20351c02dda0c0cfd62bb5", "width": 640, "height": 235}, {"url": "https://external-preview.redd.it/X7NO6r7Tf9XB1BFXyFF8JAfY1X8AoFyecR7NmWezGRA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=caf1be2bc3c474ccdb9b917b4d9040af138ff576", "width": 960, "height": 353}, {"url": "https://external-preview.redd.it/X7NO6r7Tf9XB1BFXyFF8JAfY1X8AoFyecR7NmWezGRA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05661f62da1741c56c7aa37d531b0c9df9646eb8", "width": 1080, "height": 397}], "variants": {}, "id": "l_FI4A3wAtWAb7Wyz9gf_AmQ9G6n_Qjw5u8UiTKv9q8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13bpeqx", "is_robot_indexable": true, "report_reasons": null, "author": "Joda5", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bpeqx/how_leading_data_organizations_achieve_success/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/datamindedbe/how-leading-data-organizations-achieve-success-prioritize-people-process-and-product-472cc56fd095", "subreddit_subscribers": 104603, "created_utc": 1683553148.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good day fellow Data Engineers,\n\nI am about 6 months into working with ADF. The environment I inherited had been untended for about 6 months prior, some pipelines were poorly constructed and were racking up large bills.\n\nThrough some fairly straightforward re-factoring of queries, creating better monitoring tools and other general maintenance I have got the Pipelines back up-and-running at a more respectable price.\n\nI would like to reduce costs further, and have identified things like reducing the use of mapping data flows, but wanted to see if anyone in the community had tips for reducing costs in ADF. What have people tried and was it successful?\n\nAny help and/or constructive discussion and pointers welcomed, ta!", "author_fullname": "t2_7u7yr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory: tips for reducing costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b969o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683508251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good day fellow Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;I am about 6 months into working with ADF. The environment I inherited had been untended for about 6 months prior, some pipelines were poorly constructed and were racking up large bills.&lt;/p&gt;\n\n&lt;p&gt;Through some fairly straightforward re-factoring of queries, creating better monitoring tools and other general maintenance I have got the Pipelines back up-and-running at a more respectable price.&lt;/p&gt;\n\n&lt;p&gt;I would like to reduce costs further, and have identified things like reducing the use of mapping data flows, but wanted to see if anyone in the community had tips for reducing costs in ADF. What have people tried and was it successful?&lt;/p&gt;\n\n&lt;p&gt;Any help and/or constructive discussion and pointers welcomed, ta!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13b969o", "is_robot_indexable": true, "report_reasons": null, "author": "tee_dogg", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b969o/azure_data_factory_tips_for_reducing_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b969o/azure_data_factory_tips_for_reducing_costs/", "subreddit_subscribers": 104603, "created_utc": 1683508251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI am an industrial engineering major who\u2019s going into my final year of school and after this year I think I\u2019ve learned that what I want to pursue is a career in data engineering. So far, I\u2019ve taken an intro databases course which basically I learned SQL and the fundamentals of database structures. I\u2019ve started to learn python on my own because I\u2019ve heard that I\u2019m gonna have to know that to be able to achieve my goal. In addition, over this summer I\u2019m going to take a course to attain a certification in a cloud service (don\u2019t know which to pick yet whether it\u2019s aws, gcp, or azure)\n\nSo my question to you all is if you were in my shoes what would you guys think would be the best course of action in terms of things to learn/do during my last year of schooling to be able to attain an entry level data engineering position after I graduate.\n\nThank you!", "author_fullname": "t2_2dbrp66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for a college student going into his senior year", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bbt47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683514721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am an industrial engineering major who\u2019s going into my final year of school and after this year I think I\u2019ve learned that what I want to pursue is a career in data engineering. So far, I\u2019ve taken an intro databases course which basically I learned SQL and the fundamentals of database structures. I\u2019ve started to learn python on my own because I\u2019ve heard that I\u2019m gonna have to know that to be able to achieve my goal. In addition, over this summer I\u2019m going to take a course to attain a certification in a cloud service (don\u2019t know which to pick yet whether it\u2019s aws, gcp, or azure)&lt;/p&gt;\n\n&lt;p&gt;So my question to you all is if you were in my shoes what would you guys think would be the best course of action in terms of things to learn/do during my last year of schooling to be able to attain an entry level data engineering position after I graduate.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13bbt47", "is_robot_indexable": true, "report_reasons": null, "author": "iBortex", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bbt47/recommendations_for_a_college_student_going_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bbt47/recommendations_for_a_college_student_going_into/", "subreddit_subscribers": 104603, "created_utc": 1683514721.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to understand data warehousing, particularly schemas. I\u2019ve built schemas before for individual databases (e.g inventory or sales areas of my firm). However, querying has been a nightmare whenever management requests high-level reports that requires elements from multiple databases. The firm has recently looked towards data warehousing too.\n\nMy question is: do we build a brand new schema (e.g snowflake or star) for a data warehouse? Or do we somehow combine them into one big schema? Please bear with me. I only started reading on DWs yesterday, &amp; I\u2019m really a simple financial analyst!", "author_fullname": "t2_icgo33mk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick Questions on Data Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b0hdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683488419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to understand data warehousing, particularly schemas. I\u2019ve built schemas before for individual databases (e.g inventory or sales areas of my firm). However, querying has been a nightmare whenever management requests high-level reports that requires elements from multiple databases. The firm has recently looked towards data warehousing too.&lt;/p&gt;\n\n&lt;p&gt;My question is: do we build a brand new schema (e.g snowflake or star) for a data warehouse? Or do we somehow combine them into one big schema? Please bear with me. I only started reading on DWs yesterday, &amp;amp; I\u2019m really a simple financial analyst!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13b0hdr", "is_robot_indexable": true, "report_reasons": null, "author": "DisastrousProfit97", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b0hdr/quick_questions_on_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b0hdr/quick_questions_on_data_warehouse/", "subreddit_subscribers": 104603, "created_utc": 1683488419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The crapload of work aside, is databricks a solid and future proof platform to migrate to? Also, what is the best DW architecture on DBX atm: delta live tables or just normal pyspark jobs (preferably without too much reliance on notebooks). Is this unity catalog thing only available for delta live tables pipelines?", "author_fullname": "t2_72m7mvsq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating a legacy SAS DW to Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bh2pl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683529858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The crapload of work aside, is databricks a solid and future proof platform to migrate to? Also, what is the best DW architecture on DBX atm: delta live tables or just normal pyspark jobs (preferably without too much reliance on notebooks). Is this unity catalog thing only available for delta live tables pipelines?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13bh2pl", "is_robot_indexable": true, "report_reasons": null, "author": "Majestic-Weakness239", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bh2pl/migrating_a_legacy_sas_dw_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bh2pl/migrating_a_legacy_sas_dw_to_databricks/", "subreddit_subscribers": 104603, "created_utc": 1683529858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_io93l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introducing the Atlas Kubernetes Operator: Manage database schemas with Kubernetes and Atlas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13bx99f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BulUfYRWUbEopz-iF-HFZjug7RUm7o7cD6ulHApzpaw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683563781.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "atlasgo.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://atlasgo.io/blog/2023/05/08/atlas-v011-kubernetes-operator", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NeWb6f9K1hBBCD-A-C8tc5uRlLmTvXA8-TiKe6X93Wg.jpg?auto=webp&amp;v=enabled&amp;s=fb5e5b529c54bd83b7908e9765aa23679cdcbcb9", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/NeWb6f9K1hBBCD-A-C8tc5uRlLmTvXA8-TiKe6X93Wg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76ad797d3d85f8304c43dc6c5d5f381ad18af43d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/NeWb6f9K1hBBCD-A-C8tc5uRlLmTvXA8-TiKe6X93Wg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=348a3f64e7a3e9a330dbb5a8d7d404b75ecd6e40", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/NeWb6f9K1hBBCD-A-C8tc5uRlLmTvXA8-TiKe6X93Wg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d11d25fac3bb7914270833f4f993ed800fa60735", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/NeWb6f9K1hBBCD-A-C8tc5uRlLmTvXA8-TiKe6X93Wg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6820df39e20938db1369ae8984ca804a36b406dd", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/NeWb6f9K1hBBCD-A-C8tc5uRlLmTvXA8-TiKe6X93Wg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=607fc3128291a81883ae059f02fa1105c764f0be", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/NeWb6f9K1hBBCD-A-C8tc5uRlLmTvXA8-TiKe6X93Wg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b7c492da2da4ef1e46fdc2fcf730d975b31817c", "width": 1080, "height": 607}], "variants": {}, "id": "LEjW0aWCgxoxPVnomMrIPRjGrE9BRmXFCBm6cdvV4dM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13bx99f", "is_robot_indexable": true, "report_reasons": null, "author": "rotemtam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bx99f/introducing_the_atlas_kubernetes_operator_manage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://atlasgo.io/blog/2023/05/08/atlas-v011-kubernetes-operator", "subreddit_subscribers": 104603, "created_utc": 1683563781.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6on5x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why you should start a technical reading group", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 89, "top_awarded_type": null, "hide_score": false, "name": "t3_13bro5p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6FrP-CdFHr-mQE_XbMD4xVZh1nxakbA-J8JWPJ7Czow.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683555626.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/53db2860f2c9", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fsusQ-YE_pwQBfBV3-YSFLfjeugujAcbEzwb9Zy9ZRw.jpg?auto=webp&amp;v=enabled&amp;s=80b2ba7b1dd3f95fe2ef4dd0f0d35caa7e0bf6ad", "width": 1046, "height": 666}, "resolutions": [{"url": "https://external-preview.redd.it/fsusQ-YE_pwQBfBV3-YSFLfjeugujAcbEzwb9Zy9ZRw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3591504a3124759e79dc55ee30edd53b02fd0d2", "width": 108, "height": 68}, {"url": "https://external-preview.redd.it/fsusQ-YE_pwQBfBV3-YSFLfjeugujAcbEzwb9Zy9ZRw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e8b944789d4fd7edd81650289e3538edff12b158", "width": 216, "height": 137}, {"url": "https://external-preview.redd.it/fsusQ-YE_pwQBfBV3-YSFLfjeugujAcbEzwb9Zy9ZRw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5ddadd53aff059f7aa2853d234b658ea3b62f29", "width": 320, "height": 203}, {"url": "https://external-preview.redd.it/fsusQ-YE_pwQBfBV3-YSFLfjeugujAcbEzwb9Zy9ZRw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76e3cfd7c2f0e197cb39164650f4506ca73b6a63", "width": 640, "height": 407}, {"url": "https://external-preview.redd.it/fsusQ-YE_pwQBfBV3-YSFLfjeugujAcbEzwb9Zy9ZRw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=27c4b633bb839e7a5d38b4e5ec0a6880ff62be38", "width": 960, "height": 611}], "variants": {}, "id": "GkcDdv1GjP3xziLwU2PYZ1AHN-5dTlJhHRsefMITEUU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13bro5p", "is_robot_indexable": true, "report_reasons": null, "author": "noodlesoup89", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bro5p/why_you_should_start_a_technical_reading_group/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/53db2860f2c9", "subreddit_subscribers": 104603, "created_utc": 1683555626.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in academia and I will be proposing to setup up multiple database engines (mysql, postgres, mssql) on a vm on aws. The installation of these databases will be used as a lab environment in which learners can practice to work on different sql engines without having to install anything locally. I don\u2019t expect more than 40 concurrent users at any point in time and less than 2000 users in total.\n\nI need help with the following:\n1. Only toy databases will be kept in each of these engines (&lt;50 mb), will a small machine with 4gb ram and 50gb hdd be ok?\n2. How do I administer these databases, I don\u2019t want to run any administrative sql commands, is there any opensource tool I can use to create users, grant privileges? This tool should work across db engines.\n3. What process can I use to populate databases, I can ask lecturers to provide .sql files for importing data or write scripts to do bulk copy. But is there any alternative?\n4. How do I monitor the health of databases and get alerts in case of too many open connections or ram usage? I don\u2019t want to write this functionality from scratch, are there such open source tools available?", "author_fullname": "t2_3o3jbkf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage multiple databases on a VM.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bn3rj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683547605.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in academia and I will be proposing to setup up multiple database engines (mysql, postgres, mssql) on a vm on aws. The installation of these databases will be used as a lab environment in which learners can practice to work on different sql engines without having to install anything locally. I don\u2019t expect more than 40 concurrent users at any point in time and less than 2000 users in total.&lt;/p&gt;\n\n&lt;p&gt;I need help with the following:\n1. Only toy databases will be kept in each of these engines (&amp;lt;50 mb), will a small machine with 4gb ram and 50gb hdd be ok?\n2. How do I administer these databases, I don\u2019t want to run any administrative sql commands, is there any opensource tool I can use to create users, grant privileges? This tool should work across db engines.\n3. What process can I use to populate databases, I can ask lecturers to provide .sql files for importing data or write scripts to do bulk copy. But is there any alternative?\n4. How do I monitor the health of databases and get alerts in case of too many open connections or ram usage? I don\u2019t want to write this functionality from scratch, are there such open source tools available?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13bn3rj", "is_robot_indexable": true, "report_reasons": null, "author": "gunnvant", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bn3rj/manage_multiple_databases_on_a_vm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bn3rj/manage_multiple_databases_on_a_vm/", "subreddit_subscribers": 104603, "created_utc": 1683547605.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TL;DR: need some help understanding dimensional modelling. how, why and when dimension and fact tables are created? please suggest to me any tutorial/video/ reading materials with practical examples if possible.\n\nSo recently I was going through a [youtube video](https://www.youtube.com/watch?v=WpQECq5Hx9g) to do some weekend projects. most of the time, I try to understand what the project is all about and then implement it later in my own way. \n\n (I have not seen the whole video), from what I understood, this was about creating dimension and fact tables from a dataset and visualizing them and it was for learning purposes.\n\nbut I am having trouble understanding, how one can create a fact table from a table (dataframe) that is already a kind of fact table and how a dimensional table can have transactional type data.\n\n**initial dataset columns -&gt;**  VendorID,tpep\\_pickup\\_datetime, tpep\\_dropoff\\_datetime, passenger\\_count, trip\\_distance, pickup\\_longitude, pickup\\_latitude, RatecodeID, store\\_and\\_fwd\\_flag, dropoff\\_longitude, dropoff\\_latitude, payment\\_type, fare\\_amount, extra, mta\\_tax, tip\\_amount, tolls\\_amount, improvement\\_surcharge, total\\_amount\n\n**derived fact table columns -&gt;**  \\['trip\\_id', 'VendorID', 'datetime\\_id', 'passenger\\_count\\_id',     \n 'trip\\_distance\\_id', 'rate\\_code\\_id', 'store\\_and\\_fwd\\_flag','pickup\\_location\\_id', 'dropoff\\_location\\_id', 'payment\\_type\\_id','fare\\_amount', 'extra', 'mta\\_tax', 'tip\\_amount', 'tolls\\_amount','improvement\\_surcharge', 'total\\_amount'\\] \n\nin my organisation, I mostly see, the architect giving us the er diagram, usually, we have the dimension tables and have to create the fact table from them(not transactional for my case,just for reference). so I need some help to understand these concepts.", "author_fullname": "t2_6f6khk66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need help understanding dimensional modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b0rkn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683489061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: need some help understanding dimensional modelling. how, why and when dimension and fact tables are created? please suggest to me any tutorial/video/ reading materials with practical examples if possible.&lt;/p&gt;\n\n&lt;p&gt;So recently I was going through a &lt;a href=\"https://www.youtube.com/watch?v=WpQECq5Hx9g\"&gt;youtube video&lt;/a&gt; to do some weekend projects. most of the time, I try to understand what the project is all about and then implement it later in my own way. &lt;/p&gt;\n\n&lt;p&gt;(I have not seen the whole video), from what I understood, this was about creating dimension and fact tables from a dataset and visualizing them and it was for learning purposes.&lt;/p&gt;\n\n&lt;p&gt;but I am having trouble understanding, how one can create a fact table from a table (dataframe) that is already a kind of fact table and how a dimensional table can have transactional type data.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;initial dataset columns -&amp;gt;&lt;/strong&gt;  VendorID,tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, pickup_longitude, pickup_latitude, RatecodeID, store_and_fwd_flag, dropoff_longitude, dropoff_latitude, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;derived fact table columns -&amp;gt;&lt;/strong&gt;  [&amp;#39;trip_id&amp;#39;, &amp;#39;VendorID&amp;#39;, &amp;#39;datetime_id&amp;#39;, &amp;#39;passenger_count_id&amp;#39;,&lt;br/&gt;\n &amp;#39;trip_distance_id&amp;#39;, &amp;#39;rate_code_id&amp;#39;, &amp;#39;store_and_fwd_flag&amp;#39;,&amp;#39;pickup_location_id&amp;#39;, &amp;#39;dropoff_location_id&amp;#39;, &amp;#39;payment_type_id&amp;#39;,&amp;#39;fare_amount&amp;#39;, &amp;#39;extra&amp;#39;, &amp;#39;mta_tax&amp;#39;, &amp;#39;tip_amount&amp;#39;, &amp;#39;tolls_amount&amp;#39;,&amp;#39;improvement_surcharge&amp;#39;, &amp;#39;total_amount&amp;#39;] &lt;/p&gt;\n\n&lt;p&gt;in my organisation, I mostly see, the architect giving us the er diagram, usually, we have the dimension tables and have to create the fact table from them(not transactional for my case,just for reference). so I need some help to understand these concepts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GCNe-8P7fSp5-HbPToikEahbi9XarAT706dSG534GnY.jpg?auto=webp&amp;v=enabled&amp;s=c9f7bdcbbfc5a5d9eb1e5c69246a4a86dd515b16", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GCNe-8P7fSp5-HbPToikEahbi9XarAT706dSG534GnY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a12e10aa95a9498ba4092ea435470d1ab416977", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GCNe-8P7fSp5-HbPToikEahbi9XarAT706dSG534GnY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6af654737dbebf0f26bdef8477ddc698131d68db", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GCNe-8P7fSp5-HbPToikEahbi9XarAT706dSG534GnY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2964eff873295c7ff4804ad1c0ca525c3a2671aa", "width": 320, "height": 240}], "variants": {}, "id": "0NnIPI0R0myzas-_YAqO-3Ve0PYWyJZcBKwl3qPK5ps"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13b0rkn", "is_robot_indexable": true, "report_reasons": null, "author": "mainak17", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b0rkn/need_help_understanding_dimensional_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b0rkn/need_help_understanding_dimensional_modelling/", "subreddit_subscribers": 104603, "created_utc": 1683489061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "According to Andy Pavlo it is going to be the future of databases but is it real?\nhttps://twitter.com/andy_pavlo/status/1523666179247595520", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Meta's Velox execution engine going to disrupt big data science business market in coming years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bnrwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683552420.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683549270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;According to Andy Pavlo it is going to be the future of databases but is it real?\n&lt;a href=\"https://twitter.com/andy_pavlo/status/1523666179247595520\"&gt;https://twitter.com/andy_pavlo/status/1523666179247595520&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YjQZoiR4g8v5Yi3CFzQDnnKpgxKA4ELbhRoixq0mk6w.jpg?auto=webp&amp;v=enabled&amp;s=7bc7b9027eaad69c1e67994110815c2cade4c3d1", "width": 140, "height": 140}, "resolutions": [{"url": "https://external-preview.redd.it/YjQZoiR4g8v5Yi3CFzQDnnKpgxKA4ELbhRoixq0mk6w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26fa0a75dcffe7d96da154fb322a669832382000", "width": 108, "height": 108}], "variants": {}, "id": "Z_gIDiiDmTTOtMwcwYaaGEnFJjAHqPioNSlCLMvwBkQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13bnrwr", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bnrwr/is_metas_velox_execution_engine_going_to_disrupt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bnrwr/is_metas_velox_execution_engine_going_to_disrupt/", "subreddit_subscribers": 104603, "created_utc": 1683549270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, according to this article by databricks https://docs.databricks.com/clusters/cluster-config-best-practices.html\n\nIt is good to have a large single node cluster instead of multiple smaller clusters for Complex ETL batch operations. We had autoscaling cluster config earlier and I actually tried single node implementation in production only for the job to run out of memory and job failure, the job processes around 1.6TB data in each run in production.\n\nDo any of you guys follow this advise by databricks? If yes how do you decide the size of cluster?", "author_fullname": "t2_8b8jp1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Clusters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bf6xt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683524129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, according to this article by databricks &lt;a href=\"https://docs.databricks.com/clusters/cluster-config-best-practices.html\"&gt;https://docs.databricks.com/clusters/cluster-config-best-practices.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It is good to have a large single node cluster instead of multiple smaller clusters for Complex ETL batch operations. We had autoscaling cluster config earlier and I actually tried single node implementation in production only for the job to run out of memory and job failure, the job processes around 1.6TB data in each run in production.&lt;/p&gt;\n\n&lt;p&gt;Do any of you guys follow this advise by databricks? If yes how do you decide the size of cluster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13bf6xt", "is_robot_indexable": true, "report_reasons": null, "author": "EstablishmentTop3908", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bf6xt/databricks_clusters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bf6xt/databricks_clusters/", "subreddit_subscribers": 104603, "created_utc": 1683524129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're ingesting data from Kafka topics using spark streaming and storing those topics as tables as is in base layer. And then cleaning the raw tables like trimming, basic filtering using spark SQL in notebooks. And using these tables to create other tables using join and business logic in Databricks delta lake.\n1. Using workflow to schedule notebooks or delta live table what would be the better approach from cost and CI/CD point of view to ingest Kafka real time data?\n2. The rate at which data arrives in base tables differs.What would be the ideal approach to refresh those tables created using raw tables so data there is in sync with raw tables as much as possible with least possible latency while keeping in mind the costs associated as well?", "author_fullname": "t2_wkq4zhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline advice for real time ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bbtxf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683517989.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683514778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re ingesting data from Kafka topics using spark streaming and storing those topics as tables as is in base layer. And then cleaning the raw tables like trimming, basic filtering using spark SQL in notebooks. And using these tables to create other tables using join and business logic in Databricks delta lake.\n1. Using workflow to schedule notebooks or delta live table what would be the better approach from cost and CI/CD point of view to ingest Kafka real time data?\n2. The rate at which data arrives in base tables differs.What would be the ideal approach to refresh those tables created using raw tables so data there is in sync with raw tables as much as possible with least possible latency while keeping in mind the costs associated as well?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13bbtxf", "is_robot_indexable": true, "report_reasons": null, "author": "the_aris", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bbtxf/data_pipeline_advice_for_real_time_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bbtxf/data_pipeline_advice_for_real_time_ingestion/", "subreddit_subscribers": 104603, "created_utc": 1683514778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just started a new job where they are using Keboola for ETL into snowflake for data exposure.  Wanted to hear opinions from those familiar with Keboola. Thank you!", "author_fullname": "t2_a7qtuvlq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opinions on Keboola?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b8404", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683505585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just started a new job where they are using Keboola for ETL into snowflake for data exposure.  Wanted to hear opinions from those familiar with Keboola. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13b8404", "is_robot_indexable": true, "report_reasons": null, "author": "FaithlessnessSea7467", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b8404/opinions_on_keboola/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b8404/opinions_on_keboola/", "subreddit_subscribers": 104603, "created_utc": 1683505585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have any of you worked as a consultant? I really wonder if it would be nice. How long are your projects generally? Is it interesting to see many companies or are most tech stacks used not relevant? Are you actually doing data engineering work or does it more feel like you\u2019re an analyst or architect? I feel like it\u2019s interesting, but I also feel many companies are stuck with obsolete tools and it might feel like a waste of my time. I\u2019m in Europe btw.", "author_fullname": "t2_gzpboep7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working as a consultant", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13c0wi7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683571633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have any of you worked as a consultant? I really wonder if it would be nice. How long are your projects generally? Is it interesting to see many companies or are most tech stacks used not relevant? Are you actually doing data engineering work or does it more feel like you\u2019re an analyst or architect? I feel like it\u2019s interesting, but I also feel many companies are stuck with obsolete tools and it might feel like a waste of my time. I\u2019m in Europe btw.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13c0wi7", "is_robot_indexable": true, "report_reasons": null, "author": "themouthoftruth", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13c0wi7/working_as_a_consultant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13c0wi7/working_as_a_consultant/", "subreddit_subscribers": 104603, "created_utc": 1683571633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nLooking for some suggestions.\n\nAs the result of a merger, we're on a mix of Azure and AWS/Snowflake. We are migrating away from Azure.\n\nThere is currently a process set up using Azure Logic Apps, Functions, and Tables. It is grabbing data from a csv that is sent by email to a specified email address (in Exchange) daily.\n\nCould someone suggest the 'non-MS' way to do this, given Exchange will stay, but all Azure stuff will go away?\n\nIf I understand correctly:\n\nLogic Apps -&gt; Step Functions (we don't use Step Functions, we use Airflow)\n\nFunctions -&gt; Lambdas (we spin up our Dags in Kubernetes)\n\nTables -&gt; Snowflake\n\nOpen to using AWS native if need be, however sticking within the Airflow / Python framework would be preferable.\n\nEdit: I have been trying to use exchangelib (python library), however it\u2019s not liking me. I get an invalid credentials message, noting that it\u2019s coming from \u2018https://outlook.office365.com/EWS/Exchange.asmx\u2019. If I browse there I get an old school username/password popup, and my email/password combo don\u2019t work.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Grabbing csv from Exchange email - not using MS Services", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bfxsx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683530941.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683526395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Looking for some suggestions.&lt;/p&gt;\n\n&lt;p&gt;As the result of a merger, we&amp;#39;re on a mix of Azure and AWS/Snowflake. We are migrating away from Azure.&lt;/p&gt;\n\n&lt;p&gt;There is currently a process set up using Azure Logic Apps, Functions, and Tables. It is grabbing data from a csv that is sent by email to a specified email address (in Exchange) daily.&lt;/p&gt;\n\n&lt;p&gt;Could someone suggest the &amp;#39;non-MS&amp;#39; way to do this, given Exchange will stay, but all Azure stuff will go away?&lt;/p&gt;\n\n&lt;p&gt;If I understand correctly:&lt;/p&gt;\n\n&lt;p&gt;Logic Apps -&amp;gt; Step Functions (we don&amp;#39;t use Step Functions, we use Airflow)&lt;/p&gt;\n\n&lt;p&gt;Functions -&amp;gt; Lambdas (we spin up our Dags in Kubernetes)&lt;/p&gt;\n\n&lt;p&gt;Tables -&amp;gt; Snowflake&lt;/p&gt;\n\n&lt;p&gt;Open to using AWS native if need be, however sticking within the Airflow / Python framework would be preferable.&lt;/p&gt;\n\n&lt;p&gt;Edit: I have been trying to use exchangelib (python library), however it\u2019s not liking me. I get an invalid credentials message, noting that it\u2019s coming from \u2018&lt;a href=\"https://outlook.office365.com/EWS/Exchange.asmx%E2%80%99\"&gt;https://outlook.office365.com/EWS/Exchange.asmx\u2019&lt;/a&gt;. If I browse there I get an old school username/password popup, and my email/password combo don\u2019t work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13bfxsx", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bfxsx/grabbing_csv_from_exchange_email_not_using_ms/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bfxsx/grabbing_csv_from_exchange_email_not_using_ms/", "subreddit_subscribers": 104603, "created_utc": 1683526395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ARM-based   processors are known for matching performance of x86-based instance  types at a lower cost, since they consume far less energy for the same  performance. It\u2019s not surprising then that some companies, like [Honeycomb](https://www.honeycomb.io/blog/present-future-arm-aws-graviton-honeycomb), are switching their entire infrastructure to ARM.\n\nWe ran a number of [Dask](https://www.dask.org/?utm_source=medium&amp;utm_medium=dask-graviton) workloads on both x86- and ARM-based instance types and found costs were typically 20\u201330% lower when using ARM. We also tested out the latest generation of Amazon\u2019s ARM-based processors Graviton3 instance types and looked at performance for a compute-heavy workload using Dask and XGBoost.\n\n[https://medium.com/coiled-hq/how-well-does-dask-run-on-graviton-29d5d9c20279](https://medium.com/coiled-hq/how-well-does-dask-run-on-graviton-29d5d9c20279)", "author_fullname": "t2_w7crvjmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How well does Dask run on Graviton?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bt5ul", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683556983.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ARM-based   processors are known for matching performance of x86-based instance  types at a lower cost, since they consume far less energy for the same  performance. It\u2019s not surprising then that some companies, like &lt;a href=\"https://www.honeycomb.io/blog/present-future-arm-aws-graviton-honeycomb\"&gt;Honeycomb&lt;/a&gt;, are switching their entire infrastructure to ARM.&lt;/p&gt;\n\n&lt;p&gt;We ran a number of &lt;a href=\"https://www.dask.org/?utm_source=medium&amp;amp;utm_medium=dask-graviton\"&gt;Dask&lt;/a&gt; workloads on both x86- and ARM-based instance types and found costs were typically 20\u201330% lower when using ARM. We also tested out the latest generation of Amazon\u2019s ARM-based processors Graviton3 instance types and looked at performance for a compute-heavy workload using Dask and XGBoost.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/coiled-hq/how-well-does-dask-run-on-graviton-29d5d9c20279\"&gt;https://medium.com/coiled-hq/how-well-does-dask-run-on-graviton-29d5d9c20279&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DrBfqkzjvKdSXh_oovbJvp7bA_zp0jpeKEx-kYG75Z4.jpg?auto=webp&amp;v=enabled&amp;s=36848d52a2405eb411af07bf158e05defbd8d960", "width": 5121, "height": 4012}, "resolutions": [{"url": "https://external-preview.redd.it/DrBfqkzjvKdSXh_oovbJvp7bA_zp0jpeKEx-kYG75Z4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2d47faa089063535ac75c117610c04d1c69401b", "width": 108, "height": 84}, {"url": "https://external-preview.redd.it/DrBfqkzjvKdSXh_oovbJvp7bA_zp0jpeKEx-kYG75Z4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=232b7d430646660f2e3aa78f3400bfb122e26ef9", "width": 216, "height": 169}, {"url": "https://external-preview.redd.it/DrBfqkzjvKdSXh_oovbJvp7bA_zp0jpeKEx-kYG75Z4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b59ed69349cc6a334b27157f79568c9abf9f23db", "width": 320, "height": 250}, {"url": "https://external-preview.redd.it/DrBfqkzjvKdSXh_oovbJvp7bA_zp0jpeKEx-kYG75Z4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d4d7029fdefbd8f6ea5abd2ebe267cf5df2ca6e", "width": 640, "height": 501}, {"url": "https://external-preview.redd.it/DrBfqkzjvKdSXh_oovbJvp7bA_zp0jpeKEx-kYG75Z4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e07d12e91ee6deab46423c197da02a7b8e1979ce", "width": 960, "height": 752}, {"url": "https://external-preview.redd.it/DrBfqkzjvKdSXh_oovbJvp7bA_zp0jpeKEx-kYG75Z4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2ceacedc1b125d30ef54290db8d5cf07252aaa9d", "width": 1080, "height": 846}], "variants": {}, "id": "RgYxhgS8dFH8kyPZC02Y57A12eDi7q92Ja7JKZ7FVzg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13bt5ul", "is_robot_indexable": true, "report_reasons": null, "author": "dask-jeeves", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bt5ul/how_well_does_dask_run_on_graviton/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bt5ul/how_well_does_dask_run_on_graviton/", "subreddit_subscribers": 104603, "created_utc": 1683556983.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It seems almost everyday there is a new pipelining tool coming out for data engineering that appears to do the same as existing tools, but with a key few differences. And these can be really overwhelming especially because my mindset is \u2018I now have to learn this or else I\u2019ll be left behind\u2019 thinking. I wanna hear your thoughts on how you progress through your skills development. Do you learn them when they become a need? Or do you learn in advance just to try it out, or possibly so you can mention it during a job interview? I\u2019m specifically asking about tooling, not on theories because it can really become tiring.", "author_fullname": "t2_tln2vge3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s your learning strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bymjs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683566759.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems almost everyday there is a new pipelining tool coming out for data engineering that appears to do the same as existing tools, but with a key few differences. And these can be really overwhelming especially because my mindset is \u2018I now have to learn this or else I\u2019ll be left behind\u2019 thinking. I wanna hear your thoughts on how you progress through your skills development. Do you learn them when they become a need? Or do you learn in advance just to try it out, or possibly so you can mention it during a job interview? I\u2019m specifically asking about tooling, not on theories because it can really become tiring.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13bymjs", "is_robot_indexable": true, "report_reasons": null, "author": "TheQuiteMind", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bymjs/whats_your_learning_strategy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bymjs/whats_your_learning_strategy/", "subreddit_subscribers": 104603, "created_utc": 1683566759.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I've been giving a dataset which for simplicity sake works like this with respect to measures:\n\n|ID|Employee|MeasureType|MeasureValue|\n|:-|:-|:-|:-|\n|1|Fred|Salary|100,000|\n|2|Fred|Sales|50,000|\n|3|Fred|VacationDays|20|\n\nI'm used to working with measures where each type would be a separate column like this:\n\n|ID|Employee|Salary|Sales|VacationDays|\n|:-|:-|:-|:-|:-|\n|1|Fred|100,000|50,000|20|\n\nShould I be transforming the first table into the second if I am trying to stick to a star schema approach? I'm using Tableau as my BI tool and will be working with more and more data from our financial system that provides exports structured in the first table example.\n\nThanks!", "author_fullname": "t2_5aprs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Financial/MIS Data and Star Schemas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13bsjt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683556428.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve been giving a dataset which for simplicity sake works like this with respect to measures:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;Employee&lt;/th&gt;\n&lt;th align=\"left\"&gt;MeasureType&lt;/th&gt;\n&lt;th align=\"left\"&gt;MeasureValue&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;Fred&lt;/td&gt;\n&lt;td align=\"left\"&gt;Salary&lt;/td&gt;\n&lt;td align=\"left\"&gt;100,000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;td align=\"left\"&gt;Fred&lt;/td&gt;\n&lt;td align=\"left\"&gt;Sales&lt;/td&gt;\n&lt;td align=\"left\"&gt;50,000&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Fred&lt;/td&gt;\n&lt;td align=\"left\"&gt;VacationDays&lt;/td&gt;\n&lt;td align=\"left\"&gt;20&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I&amp;#39;m used to working with measures where each type would be a separate column like this:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;ID&lt;/th&gt;\n&lt;th align=\"left\"&gt;Employee&lt;/th&gt;\n&lt;th align=\"left\"&gt;Salary&lt;/th&gt;\n&lt;th align=\"left\"&gt;Sales&lt;/th&gt;\n&lt;th align=\"left\"&gt;VacationDays&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;Fred&lt;/td&gt;\n&lt;td align=\"left\"&gt;100,000&lt;/td&gt;\n&lt;td align=\"left\"&gt;50,000&lt;/td&gt;\n&lt;td align=\"left\"&gt;20&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Should I be transforming the first table into the second if I am trying to stick to a star schema approach? I&amp;#39;m using Tableau as my BI tool and will be working with more and more data from our financial system that provides exports structured in the first table example.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13bsjt9", "is_robot_indexable": true, "report_reasons": null, "author": "kaslokid", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bsjt9/financialmis_data_and_star_schemas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bsjt9/financialmis_data_and_star_schemas/", "subreddit_subscribers": 104603, "created_utc": 1683556428.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have about 4 yoe mostly at one place. Current job is not working out so I\u2019m looking to leave. \n\nLast go around I had a lot of interviews (last summer) but I hear it\u2019s a little more scarce now. I haven\u2019t paid much attention so wondering what y\u2019all\u2019s experience has been for non-entry level roles this season?", "author_fullname": "t2_2tu8n7l9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone currently hunting for their 2nd job? How\u2019s the market?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13boa43", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683550544.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have about 4 yoe mostly at one place. Current job is not working out so I\u2019m looking to leave. &lt;/p&gt;\n\n&lt;p&gt;Last go around I had a lot of interviews (last summer) but I hear it\u2019s a little more scarce now. I haven\u2019t paid much attention so wondering what y\u2019all\u2019s experience has been for non-entry level roles this season?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13boa43", "is_robot_indexable": true, "report_reasons": null, "author": "Firm_Bit", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13boa43/anyone_currently_hunting_for_their_2nd_job_hows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13boa43/anyone_currently_hunting_for_their_2nd_job_hows/", "subreddit_subscribers": 104603, "created_utc": 1683550544.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have developers needing queries such as \"how much quota/hours\" does someone have in their subscription. \n\nThey are currently querying snowflake via the api as if it worth their own psql db.\n\nI'm concerned about performance scale and cost. Is this a good approach?\n\nOther options I'm thinking is\n- let them pull aggregated analytics like hours for customer from s3\n- storing the gold data in open search for them (aws kind of suggests this)\n- string the data in a segment schema in their transactional db (i.e. Mongo or psql, but have it be read only from their side)\n\nThanks in advance!", "author_fullname": "t2_sgq4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations on Analytics for Apps from Gold lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b160x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683489957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have developers needing queries such as &amp;quot;how much quota/hours&amp;quot; does someone have in their subscription. &lt;/p&gt;\n\n&lt;p&gt;They are currently querying snowflake via the api as if it worth their own psql db.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m concerned about performance scale and cost. Is this a good approach?&lt;/p&gt;\n\n&lt;p&gt;Other options I&amp;#39;m thinking is\n- let them pull aggregated analytics like hours for customer from s3\n- storing the gold data in open search for them (aws kind of suggests this)\n- string the data in a segment schema in their transactional db (i.e. Mongo or psql, but have it be read only from their side)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13b160x", "is_robot_indexable": true, "report_reasons": null, "author": "Thehollidayinn", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b160x/recommendations_on_analytics_for_apps_from_gold/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b160x/recommendations_on_analytics_for_apps_from_gold/", "subreddit_subscribers": 104603, "created_utc": 1683489957.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}