{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark", "author_fullname": "t2_5t56uq7x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Redpanda going to replace Apache Kafka?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ahkh7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 73, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 73, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683448250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark\"&gt;https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tAvhCYL_CPz8Snhn1nCU56SiT5G091t6Puaqgeog2LA.jpg?auto=webp&amp;v=enabled&amp;s=a34b1613855b1e3e7b23d18369adfc586ca53c77", "width": 1170, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/tAvhCYL_CPz8Snhn1nCU56SiT5G091t6Puaqgeog2LA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f8e4522268a5fdf969f9f3c55606fe5640829f5", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/tAvhCYL_CPz8Snhn1nCU56SiT5G091t6Puaqgeog2LA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bc8b728fb1562baafc9b4396407421db515dc3e5", "width": 216, "height": 132}, {"url": "https://external-preview.redd.it/tAvhCYL_CPz8Snhn1nCU56SiT5G091t6Puaqgeog2LA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c62e08b51037feaf8b3a0c4fc03720827c34a802", "width": 320, "height": 196}, {"url": "https://external-preview.redd.it/tAvhCYL_CPz8Snhn1nCU56SiT5G091t6Puaqgeog2LA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adee7ca243c32a9e2fd4a4639272d58b4aaa38fe", "width": 640, "height": 393}, {"url": "https://external-preview.redd.it/tAvhCYL_CPz8Snhn1nCU56SiT5G091t6Puaqgeog2LA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e45d1ac1b4b9e4a552c1fd3f3f0e8b93cbf8fc17", "width": 960, "height": 590}, {"url": "https://external-preview.redd.it/tAvhCYL_CPz8Snhn1nCU56SiT5G091t6Puaqgeog2LA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=197bb75b3831c3efb678f141bb79eb4090f52817", "width": 1080, "height": 664}], "variants": {}, "id": "b02hSD6fK-02oym8-LDsyi4dLz-sGMVoqadlnXaGuj0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13ahkh7", "is_robot_indexable": true, "report_reasons": null, "author": "Born-Comment3359", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ahkh7/is_redpanda_going_to_replace_apache_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ahkh7/is_redpanda_going_to_replace_apache_kafka/", "subreddit_subscribers": 104535, "created_utc": 1683448250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What's the best way to load delta records from on prem sql server db to snowflake?\n\nWe are planning on ADF copy feature which seems use blob storage as intermediate stage - which I feel may cause performance issues, so are there any better ways people do? Is there a way to directly load to snowflake?  Delta extract would be based on record update date.. and need to cover 100s of tables.. with a hourly job\n\n.", "author_fullname": "t2_khph1234", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best way to ELT from on-prem sql server db to snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13auxhk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683476007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best way to load delta records from on prem sql server db to snowflake?&lt;/p&gt;\n\n&lt;p&gt;We are planning on ADF copy feature which seems use blob storage as intermediate stage - which I feel may cause performance issues, so are there any better ways people do? Is there a way to directly load to snowflake?  Delta extract would be based on record update date.. and need to cover 100s of tables.. with a hourly job&lt;/p&gt;\n\n&lt;p&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13auxhk", "is_robot_indexable": true, "report_reasons": null, "author": "misc0007", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13auxhk/whats_the_best_way_to_elt_from_onprem_sql_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13auxhk/whats_the_best_way_to_elt_from_onprem_sql_server/", "subreddit_subscribers": 104535, "created_utc": 1683476007.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone , i have a python script that scrapes a website for data.\n\nI was wondering how i can automate it to run everyday  and store the content into a csv file then  visualize it with powerbi. \n\nTIA", "author_fullname": "t2_5phrmwmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running a python scraper every day and update", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13an1zf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683464433.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone , i have a python script that scrapes a website for data.&lt;/p&gt;\n\n&lt;p&gt;I was wondering how i can automate it to run everyday  and store the content into a csv file then  visualize it with powerbi. &lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13an1zf", "is_robot_indexable": true, "report_reasons": null, "author": "naffra", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13an1zf/running_a_python_scraper_every_day_and_update/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13an1zf/running_a_python_scraper_every_day_and_update/", "subreddit_subscribers": 104535, "created_utc": 1683464433.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently been tasked with exporting around 5 years - approximately 13 million records - worth of data from an Azure SQL database to monthly csv files. This format is required by the vendor. \n\nThe data itself is an MS Dynamics table of emails - with one of the columns being the HTML content of the email itself - which is understandably massive. \n\nI\u2019m currently using Pandas to connect to the database via ODBC and reading the records in chunks before using the to_csv method to spit the files out to an SFTP location. Due to the sheer size of the HTML description column (some of which are around 900k characters), this process is taking an extremely long time. \n\nI\u2019ve tried the write process using Polars, which seems significantly quicker but I\u2019m having issues connecting to the database using it so I\u2019ve been reading the files through the above Pandas routine and converting them to Polars dataframes in my tests. \n\nDoes anybody have any better ideas or experience doing something similar?", "author_fullname": "t2_xu27r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exporting Large Datasets to csv", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13aru6c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683471467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently been tasked with exporting around 5 years - approximately 13 million records - worth of data from an Azure SQL database to monthly csv files. This format is required by the vendor. &lt;/p&gt;\n\n&lt;p&gt;The data itself is an MS Dynamics table of emails - with one of the columns being the HTML content of the email itself - which is understandably massive. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently using Pandas to connect to the database via ODBC and reading the records in chunks before using the to_csv method to spit the files out to an SFTP location. Due to the sheer size of the HTML description column (some of which are around 900k characters), this process is taking an extremely long time. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve tried the write process using Polars, which seems significantly quicker but I\u2019m having issues connecting to the database using it so I\u2019ve been reading the files through the above Pandas routine and converting them to Polars dataframes in my tests. &lt;/p&gt;\n\n&lt;p&gt;Does anybody have any better ideas or experience doing something similar?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13aru6c", "is_robot_indexable": true, "report_reasons": null, "author": "PurpleChicken7", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13aru6c/exporting_large_datasets_to_csv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13aru6c/exporting_large_datasets_to_csv/", "subreddit_subscribers": 104535, "created_utc": 1683471467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an economics undergrad and 2 years of experience as a reporting analyst. I basically extract data from our data warehouse (Snowflake) using SQL (joins, aggregates, filters, etc.) into Tableau where I build dashboards. \nI\u2019ve realized I really like SQL and programming more than making dashboards, so now I\u2019m looking to become a data engineer. Would it make sense to continue at this job full-time while doing an online BSCS? I\u2019ve mainly been looking at WGU since I could afford it without loans.", "author_fullname": "t2_hyamjn58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Second bachelor\u2019s worth it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b3gr1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683494880.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an economics undergrad and 2 years of experience as a reporting analyst. I basically extract data from our data warehouse (Snowflake) using SQL (joins, aggregates, filters, etc.) into Tableau where I build dashboards. \nI\u2019ve realized I really like SQL and programming more than making dashboards, so now I\u2019m looking to become a data engineer. Would it make sense to continue at this job full-time while doing an online BSCS? I\u2019ve mainly been looking at WGU since I could afford it without loans.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13b3gr1", "is_robot_indexable": true, "report_reasons": null, "author": "ggmmz", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b3gr1/second_bachelors_worth_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b3gr1/second_bachelors_worth_it/", "subreddit_subscribers": 104535, "created_utc": 1683494880.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uu592ayo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Step-by-Step Guide to Building a High-Performing Risk Data Mart", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_13au2ky", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Pj5nWuQJkLDYKSEuGmj4lEvutY3IAbcvOSnzEqDkags.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1683474133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/dev-genius/step-by-step-guide-to-building-a-high-performing-risk-data-mart-8a7aaaac9535", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SosrA9B-xbCffaRvfwZlqk1MVIfgf89wvKFgBbBXczk.jpg?auto=webp&amp;v=enabled&amp;s=7ec5db1777f5770010c1d86f59e3e7a6c4458c87", "width": 1200, "height": 798}, "resolutions": [{"url": "https://external-preview.redd.it/SosrA9B-xbCffaRvfwZlqk1MVIfgf89wvKFgBbBXczk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1438ed3094ceb74db1e3b63b09e4e0c897ba46cb", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/SosrA9B-xbCffaRvfwZlqk1MVIfgf89wvKFgBbBXczk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=927a7c4f808cd46b681e4839c7e815aa29c0778c", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/SosrA9B-xbCffaRvfwZlqk1MVIfgf89wvKFgBbBXczk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f83a4f7d91452da3fa203026cc73c849cac37db", "width": 320, "height": 212}, {"url": "https://external-preview.redd.it/SosrA9B-xbCffaRvfwZlqk1MVIfgf89wvKFgBbBXczk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c3cb1306f9391a9e114547cb74534847c3f7af5", "width": 640, "height": 425}, {"url": "https://external-preview.redd.it/SosrA9B-xbCffaRvfwZlqk1MVIfgf89wvKFgBbBXczk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5fbf8d59a9b011df666323c50afe9882fde7dc81", "width": 960, "height": 638}, {"url": "https://external-preview.redd.it/SosrA9B-xbCffaRvfwZlqk1MVIfgf89wvKFgBbBXczk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d17d3eb3d990e8a6afc717af80b291d3fc5e0551", "width": 1080, "height": 718}], "variants": {}, "id": "lrpJC_WbxYIlrkQfUB3wVqpwlxjH31HAzXfKIQP3RrI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13au2ky", "is_robot_indexable": true, "report_reasons": null, "author": "Any_Opportunity1234", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13au2ky/stepbystep_guide_to_building_a_highperforming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/dev-genius/step-by-step-guide-to-building-a-high-performing-risk-data-mart-8a7aaaac9535", "subreddit_subscribers": 104535, "created_utc": 1683474133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m trying to understand data warehousing, particularly schemas. I\u2019ve built schemas before for individual databases (e.g inventory or sales areas of my firm). However, querying has been a nightmare whenever management requests high-level reports that requires elements from multiple databases. The firm has recently looked towards data warehousing too.\n\nMy question is: do we build a brand new schema (e.g snowflake or star) for a data warehouse? Or do we somehow combine them into one big schema? Please bear with me. I only started reading on DWs yesterday, &amp; I\u2019m really a simple financial analyst!", "author_fullname": "t2_icgo33mk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick Questions on Data Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b0hdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683488419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to understand data warehousing, particularly schemas. I\u2019ve built schemas before for individual databases (e.g inventory or sales areas of my firm). However, querying has been a nightmare whenever management requests high-level reports that requires elements from multiple databases. The firm has recently looked towards data warehousing too.&lt;/p&gt;\n\n&lt;p&gt;My question is: do we build a brand new schema (e.g snowflake or star) for a data warehouse? Or do we somehow combine them into one big schema? Please bear with me. I only started reading on DWs yesterday, &amp;amp; I\u2019m really a simple financial analyst!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13b0hdr", "is_robot_indexable": true, "report_reasons": null, "author": "DisastrousProfit97", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b0hdr/quick_questions_on_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b0hdr/quick_questions_on_data_warehouse/", "subreddit_subscribers": 104535, "created_utc": 1683488419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Work in a large organisation with a big focus on data initiatives.\nWe have some smaller data engineering groups out in the business units. \nBut IT centrally is responsible for the overall infrastructure.\n\nMain problem is that IT centrally are filled up with architects with non-technical background (ex-project managers, ex business-analyst etc). We feel like often they make really stupid architectural decisions, and don\u2019t want to listen to the actual engineers out in the BUs. \nIs this common elsewhere? \n\nI would have thought that the best architects would have some hands on experience with developing solutions in order to make informed decisions.", "author_fullname": "t2_55lwhlsm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are your orgs architects technical?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ajf2d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683453982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Work in a large organisation with a big focus on data initiatives.\nWe have some smaller data engineering groups out in the business units. \nBut IT centrally is responsible for the overall infrastructure.&lt;/p&gt;\n\n&lt;p&gt;Main problem is that IT centrally are filled up with architects with non-technical background (ex-project managers, ex business-analyst etc). We feel like often they make really stupid architectural decisions, and don\u2019t want to listen to the actual engineers out in the BUs. \nIs this common elsewhere? &lt;/p&gt;\n\n&lt;p&gt;I would have thought that the best architects would have some hands on experience with developing solutions in order to make informed decisions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13ajf2d", "is_robot_indexable": true, "report_reasons": null, "author": "scalahtmlsql", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ajf2d/are_your_orgs_architects_technical/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ajf2d/are_your_orgs_architects_technical/", "subreddit_subscribers": 104535, "created_utc": 1683453982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TL;DR: need some help understanding dimensional modelling. how, why and when dimension and fact tables are created? please suggest to me any tutorial/video/ reading materials with practical examples if possible.\n\nSo recently I was going through a [youtube video](https://www.youtube.com/watch?v=WpQECq5Hx9g) to do some weekend projects. most of the time, I try to understand what the project is all about and then implement it later in my own way. \n\n (I have not seen the whole video), from what I understood, this was about creating dimension and fact tables from a dataset and visualizing them and it was for learning purposes.\n\nbut I am having trouble understanding, how one can create a fact table from a table (dataframe) that is already a kind of fact table and how a dimensional table can have transactional type data.\n\n**initial dataset columns -&gt;**  VendorID,tpep\\_pickup\\_datetime, tpep\\_dropoff\\_datetime, passenger\\_count, trip\\_distance, pickup\\_longitude, pickup\\_latitude, RatecodeID, store\\_and\\_fwd\\_flag, dropoff\\_longitude, dropoff\\_latitude, payment\\_type, fare\\_amount, extra, mta\\_tax, tip\\_amount, tolls\\_amount, improvement\\_surcharge, total\\_amount\n\n**derived fact table columns -&gt;**  \\['trip\\_id', 'VendorID', 'datetime\\_id', 'passenger\\_count\\_id',     \n 'trip\\_distance\\_id', 'rate\\_code\\_id', 'store\\_and\\_fwd\\_flag','pickup\\_location\\_id', 'dropoff\\_location\\_id', 'payment\\_type\\_id','fare\\_amount', 'extra', 'mta\\_tax', 'tip\\_amount', 'tolls\\_amount','improvement\\_surcharge', 'total\\_amount'\\] \n\nin my organisation, I mostly see, the architect giving us the er diagram, usually, we have the dimension tables and have to create the fact table from them(not transactional for my case,just for reference). so I need some help to understand these concepts.", "author_fullname": "t2_6f6khk66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "need help understanding dimensional modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b0rkn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1683489061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: need some help understanding dimensional modelling. how, why and when dimension and fact tables are created? please suggest to me any tutorial/video/ reading materials with practical examples if possible.&lt;/p&gt;\n\n&lt;p&gt;So recently I was going through a &lt;a href=\"https://www.youtube.com/watch?v=WpQECq5Hx9g\"&gt;youtube video&lt;/a&gt; to do some weekend projects. most of the time, I try to understand what the project is all about and then implement it later in my own way. &lt;/p&gt;\n\n&lt;p&gt;(I have not seen the whole video), from what I understood, this was about creating dimension and fact tables from a dataset and visualizing them and it was for learning purposes.&lt;/p&gt;\n\n&lt;p&gt;but I am having trouble understanding, how one can create a fact table from a table (dataframe) that is already a kind of fact table and how a dimensional table can have transactional type data.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;initial dataset columns -&amp;gt;&lt;/strong&gt;  VendorID,tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, pickup_longitude, pickup_latitude, RatecodeID, store_and_fwd_flag, dropoff_longitude, dropoff_latitude, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;derived fact table columns -&amp;gt;&lt;/strong&gt;  [&amp;#39;trip_id&amp;#39;, &amp;#39;VendorID&amp;#39;, &amp;#39;datetime_id&amp;#39;, &amp;#39;passenger_count_id&amp;#39;,&lt;br/&gt;\n &amp;#39;trip_distance_id&amp;#39;, &amp;#39;rate_code_id&amp;#39;, &amp;#39;store_and_fwd_flag&amp;#39;,&amp;#39;pickup_location_id&amp;#39;, &amp;#39;dropoff_location_id&amp;#39;, &amp;#39;payment_type_id&amp;#39;,&amp;#39;fare_amount&amp;#39;, &amp;#39;extra&amp;#39;, &amp;#39;mta_tax&amp;#39;, &amp;#39;tip_amount&amp;#39;, &amp;#39;tolls_amount&amp;#39;,&amp;#39;improvement_surcharge&amp;#39;, &amp;#39;total_amount&amp;#39;] &lt;/p&gt;\n\n&lt;p&gt;in my organisation, I mostly see, the architect giving us the er diagram, usually, we have the dimension tables and have to create the fact table from them(not transactional for my case,just for reference). so I need some help to understand these concepts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GCNe-8P7fSp5-HbPToikEahbi9XarAT706dSG534GnY.jpg?auto=webp&amp;v=enabled&amp;s=c9f7bdcbbfc5a5d9eb1e5c69246a4a86dd515b16", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GCNe-8P7fSp5-HbPToikEahbi9XarAT706dSG534GnY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a12e10aa95a9498ba4092ea435470d1ab416977", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GCNe-8P7fSp5-HbPToikEahbi9XarAT706dSG534GnY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6af654737dbebf0f26bdef8477ddc698131d68db", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GCNe-8P7fSp5-HbPToikEahbi9XarAT706dSG534GnY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2964eff873295c7ff4804ad1c0ca525c3a2671aa", "width": 320, "height": 240}], "variants": {}, "id": "0NnIPI0R0myzas-_YAqO-3Ve0PYWyJZcBKwl3qPK5ps"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13b0rkn", "is_robot_indexable": true, "report_reasons": null, "author": "mainak17", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b0rkn/need_help_understanding_dimensional_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b0rkn/need_help_understanding_dimensional_modelling/", "subreddit_subscribers": 104535, "created_utc": 1683489061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Good day fellow Data Engineers,\n\nI am about 6 months into working with ADF. The environment I inherited had been untended for about 6 months prior, some pipelines were poorly constructed and were racking up large bills.\n\nThrough some fairly straightforward re-factoring of queries, creating better monitoring tools and other general maintenance I have got the Pipelines back up-and-running at a more respectable price.\n\nI would like to reduce costs further, and have identified things like reducing the use of mapping data flows, but wanted to see if anyone in the community had tips for reducing costs in ADF. What have people tried and was it successful?\n\nAny help and/or constructive discussion and pointers welcomed, ta!", "author_fullname": "t2_7u7yr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure Data Factory: tips for reducing costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b969o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683508251.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good day fellow Data Engineers,&lt;/p&gt;\n\n&lt;p&gt;I am about 6 months into working with ADF. The environment I inherited had been untended for about 6 months prior, some pipelines were poorly constructed and were racking up large bills.&lt;/p&gt;\n\n&lt;p&gt;Through some fairly straightforward re-factoring of queries, creating better monitoring tools and other general maintenance I have got the Pipelines back up-and-running at a more respectable price.&lt;/p&gt;\n\n&lt;p&gt;I would like to reduce costs further, and have identified things like reducing the use of mapping data flows, but wanted to see if anyone in the community had tips for reducing costs in ADF. What have people tried and was it successful?&lt;/p&gt;\n\n&lt;p&gt;Any help and/or constructive discussion and pointers welcomed, ta!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13b969o", "is_robot_indexable": true, "report_reasons": null, "author": "tee_dogg", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b969o/azure_data_factory_tips_for_reducing_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b969o/azure_data_factory_tips_for_reducing_costs/", "subreddit_subscribers": 104535, "created_utc": 1683508251.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just started a new job where they are using Keboola for ETL into snowflake for data exposure.  Wanted to hear opinions from those familiar with Keboola. Thank you!", "author_fullname": "t2_a7qtuvlq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Opinions on Keboola?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b8404", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683505585.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just started a new job where they are using Keboola for ETL into snowflake for data exposure.  Wanted to hear opinions from those familiar with Keboola. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13b8404", "is_robot_indexable": true, "report_reasons": null, "author": "FaithlessnessSea7467", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b8404/opinions_on_keboola/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b8404/opinions_on_keboola/", "subreddit_subscribers": 104535, "created_utc": 1683505585.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're ingesting data from Kafka topics using spark streaming and storing those topics as tables as is in base layer. And then cleaning the raw tables like trimming, basic filtering using spark SQL in notebooks. And using these tables to create other tables using join and business logic in Databricks delta lake.\n1. Using workflow to schedule notebooks or delta live table what would be the better approach from cost and CI/CD point of view to ingest Kafka real time data?\n2. The rate at which data arrives in base tables differs.What would be the ideal approach to refresh those tables created using raw tables so data there is in sync with raw tables as much as possible with least possible latency while keeping in mind the costs associated as well?", "author_fullname": "t2_wkq4zhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data pipeline advice for real time ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13bbtxf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1683517989.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683514778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re ingesting data from Kafka topics using spark streaming and storing those topics as tables as is in base layer. And then cleaning the raw tables like trimming, basic filtering using spark SQL in notebooks. And using these tables to create other tables using join and business logic in Databricks delta lake.\n1. Using workflow to schedule notebooks or delta live table what would be the better approach from cost and CI/CD point of view to ingest Kafka real time data?\n2. The rate at which data arrives in base tables differs.What would be the ideal approach to refresh those tables created using raw tables so data there is in sync with raw tables as much as possible with least possible latency while keeping in mind the costs associated as well?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13bbtxf", "is_robot_indexable": true, "report_reasons": null, "author": "the_aris", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bbtxf/data_pipeline_advice_for_real_time_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bbtxf/data_pipeline_advice_for_real_time_ingestion/", "subreddit_subscribers": 104535, "created_utc": 1683514778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI am an industrial engineering major who\u2019s going into my final year of school and after this year I think I\u2019ve learned that what I want to pursue is a career in data engineering. So far, I\u2019ve taken an intro databases course which basically I learned SQL and the fundamentals of database structures. I\u2019ve started to learn python on my own because I\u2019ve heard that I\u2019m gonna have to know that to be able to achieve my goal. In addition, over this summer I\u2019m going to take a course to attain a certification in a cloud service (don\u2019t know which to pick yet whether it\u2019s aws, gcp, or azure)\n\nSo my question to you all is if you were in my shoes what would you guys think would be the best course of action in terms of things to learn/do during my last year of schooling to be able to attain an entry level data engineering position after I graduate.\n\nThank you!", "author_fullname": "t2_2dbrp66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for a college student going into his senior year", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13bbt47", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683514721.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I am an industrial engineering major who\u2019s going into my final year of school and after this year I think I\u2019ve learned that what I want to pursue is a career in data engineering. So far, I\u2019ve taken an intro databases course which basically I learned SQL and the fundamentals of database structures. I\u2019ve started to learn python on my own because I\u2019ve heard that I\u2019m gonna have to know that to be able to achieve my goal. In addition, over this summer I\u2019m going to take a course to attain a certification in a cloud service (don\u2019t know which to pick yet whether it\u2019s aws, gcp, or azure)&lt;/p&gt;\n\n&lt;p&gt;So my question to you all is if you were in my shoes what would you guys think would be the best course of action in terms of things to learn/do during my last year of schooling to be able to attain an entry level data engineering position after I graduate.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13bbt47", "is_robot_indexable": true, "report_reasons": null, "author": "iBortex", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13bbt47/recommendations_for_a_college_student_going_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13bbt47/recommendations_for_a_college_student_going_into/", "subreddit_subscribers": 104535, "created_utc": 1683514721.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently have developers needing queries such as \"how much quota/hours\" does someone have in their subscription. \n\nThey are currently querying snowflake via the api as if it worth their own psql db.\n\nI'm concerned about performance scale and cost. Is this a good approach?\n\nOther options I'm thinking is\n- let them pull aggregated analytics like hours for customer from s3\n- storing the gold data in open search for them (aws kind of suggests this)\n- string the data in a segment schema in their transactional db (i.e. Mongo or psql, but have it be read only from their side)\n\nThanks in advance!", "author_fullname": "t2_sgq4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations on Analytics for Apps from Gold lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13b160x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1683489957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have developers needing queries such as &amp;quot;how much quota/hours&amp;quot; does someone have in their subscription. &lt;/p&gt;\n\n&lt;p&gt;They are currently querying snowflake via the api as if it worth their own psql db.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m concerned about performance scale and cost. Is this a good approach?&lt;/p&gt;\n\n&lt;p&gt;Other options I&amp;#39;m thinking is\n- let them pull aggregated analytics like hours for customer from s3\n- storing the gold data in open search for them (aws kind of suggests this)\n- string the data in a segment schema in their transactional db (i.e. Mongo or psql, but have it be read only from their side)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13b160x", "is_robot_indexable": true, "report_reasons": null, "author": "Thehollidayinn", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13b160x/recommendations_on_analytics_for_apps_from_gold/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13b160x/recommendations_on_analytics_for_apps_from_gold/", "subreddit_subscribers": 104535, "created_utc": 1683489957.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}