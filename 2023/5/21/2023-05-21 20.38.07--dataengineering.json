{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Share your best practices for designing robust and adaptable data pipelines that consistently perform well and can handle future changes effectively. \n\nEspecially while working in environments where data failures are expensive and incorrect data has massive negative impacts.\n\n\nWhat are the design principles you've found helpful, what about the QA , versioning &amp; change management. How do you document everything above! \ud83d\udc68\u200d\ud83d\udcbb", "author_fullname": "t2_8onr2vji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which best practices do you follow to build robust &amp; extensible ETL jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nn9j7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 80, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 80, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684657701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Share your best practices for designing robust and adaptable data pipelines that consistently perform well and can handle future changes effectively. &lt;/p&gt;\n\n&lt;p&gt;Especially while working in environments where data failures are expensive and incorrect data has massive negative impacts.&lt;/p&gt;\n\n&lt;p&gt;What are the design principles you&amp;#39;ve found helpful, what about the QA , versioning &amp;amp; change management. How do you document everything above! \ud83d\udc68\u200d\ud83d\udcbb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nn9j7", "is_robot_indexable": true, "report_reasons": null, "author": "kryon-X", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nn9j7/which_best_practices_do_you_follow_to_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nn9j7/which_best_practices_do_you_follow_to_build/", "subreddit_subscribers": 106577, "created_utc": 1684657701.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, what are some of the best practices for logging data pipelines especially exceptions? Are there any design or code I can refer to?", "author_fullname": "t2_6zaja793", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Logging pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nh93s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684638651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, what are some of the best practices for logging data pipelines especially exceptions? Are there any design or code I can refer to?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nh93s", "is_robot_indexable": true, "report_reasons": null, "author": "Ambitious_Cucumber96", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nh93s/logging_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nh93s/logging_pipelines/", "subreddit_subscribers": 106577, "created_utc": 1684638651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I work as a data engineer at a consulting firm. I've been doing data engineering from past 2 years for variety of clients ranging from logistics to real-estate. Most of the projects - we get as a consulting firm are analytics project. So - the clients usually have their data in multiple sources like CRM software's, SAP, manual files etc. As data engineers in company - We build pipelines to migrate their data from multiple sources to aa single cloud warehouse (AWS/Azure). Then we write SQL queries to transform raw data, do some calculations in order to calculate some key KPIs that clients need to track. Then BI engineers take that data and build dashboards on Tableau/PBI.\n\nEvery project I've been working on - is somewhat similar to what I explained above. I work at a very small company - where most of the employees are MBA grads - senior project managers and analytics managers. So the engineering team usually dont follow standard practices - like maintaining GitHub, making good technical documentations and yeah - no DevOps practices or CI/CD or anything of that sort. \n\nI really appreciate any suggestions/tips that I can use to implement such important and must-be-followed practices in the projects that I'm working on. \n\n(I'm not a native english speaker - please excuse any mistakes, lol!)", "author_fullname": "t2_6dn8cxrd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I start learning/implementing DevOps in data engineering projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nxz5n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684683053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I work as a data engineer at a consulting firm. I&amp;#39;ve been doing data engineering from past 2 years for variety of clients ranging from logistics to real-estate. Most of the projects - we get as a consulting firm are analytics project. So - the clients usually have their data in multiple sources like CRM software&amp;#39;s, SAP, manual files etc. As data engineers in company - We build pipelines to migrate their data from multiple sources to aa single cloud warehouse (AWS/Azure). Then we write SQL queries to transform raw data, do some calculations in order to calculate some key KPIs that clients need to track. Then BI engineers take that data and build dashboards on Tableau/PBI.&lt;/p&gt;\n\n&lt;p&gt;Every project I&amp;#39;ve been working on - is somewhat similar to what I explained above. I work at a very small company - where most of the employees are MBA grads - senior project managers and analytics managers. So the engineering team usually dont follow standard practices - like maintaining GitHub, making good technical documentations and yeah - no DevOps practices or CI/CD or anything of that sort. &lt;/p&gt;\n\n&lt;p&gt;I really appreciate any suggestions/tips that I can use to implement such important and must-be-followed practices in the projects that I&amp;#39;m working on. &lt;/p&gt;\n\n&lt;p&gt;(I&amp;#39;m not a native english speaker - please excuse any mistakes, lol!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nxz5n", "is_robot_indexable": true, "report_reasons": null, "author": "saiyan6174", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/13nxz5n/how_should_i_start_learningimplementing_devops_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nxz5n/how_should_i_start_learningimplementing_devops_in/", "subreddit_subscribers": 106577, "created_utc": 1684683053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Bit inexperienced, so unsure exactly how to design something like this.\n\nSay, for example, you're extracting from a Sales table within an operational database once a day at midnight. You want to utilise an incremental approach, so you're only pulling data that has been updated, deleted, or inserted since last execution date of pipeline and storing that in a raw storage area. You're then running a series of transformations on this raw data and loading into a target table.\n\nI have a few questions, which hopefully make sense:\n\n1. For an incremental approach, is one way of achieving this to (somehow) identify records that have been updated or inserted since the last execution date of your pipeline, delete the corresponding records in downstream tables/storage, and then insert the new records.\n2. Also, I can see how we would detect records that have been newly inserted or updated, but how would you detect deletions in the source system, and ensure these are removed or flagged as deleted in the target system?\n3. Finally, what considerations need to be given to backfilling? If our logic was to simply detect changes that happened since the last run of the pipeline and only process that, would it make sense to implement additional logic that would only look at changes that happened a day before the execution date (in our example). So for example, we detect an issue with out pipeline and want to re-run all our DAGs from the past week. The first DAG that runs here would have an execution date of 14/05/2023. So our logic could be that this particular DAG will only extract and transform records that have been newly inserted or updated between the dates of 13/05/2023 Midnight and 14/05/2023 Midnight. Again, not sure how exactly we would detect when these changes happen in a database, but just want to check if this is a common approach.", "author_fullname": "t2_osh2xtjb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there common approaches to designing a pipeline with incremental loading, which also allows for backfilling, idempotence, and handles source deletions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n60lq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684613223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bit inexperienced, so unsure exactly how to design something like this.&lt;/p&gt;\n\n&lt;p&gt;Say, for example, you&amp;#39;re extracting from a Sales table within an operational database once a day at midnight. You want to utilise an incremental approach, so you&amp;#39;re only pulling data that has been updated, deleted, or inserted since last execution date of pipeline and storing that in a raw storage area. You&amp;#39;re then running a series of transformations on this raw data and loading into a target table.&lt;/p&gt;\n\n&lt;p&gt;I have a few questions, which hopefully make sense:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For an incremental approach, is one way of achieving this to (somehow) identify records that have been updated or inserted since the last execution date of your pipeline, delete the corresponding records in downstream tables/storage, and then insert the new records.&lt;/li&gt;\n&lt;li&gt;Also, I can see how we would detect records that have been newly inserted or updated, but how would you detect deletions in the source system, and ensure these are removed or flagged as deleted in the target system?&lt;/li&gt;\n&lt;li&gt;Finally, what considerations need to be given to backfilling? If our logic was to simply detect changes that happened since the last run of the pipeline and only process that, would it make sense to implement additional logic that would only look at changes that happened a day before the execution date (in our example). So for example, we detect an issue with out pipeline and want to re-run all our DAGs from the past week. The first DAG that runs here would have an execution date of 14/05/2023. So our logic could be that this particular DAG will only extract and transform records that have been newly inserted or updated between the dates of 13/05/2023 Midnight and 14/05/2023 Midnight. Again, not sure how exactly we would detect when these changes happen in a database, but just want to check if this is a common approach.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13n60lq", "is_robot_indexable": true, "report_reasons": null, "author": "TheDataPanda", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n60lq/are_there_common_approaches_to_designing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n60lq/are_there_common_approaches_to_designing_a/", "subreddit_subscribers": 106577, "created_utc": 1684613223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have multiple delta lake tables refreshing near real time with last_updated date coming from source tables and only having incremental data loaded and from these tables I am creating a joined delta lake table after applying some calculations.\n\nSo far I have been using merge using the entire tables, which is not ideal. I would like to change it to load only incremental load using upsert and run it every 5 mins. What is the best way to approach this. \nShould I leverage last_job_end_ts and last_updated or explore auto loader or any other way to achieve this?", "author_fullname": "t2_wkq4zhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental load for multiple joined tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nuoim", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684676227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have multiple delta lake tables refreshing near real time with last_updated date coming from source tables and only having incremental data loaded and from these tables I am creating a joined delta lake table after applying some calculations.&lt;/p&gt;\n\n&lt;p&gt;So far I have been using merge using the entire tables, which is not ideal. I would like to change it to load only incremental load using upsert and run it every 5 mins. What is the best way to approach this. \nShould I leverage last_job_end_ts and last_updated or explore auto loader or any other way to achieve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nuoim", "is_robot_indexable": true, "report_reasons": null, "author": "the_aris", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nuoim/incremental_load_for_multiple_joined_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nuoim/incremental_load_for_multiple_joined_tables/", "subreddit_subscribers": 106577, "created_utc": 1684676227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a medium sized retail business where I am the first proper (and only) data person. Before I joined, they did all analysis on excel. My experience is 3 years as a data analyst - my experience with data engineering is minimal.\n\nWhen I joined I created a POC on my local laptop - I installed MariaDB Columnstore on a Linux hyper VM within Windows 10 and stored the data there. Over the past few months this has become really useful and everybody loves how I give them answers/ data reports with little turnaround. Now, the challenge is, my laptop is running out of space and I know this can't be a long term solution.\n\nI am looking to move the data to a cloud data warehouse and then create dashboards in a BI tool so a lot of things can be made self-serve. The pricing details of every cloud vendor seems to be too complicated to understand. \n\nIn terms of a small scale data warehouse, what vendor would be the most cost effective? \n\nPoints about the data:\n1. The data source is Rest API - I can give the required date and the the API call fetches the data\n2. In terms of storage, every month, less than 1GB of data would be stored in the data warehouse\n3. We currently have powerbi license but we're happy to switch or move to a cloud native BI tool that will make all this easier \n\nPlease also advice on how best to learn about the solution you prefer - does the vendor's certifications help, is there any other book or online course that would be best to learn about it.", "author_fullname": "t2_t05ji4fs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on a cost effective solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13o2sb9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684694602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a medium sized retail business where I am the first proper (and only) data person. Before I joined, they did all analysis on excel. My experience is 3 years as a data analyst - my experience with data engineering is minimal.&lt;/p&gt;\n\n&lt;p&gt;When I joined I created a POC on my local laptop - I installed MariaDB Columnstore on a Linux hyper VM within Windows 10 and stored the data there. Over the past few months this has become really useful and everybody loves how I give them answers/ data reports with little turnaround. Now, the challenge is, my laptop is running out of space and I know this can&amp;#39;t be a long term solution.&lt;/p&gt;\n\n&lt;p&gt;I am looking to move the data to a cloud data warehouse and then create dashboards in a BI tool so a lot of things can be made self-serve. The pricing details of every cloud vendor seems to be too complicated to understand. &lt;/p&gt;\n\n&lt;p&gt;In terms of a small scale data warehouse, what vendor would be the most cost effective? &lt;/p&gt;\n\n&lt;p&gt;Points about the data:\n1. The data source is Rest API - I can give the required date and the the API call fetches the data\n2. In terms of storage, every month, less than 1GB of data would be stored in the data warehouse\n3. We currently have powerbi license but we&amp;#39;re happy to switch or move to a cloud native BI tool that will make all this easier &lt;/p&gt;\n\n&lt;p&gt;Please also advice on how best to learn about the solution you prefer - does the vendor&amp;#39;s certifications help, is there any other book or online course that would be best to learn about it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13o2sb9", "is_robot_indexable": true, "report_reasons": null, "author": "kkchn001", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o2sb9/advice_on_a_cost_effective_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o2sb9/advice_on_a_cost_effective_solution/", "subreddit_subscribers": 106577, "created_utc": 1684694602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I'm currently learning Apache Spark. I have followed the tutorial from common crawl \\[1\\] to query their dataset, which is stored as partitioned parquet format in a public S3 bucket.  \nThe query is as follows:\n\n    SELECT  COUNT(*) AS count\n    FROM    ccindex\n    WHERE   crawl = 'CC-MAIN-2018-05'\n            AND subnet = 'warc'\n            AND content_mime_type = 'application/pdf'\n\nThe dataset is partitioned on **crawl** and **subnet** columns.\n\nThe query result on AWS Athena scans 76.89 MB of data (the query used to create table is specified in the link \\[1\\]).\n\nhttps://preview.redd.it/foetpw9c971b1.png?width=842&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0\n\nBut when I try to query from my Spark standalone setup in my local machine, Spark UI shows that the query has read 400 MB of data (5 times compare to Athena!). The command used to read the data is just simply `spark.read.parquet(\"s3a://...\")`.\n\nhttps://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69\n\nPlease help me understand what happened and what can I do to improve my Spark query performance.\n\nAdditional query plan:\n\n\\- The query plan from Athena (just ScanFilterProject and Aggregate)\n\n        ScanFilterProject\n    [table = awsdatacatalog:HiveTableHandle{schemaName=ccindex, tableName=ccindex, analyzePartitionValues=Optional.empty}, filterPredicate = (\"content_mime_type\" = CAST('application/pdf' AS varchar))]\n\n\\- The query plan from Spark (could it be that Spark do an unnecessary effort in reading **crawl** and **subset** columns from data when it wasn't needed in the query and can filter data on them without the need to read them since the data is partitioned on these column)\n\n    == Physical Plan ==\n    AdaptiveSparkPlan (14)\n    +- == Final Plan ==\n       * HashAggregate (8)\n       +- ShuffleQueryStage (7), Statistics(sizeInBytes=31.7 KiB, rowCount=2.03E+3)\n          +- Exchange (6)\n             +- * HashAggregate (5)\n                +- * Project (4)\n                   +- * Filter (3)\n                      +- * ColumnarToRow (2)\n                         +- Scan parquet  (1)\n    \n    (1) Scan parquet \n    Output [3]: [content_mime_type#19, crawl#25, subset#26]\n    Batched: true\n    Location: InMemoryFileIndex [s3a://commoncrawl/cc-index/table/cc-main/warc]\n    PartitionFilters: [isnotnull(crawl#25), isnotnull(subset#26), (crawl#25 = CC-MAIN-2018-05), (subset#26 = warc)]\n    PushedFilters: [IsNotNull(content_mime_type), EqualTo(content_mime_type,application/pdf)]\n    ReadSchema: struct&lt;content_mime_type:string&gt;\n    \n    (2) ColumnarToRow [codegen id : 1]\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (3) Filter [codegen id : 1]\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    Condition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n    \n    (4) Project [codegen id : 1]\n    Output: []\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (5) HashAggregate [codegen id : 1]\n    Input: []\n    Keys: []\n    Functions [1]: [partial_count(1)]\n    Aggregate Attributes [1]: [count#154L]\n    Results [1]: [count#155L]\n    \n    (6) Exchange\n    Input [1]: [count#155L]\n    Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=146]\n    \n    (7) ShuffleQueryStage\n    Output [1]: [count#155L]\n    Arguments: 0\n    \n    (8) HashAggregate [codegen id : 2]\n    Input [1]: [count#155L]\n    Keys: []\n    Functions [1]: [count(1)]\n    Aggregate Attributes [1]: [count(1)#151L]\n    Results [1]: [count(1)#151L AS count#152L]\n    \n    (9) Filter\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    Condition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n    \n    (10) Project\n    Output: []\n    Input [3]: [content_mime_type#19, crawl#25, subset#26]\n    \n    (11) HashAggregate\n    Input: []\n    Keys: []\n    Functions [1]: [partial_count(1)]\n    Aggregate Attributes [1]: [count#154L]\n    Results [1]: [count#155L]\n    \n    (12) Exchange\n    Input [1]: [count#155L]\n    Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=126]\n    \n    (13) HashAggregate\n    Input [1]: [count#155L]\n    Keys: []\n    Functions [1]: [count(1)]\n    Aggregate Attributes [1]: [count(1)#151L]\n    Results [1]: [count(1)#151L AS count#152L]\n    \n    (14) AdaptiveSparkPlan\n    Output [1]: [count#152L]\n    Arguments: isFinalPlan=true\n\n  \nReferences:  \n\\[1\\] [Index to WARC Files and URLs in Columnar Format \u2013 Common Crawl](https://commoncrawl.org/2018/03/index-to-warc-files-and-urls-in-columnar-format/)", "author_fullname": "t2_558fsgez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why does AWS Athena read fewer data from parquet files in S3 than Apache Spark with the same query?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 124, "top_awarded_type": null, "hide_score": false, "media_metadata": {"foetpw9c971b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 96, "x": 108, "u": "https://preview.redd.it/foetpw9c971b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe188b60ec0c7d4019bb494d5a883b7c68ba4a9a"}, {"y": 192, "x": 216, "u": "https://preview.redd.it/foetpw9c971b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c4076a66f3443af8c0fb34fef151b8448c235414"}, {"y": 285, "x": 320, "u": "https://preview.redd.it/foetpw9c971b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62b281a8812fd6b15f909d30e68bd1ba2f349afa"}, {"y": 570, "x": 640, "u": "https://preview.redd.it/foetpw9c971b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de49223382592fdd7ac215139c73fa909523583b"}], "s": {"y": 750, "x": 842, "u": "https://preview.redd.it/foetpw9c971b1.png?width=842&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0"}, "id": "foetpw9c971b1"}, "rnvum8fr481b1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 19, "x": 108, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba61245e63fbdadc8e156f573f39263916daffcd"}, {"y": 39, "x": 216, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7004f5b5050a318b295fcebb73dac71c58e97e51"}, {"y": 58, "x": 320, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=750675fcb9672f051c578a40c4eecca822a4ec7c"}, {"y": 116, "x": 640, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c96e68e18ec8d4ae5b4056f7aaac2a4b1137279"}, {"y": 174, "x": 960, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b0835006fa32e627c10a5af0bc7518ee365162d"}, {"y": 195, "x": 1080, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea10a5a3a58c0e50951a7fc79cc3b72ae61937fe"}], "s": {"y": 231, "x": 1274, "u": "https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69"}, "id": "rnvum8fr481b1"}}, "name": "t3_13o20uv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nQOtvmzBNrFBTuk2ccT8nJU1lOvf_tI6_NvRP0SIVXw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684692777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m currently learning Apache Spark. I have followed the tutorial from common crawl [1] to query their dataset, which is stored as partitioned parquet format in a public S3 bucket.&lt;br/&gt;\nThe query is as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT  COUNT(*) AS count\nFROM    ccindex\nWHERE   crawl = &amp;#39;CC-MAIN-2018-05&amp;#39;\n        AND subnet = &amp;#39;warc&amp;#39;\n        AND content_mime_type = &amp;#39;application/pdf&amp;#39;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The dataset is partitioned on &lt;strong&gt;crawl&lt;/strong&gt; and &lt;strong&gt;subnet&lt;/strong&gt; columns.&lt;/p&gt;\n\n&lt;p&gt;The query result on AWS Athena scans 76.89 MB of data (the query used to create table is specified in the link [1]).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/foetpw9c971b1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0\"&gt;https://preview.redd.it/foetpw9c971b1.png?width=842&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=231a35ee15e674e5d02b4cf6558f4331f4c5bce0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But when I try to query from my Spark standalone setup in my local machine, Spark UI shows that the query has read 400 MB of data (5 times compare to Athena!). The command used to read the data is just simply &lt;code&gt;spark.read.parquet(&amp;quot;s3a://...&amp;quot;)&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69\"&gt;https://preview.redd.it/rnvum8fr481b1.png?width=1274&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3576f18b82c275ab8504c7aaa4fa84e9c1851a69&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please help me understand what happened and what can I do to improve my Spark query performance.&lt;/p&gt;\n\n&lt;p&gt;Additional query plan:&lt;/p&gt;\n\n&lt;p&gt;- The query plan from Athena (just ScanFilterProject and Aggregate)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    ScanFilterProject\n[table = awsdatacatalog:HiveTableHandle{schemaName=ccindex, tableName=ccindex, analyzePartitionValues=Optional.empty}, filterPredicate = (&amp;quot;content_mime_type&amp;quot; = CAST(&amp;#39;application/pdf&amp;#39; AS varchar))]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;- The query plan from Spark (could it be that Spark do an unnecessary effort in reading &lt;strong&gt;crawl&lt;/strong&gt; and &lt;strong&gt;subset&lt;/strong&gt; columns from data when it wasn&amp;#39;t needed in the query and can filter data on them without the need to read them since the data is partitioned on these column)&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;== Physical Plan ==\nAdaptiveSparkPlan (14)\n+- == Final Plan ==\n   * HashAggregate (8)\n   +- ShuffleQueryStage (7), Statistics(sizeInBytes=31.7 KiB, rowCount=2.03E+3)\n      +- Exchange (6)\n         +- * HashAggregate (5)\n            +- * Project (4)\n               +- * Filter (3)\n                  +- * ColumnarToRow (2)\n                     +- Scan parquet  (1)\n\n(1) Scan parquet \nOutput [3]: [content_mime_type#19, crawl#25, subset#26]\nBatched: true\nLocation: InMemoryFileIndex [s3a://commoncrawl/cc-index/table/cc-main/warc]\nPartitionFilters: [isnotnull(crawl#25), isnotnull(subset#26), (crawl#25 = CC-MAIN-2018-05), (subset#26 = warc)]\nPushedFilters: [IsNotNull(content_mime_type), EqualTo(content_mime_type,application/pdf)]\nReadSchema: struct&amp;lt;content_mime_type:string&amp;gt;\n\n(2) ColumnarToRow [codegen id : 1]\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(3) Filter [codegen id : 1]\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\nCondition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n\n(4) Project [codegen id : 1]\nOutput: []\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(5) HashAggregate [codegen id : 1]\nInput: []\nKeys: []\nFunctions [1]: [partial_count(1)]\nAggregate Attributes [1]: [count#154L]\nResults [1]: [count#155L]\n\n(6) Exchange\nInput [1]: [count#155L]\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=146]\n\n(7) ShuffleQueryStage\nOutput [1]: [count#155L]\nArguments: 0\n\n(8) HashAggregate [codegen id : 2]\nInput [1]: [count#155L]\nKeys: []\nFunctions [1]: [count(1)]\nAggregate Attributes [1]: [count(1)#151L]\nResults [1]: [count(1)#151L AS count#152L]\n\n(9) Filter\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\nCondition : (isnotnull(content_mime_type#19) AND (content_mime_type#19 = application/pdf))\n\n(10) Project\nOutput: []\nInput [3]: [content_mime_type#19, crawl#25, subset#26]\n\n(11) HashAggregate\nInput: []\nKeys: []\nFunctions [1]: [partial_count(1)]\nAggregate Attributes [1]: [count#154L]\nResults [1]: [count#155L]\n\n(12) Exchange\nInput [1]: [count#155L]\nArguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=126]\n\n(13) HashAggregate\nInput [1]: [count#155L]\nKeys: []\nFunctions [1]: [count(1)]\nAggregate Attributes [1]: [count(1)#151L]\nResults [1]: [count(1)#151L AS count#152L]\n\n(14) AdaptiveSparkPlan\nOutput [1]: [count#152L]\nArguments: isFinalPlan=true\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;References:&lt;br/&gt;\n[1] &lt;a href=\"https://commoncrawl.org/2018/03/index-to-warc-files-and-urls-in-columnar-format/\"&gt;Index to WARC Files and URLs in Columnar Format \u2013 Common Crawl&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o20uv", "is_robot_indexable": true, "report_reasons": null, "author": "VLoZg", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o20uv/why_does_aws_athena_read_fewer_data_from_parquet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o20uv/why_does_aws_athena_read_fewer_data_from_parquet/", "subreddit_subscribers": 106577, "created_utc": 1684692777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi there!\n\nI'm having some trouble developing a Shiny dashboard for a client for whom I have no access to its data in a form of a DB connection, but I need to be able to stream it any time I needed it.\n\nDue to the industry, I know which columns I need in order to generate triggers, statics reports (done through Python and/or R), and dynamic ones.\n\nDo you know what are the possibilities of streaming data from a client to me?\n\nThanks", "author_fullname": "t2_3jpw1rxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting data from a client", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nryyn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684672697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having some trouble developing a Shiny dashboard for a client for whom I have no access to its data in a form of a DB connection, but I need to be able to stream it any time I needed it.&lt;/p&gt;\n\n&lt;p&gt;Due to the industry, I know which columns I need in order to generate triggers, statics reports (done through Python and/or R), and dynamic ones.&lt;/p&gt;\n\n&lt;p&gt;Do you know what are the possibilities of streaming data from a client to me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13nryyn", "is_robot_indexable": true, "report_reasons": null, "author": "eviljia", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nryyn/getting_data_from_a_client/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nryyn/getting_data_from_a_client/", "subreddit_subscribers": 106577, "created_utc": 1684672697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A frequent question in interviews and except the fact that I had experience with Data analysis which introduce it me to DE and I liked it, I keep wondering what are the reasons for you guys.", "author_fullname": "t2_um2qwii8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What made you choose DE as a career ? Especially for the ones coming from other disciplines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nwwn1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684680432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A frequent question in interviews and except the fact that I had experience with Data analysis which introduce it me to DE and I liked it, I keep wondering what are the reasons for you guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13nwwn1", "is_robot_indexable": true, "report_reasons": null, "author": "NoChemical1223", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nwwn1/what_made_you_choose_de_as_a_career_especially/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nwwn1/what_made_you_choose_de_as_a_career_especially/", "subreddit_subscribers": 106577, "created_utc": 1684680432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys.\nCan you please suggest a few delta table optimization startergies?\n\nI've used a few like\n1. Vacuum\n2. Optimize \n3. Z ordering \n4. Partitioning \n\nWould love to learn about more possible optimization strategies.", "author_fullname": "t2_2v7ell6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Table Optimization Methods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nmpfj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684655845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys.\nCan you please suggest a few delta table optimization startergies?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used a few like\n1. Vacuum\n2. Optimize \n3. Z ordering \n4. Partitioning &lt;/p&gt;\n\n&lt;p&gt;Would love to learn about more possible optimization strategies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13nmpfj", "is_robot_indexable": true, "report_reasons": null, "author": "RithwikChhugani", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nmpfj/delta_table_optimization_methods/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nmpfj/delta_table_optimization_methods/", "subreddit_subscribers": 106577, "created_utc": 1684655845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\nSomeone knows a visualization tool with OR &amp; AND slicers?\nI can\u2019t use the DAX solution because the customer want to use the slicers like:\nwhere ID = 1 and customer like \u201cabc\u201d or customer like \u201cdef\u201d\n\nIs there any solution for this?", "author_fullname": "t2_aabuvwgh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualization tool with OR &amp; AND slicers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nknre", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684649147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,\nSomeone knows a visualization tool with OR &amp;amp; AND slicers?\nI can\u2019t use the DAX solution because the customer want to use the slicers like:\nwhere ID = 1 and customer like \u201cabc\u201d or customer like \u201cdef\u201d&lt;/p&gt;\n\n&lt;p&gt;Is there any solution for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13nknre", "is_robot_indexable": true, "report_reasons": null, "author": "ImplementKind8414", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nknre/visualization_tool_with_or_and_slicers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nknre/visualization_tool_with_or_and_slicers/", "subreddit_subscribers": 106577, "created_utc": 1684649147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Community, I'm tasked with extracting individual tables from a pdf as seen in the pdf screenshot, what is the best way to do it .?\n\nhttps://preview.redd.it/55fgklwpn31b1.jpg?width=908&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7ac5a475e24b00a1ba4025604b800759bb5790ba\n\nI want each table to be on a separate page ( it can be again written as a pdf file ). Have anyone worked on this before I would really appreciate any light on how I can achieve this", "author_fullname": "t2_6psngyc6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help to extract numerous tables from pdf.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"55fgklwpn31b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10e435e2451d32a3158580ad8743a51a363630e5"}, {"y": 143, "x": 216, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f8c1b56fa6c5066d984bd9db6dd18ca3ebf7c5d"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85c1be8dbebae82ee7f47530f61c4bc8dd6b1307"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6120a86f0945037dc94c2a03eafc8724736dbeae"}], "s": {"y": 605, "x": 908, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=908&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7ac5a475e24b00a1ba4025604b800759bb5790ba"}, "id": "55fgklwpn31b1"}}, "name": "t3_13ngynt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/OuywNvamZrydEgnKYycxF8TpH_9opuxVrEduMrXz5bE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684637833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Community, I&amp;#39;m tasked with extracting individual tables from a pdf as seen in the pdf screenshot, what is the best way to do it .?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/55fgklwpn31b1.jpg?width=908&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7ac5a475e24b00a1ba4025604b800759bb5790ba\"&gt;https://preview.redd.it/55fgklwpn31b1.jpg?width=908&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7ac5a475e24b00a1ba4025604b800759bb5790ba&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I want each table to be on a separate page ( it can be again written as a pdf file ). Have anyone worked on this before I would really appreciate any light on how I can achieve this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ngynt", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo_51799", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ngynt/need_help_to_extract_numerous_tables_from_pdf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ngynt/need_help_to_extract_numerous_tables_from_pdf/", "subreddit_subscribers": 106577, "created_utc": 1684637833.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a big household-name insurance company as a claims professional.  I have been with the company 8 years and in the industry for 12 years.  I've always been a stellar employee with every review meeting or exceeding expectations, and lots of kudos from the higher ups.  I do have a bachelor's degree in an unrelated field.   I've advanced as high as I can go in my current career track without being a people manager, so I'd like to transition out of claims and into an area where I can continue to grow and make better money. \n\nThis is a job that gets posted occasionally on our internal job board:\n\n \n\n&gt;**Data Engineer Cons III**  \n&gt;  \n&gt;J**ob Summary:**  \n&gt;  \n&gt;This role is responsible for the use and development of data infrastructure projects and proof of concept business solutions for users in analytics and Data Science. This person coordinates with data scientists and analytic engineers on tactical solutions. This role participates on new data engineering work tracks for Data Science and analytics from inception and prototyping to fully developed solutions.  \n&gt;  \n&gt;**Key Responsibilities:**  \n&gt;  \n&gt;\u2022 Work with Data Scientists and business partners on embedded cross functional teams; developing subject matter expertise in the business as well as advanced analytics.  \n\u2022 Participate in building requirements to develop analytic data sources for use by data scientists, research, and business users  \n\u2022 Participate in rapid development of new data work tracks with fast iteration over quick sprints  \n\u2022 Leverage the data infrastructure required to support needs of data science and analytics.  \n\u2022 Provides input to the team with test scripts, executes testing, and works with data scientists and business to ensure end user acceptance  \n\u2022 Leverage agile data analysis with technology fluency in parallel processing/programming, software/programming languages and technologies (Oracle, MongoDB, SQL, Python, Spark, Kafka, Scala and Hadoop), paired with analytic agility to meet fluid and dynamic business needs in this space  \n\u2022 Support innovation and process improvement.  \n\u2022 Provides support to team on the development of enterprise data assets, information platforms or data spaces designed for exploring and understanding the data.  \n&gt;  \n&gt;**Supervisory Responsibilities:**  \n&gt;  \n&gt;This job does not have supervisory duties.  \n&gt;  \n&gt;**Education and Experience:**  \n&gt;  \n&gt;\u2022 Bachelor's degree or equivalent experience  \n\u2022 0-2 years of related experience  \n&gt;  \n&gt;**Certificates, Licenses, Registrations:**  \n&gt;  \n&gt;\u2022 None  \n&gt;  \n&gt;**Functional Skills:**  \n&gt;  \n&gt;\u2022 Database development experience and knowledge (i.e. SQL)  \n\u2022 Programming skills (examples: Python, R, Java)  \n\u2022 Computer proficiency in UNIX/Linux  \n\u2022 Ability to extract data from various data sources.  \n\u2022 Analytic, Data Sourcing, and Data Management skills  \n\u2022 Experience in time and task management  \n\u2022 Ability to learn new technologies  \n\u2022 Strong attention to detail  \n\u2022 Intermediate written and verbal communication skills, including the ability to effectively collaborate with multi-disciplinary groups and all organizational levels\n\nI would like to be able to apply for this job next time it becomes available.  I have taken the Colt Steele SQL bootcamp on Udemy.  I'm pretty handy with Leetcode-style problems in JavaScript up to intermediate level.  (I'm aware JavaScript and Java aren't the same thing)   I'm weak in Python-- I have done the first 15 days of Angela Yu's 100 days of Python but know I'm weak there.  I'm proficient with Linux.  \n\n&amp;#x200B;\n\nCan anyone recommend a good online course to fill in my knowledge gaps so I would be able to be a strong applicant within the next 6-9 months?  I was thinking of taking the Data Engineer course from Meta that is available on Coursera, or the Data Engineer track at Dataquest.  Or is there something better out there?  \n\nThanks very much for taking the time to read and reply.", "author_fullname": "t2_vqn5tkw9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would like advice for transitioning to a Data Engineer role within my current company", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13o4d7f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684698414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a big household-name insurance company as a claims professional.  I have been with the company 8 years and in the industry for 12 years.  I&amp;#39;ve always been a stellar employee with every review meeting or exceeding expectations, and lots of kudos from the higher ups.  I do have a bachelor&amp;#39;s degree in an unrelated field.   I&amp;#39;ve advanced as high as I can go in my current career track without being a people manager, so I&amp;#39;d like to transition out of claims and into an area where I can continue to grow and make better money. &lt;/p&gt;\n\n&lt;p&gt;This is a job that gets posted occasionally on our internal job board:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Data Engineer Cons III&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;J&lt;strong&gt;ob Summary:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;This role is responsible for the use and development of data infrastructure projects and proof of concept business solutions for users in analytics and Data Science. This person coordinates with data scientists and analytic engineers on tactical solutions. This role participates on new data engineering work tracks for Data Science and analytics from inception and prototyping to fully developed solutions.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Responsibilities:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;\u2022 Work with Data Scientists and business partners on embedded cross functional teams; developing subject matter expertise in the business as well as advanced analytics.&lt;br/&gt;\n\u2022 Participate in building requirements to develop analytic data sources for use by data scientists, research, and business users&lt;br/&gt;\n\u2022 Participate in rapid development of new data work tracks with fast iteration over quick sprints&lt;br/&gt;\n\u2022 Leverage the data infrastructure required to support needs of data science and analytics.&lt;br/&gt;\n\u2022 Provides input to the team with test scripts, executes testing, and works with data scientists and business to ensure end user acceptance&lt;br/&gt;\n\u2022 Leverage agile data analysis with technology fluency in parallel processing/programming, software/programming languages and technologies (Oracle, MongoDB, SQL, Python, Spark, Kafka, Scala and Hadoop), paired with analytic agility to meet fluid and dynamic business needs in this space&lt;br/&gt;\n\u2022 Support innovation and process improvement.&lt;br/&gt;\n\u2022 Provides support to team on the development of enterprise data assets, information platforms or data spaces designed for exploring and understanding the data.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Supervisory Responsibilities:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;This job does not have supervisory duties.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Education and Experience:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;\u2022 Bachelor&amp;#39;s degree or equivalent experience&lt;br/&gt;\n\u2022 0-2 years of related experience  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Certificates, Licenses, Registrations:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;\u2022 None  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Functional Skills:&lt;/strong&gt;  &lt;/p&gt;\n\n&lt;p&gt;\u2022 Database development experience and knowledge (i.e. SQL)&lt;br/&gt;\n\u2022 Programming skills (examples: Python, R, Java)&lt;br/&gt;\n\u2022 Computer proficiency in UNIX/Linux&lt;br/&gt;\n\u2022 Ability to extract data from various data sources.&lt;br/&gt;\n\u2022 Analytic, Data Sourcing, and Data Management skills&lt;br/&gt;\n\u2022 Experience in time and task management&lt;br/&gt;\n\u2022 Ability to learn new technologies&lt;br/&gt;\n\u2022 Strong attention to detail&lt;br/&gt;\n\u2022 Intermediate written and verbal communication skills, including the ability to effectively collaborate with multi-disciplinary groups and all organizational levels&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I would like to be able to apply for this job next time it becomes available.  I have taken the Colt Steele SQL bootcamp on Udemy.  I&amp;#39;m pretty handy with Leetcode-style problems in JavaScript up to intermediate level.  (I&amp;#39;m aware JavaScript and Java aren&amp;#39;t the same thing)   I&amp;#39;m weak in Python-- I have done the first 15 days of Angela Yu&amp;#39;s 100 days of Python but know I&amp;#39;m weak there.  I&amp;#39;m proficient with Linux.  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a good online course to fill in my knowledge gaps so I would be able to be a strong applicant within the next 6-9 months?  I was thinking of taking the Data Engineer course from Meta that is available on Coursera, or the Data Engineer track at Dataquest.  Or is there something better out there?  &lt;/p&gt;\n\n&lt;p&gt;Thanks very much for taking the time to read and reply.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13o4d7f", "is_robot_indexable": true, "report_reasons": null, "author": "Miles-From-Nowhere0", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o4d7f/would_like_advice_for_transitioning_to_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o4d7f/would_like_advice_for_transitioning_to_a_data/", "subreddit_subscribers": 106577, "created_utc": 1684698414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any resources to learn dataops?? I work as a DE and i want to learn dataops / devops to use in my projects", "author_fullname": "t2_d6bmxkhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dataops learning resourses", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13o3xm9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684697357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any resources to learn dataops?? I work as a DE and i want to learn dataops / devops to use in my projects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o3xm9", "is_robot_indexable": true, "report_reasons": null, "author": "PinPrestigious2327", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o3xm9/dataops_learning_resourses/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o3xm9/dataops_learning_resourses/", "subreddit_subscribers": 106577, "created_utc": 1684697357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI'm a software developer with a strong Python background, and I'm currently working on a project that involves extracting data from PDF circulars issued by the government. As a newcomer to the field of data engineering, I'm seeking guidance on how to efficiently extract the data and save it to a database.\n\nHere's a brief overview of my project and the steps I'm considering:\n\nExtraction: I need to extract text from multiple pages of PDF circulars. I plan to use the PyPDF2 library in Python to accomplish this. Is this a suitable approach, or are there other tools or libraries I should consider for better performance?\n\nData Cleaning: Once I have the extracted text, I will need to clean and preprocess it before storing it in a database. Are there any recommended best practices or tools I should consider for data cleaning and transformation? I want to ensure the data is accurate and consistent.\n\nDatabase Storage: I'm looking for guidance on the recommended approaches for storing the cleaned and transformed data in a database. Are there any specific considerations I should keep in mind for efficient database storage?\n\nI would greatly appreciate any insights, recommendations, or resources you can provide to help me effectively tackle this data extraction and storage task. Additionally, if there are any specific data engineering concepts or practices I should familiarize myself with for this project, please let me know.", "author_fullname": "t2_3rrudcgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting Data from PDF Circulars and Saving to a Database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o2lvu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684694174.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a software developer with a strong Python background, and I&amp;#39;m currently working on a project that involves extracting data from PDF circulars issued by the government. As a newcomer to the field of data engineering, I&amp;#39;m seeking guidance on how to efficiently extract the data and save it to a database.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a brief overview of my project and the steps I&amp;#39;m considering:&lt;/p&gt;\n\n&lt;p&gt;Extraction: I need to extract text from multiple pages of PDF circulars. I plan to use the PyPDF2 library in Python to accomplish this. Is this a suitable approach, or are there other tools or libraries I should consider for better performance?&lt;/p&gt;\n\n&lt;p&gt;Data Cleaning: Once I have the extracted text, I will need to clean and preprocess it before storing it in a database. Are there any recommended best practices or tools I should consider for data cleaning and transformation? I want to ensure the data is accurate and consistent.&lt;/p&gt;\n\n&lt;p&gt;Database Storage: I&amp;#39;m looking for guidance on the recommended approaches for storing the cleaned and transformed data in a database. Are there any specific considerations I should keep in mind for efficient database storage?&lt;/p&gt;\n\n&lt;p&gt;I would greatly appreciate any insights, recommendations, or resources you can provide to help me effectively tackle this data extraction and storage task. Additionally, if there are any specific data engineering concepts or practices I should familiarize myself with for this project, please let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13o2lvu", "is_robot_indexable": true, "report_reasons": null, "author": "adivhaho_m", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13o2lvu/extracting_data_from_pdf_circulars_and_saving_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13o2lvu/extracting_data_from_pdf_circulars_and_saving_to/", "subreddit_subscribers": 106577, "created_utc": 1684694174.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to understand the thought process behind not having a valid to data field in a Data Vault Satellite. \n\nIt seems that given you generally have multiple satellites off a hub, you\u2019re going to be building a point in time table. A PIT is going to need either a valid to date in the satellites, else a LEAD window function on each satellite each time it loads. Surely the former is more performant?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No valid to data in DV2.0 Satellites. Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nnujn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684659656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to understand the thought process behind not having a valid to data field in a Data Vault Satellite. &lt;/p&gt;\n\n&lt;p&gt;It seems that given you generally have multiple satellites off a hub, you\u2019re going to be building a point in time table. A PIT is going to need either a valid to date in the satellites, else a LEAD window function on each satellite each time it loads. Surely the former is more performant?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13nnujn", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nnujn/no_valid_to_data_in_dv20_satellites_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nnujn/no_valid_to_data_in_dv20_satellites_why/", "subreddit_subscribers": 106577, "created_utc": 1684659656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nI've recently joined a e-commere SaaS company that provides services for around 20k direct clients and around 100x indirect clients, and that has a weird data stack with tons of technical debt (absence of proper data modeling, broken pipelines, teams working in silos, bad usage of dbt, etc.).\n\nTo summarize things:  \n\n\n* We ingest some transactional data coming from MySQL, that's mainly composed by customer purchases and other interactions with the website. We also ingest some dimension tables coming from this MySQL.\n* We ingest a lot of transactional data (web events) coming directly from nginx and going to Clickhouse through Kafka, to be transformed later and ingested into a PostgreSQL (yes, we use it as DWH).\n* We ingest data coming from diferent apps (Hubspot, Odoo, etc.) into PostgreSQL, using NiFi.\n\nFor now, I've been thinking on using self-hosted Rudderstack both for ingestion and reverse ETL, [cube.dev](https://cube.dev) as the abstraction later for building webapps and providing catching for the BI layer, and dbt for transformations. But I have doubts with the following elements:  \n\n\n* Should I pick a single DWH, like Redshift, Snowflake, etc., or would it be better to go for a combination of solutions? My point is that our current bills coming from AWS are high, and even considering scaling capabilities, I'm tempted of storing some data in S3, some other in a 'normal' OLAP DWH, and using something like TimescaleDB / Clickhouse just for event analysis or CDS.\n\nThanks in advance!", "author_fullname": "t2_11ud20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice / Questions on Modern Data Stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13na68z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684619361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently joined a e-commere SaaS company that provides services for around 20k direct clients and around 100x indirect clients, and that has a weird data stack with tons of technical debt (absence of proper data modeling, broken pipelines, teams working in silos, bad usage of dbt, etc.).&lt;/p&gt;\n\n&lt;p&gt;To summarize things:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We ingest some transactional data coming from MySQL, that&amp;#39;s mainly composed by customer purchases and other interactions with the website. We also ingest some dimension tables coming from this MySQL.&lt;/li&gt;\n&lt;li&gt;We ingest a lot of transactional data (web events) coming directly from nginx and going to Clickhouse through Kafka, to be transformed later and ingested into a PostgreSQL (yes, we use it as DWH).&lt;/li&gt;\n&lt;li&gt;We ingest data coming from diferent apps (Hubspot, Odoo, etc.) into PostgreSQL, using NiFi.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For now, I&amp;#39;ve been thinking on using self-hosted Rudderstack both for ingestion and reverse ETL, &lt;a href=\"https://cube.dev\"&gt;cube.dev&lt;/a&gt; as the abstraction later for building webapps and providing catching for the BI layer, and dbt for transformations. But I have doubts with the following elements:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Should I pick a single DWH, like Redshift, Snowflake, etc., or would it be better to go for a combination of solutions? My point is that our current bills coming from AWS are high, and even considering scaling capabilities, I&amp;#39;m tempted of storing some data in S3, some other in a &amp;#39;normal&amp;#39; OLAP DWH, and using something like TimescaleDB / Clickhouse just for event analysis or CDS.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?auto=webp&amp;v=enabled&amp;s=4c2ee9ced32cf7f44c9acfadaf0fc6138d934235", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fc2899acfe1fd7fec2ad6a9c6a16ed630cc31d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb8a7f81c9f0c3327863c615505b600bdd32ead4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0747c7e295fa50f4a169ff2c77b4e38dfe3c70c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ddb5bc88a05e68b02981b71d6f63b8130ecb7a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd7a3df58c7294394cdfee68c359fedcde4abc6f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450ad0a42363c74fd12f5edf265b485734b70be3", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13na68z", "is_robot_indexable": true, "report_reasons": null, "author": "putokaos", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13na68z/advice_questions_on_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13na68z/advice_questions_on_modern_data_stack/", "subreddit_subscribers": 106577, "created_utc": 1684619361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On our client-facing UI, we have a few dashboards filled with normal analytics you'd expect: line/bar charts, data-tables, etc. Most of these we want to serve in a timeseries fashion e.g. for a given timeline (from/to dates or all time) we display data (bucketed per day/week/...). We also have some data tables which display an enriched view for some of these counters.\n\nRecently, we've been getting a lot of traffic so our analytics events table is growing quite rapidly. We expect this traffic to go up considerably in the next months. We're worried that our loading times will become worse as it can already take 10+ seconds for some of the queries (especially all-time).\n\nFor some context, our backend follows a semi \"event-driven\" paradigm. For instance, assume the following setup:\n\n1. Our clients can create different \"graphs\" which their users can enter/leave\n2. Each graph node can \"convert\" a user\n3. We persits \"events\" in our DB e.g. UserEnteredGraph, UserEnteredGraphNode, UserConverted (scoped appropriately per client, graph, etc)\n\nSo for the following rules, we want to export analytics. Note, all of these are scoped per client:\n\n1. How many unique users have entered a graph (sum across all graphs) per day/month/all-time\n2. How many unique users have entered a specific graph (per graph) per day/month/all-time\n3. How many unique users have reached a graph node (per all nodes in a graph) per day/month/all-time\n4. How many unique users have \"converted\" a graph node (per all nodes in a graph) ...\n5. Some data-tables which group users, say per \"Country\". For instance a table that shows top 10 countries with most user conversions. Here, we infer the information using the \\`analytics\\_events\\` table (count conversions) joined with our \\`users\\` table (enrich the country)\n\nAs you can see, we want to display these in a timeseries fashion (for a given timerange, bucketed per day/week/...). We initially decided to persist all of these events into a singular postgres table \\`analytics\\_events\\` which basically has:\n\n* `event_type` (UserEnteredGraph, UserConverted..)\n* `payload` JSONB column that stores any event specific data (which we use to query through a gin-index). Say for a \\`UserConverted\\` the payload column will contain the \\`nodeId\\` of the graph.\n* `inserted_at` timestamp\n\nTo serve our UI with timeseries (say 1day bucketed info for a from/to timeframe). To do so, we do some SQL acrobatics, including a `CROSS JOIN` with a `generate_series(?, ?, INTERVAL '1 day')`. Further, for some of the queries we use a gin-index to access data from the \\`payload\\` column.\n\nTo me, serving analytics like this seems like a solved problem. I suspect we're severely over-complicating and that instead of reinventing the wheel we can use a 3rd party to do all the heavy lifting.  I had a look at Redis time-series but it doesn't feel flexible enough for our data (query per client/graph/node/etc). Timescaledb was also another solution I had a look at, but unfortunately RDS does not support it.  What stack/tools/architectures would you recommend for this?", "author_fullname": "t2_xk16u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to store and serve client data analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n93el", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684616993.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684616692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On our client-facing UI, we have a few dashboards filled with normal analytics you&amp;#39;d expect: line/bar charts, data-tables, etc. Most of these we want to serve in a timeseries fashion e.g. for a given timeline (from/to dates or all time) we display data (bucketed per day/week/...). We also have some data tables which display an enriched view for some of these counters.&lt;/p&gt;\n\n&lt;p&gt;Recently, we&amp;#39;ve been getting a lot of traffic so our analytics events table is growing quite rapidly. We expect this traffic to go up considerably in the next months. We&amp;#39;re worried that our loading times will become worse as it can already take 10+ seconds for some of the queries (especially all-time).&lt;/p&gt;\n\n&lt;p&gt;For some context, our backend follows a semi &amp;quot;event-driven&amp;quot; paradigm. For instance, assume the following setup:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Our clients can create different &amp;quot;graphs&amp;quot; which their users can enter/leave&lt;/li&gt;\n&lt;li&gt;Each graph node can &amp;quot;convert&amp;quot; a user&lt;/li&gt;\n&lt;li&gt;We persits &amp;quot;events&amp;quot; in our DB e.g. UserEnteredGraph, UserEnteredGraphNode, UserConverted (scoped appropriately per client, graph, etc)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So for the following rules, we want to export analytics. Note, all of these are scoped per client:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How many unique users have entered a graph (sum across all graphs) per day/month/all-time&lt;/li&gt;\n&lt;li&gt;How many unique users have entered a specific graph (per graph) per day/month/all-time&lt;/li&gt;\n&lt;li&gt;How many unique users have reached a graph node (per all nodes in a graph) per day/month/all-time&lt;/li&gt;\n&lt;li&gt;How many unique users have &amp;quot;converted&amp;quot; a graph node (per all nodes in a graph) ...&lt;/li&gt;\n&lt;li&gt;Some data-tables which group users, say per &amp;quot;Country&amp;quot;. For instance a table that shows top 10 countries with most user conversions. Here, we infer the information using the `analytics_events` table (count conversions) joined with our `users` table (enrich the country)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;As you can see, we want to display these in a timeseries fashion (for a given timerange, bucketed per day/week/...). We initially decided to persist all of these events into a singular postgres table `analytics_events` which basically has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;event_type&lt;/code&gt; (UserEnteredGraph, UserConverted..)&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;payload&lt;/code&gt; JSONB column that stores any event specific data (which we use to query through a gin-index). Say for a `UserConverted` the payload column will contain the `nodeId` of the graph.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;inserted_at&lt;/code&gt; timestamp&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To serve our UI with timeseries (say 1day bucketed info for a from/to timeframe). To do so, we do some SQL acrobatics, including a &lt;code&gt;CROSS JOIN&lt;/code&gt; with a &lt;code&gt;generate_series(?, ?, INTERVAL &amp;#39;1 day&amp;#39;)&lt;/code&gt;. Further, for some of the queries we use a gin-index to access data from the `payload` column.&lt;/p&gt;\n\n&lt;p&gt;To me, serving analytics like this seems like a solved problem. I suspect we&amp;#39;re severely over-complicating and that instead of reinventing the wheel we can use a 3rd party to do all the heavy lifting.  I had a look at Redis time-series but it doesn&amp;#39;t feel flexible enough for our data (query per client/graph/node/etc). Timescaledb was also another solution I had a look at, but unfortunately RDS does not support it.  What stack/tools/architectures would you recommend for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13n93el", "is_robot_indexable": true, "report_reasons": null, "author": "TheWalkingFridge", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n93el/how_to_store_and_serve_client_data_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n93el/how_to_store_and_serve_client_data_analytics/", "subreddit_subscribers": 106577, "created_utc": 1684616692.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}