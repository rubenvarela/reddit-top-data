{"kind": "Listing", "data": {"after": "t3_13nrs9v", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_vvm4y7n0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any reason for ssd to drop to 1% of its speed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 94, "top_awarded_type": null, "hide_score": false, "name": "t3_13nnb08", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 454, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 454, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mJgyQRauvnYVKh9oYuIoxkqkOFQjhU2-3fU7jvgeKEM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684657839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/vu1pny13b51b1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/vu1pny13b51b1.png?auto=webp&amp;v=enabled&amp;s=7948768363dc3dc4cc2241677c336eac897a5d43", "width": 558, "height": 375}, "resolutions": [{"url": "https://preview.redd.it/vu1pny13b51b1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9796951e1efa37af8c2d4f934fa03035b111163f", "width": 108, "height": 72}, {"url": "https://preview.redd.it/vu1pny13b51b1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64c9ac2160178860b7cbae575f1707f2957d4114", "width": 216, "height": 145}, {"url": "https://preview.redd.it/vu1pny13b51b1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b92079ad1a265cd6423318c23f2c67cbe1c84dbb", "width": 320, "height": 215}], "variants": {}, "id": "dJ0Gf0rslrZZlkr5nzu3vEzExIbzp88oUagAj0OLleQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nnb08", "is_robot_indexable": true, "report_reasons": null, "author": "lost_bytes", "discussion_type": null, "num_comments": 113, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nnb08/any_reason_for_ssd_to_drop_to_1_of_its_speed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/vu1pny13b51b1.png", "subreddit_subscribers": 683782, "created_utc": 1684657839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I hope this isn't interpreted as breaking rule 8 - I think there is a legitimate discussion around the risk some communities face as reddit corporate starts purging and sanitizing the site in preparation for sale to investors. \n\nIn case you're not familiar with it, r/CombatFootage is a community for posting and discussing records of human conflicts across time and locations. (Lately it's been dominated by footage from the war in Ukraine, but that's because it's the largest war in Europe since WW2 and so many of the combatants have go pros/smartphones) The photos and videos on that sub have great historical value not only for future historians who might analyze a conflict, but as a record of one of the most extreme experiences in the human condition. Naturally, because of the content in these records, they are very much not \"advertiser friendly\". \n\nLately reddit has been slowly purging unsavory communities in an effort to become more attractive for sale. A few days ago [I asked in that sub](https://www.reddit.com/r/CombatFootage/comments/13kvenz/this_sub_has_great_historical_value_is_there_any/?utm_source=share&amp;utm_medium=web2x&amp;context=3) if anyone was aware of any archival efforts being undertaken. Many people shared my concern, but were unaware if anything was being done. Some suggested asking in r/DataHoarder.\n\nSlight tangent: Ironically, my post was later removed - after generating thousands of upvotes and comments/discussions - with no message from the mods stating the reason it was being removed. I could understand if it had broken some kind of sub rule, but normally when mods remove something for breaking a rule, they cite the rule when removing it - and I had paid extra close attention to flairing my post correctly to abide by the rules. It was simple removed with no explanation - which makes me believe it was removed because I was in disagreement with reddit corporate's aims to sanitize the site.\n\nBack to the point of this question: I realize there are a lot of logistical issues when it comes to archiving and especially hosting the content in r/CombatFootage \\- which makes me believe that realistically nothing will be done and the content/community there will inevitably be lost to time when reddit corporate deletes the community. It's one thing if someone scrapes all the content from that community and saves it on their personal server - it's a lot more complicated to also host a location that people know to go to in case the community on reddit is destroyed. \n\nOn the bright side the US supreme court [dismissed a case](https://www.reuters.com/legal/us-supreme-court-lets-twitter-off-hook-terrorism-lawsuit-over-istanbul-massacre-2023-05-18/) that would have held internet platforms liable for the content posted on them by their users, so maybe that gives us some more time. Call me a pessimist though - but it feels like that issue is another example of just one of those things the gov has decided it wants to do (like banning strong encryption) and they will keep hammering it over and over and over again until they eventually codify it into law - so if anything this just kicked the can down the road.", "author_fullname": "t2_i3rjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone by chance archiving r/combatfootage? Concerned about subs that are not \"advertiser friendly\", yet that hold high historical value being purged by reddit corporate.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nmftf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 99, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 99, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684654966.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I hope this isn&amp;#39;t interpreted as breaking rule 8 - I think there is a legitimate discussion around the risk some communities face as reddit corporate starts purging and sanitizing the site in preparation for sale to investors. &lt;/p&gt;\n\n&lt;p&gt;In case you&amp;#39;re not familiar with it, &lt;a href=\"/r/CombatFootage\"&gt;r/CombatFootage&lt;/a&gt; is a community for posting and discussing records of human conflicts across time and locations. (Lately it&amp;#39;s been dominated by footage from the war in Ukraine, but that&amp;#39;s because it&amp;#39;s the largest war in Europe since WW2 and so many of the combatants have go pros/smartphones) The photos and videos on that sub have great historical value not only for future historians who might analyze a conflict, but as a record of one of the most extreme experiences in the human condition. Naturally, because of the content in these records, they are very much not &amp;quot;advertiser friendly&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;Lately reddit has been slowly purging unsavory communities in an effort to become more attractive for sale. A few days ago &lt;a href=\"https://www.reddit.com/r/CombatFootage/comments/13kvenz/this_sub_has_great_historical_value_is_there_any/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;I asked in that sub&lt;/a&gt; if anyone was aware of any archival efforts being undertaken. Many people shared my concern, but were unaware if anything was being done. Some suggested asking in &lt;a href=\"/r/DataHoarder\"&gt;r/DataHoarder&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Slight tangent: Ironically, my post was later removed - after generating thousands of upvotes and comments/discussions - with no message from the mods stating the reason it was being removed. I could understand if it had broken some kind of sub rule, but normally when mods remove something for breaking a rule, they cite the rule when removing it - and I had paid extra close attention to flairing my post correctly to abide by the rules. It was simple removed with no explanation - which makes me believe it was removed because I was in disagreement with reddit corporate&amp;#39;s aims to sanitize the site.&lt;/p&gt;\n\n&lt;p&gt;Back to the point of this question: I realize there are a lot of logistical issues when it comes to archiving and especially hosting the content in &lt;a href=\"/r/CombatFootage\"&gt;r/CombatFootage&lt;/a&gt; - which makes me believe that realistically nothing will be done and the content/community there will inevitably be lost to time when reddit corporate deletes the community. It&amp;#39;s one thing if someone scrapes all the content from that community and saves it on their personal server - it&amp;#39;s a lot more complicated to also host a location that people know to go to in case the community on reddit is destroyed. &lt;/p&gt;\n\n&lt;p&gt;On the bright side the US supreme court &lt;a href=\"https://www.reuters.com/legal/us-supreme-court-lets-twitter-off-hook-terrorism-lawsuit-over-istanbul-massacre-2023-05-18/\"&gt;dismissed a case&lt;/a&gt; that would have held internet platforms liable for the content posted on them by their users, so maybe that gives us some more time. Call me a pessimist though - but it feels like that issue is another example of just one of those things the gov has decided it wants to do (like banning strong encryption) and they will keep hammering it over and over and over again until they eventually codify it into law - so if anything this just kicked the can down the road.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9vNtjBKn6sdiLNZR34-uVUKtPofe0sBtkh2SMSvv5-Q.jpg?auto=webp&amp;v=enabled&amp;s=1c73b02a7f47acd48c74c6e19fe959f4b9a8f620", "width": 1200, "height": 628}, "resolutions": [{"url": "https://external-preview.redd.it/9vNtjBKn6sdiLNZR34-uVUKtPofe0sBtkh2SMSvv5-Q.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f8d31796080c475575bb0fab120a70b131a3af3", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/9vNtjBKn6sdiLNZR34-uVUKtPofe0sBtkh2SMSvv5-Q.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d68e4b22f51043735e2b3a857993057f2f34f487", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/9vNtjBKn6sdiLNZR34-uVUKtPofe0sBtkh2SMSvv5-Q.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2501da1041d17159cc723482ddbaad232fc9707a", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/9vNtjBKn6sdiLNZR34-uVUKtPofe0sBtkh2SMSvv5-Q.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67dd2597107becbcead6840c978f2a04b9cf469c", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/9vNtjBKn6sdiLNZR34-uVUKtPofe0sBtkh2SMSvv5-Q.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45fe18e88f4aed86e5ac9e110574e59f721d57d4", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/9vNtjBKn6sdiLNZR34-uVUKtPofe0sBtkh2SMSvv5-Q.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41894eb9d0e16177b429b5fd5f613ee9556ea06a", "width": 1080, "height": 565}], "variants": {}, "id": "Cp2NTCctSZ4hmHtNWkIFqAM450Dg5QRLb0lZZ5mARio"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13nmftf", "is_robot_indexable": true, "report_reasons": null, "author": "Prophet_60091_", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nmftf/is_anyone_by_chance_archiving_rcombatfootage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nmftf/is_anyone_by_chance_archiving_rcombatfootage/", "subreddit_subscribers": 683782, "created_utc": 1684654966.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've checked in with Google support and got transferred twice all the way to a specialist and after much probing I got them to make  the following clear statement:\n\n&gt;\"Google Workspace Support: ...if there is no limit set per Shared Drive and a single Shared Drive is not reaching the item cap of 400,000 even if the files stored is too large it will still not trigger a limit until it reaches the 400,000 item cap.\"\n\nIn other words, you can see your pooled storage being exceeded but shared drive storage won't cause any problems unless the admin set a shared drive storage limit or if you reach the 400,000 file count cap. So without any storage limit, you only  have the file count cap and the folder ne sting limits to worry about. \n\nThey also made references trying to curb abuse and I think the way they are showing the storage limit in Google Workspace is designed to push people to limit their storage usage. it seems to work based on all the Reddit threads I've seen about this. \n\nI hope this clams some people's nerves. Continue enjoying unlimited shared drive storage :)", "author_fullname": "t2_aq05j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Workspace Shared Drive Storage is still unlimited abiding to Google Support", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nl7qv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684650993.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve checked in with Google support and got transferred twice all the way to a specialist and after much probing I got them to make  the following clear statement:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&amp;quot;Google Workspace Support: ...if there is no limit set per Shared Drive and a single Shared Drive is not reaching the item cap of 400,000 even if the files stored is too large it will still not trigger a limit until it reaches the 400,000 item cap.&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;In other words, you can see your pooled storage being exceeded but shared drive storage won&amp;#39;t cause any problems unless the admin set a shared drive storage limit or if you reach the 400,000 file count cap. So without any storage limit, you only  have the file count cap and the folder ne sting limits to worry about. &lt;/p&gt;\n\n&lt;p&gt;They also made references trying to curb abuse and I think the way they are showing the storage limit in Google Workspace is designed to push people to limit their storage usage. it seems to work based on all the Reddit threads I&amp;#39;ve seen about this. &lt;/p&gt;\n\n&lt;p&gt;I hope this clams some people&amp;#39;s nerves. Continue enjoying unlimited shared drive storage :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13nl7qv", "is_robot_indexable": true, "report_reasons": null, "author": "coins4bits", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nl7qv/google_workspace_shared_drive_storage_is_still/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nl7qv/google_workspace_shared_drive_storage_is_still/", "subreddit_subscribers": 683782, "created_utc": 1684650993.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I started playing around with OneDrive but I couldn't figure out a way to upload the folder itself from my gallery there. It also seems like it wants to sync to my phone's gallery, so if I delete it from my phone, it'll delete from OneDrive - not what I want.", "author_fullname": "t2_a3v0n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to figure out how to back up photos the way I want from my Galaxy S20 to cloud storage. I want to upload folders from my gallery to the cloud, have them appear as those folders in the cloud, and then delete them from my phone without them being deleted from the cloud. Any advice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13neyyt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684632146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started playing around with OneDrive but I couldn&amp;#39;t figure out a way to upload the folder itself from my gallery there. It also seems like it wants to sync to my phone&amp;#39;s gallery, so if I delete it from my phone, it&amp;#39;ll delete from OneDrive - not what I want.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13neyyt", "is_robot_indexable": true, "report_reasons": null, "author": "StarDestinyGuy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13neyyt/trying_to_figure_out_how_to_back_up_photos_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13neyyt/trying_to_figure_out_how_to_back_up_photos_the/", "subreddit_subscribers": 683782, "created_utc": 1684632146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI'm a terrible Data Hoarder, I collect games, movies, series, save videos I've made when gaming, screenshots, full backups of my PC, everything..\n\nI have a very good habit of compressing / getting the best data to performance ratio, but eventually, I still fill up.\n\nMy bad habit however is I keep buying external hard drives, ( I currently own 5-6 4TB's and multiple 2TB's) and they're annoying to keep record of, it's getting out of control.. and the thought of one dying on me randomly is sickening. It happened once but luckily nothing on it was important.\n\nMy goal is to have at least 24TB of data, possibly more with RAID 5.\n\nI'm afraid of JBODs, I'm unsure how to properly set that kind of thing up.. but I'm still open to suggestions.\n\nCould anyone please offer the cheapest, economical route of a NAS to buy, so I can fill it with HDDs?\n\n**My requirements are:**\n\nRaid 5\n\nAbility to reach storage over the internet / LAN\n\nA GUI would be nice, app controlled via iPhone if possible? IP address is fine.\n\n4 bays would be okay, 5-6 would be better for scalability.\n\nThe device MUST have a sleep mode. It MUST be power efficient.. and the disks to only spin when a request for data be made for either writing or reading.\n\nDoesn't need to be blazing fast, 500mb speed is fine, more speed is welcome.\n\n&amp;#x200B;\n\n**My main intent is:**\n\nTo hold my data I've collected over my life, without fear of loss by HDD death, given most my external drives are using proprietary slots (faff you WD, probably my fault for buying passport versions though).\n\nKeep safe, clear away from things.\n\nLeave it alone and only interact with it via WIFI / internet.", "author_fullname": "t2_s7su4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Hoarder here. Need solutions.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o0kvm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684689695.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684689402.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a terrible Data Hoarder, I collect games, movies, series, save videos I&amp;#39;ve made when gaming, screenshots, full backups of my PC, everything..&lt;/p&gt;\n\n&lt;p&gt;I have a very good habit of compressing / getting the best data to performance ratio, but eventually, I still fill up.&lt;/p&gt;\n\n&lt;p&gt;My bad habit however is I keep buying external hard drives, ( I currently own 5-6 4TB&amp;#39;s and multiple 2TB&amp;#39;s) and they&amp;#39;re annoying to keep record of, it&amp;#39;s getting out of control.. and the thought of one dying on me randomly is sickening. It happened once but luckily nothing on it was important.&lt;/p&gt;\n\n&lt;p&gt;My goal is to have at least 24TB of data, possibly more with RAID 5.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m afraid of JBODs, I&amp;#39;m unsure how to properly set that kind of thing up.. but I&amp;#39;m still open to suggestions.&lt;/p&gt;\n\n&lt;p&gt;Could anyone please offer the cheapest, economical route of a NAS to buy, so I can fill it with HDDs?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My requirements are:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Raid 5&lt;/p&gt;\n\n&lt;p&gt;Ability to reach storage over the internet / LAN&lt;/p&gt;\n\n&lt;p&gt;A GUI would be nice, app controlled via iPhone if possible? IP address is fine.&lt;/p&gt;\n\n&lt;p&gt;4 bays would be okay, 5-6 would be better for scalability.&lt;/p&gt;\n\n&lt;p&gt;The device MUST have a sleep mode. It MUST be power efficient.. and the disks to only spin when a request for data be made for either writing or reading.&lt;/p&gt;\n\n&lt;p&gt;Doesn&amp;#39;t need to be blazing fast, 500mb speed is fine, more speed is welcome.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My main intent is:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;To hold my data I&amp;#39;ve collected over my life, without fear of loss by HDD death, given most my external drives are using proprietary slots (faff you WD, probably my fault for buying passport versions though).&lt;/p&gt;\n\n&lt;p&gt;Keep safe, clear away from things.&lt;/p&gt;\n\n&lt;p&gt;Leave it alone and only interact with it via WIFI / internet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o0kvm", "is_robot_indexable": true, "report_reasons": null, "author": "SumonaFlorence", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o0kvm/data_hoarder_here_need_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o0kvm/data_hoarder_here_need_solutions/", "subreddit_subscribers": 683782, "created_utc": 1684689402.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to copy an external drive to another external drive.  \nSo If I plug in the external drive to one side of my laptop and then the other external drive to the other side, to make a copy between eachother.... my laptop is acting as a transfer or connection point, so does it mean all the data is briefly being written onto my laptops SSD before transferring over to the other external drive?  \nOr is it all just done on the ram level of my laptop?", "author_fullname": "t2_bib2a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Copying 1 External Drive to Another, Write on the Computers Drive all?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nds3z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684628853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to copy an external drive to another external drive.&lt;br/&gt;\nSo If I plug in the external drive to one side of my laptop and then the other external drive to the other side, to make a copy between eachother.... my laptop is acting as a transfer or connection point, so does it mean all the data is briefly being written onto my laptops SSD before transferring over to the other external drive?&lt;br/&gt;\nOr is it all just done on the ram level of my laptop?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13nds3z", "is_robot_indexable": true, "report_reasons": null, "author": "LeoWitt", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nds3z/does_copying_1_external_drive_to_another_write_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nds3z/does_copying_1_external_drive_to_another_write_on/", "subreddit_subscribers": 683782, "created_utc": 1684628853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have family photos I would like encrypted so I can save them on the cloud as well as other places. Currently I just zip a bunch of them into 30gb .zip files and then encrypt that with AES256 bit. \nWould having them all zipped have any down sides?\nAre there any better methods for doing this?", "author_fullname": "t2_rqz7q6ye", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should I be encrypting files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o5leb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684701427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have family photos I would like encrypted so I can save them on the cloud as well as other places. Currently I just zip a bunch of them into 30gb .zip files and then encrypt that with AES256 bit. \nWould having them all zipped have any down sides?\nAre there any better methods for doing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o5leb", "is_robot_indexable": true, "report_reasons": null, "author": "SiliconMillikan", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o5leb/how_should_i_be_encrypting_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o5leb/how_should_i_be_encrypting_files/", "subreddit_subscribers": 683782, "created_utc": 1684701427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys.\n\nSo, over the past 15 years i have collected 80-90GB of photos / media through several different phones  / file systems. \n\nAny chance is there a program which could look through all of them, managing duplicates, and give me a rough idea on categorising them based on exif, or other details? In-between the folders i have also a bunch of screenshots, downloaded pics, and all that stuff as well. \n\n&amp;#x200B;\n\nThanks guys.", "author_fullname": "t2_6q8rm9or", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(windows) Recommendations for Organizing / Managing duplicates of a bulk 80-90GB personal photo folder", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nrerk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684671109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys.&lt;/p&gt;\n\n&lt;p&gt;So, over the past 15 years i have collected 80-90GB of photos / media through several different phones  / file systems. &lt;/p&gt;\n\n&lt;p&gt;Any chance is there a program which could look through all of them, managing duplicates, and give me a rough idea on categorising them based on exif, or other details? In-between the folders i have also a bunch of screenshots, downloaded pics, and all that stuff as well. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nrerk", "is_robot_indexable": true, "report_reasons": null, "author": "Kekzarc", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nrerk/windows_recommendations_for_organizing_managing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nrerk/windows_recommendations_for_organizing_managing/", "subreddit_subscribers": 683782, "created_utc": 1684671109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a WD MyCloud EX2 Ultra which is currently equipped with 2 original 8TB drives. I am planning on building a new PC and though about upgrading the NAS instead of packing huge HDDs in the new PC.\n\nThe drives listed on WDs drive compatibility list are extremely expensive. Does anyone here know if drives from other vendors (like Toshiba or Seagate) will work in my NAS? Or even WD drives that are not on the list?", "author_fullname": "t2_125scv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with non-WD drives in MyCloud EX2 Ultra", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nmd2l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684654717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a WD MyCloud EX2 Ultra which is currently equipped with 2 original 8TB drives. I am planning on building a new PC and though about upgrading the NAS instead of packing huge HDDs in the new PC.&lt;/p&gt;\n\n&lt;p&gt;The drives listed on WDs drive compatibility list are extremely expensive. Does anyone here know if drives from other vendors (like Toshiba or Seagate) will work in my NAS? Or even WD drives that are not on the list?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nmd2l", "is_robot_indexable": true, "report_reasons": null, "author": "TheColin21", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nmd2l/experience_with_nonwd_drives_in_mycloud_ex2_ultra/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nmd2l/experience_with_nonwd_drives_in_mycloud_ex2_ultra/", "subreddit_subscribers": 683782, "created_utc": 1684654717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everybody. I am not the most tech savvy when it comes to network/storage type stuff so I could use some help figuring out a new setup in light of google's decision to cut off our unlimited storage.\n\nPreviously I was using gdrive to store all of my TV shows and movies that are on my plex server. I was using a seedbox to download media, and vnc viewer to upload those files to gdrive. I had tried to run my plex server from the seedbox at one point, but ran into lots of buffering issues. So, I currently have my plex setup locally on a pc, and am using netdrive3 to mount gdrive as a local drive, and it is working great. \n\nSince I have plex running well from my local pc, my first thought was, all I need is a docking station. \n\nLike this: [https://www.amazon.com/SABRENT-Tray-Less-Docking-Station-DS-UCTB/dp/B09TV1XPDD/ref=asc\\_df\\_B09TV1XPDD/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=583376942549&amp;hvpos=&amp;hvnetw=g&amp;hvrand=460423300124451486&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9021936&amp;hvtargid=pla-1646314242856&amp;th=1](https://www.amazon.com/SABRENT-Tray-Less-Docking-Station-DS-UCTB/dp/B09TV1XPDD/ref=asc_df_B09TV1XPDD/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=583376942549&amp;hvpos=&amp;hvnetw=g&amp;hvrand=460423300124451486&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9021936&amp;hvtargid=pla-1646314242856&amp;th=1) \n\nI currently have about 60 tb of media on my gdrive, and this station could easily store that much, and more. Do you all think that something like this would be a viable option in my case? I am open to suggestions about NAS as well. I don't know much about them, but figured I could do without, since I already have my plex server running on my local pc. \n\nI mainly get media from private trackers as well, so if need be, I can ditch the seedbox. And if the docking station route is the way to go, I am open to different brands/product suggestions, as I haven't done much research on them and their different price ranges.\n\nObviously I am trying to be mindful of cost, but will spend the money where needed to figure out a new storage method.\n\nHope I explained everything clearly, and looking forward to the help. Thanks!", "author_fullname": "t2_81h2r164q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help figure out new storage setup since gdrive unlimited storage is now gone", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ni4ux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Setup Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684641266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody. I am not the most tech savvy when it comes to network/storage type stuff so I could use some help figuring out a new setup in light of google&amp;#39;s decision to cut off our unlimited storage.&lt;/p&gt;\n\n&lt;p&gt;Previously I was using gdrive to store all of my TV shows and movies that are on my plex server. I was using a seedbox to download media, and vnc viewer to upload those files to gdrive. I had tried to run my plex server from the seedbox at one point, but ran into lots of buffering issues. So, I currently have my plex setup locally on a pc, and am using netdrive3 to mount gdrive as a local drive, and it is working great. &lt;/p&gt;\n\n&lt;p&gt;Since I have plex running well from my local pc, my first thought was, all I need is a docking station. &lt;/p&gt;\n\n&lt;p&gt;Like this: &lt;a href=\"https://www.amazon.com/SABRENT-Tray-Less-Docking-Station-DS-UCTB/dp/B09TV1XPDD/ref=asc_df_B09TV1XPDD/?tag=hyprod-20&amp;amp;linkCode=df0&amp;amp;hvadid=583376942549&amp;amp;hvpos=&amp;amp;hvnetw=g&amp;amp;hvrand=460423300124451486&amp;amp;hvpone=&amp;amp;hvptwo=&amp;amp;hvqmt=&amp;amp;hvdev=c&amp;amp;hvdvcmdl=&amp;amp;hvlocint=&amp;amp;hvlocphy=9021936&amp;amp;hvtargid=pla-1646314242856&amp;amp;th=1\"&gt;https://www.amazon.com/SABRENT-Tray-Less-Docking-Station-DS-UCTB/dp/B09TV1XPDD/ref=asc_df_B09TV1XPDD/?tag=hyprod-20&amp;amp;linkCode=df0&amp;amp;hvadid=583376942549&amp;amp;hvpos=&amp;amp;hvnetw=g&amp;amp;hvrand=460423300124451486&amp;amp;hvpone=&amp;amp;hvptwo=&amp;amp;hvqmt=&amp;amp;hvdev=c&amp;amp;hvdvcmdl=&amp;amp;hvlocint=&amp;amp;hvlocphy=9021936&amp;amp;hvtargid=pla-1646314242856&amp;amp;th=1&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;I currently have about 60 tb of media on my gdrive, and this station could easily store that much, and more. Do you all think that something like this would be a viable option in my case? I am open to suggestions about NAS as well. I don&amp;#39;t know much about them, but figured I could do without, since I already have my plex server running on my local pc. &lt;/p&gt;\n\n&lt;p&gt;I mainly get media from private trackers as well, so if need be, I can ditch the seedbox. And if the docking station route is the way to go, I am open to different brands/product suggestions, as I haven&amp;#39;t done much research on them and their different price ranges.&lt;/p&gt;\n\n&lt;p&gt;Obviously I am trying to be mindful of cost, but will spend the money where needed to figure out a new storage method.&lt;/p&gt;\n\n&lt;p&gt;Hope I explained everything clearly, and looking forward to the help. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13ni4ux", "is_robot_indexable": true, "report_reasons": null, "author": "7saltypirates", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ni4ux/help_figure_out_new_storage_setup_since_gdrive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ni4ux/help_figure_out_new_storage_setup_since_gdrive/", "subreddit_subscribers": 683782, "created_utc": 1684641266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just bought an external 2tb Seagate Expansion to basically clone one of the drives I have in my PC in which I store important photos and videos.\n\nWhen I tried copying 600GB to it, the process crashed midway. Two times.\n\nThen I tried copying folder by folder, which the heaviest one was no more than 150GB, and I was successful that way (The process that I used was just copying it with the files explorer on Windows from one disk to the other).\n\nI tested both the drive in my PC and the new external one with CrystalDiskInfo and they were showing all good (Before and after). And also I did a full format of the new one before doing anything.\n\nShould I be worried about the state of the new HDD?", "author_fullname": "t2_1349pz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External HDD is crashing when copying all at once, but no problems when doing it by sections", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ndncb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684628490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just bought an external 2tb Seagate Expansion to basically clone one of the drives I have in my PC in which I store important photos and videos.&lt;/p&gt;\n\n&lt;p&gt;When I tried copying 600GB to it, the process crashed midway. Two times.&lt;/p&gt;\n\n&lt;p&gt;Then I tried copying folder by folder, which the heaviest one was no more than 150GB, and I was successful that way (The process that I used was just copying it with the files explorer on Windows from one disk to the other).&lt;/p&gt;\n\n&lt;p&gt;I tested both the drive in my PC and the new external one with CrystalDiskInfo and they were showing all good (Before and after). And also I did a full format of the new one before doing anything.&lt;/p&gt;\n\n&lt;p&gt;Should I be worried about the state of the new HDD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "10TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ndncb", "is_robot_indexable": true, "report_reasons": null, "author": "ixanol", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13ndncb/external_hdd_is_crashing_when_copying_all_at_once/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ndncb/external_hdd_is_crashing_when_copying_all_at_once/", "subreddit_subscribers": 683782, "created_utc": 1684628490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Good afternoon people!\n\nFor some reason I\u2019ve become somewhat nervous about keeping my emails on Google\u2019s servers. I have a gmail account (for more than 15 years at this point) a private workspace account and a work assigned workspace account.\n \nI am using rclone to backup my Google drives (about 150 gigabytes total). I now want to get all my emails from each account.\n\nWould this be overkill? I\u2019ve been digging down this rabbit hole of people telling horror stories about losing all their emails.\n\nAny advice on how to do it? I\u2019ve tried getmail6 but cannot make heads or tails of the documentation.", "author_fullname": "t2_hrtc9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up gmail emails", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13o91rx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684709699.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good afternoon people!&lt;/p&gt;\n\n&lt;p&gt;For some reason I\u2019ve become somewhat nervous about keeping my emails on Google\u2019s servers. I have a gmail account (for more than 15 years at this point) a private workspace account and a work assigned workspace account.&lt;/p&gt;\n\n&lt;p&gt;I am using rclone to backup my Google drives (about 150 gigabytes total). I now want to get all my emails from each account.&lt;/p&gt;\n\n&lt;p&gt;Would this be overkill? I\u2019ve been digging down this rabbit hole of people telling horror stories about losing all their emails.&lt;/p&gt;\n\n&lt;p&gt;Any advice on how to do it? I\u2019ve tried getmail6 but cannot make heads or tails of the documentation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o91rx", "is_robot_indexable": true, "report_reasons": null, "author": "py2gb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o91rx/backing_up_gmail_emails/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o91rx/backing_up_gmail_emails/", "subreddit_subscribers": 683782, "created_utc": 1684709699.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Tried different tapes. Different SAS controllers. \n\nSame controller on a Mac works great with Retrospect. 80-150MB/s\n\nCould this be a Veeam issue ? Or a sas controller driver  issue?", "author_fullname": "t2_4emv3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any idea why Ultrium LTO4 only writes at 150KB/s using Veeam?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13o8t9w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684709102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tried different tapes. Different SAS controllers. &lt;/p&gt;\n\n&lt;p&gt;Same controller on a Mac works great with Retrospect. 80-150MB/s&lt;/p&gt;\n\n&lt;p&gt;Could this be a Veeam issue ? Or a sas controller driver  issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "25TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o8t9w", "is_robot_indexable": true, "report_reasons": null, "author": "dangil", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13o8t9w/any_idea_why_ultrium_lto4_only_writes_at_150kbs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o8t9w/any_idea_why_ultrium_lto4_only_writes_at_150kbs/", "subreddit_subscribers": 683782, "created_utc": 1684709102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A slight update on the Ultrium 5 FC LTO drive I mistakenly purchased the other day. Thanks again to those who offered advice. I got it installed in my win10 machine and after a few restarts, it and the HBA were detected. Have run some of the tests through HPE Library &amp; Tape Tools. Is the following an outright error or just the software testing the drive at higher than it is capable of writing?  \n\n|\\_\\_ Sense Key 0x00, Sense Code 0x0000 (No additional sense information)  \n|\\_\\_        2.3 m/sec. tape speed:   \n|\\_\\_                  Great margin (Data written: 386.6 MB)  \n|\\_\\_        2.8 m/sec. tape speed:   \n|\\_\\_                  Great margin (Data written: 386.6 MB)  \n|\\_\\_        3.4 m/sec. tape speed:   \n|\\_\\_                  Great margin (Data written: 386.6 MB)  \n|\\_\\_        3.9 m/sec. tape speed:   \n|\\_\\_                  Warning (Data written: 386.6 MB)  \n|\\_\\_        4.5 m/sec. tape speed:   \n|\\_\\_                  Warning (Data written: 0.0 MB)  \n|\\_\\_        forward direction:   \n|\\_\\_                  Great margin (Data written: 1546.5 MB)  \n|\\_\\_        reverse direction:   \n|\\_\\_                  Warning (Data written: 0.0 MB)  \n|\\_\\_ The LTO Drive Assessment Test has checked the history and operation of the selected drive, and   \n|\\_\\_ problems have been reported.  \n|\\_\\_ The test tape used was not HPE-labelled tape. HPE-labelled tape is preferred  \n|\\_\\_ for this test, so if possible please re-run the test using HPE-labelled tape.  \n|\\_\\_ Test time: 4:57\n\nI also received a \"fibre channel receiver rx low power\" alarm. The FC is only 50cm and while it is looping back on itself, its quite a gentle curve and there are no kinks or sharp angles. From my limited understanding this could be caused by a wavelength issue?\n\nAdditionally, Uranium Backup fails to see the drive. Iperius does, but all write operations fail with 'Error: 2 - The system cannot find the file specified'\n\nSo, some progress but some issues still to work out! If anyone has win10 drivers I'd be very grateful for a copy.", "author_fullname": "t2_vdieh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Further adventures with Ultrium 5", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13o88fj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684707702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A slight update on the Ultrium 5 FC LTO drive I mistakenly purchased the other day. Thanks again to those who offered advice. I got it installed in my win10 machine and after a few restarts, it and the HBA were detected. Have run some of the tests through HPE Library &amp;amp; Tape Tools. Is the following an outright error or just the software testing the drive at higher than it is capable of writing?  &lt;/p&gt;\n\n&lt;p&gt;|__ Sense Key 0x00, Sense Code 0x0000 (No additional sense information)&lt;br/&gt;\n|__        2.3 m/sec. tape speed:&lt;br/&gt;\n|__                  Great margin (Data written: 386.6 MB)&lt;br/&gt;\n|__        2.8 m/sec. tape speed:&lt;br/&gt;\n|__                  Great margin (Data written: 386.6 MB)&lt;br/&gt;\n|__        3.4 m/sec. tape speed:&lt;br/&gt;\n|__                  Great margin (Data written: 386.6 MB)&lt;br/&gt;\n|__        3.9 m/sec. tape speed:&lt;br/&gt;\n|__                  Warning (Data written: 386.6 MB)&lt;br/&gt;\n|__        4.5 m/sec. tape speed:&lt;br/&gt;\n|__                  Warning (Data written: 0.0 MB)&lt;br/&gt;\n|__        forward direction:&lt;br/&gt;\n|__                  Great margin (Data written: 1546.5 MB)&lt;br/&gt;\n|__        reverse direction:&lt;br/&gt;\n|__                  Warning (Data written: 0.0 MB)&lt;br/&gt;\n|__ The LTO Drive Assessment Test has checked the history and operation of the selected drive, and&lt;br/&gt;\n|__ problems have been reported.&lt;br/&gt;\n|__ The test tape used was not HPE-labelled tape. HPE-labelled tape is preferred&lt;br/&gt;\n|__ for this test, so if possible please re-run the test using HPE-labelled tape.&lt;br/&gt;\n|__ Test time: 4:57&lt;/p&gt;\n\n&lt;p&gt;I also received a &amp;quot;fibre channel receiver rx low power&amp;quot; alarm. The FC is only 50cm and while it is looping back on itself, its quite a gentle curve and there are no kinks or sharp angles. From my limited understanding this could be caused by a wavelength issue?&lt;/p&gt;\n\n&lt;p&gt;Additionally, Uranium Backup fails to see the drive. Iperius does, but all write operations fail with &amp;#39;Error: 2 - The system cannot find the file specified&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;So, some progress but some issues still to work out! If anyone has win10 drivers I&amp;#39;d be very grateful for a copy.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o88fj", "is_robot_indexable": true, "report_reasons": null, "author": "pennanbeach", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o88fj/further_adventures_with_ultrium_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o88fj/further_adventures_with_ultrium_5/", "subreddit_subscribers": 683782, "created_utc": 1684707702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi to everyone, I would like an advice:\n\nI have a lot of photos, currently I store them in folders / subfolders with the following structure:\n\n* Person\n   * Year\n      * Month + day\n\nThe problem is that it is impossible to retrieve photos in an efficient way.  \nI would like some suggestion: does exist a software (open source and self hostable) that allows to keep a kind of general gallery, like google photos, where I have all metadata of the photo (date, time, geotag), and where I can add personal tags defined by me? (in order to be able to retrieve for example all the photos of birthdays)\n\nThanks in advance to everyone!", "author_fullname": "t2_3hwuhm84", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to store photos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13o7xdu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684706945.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi to everyone, I would like an advice:&lt;/p&gt;\n\n&lt;p&gt;I have a lot of photos, currently I store them in folders / subfolders with the following structure:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Person\n\n&lt;ul&gt;\n&lt;li&gt;Year\n\n&lt;ul&gt;\n&lt;li&gt;Month + day&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The problem is that it is impossible to retrieve photos in an efficient way.&lt;br/&gt;\nI would like some suggestion: does exist a software (open source and self hostable) that allows to keep a kind of general gallery, like google photos, where I have all metadata of the photo (date, time, geotag), and where I can add personal tags defined by me? (in order to be able to retrieve for example all the photos of birthdays)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance to everyone!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o7xdu", "is_robot_indexable": true, "report_reasons": null, "author": "Landomix", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o7xdu/best_way_to_store_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o7xdu/best_way_to_store_photos/", "subreddit_subscribers": 683782, "created_utc": 1684706945.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, just added a new line in my checklist for n services with 2FA. To check the fallback before it's needed. \n\nMy previous phone died and I lost my two factor authentication app with all registered apps... So I have been getting everything sorted on my new phone using the fallback option. So far I had no issues with Google, amazon, plex, etc. Until I tried backblaze, which simply doesn't work. \nI have contacted their support and they confirm that the phone number in my account is indeed my current number, and they require the master application key and key id to remove the 2FA. But I don't have these as I never needed them.\n\nHas anyone successfully used the sms fallback on backblaze? \n\nI am just glad that it happened now and not while trying to retrieve data that were gone from any local storage. But still, I no longer concider the roughly 1.5 tb of data as readily available on theor service.\n\nHas anyone else had similar issues? \n\nTo add insult to the injury, I am not sure if I am being charged for the fallback codes I am not receiving. As \"messaging rates may apply\".\n\nTLDR: lost my 2FA app and I am locked out of my backblaze account that I have my backups.", "author_fullname": "t2_12il39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backblaze 2FA has no fallback?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o5g7z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684701084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, just added a new line in my checklist for n services with 2FA. To check the fallback before it&amp;#39;s needed. &lt;/p&gt;\n\n&lt;p&gt;My previous phone died and I lost my two factor authentication app with all registered apps... So I have been getting everything sorted on my new phone using the fallback option. So far I had no issues with Google, amazon, plex, etc. Until I tried backblaze, which simply doesn&amp;#39;t work. \nI have contacted their support and they confirm that the phone number in my account is indeed my current number, and they require the master application key and key id to remove the 2FA. But I don&amp;#39;t have these as I never needed them.&lt;/p&gt;\n\n&lt;p&gt;Has anyone successfully used the sms fallback on backblaze? &lt;/p&gt;\n\n&lt;p&gt;I am just glad that it happened now and not while trying to retrieve data that were gone from any local storage. But still, I no longer concider the roughly 1.5 tb of data as readily available on theor service.&lt;/p&gt;\n\n&lt;p&gt;Has anyone else had similar issues? &lt;/p&gt;\n\n&lt;p&gt;To add insult to the injury, I am not sure if I am being charged for the fallback codes I am not receiving. As &amp;quot;messaging rates may apply&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;TLDR: lost my 2FA app and I am locked out of my backblaze account that I have my backups.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o5g7z", "is_robot_indexable": true, "report_reasons": null, "author": "ebrembo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o5g7z/backblaze_2fa_has_no_fallback/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o5g7z/backblaze_2fa_has_no_fallback/", "subreddit_subscribers": 683782, "created_utc": 1684701084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I'm trying to backup some OnlyFans pages but I can't really find any reliable tools. I tried WFDownloader but it doesn't support OnlyFans. Thanks", "author_fullname": "t2_uegnp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which are some good tools to backup OnlyFans content?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o17my", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684690884.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I&amp;#39;m trying to backup some OnlyFans pages but I can&amp;#39;t really find any reliable tools. I tried WFDownloader but it doesn&amp;#39;t support OnlyFans. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o17my", "is_robot_indexable": true, "report_reasons": null, "author": "CrazyYAY", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o17my/which_are_some_good_tools_to_backup_onlyfans/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o17my/which_are_some_good_tools_to_backup_onlyfans/", "subreddit_subscribers": 683782, "created_utc": 1684690884.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "ive been wrecking my brain on this for the past weekend, since i cant seem to find anything, but.  \n\n\nare there any good sellers in europe/ sellers in general that ship to the EU?  \n\n\nas i'm kind of long overdue for building myself this, but money is kind of the biggest problem in my alleyway", "author_fullname": "t2_6i9v5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(my first nas-question) recertified frive sellers are in europe/ship to europe ? (outside of amazon)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o0vh0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684690100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ive been wrecking my brain on this for the past weekend, since i cant seem to find anything, but.  &lt;/p&gt;\n\n&lt;p&gt;are there any good sellers in europe/ sellers in general that ship to the EU?  &lt;/p&gt;\n\n&lt;p&gt;as i&amp;#39;m kind of long overdue for building myself this, but money is kind of the biggest problem in my alleyway&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o0vh0", "is_robot_indexable": true, "report_reasons": null, "author": "claudybunni", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o0vh0/my_first_nasquestion_recertified_frive_sellers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o0vh0/my_first_nasquestion_recertified_frive_sellers/", "subreddit_subscribers": 683782, "created_utc": 1684690100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is a used DS212j in 2023 worth it?  I found someone selling one for $50.  I already have the hard drives.  Thoughts.  I figure I could use it to store photos and practice with a NAS since I\u2019ve never owned one.r/question", "author_fullname": "t2_94p9h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DS212J in 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nxq8q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684682482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is a used DS212j in 2023 worth it?  I found someone selling one for $50.  I already have the hard drives.  Thoughts.  I figure I could use it to store photos and practice with a NAS since I\u2019ve never owned one.&lt;a href=\"/r/question\"&gt;r/question&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nxq8q", "is_robot_indexable": true, "report_reasons": null, "author": "suprkain", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nxq8q/ds212j_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nxq8q/ds212j_in_2023/", "subreddit_subscribers": 683782, "created_utc": 1684682482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently started playing around with restic and borgbackup. I'm confused about whether they are incremental or differential. I know they use snapshots and de-duplication. But I want to know what happens if a snapshot is corrupt, do all subsequent snapshots contain the corrupt data and are therefore all corrupt, as is the case with incremental backups?\n\nAs I understand it, with incremental backups, if an incremental backup is corrupt (or the original), all subsequent backups will be too. Differential backups only backup the original + all new changes, leading to (in my opinion) a more reliable backup method (let me know if my reasoning is flawed). Since restic uses de-duplication, no duplicates will exist, therefore I don't see how it can be differential since differential backups contain duplicates (every backup contains all changes made since the last full backup, while incremental only contains changes made since the last incremental backup).", "author_fullname": "t2_w2rphh1j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is restic incremental or differential?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nnq1r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684659246.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started playing around with restic and borgbackup. I&amp;#39;m confused about whether they are incremental or differential. I know they use snapshots and de-duplication. But I want to know what happens if a snapshot is corrupt, do all subsequent snapshots contain the corrupt data and are therefore all corrupt, as is the case with incremental backups?&lt;/p&gt;\n\n&lt;p&gt;As I understand it, with incremental backups, if an incremental backup is corrupt (or the original), all subsequent backups will be too. Differential backups only backup the original + all new changes, leading to (in my opinion) a more reliable backup method (let me know if my reasoning is flawed). Since restic uses de-duplication, no duplicates will exist, therefore I don&amp;#39;t see how it can be differential since differential backups contain duplicates (every backup contains all changes made since the last full backup, while incremental only contains changes made since the last incremental backup).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nnq1r", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Studio4716", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nnq1r/is_restic_incremental_or_differential/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nnq1r/is_restic_incremental_or_differential/", "subreddit_subscribers": 683782, "created_utc": 1684659246.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Title.  I know this is kinda a googlable thing, but I find mentions MTBF of \"2.5million hours\".  Surely that's not power on hours, as a normal HDD lasts \\~43k hours.  Could someone explain how MTBF translates to when I should actually replace my drive?  As someone trying to get more serious about protecting data, performing backups, and maintaining the server (old PC, not anything special) going forward, I'm a bit confused by this.\n\nFurthermore, my plan, until I can afford a proper raid setup (I don't even have enough HDDs, let alone the money for an 8bay NAS or a sata card, or other means to setup raid in my PC) I plan to cold storage HDDs with data backed up on a shelf.  See, I went cheap when I was first hoarding, due to the speed at which it was growing, and went 10tb externals for like $200 each.  They're find enough, and after replacing my oldest at 39k hours, the next oldest has 18k hours, so I've got a bit of time.  My plan is to slowly purchase more 16tb, copy the data on the 10tb over to the 16tb, and then shelve the external HDDs as my backup.  Eventually I will buy more 16tb, but for now this will minimize potential losses.\n\nAm I correct in understanding that this is generally ok, so long as I power them on occasionally?  And to this end, what's a good policy for timing to do so?  Yearly?  Every other year?  Appreciate some more experienced insight.", "author_fullname": "t2_hdqpz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Some questions on when to replace Exos HDDs and Backup methods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o4x9m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684699799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title.  I know this is kinda a googlable thing, but I find mentions MTBF of &amp;quot;2.5million hours&amp;quot;.  Surely that&amp;#39;s not power on hours, as a normal HDD lasts ~43k hours.  Could someone explain how MTBF translates to when I should actually replace my drive?  As someone trying to get more serious about protecting data, performing backups, and maintaining the server (old PC, not anything special) going forward, I&amp;#39;m a bit confused by this.&lt;/p&gt;\n\n&lt;p&gt;Furthermore, my plan, until I can afford a proper raid setup (I don&amp;#39;t even have enough HDDs, let alone the money for an 8bay NAS or a sata card, or other means to setup raid in my PC) I plan to cold storage HDDs with data backed up on a shelf.  See, I went cheap when I was first hoarding, due to the speed at which it was growing, and went 10tb externals for like $200 each.  They&amp;#39;re find enough, and after replacing my oldest at 39k hours, the next oldest has 18k hours, so I&amp;#39;ve got a bit of time.  My plan is to slowly purchase more 16tb, copy the data on the 10tb over to the 16tb, and then shelve the external HDDs as my backup.  Eventually I will buy more 16tb, but for now this will minimize potential losses.&lt;/p&gt;\n\n&lt;p&gt;Am I correct in understanding that this is generally ok, so long as I power them on occasionally?  And to this end, what&amp;#39;s a good policy for timing to do so?  Yearly?  Every other year?  Appreciate some more experienced insight.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "101TB and no sign of slowing down", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o4x9m", "is_robot_indexable": true, "report_reasons": null, "author": "TLunchFTW", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/13o4x9m/some_questions_on_when_to_replace_exos_hdds_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o4x9m/some_questions_on_when_to_replace_exos_hdds_and/", "subreddit_subscribers": 683782, "created_utc": 1684699799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.mobygames.com/](https://www.mobygames.com/)", "author_fullname": "t2_mv8vsqku", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How i can get not resized images from MobyGames site? Cover arts for example", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o3d09", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684695982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.mobygames.com/\"&gt;https://www.mobygames.com/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DpZkPbQuAMKIi_0hFGKiKXfpe8tFRWBBzBA5flqJYAU.jpg?auto=webp&amp;v=enabled&amp;s=80a8d2b78e6e526ca1f1f3421e1800374621f41e", "width": 679, "height": 312}, "resolutions": [{"url": "https://external-preview.redd.it/DpZkPbQuAMKIi_0hFGKiKXfpe8tFRWBBzBA5flqJYAU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=717fda024ed7c89c4050f46b534c3d8fe83be147", "width": 108, "height": 49}, {"url": "https://external-preview.redd.it/DpZkPbQuAMKIi_0hFGKiKXfpe8tFRWBBzBA5flqJYAU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5abf514eb87734ed906405db2aebfdbf2df42801", "width": 216, "height": 99}, {"url": "https://external-preview.redd.it/DpZkPbQuAMKIi_0hFGKiKXfpe8tFRWBBzBA5flqJYAU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9d39e497507a94d072e507b0d69d28ed8e4aaf9", "width": 320, "height": 147}, {"url": "https://external-preview.redd.it/DpZkPbQuAMKIi_0hFGKiKXfpe8tFRWBBzBA5flqJYAU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df717ace96bea53ea07830b09d60563c012a91c9", "width": 640, "height": 294}], "variants": {}, "id": "N4h_hzhJ0KplcJCeIIDxj1mvDCijiV3cHU3xJqFd1_c"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o3d09", "is_robot_indexable": true, "report_reasons": null, "author": "randomknight444", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o3d09/how_i_can_get_not_resized_images_from_mobygames/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o3d09/how_i_can_get_not_resized_images_from_mobygames/", "subreddit_subscribers": 683782, "created_utc": 1684695982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "posting so others might possibly benefit\n\nfor some time, i have been trying to get a working nitroflare premium account setup\n\nit has been really frustrating because (a) none of the debrid services seem to have consistently working nitroflare host support - i have paid for and tried many, and (b) US-based gift cards/etc don't seem to process correctly by nitroflare's payment processors (i've tried many times)\n\ni finally came up with the following - i still did not want to put my real CC# into their payment processors website, however, one of my CC issuers has an interface to create a virtual CC number. you can choose the expire date, daily dollar limit, and deactivate it early if you want. so i did this, set the daily dollar limit to 100, and immediately had a CC#, expire, cvv.  i used nitroflare's CCbill payment processor, bought 120 days for $45.50 USD.  the payment went through right away - note, I did get an email from my credit card company since i had alerts set for card not present on int'l transactions. it shows up on the statement as something like \"keep vpn\"\n\nwith a working nitroflare premium account finally, i used my pyload instance for bulk downloading. Pyload runs in a docker container so it was super easy to setup. In the pyload config, i added my premium nitroflare account and now i can feed pyload either a nitroflare folder URL or a list of nitroflare links (i use a short one-liner command-line to bulk extract all of these URLs from HTML files in various webpages that have stuff i want to snag)\n\nfeel free to ask any questions here in comments or DM me\n\nfwiw - the specific driver responsible for me going down this road was a website that contained a lot of direct download links (via nitroflare) for multitrack mogg audio files, which i am a bit of a collector of. Whoever posted the files to nitroflare has them all set as only being downloadable via premium accounts.  I'm also using pyload for bulk downloading some music-theory related youtube playlists that I want to archive.", "author_fullname": "t2_4p0tv0o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "my nitroflare + pyload setup", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13o1iev", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684693199.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684691593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;posting so others might possibly benefit&lt;/p&gt;\n\n&lt;p&gt;for some time, i have been trying to get a working nitroflare premium account setup&lt;/p&gt;\n\n&lt;p&gt;it has been really frustrating because (a) none of the debrid services seem to have consistently working nitroflare host support - i have paid for and tried many, and (b) US-based gift cards/etc don&amp;#39;t seem to process correctly by nitroflare&amp;#39;s payment processors (i&amp;#39;ve tried many times)&lt;/p&gt;\n\n&lt;p&gt;i finally came up with the following - i still did not want to put my real CC# into their payment processors website, however, one of my CC issuers has an interface to create a virtual CC number. you can choose the expire date, daily dollar limit, and deactivate it early if you want. so i did this, set the daily dollar limit to 100, and immediately had a CC#, expire, cvv.  i used nitroflare&amp;#39;s CCbill payment processor, bought 120 days for $45.50 USD.  the payment went through right away - note, I did get an email from my credit card company since i had alerts set for card not present on int&amp;#39;l transactions. it shows up on the statement as something like &amp;quot;keep vpn&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;with a working nitroflare premium account finally, i used my pyload instance for bulk downloading. Pyload runs in a docker container so it was super easy to setup. In the pyload config, i added my premium nitroflare account and now i can feed pyload either a nitroflare folder URL or a list of nitroflare links (i use a short one-liner command-line to bulk extract all of these URLs from HTML files in various webpages that have stuff i want to snag)&lt;/p&gt;\n\n&lt;p&gt;feel free to ask any questions here in comments or DM me&lt;/p&gt;\n\n&lt;p&gt;fwiw - the specific driver responsible for me going down this road was a website that contained a lot of direct download links (via nitroflare) for multitrack mogg audio files, which i am a bit of a collector of. Whoever posted the files to nitroflare has them all set as only being downloadable via premium accounts.  I&amp;#39;m also using pyload for bulk downloading some music-theory related youtube playlists that I want to archive.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13o1iev", "is_robot_indexable": true, "report_reasons": null, "author": "xr4cy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13o1iev/my_nitroflare_pyload_setup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13o1iev/my_nitroflare_pyload_setup/", "subreddit_subscribers": 683782, "created_utc": 1684691593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A week ago, I bought a WD blue 4tb hard drive to store videos, which I keep in my Sabrent hard drive enclosure. I copied about 500gb worth of videos onto it. The last day or two I've noticed a lot of the videos became unplayable and some of the folders I can't even get into anymore, just gives the message \"this filepath is not valid\". when I view the properties of the videos, they still show their full size. I ran chkdsk /f, which seemed to fix the folders, but not the videos themselves. Thankfully I didn't delete the videos from my main drive in case something like this went down, but I still don't know what the problem is. Is the drive bad or what? what should I do?\n\n&amp;#x200B;\n\nedit: I ran chkdsk /r which KIND of restored SOME of the videos... many of the ones that are playable are still corrupted (visual artifacts, audio scrambled)\n\namazon listing for hard drive [https://www.amazon.com/gp/product/B087QTVCHH](https://www.amazon.com/gp/product/B087QTVCHH)\n\nexternal enclosure [https://www.amazon.com/gp/product/B00LS5NFQ2](https://www.amazon.com/gp/product/B00LS5NFQ2)", "author_fullname": "t2_gdd14qwb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"corrupted\" files on external hard drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13ny2zy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684701635.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684683317.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A week ago, I bought a WD blue 4tb hard drive to store videos, which I keep in my Sabrent hard drive enclosure. I copied about 500gb worth of videos onto it. The last day or two I&amp;#39;ve noticed a lot of the videos became unplayable and some of the folders I can&amp;#39;t even get into anymore, just gives the message &amp;quot;this filepath is not valid&amp;quot;. when I view the properties of the videos, they still show their full size. I ran chkdsk /f, which seemed to fix the folders, but not the videos themselves. Thankfully I didn&amp;#39;t delete the videos from my main drive in case something like this went down, but I still don&amp;#39;t know what the problem is. Is the drive bad or what? what should I do?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;edit: I ran chkdsk /r which KIND of restored SOME of the videos... many of the ones that are playable are still corrupted (visual artifacts, audio scrambled)&lt;/p&gt;\n\n&lt;p&gt;amazon listing for hard drive &lt;a href=\"https://www.amazon.com/gp/product/B087QTVCHH\"&gt;https://www.amazon.com/gp/product/B087QTVCHH&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;external enclosure &lt;a href=\"https://www.amazon.com/gp/product/B00LS5NFQ2\"&gt;https://www.amazon.com/gp/product/B00LS5NFQ2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13ny2zy", "is_robot_indexable": true, "report_reasons": null, "author": "Diesel_199", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13ny2zy/corrupted_files_on_external_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13ny2zy/corrupted_files_on_external_hard_drive/", "subreddit_subscribers": 683782, "created_utc": 1684683317.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was looking at various backup solutions for my local home NAS. It is about \\~8TB large, running Ubuntu server.I would prefer to have regular backups, since I don't want to lose all that data. My plan is to back up to a local secondary server; an 8TB external HDD on a raspberry pi.\n\nNow, apparently there are a few different solutions, like BorgBackup, Urbackup, Duplicati, and rclone. Unfortunately, I am not clear on what the differences between these solutions are. Does this community have any recommendations for local backup?\n\nEdit: Some additional factors:\n\nI dont care about Amazon/Google/MS integrations. Don't use them\n\nThe software having the ability to remote sync to an offsite server, with encryption, would be good (though not a must)  \n\n\nOpen source software is a must  \n", "author_fullname": "t2_ny64x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Community recommended backup solutions for home NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nrs9v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684672831.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684672177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was looking at various backup solutions for my local home NAS. It is about ~8TB large, running Ubuntu server.I would prefer to have regular backups, since I don&amp;#39;t want to lose all that data. My plan is to back up to a local secondary server; an 8TB external HDD on a raspberry pi.&lt;/p&gt;\n\n&lt;p&gt;Now, apparently there are a few different solutions, like BorgBackup, Urbackup, Duplicati, and rclone. Unfortunately, I am not clear on what the differences between these solutions are. Does this community have any recommendations for local backup?&lt;/p&gt;\n\n&lt;p&gt;Edit: Some additional factors:&lt;/p&gt;\n\n&lt;p&gt;I dont care about Amazon/Google/MS integrations. Don&amp;#39;t use them&lt;/p&gt;\n\n&lt;p&gt;The software having the ability to remote sync to an offsite server, with encryption, would be good (though not a must)  &lt;/p&gt;\n\n&lt;p&gt;Open source software is a must  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13nrs9v", "is_robot_indexable": true, "report_reasons": null, "author": "cfs3corsair", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13nrs9v/community_recommended_backup_solutions_for_home/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13nrs9v/community_recommended_backup_solutions_for_home/", "subreddit_subscribers": 683782, "created_utc": 1684672177.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}