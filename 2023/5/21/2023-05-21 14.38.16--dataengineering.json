{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_31ilk7t8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Missing Manual: Everything you need to know about Snowflake optimization", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_13mzqi0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vOJ2P8ii1Q27tTHHi-uP2v12Jt8nuIapf6UTSoc4wgU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1684600863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "select.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://select.dev/posts/data-council-2023", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?auto=webp&amp;v=enabled&amp;s=f44ab13b30516636449503abbe13b5491659de76", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57e178e36418528083e9cccf93b7b7536725106b", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ff884a3de09942e334816090be5175a109f298b", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9bde60471daa25d8a95b111a70976f82c83099e5", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e36b8f313fd5fd2d94e48ee4eaaad9dc670d908", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99a086e7d4f20be8cb61319a29daa1fbdf17a498", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/ZIBcKjWCfR_T4lHecgk4Ae37KAlhGeI6nK-IF77nMq4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e668351dbd1876d1c22dd1ea99ac39c518329ba3", "width": 1080, "height": 607}], "variants": {}, "id": "3cvzgn3lBI59zBR6hi-N7QXE5Uy9SODtfe-dJlJOmEI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13mzqi0", "is_robot_indexable": true, "report_reasons": null, "author": "ian-whitestone", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13mzqi0/the_missing_manual_everything_you_need_to_know/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://select.dev/posts/data-council-2023", "subreddit_subscribers": 106521, "created_utc": 1684600863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Share your best practices for designing robust and adaptable data pipelines that consistently perform well and can handle future changes effectively. \n\nEspecially while working in environments where data failures are expensive and incorrect data has massive negative impacts.\n\n\nWhat are the design principles you've found helpful, what about the QA , versioning &amp; change management. How do you document everything above! \ud83d\udc68\u200d\ud83d\udcbb", "author_fullname": "t2_8onr2vji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which best practices do you follow to build robust &amp; extensible ETL jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nn9j7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684657701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Share your best practices for designing robust and adaptable data pipelines that consistently perform well and can handle future changes effectively. &lt;/p&gt;\n\n&lt;p&gt;Especially while working in environments where data failures are expensive and incorrect data has massive negative impacts.&lt;/p&gt;\n\n&lt;p&gt;What are the design principles you&amp;#39;ve found helpful, what about the QA , versioning &amp;amp; change management. How do you document everything above! \ud83d\udc68\u200d\ud83d\udcbb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nn9j7", "is_robot_indexable": true, "report_reasons": null, "author": "kryon-X", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nn9j7/which_best_practices_do_you_follow_to_build/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nn9j7/which_best_practices_do_you_follow_to_build/", "subreddit_subscribers": 106521, "created_utc": 1684657701.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just curious how is everyone using spark to load fact tables in s3. \n\nIn my environment,  there is no delta, so I believe all my coworkers is just using parquet. We just partition it by data source system and date. Then use dynamic partition overwrite to load the tables by partitions.\n\nHowever,  I feel like this inefficient since it can lead to lots of small files and spark is painfully slow with that.\n\nJust curious what is everyone else doing with it.", "author_fullname": "t2_c3yqm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Pattern to Incrementally update fact tables in s3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n4btc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684611586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just curious how is everyone using spark to load fact tables in s3. &lt;/p&gt;\n\n&lt;p&gt;In my environment,  there is no delta, so I believe all my coworkers is just using parquet. We just partition it by data source system and date. Then use dynamic partition overwrite to load the tables by partitions.&lt;/p&gt;\n\n&lt;p&gt;However,  I feel like this inefficient since it can lead to lots of small files and spark is painfully slow with that.&lt;/p&gt;\n\n&lt;p&gt;Just curious what is everyone else doing with it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13n4btc", "is_robot_indexable": true, "report_reasons": null, "author": "543254447", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n4btc/spark_pattern_to_incrementally_update_fact_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n4btc/spark_pattern_to_incrementally_update_fact_tables/", "subreddit_subscribers": 106521, "created_utc": 1684611586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm asking as I'm sure this has been solved a million times before, just not by me :) \n\nSuppose I have a GCP cloud function with memory set to 512MB or whatever (this value doesn't really matter) and all it does it read CSV files from `bucket_raw` and write them to `bucket_processed` as `.parquet` files. \n\nFor CSV files which are 20MB or so that's fine, but if I have a CSV that's 513MB (_or _whatever_ size necessary to make the cloud function run out of memory_) a different approach is needed. \n\nI can read from blob storage in chunks using pandas, so \n```\npd.read_csv(f\"gs://{event['bucket']}/{event['name']}\", chunksize=&lt;chunk_size&gt;, iterator=True)\n``` \nwill read in chunks, and I could write each chunk out into `bucket_processed`  looping over that.\n\nSo for example - I might have an initial file inserted: \n```\ngs://bucket_raw/some_file.csv\n```\nWhich triggers the cloud function, and after processing it using the chunks from `read_csv` I could have something like: \n```\ngs://bucket_processed/some_file__0.csv\ngs://bucket_processed/some_file__1.csv\ngs://bucket_processed/some_file__2.csv\ngs://bucket_processed/some_file__3.csv\ngs://bucket_processed/some_file__4.csv\ngs://bucket_processed/some_file__5.csv\n```\nRather than \n```\ngs://bucket_processed/some_file.csv\n```\n\nThis approach (naming with a suffix `_&lt;loop&gt;`) feels a bit clunky, and given I've not had to do much of this I'm sure there's something I'm missing. \n\nIt would be great to get some views on how to better handle this, thanks", "author_fullname": "t2_tczfts4v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to process data larger than memory using a GCP python cloud function (csv -&gt; parquet)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n2f63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684607498.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m asking as I&amp;#39;m sure this has been solved a million times before, just not by me :) &lt;/p&gt;\n\n&lt;p&gt;Suppose I have a GCP cloud function with memory set to 512MB or whatever (this value doesn&amp;#39;t really matter) and all it does it read CSV files from &lt;code&gt;bucket_raw&lt;/code&gt; and write them to &lt;code&gt;bucket_processed&lt;/code&gt; as &lt;code&gt;.parquet&lt;/code&gt; files. &lt;/p&gt;\n\n&lt;p&gt;For CSV files which are 20MB or so that&amp;#39;s fine, but if I have a CSV that&amp;#39;s 513MB (&lt;em&gt;or _whatever&lt;/em&gt; size necessary to make the cloud function run out of memory_) a different approach is needed. &lt;/p&gt;\n\n&lt;p&gt;I can read from blob storage in chunks using pandas, so \n&lt;code&gt;\npd.read_csv(f&amp;quot;gs://{event[&amp;#39;bucket&amp;#39;]}/{event[&amp;#39;name&amp;#39;]}&amp;quot;, chunksize=&amp;lt;chunk_size&amp;gt;, iterator=True)\n&lt;/code&gt; \nwill read in chunks, and I could write each chunk out into &lt;code&gt;bucket_processed&lt;/code&gt;  looping over that.&lt;/p&gt;\n\n&lt;p&gt;So for example - I might have an initial file inserted: \n&lt;code&gt;\ngs://bucket_raw/some_file.csv\n&lt;/code&gt;\nWhich triggers the cloud function, and after processing it using the chunks from &lt;code&gt;read_csv&lt;/code&gt; I could have something like: \n&lt;code&gt;\ngs://bucket_processed/some_file__0.csv\ngs://bucket_processed/some_file__1.csv\ngs://bucket_processed/some_file__2.csv\ngs://bucket_processed/some_file__3.csv\ngs://bucket_processed/some_file__4.csv\ngs://bucket_processed/some_file__5.csv\n&lt;/code&gt;\nRather than \n&lt;code&gt;\ngs://bucket_processed/some_file.csv\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This approach (naming with a suffix &lt;code&gt;_&amp;lt;loop&amp;gt;&lt;/code&gt;) feels a bit clunky, and given I&amp;#39;ve not had to do much of this I&amp;#39;m sure there&amp;#39;s something I&amp;#39;m missing. &lt;/p&gt;\n\n&lt;p&gt;It would be great to get some views on how to better handle this, thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13n2f63", "is_robot_indexable": true, "report_reasons": null, "author": "Subject_Fix2471", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n2f63/how_to_process_data_larger_than_memory_using_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n2f63/how_to_process_data_larger_than_memory_using_a/", "subreddit_subscribers": 106521, "created_utc": 1684607498.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, what are some of the best practices for logging data pipelines especially exceptions? Are there any design or code I can refer to?", "author_fullname": "t2_6zaja793", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Logging pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nh93s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684638651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, what are some of the best practices for logging data pipelines especially exceptions? Are there any design or code I can refer to?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nh93s", "is_robot_indexable": true, "report_reasons": null, "author": "Ambitious_Cucumber96", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nh93s/logging_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nh93s/logging_pipelines/", "subreddit_subscribers": 106521, "created_utc": 1684638651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Bit inexperienced, so unsure exactly how to design something like this.\n\nSay, for example, you're extracting from a Sales table within an operational database once a day at midnight. You want to utilise an incremental approach, so you're only pulling data that has been updated, deleted, or inserted since last execution date of pipeline and storing that in a raw storage area. You're then running a series of transformations on this raw data and loading into a target table.\n\nI have a few questions, which hopefully make sense:\n\n1. For an incremental approach, is one way of achieving this to (somehow) identify records that have been updated or inserted since the last execution date of your pipeline, delete the corresponding records in downstream tables/storage, and then insert the new records.\n2. Also, I can see how we would detect records that have been newly inserted or updated, but how would you detect deletions in the source system, and ensure these are removed or flagged as deleted in the target system?\n3. Finally, what considerations need to be given to backfilling? If our logic was to simply detect changes that happened since the last run of the pipeline and only process that, would it make sense to implement additional logic that would only look at changes that happened a day before the execution date (in our example). So for example, we detect an issue with out pipeline and want to re-run all our DAGs from the past week. The first DAG that runs here would have an execution date of 14/05/2023. So our logic could be that this particular DAG will only extract and transform records that have been newly inserted or updated between the dates of 13/05/2023 Midnight and 14/05/2023 Midnight. Again, not sure how exactly we would detect when these changes happen in a database, but just want to check if this is a common approach.", "author_fullname": "t2_osh2xtjb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there common approaches to designing a pipeline with incremental loading, which also allows for backfilling, idempotence, and handles source deletions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n60lq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684613223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bit inexperienced, so unsure exactly how to design something like this.&lt;/p&gt;\n\n&lt;p&gt;Say, for example, you&amp;#39;re extracting from a Sales table within an operational database once a day at midnight. You want to utilise an incremental approach, so you&amp;#39;re only pulling data that has been updated, deleted, or inserted since last execution date of pipeline and storing that in a raw storage area. You&amp;#39;re then running a series of transformations on this raw data and loading into a target table.&lt;/p&gt;\n\n&lt;p&gt;I have a few questions, which hopefully make sense:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For an incremental approach, is one way of achieving this to (somehow) identify records that have been updated or inserted since the last execution date of your pipeline, delete the corresponding records in downstream tables/storage, and then insert the new records.&lt;/li&gt;\n&lt;li&gt;Also, I can see how we would detect records that have been newly inserted or updated, but how would you detect deletions in the source system, and ensure these are removed or flagged as deleted in the target system?&lt;/li&gt;\n&lt;li&gt;Finally, what considerations need to be given to backfilling? If our logic was to simply detect changes that happened since the last run of the pipeline and only process that, would it make sense to implement additional logic that would only look at changes that happened a day before the execution date (in our example). So for example, we detect an issue with out pipeline and want to re-run all our DAGs from the past week. The first DAG that runs here would have an execution date of 14/05/2023. So our logic could be that this particular DAG will only extract and transform records that have been newly inserted or updated between the dates of 13/05/2023 Midnight and 14/05/2023 Midnight. Again, not sure how exactly we would detect when these changes happen in a database, but just want to check if this is a common approach.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13n60lq", "is_robot_indexable": true, "report_reasons": null, "author": "TheDataPanda", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n60lq/are_there_common_approaches_to_designing_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n60lq/are_there_common_approaches_to_designing_a/", "subreddit_subscribers": 106521, "created_utc": 1684613223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, I\u2019ve been thinking about going solo and starting a Data Engineering company and I was wondering how many people here have done it and are currently doing well or tried and failed? \n\nIf there\u2019s any place in the market for a company like this that would offer DB migrations, Pipeline creations, DB Administration,etc for other businesses with maintenance contracts or if most of the work would be contract/freelance work of companies just needing a an extra person on staff?\n\nAny advice you can give is appreciated! Thank you", "author_fullname": "t2_xejw1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering as a business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n0ta8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684603555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, I\u2019ve been thinking about going solo and starting a Data Engineering company and I was wondering how many people here have done it and are currently doing well or tried and failed? &lt;/p&gt;\n\n&lt;p&gt;If there\u2019s any place in the market for a company like this that would offer DB migrations, Pipeline creations, DB Administration,etc for other businesses with maintenance contracts or if most of the work would be contract/freelance work of companies just needing a an extra person on staff?&lt;/p&gt;\n\n&lt;p&gt;Any advice you can give is appreciated! Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13n0ta8", "is_robot_indexable": true, "report_reasons": null, "author": "angelnator1998", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n0ta8/data_engineering_as_a_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n0ta8/data_engineering_as_a_business/", "subreddit_subscribers": 106521, "created_utc": 1684603555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I thought the community might be interested in a recent update to [Substation](https://github.com/brexhq/substation) that makes setting up change data capture (CDC) on AWS DynamoDB easy. Features include:\n\n* All DynamoDB tables have CDC enabled by default\n* Runs 100% on AWS serverless (pay per use, little-to-no maintenance)\n* Generates Debezium-compatible CDC events (create, update, delete actions)\n* Use cases include database observability,  database replication, and real-time notifications for other services\n\nIf you're interested, then you can [read more here](https://substation.readme.io/docs/change-data-capture-cdc)!", "author_fullname": "t2_kuumsieq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple Change Data Capture (CDC) using AWS DynamoDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mwy8d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684594901.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684594682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I thought the community might be interested in a recent update to &lt;a href=\"https://github.com/brexhq/substation\"&gt;Substation&lt;/a&gt; that makes setting up change data capture (CDC) on AWS DynamoDB easy. Features include:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;All DynamoDB tables have CDC enabled by default&lt;/li&gt;\n&lt;li&gt;Runs 100% on AWS serverless (pay per use, little-to-no maintenance)&lt;/li&gt;\n&lt;li&gt;Generates Debezium-compatible CDC events (create, update, delete actions)&lt;/li&gt;\n&lt;li&gt;Use cases include database observability,  database replication, and real-time notifications for other services&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you&amp;#39;re interested, then you can &lt;a href=\"https://substation.readme.io/docs/change-data-capture-cdc\"&gt;read more here&lt;/a&gt;!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?auto=webp&amp;v=enabled&amp;s=5768c522562e252ddff12c04c15373c0ef5f890f", "width": 640, "height": 320}, "resolutions": [{"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2b3d76a94d7f0c93e92dc31221b3289117f534b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48f3763c62890067459b10bcc7ddf538649fb75e", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=71945e8654ebfcd208775c1530abcf74bc666ffb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/8Zw2s3HB35q64h3kHf-7EjhHWqFlZGa2rb3NokdMzlM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9cb89bb411f11d2dc2ea98616f242941525d0bc9", "width": 640, "height": 320}], "variants": {}, "id": "xv1e5O9fm8KLjRaTRRPYd9oJCEFoBjTun2Ww3C8XHpQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "13mwy8d", "is_robot_indexable": true, "report_reasons": null, "author": "jshlbrd-brex", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13mwy8d/simple_change_data_capture_cdc_using_aws_dynamodb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13mwy8d/simple_change_data_capture_cdc_using_aws_dynamodb/", "subreddit_subscribers": 106521, "created_utc": 1684594682.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have multiple delta lake tables refreshing near real time with last_updated date coming from source tables and only having incremental data loaded and from these tables I am creating a joined delta lake table after applying some calculations.\n\nSo far I have been using merge using the entire tables, which is not ideal. I would like to change it to load only incremental load using upsert and run it every 5 mins. What is the best way to approach this. \nShould I leverage last_job_end_ts and last_updated or explore auto loader or any other way to achieve this?", "author_fullname": "t2_wkq4zhf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental load for multiple joined tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13nuoim", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684676227.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have multiple delta lake tables refreshing near real time with last_updated date coming from source tables and only having incremental data loaded and from these tables I am creating a joined delta lake table after applying some calculations.&lt;/p&gt;\n\n&lt;p&gt;So far I have been using merge using the entire tables, which is not ideal. I would like to change it to load only incremental load using upsert and run it every 5 mins. What is the best way to approach this. \nShould I leverage last_job_end_ts and last_updated or explore auto loader or any other way to achieve this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13nuoim", "is_robot_indexable": true, "report_reasons": null, "author": "the_aris", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nuoim/incremental_load_for_multiple_joined_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nuoim/incremental_load_for_multiple_joined_tables/", "subreddit_subscribers": 106521, "created_utc": 1684676227.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys.\nCan you please suggest a few delta table optimization startergies?\n\nI've used a few like\n1. Vacuum\n2. Optimize \n3. Z ordering \n4. Partitioning \n\nWould love to learn about more possible optimization strategies.", "author_fullname": "t2_2v7ell6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Table Optimization Methods", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nmpfj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684655845.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys.\nCan you please suggest a few delta table optimization startergies?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve used a few like\n1. Vacuum\n2. Optimize \n3. Z ordering \n4. Partitioning &lt;/p&gt;\n\n&lt;p&gt;Would love to learn about more possible optimization strategies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13nmpfj", "is_robot_indexable": true, "report_reasons": null, "author": "RithwikChhugani", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nmpfj/delta_table_optimization_methods/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nmpfj/delta_table_optimization_methods/", "subreddit_subscribers": 106521, "created_utc": 1684655845.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\nSomeone knows a visualization tool with OR &amp; AND slicers?\nI can\u2019t use the DAX solution because the customer want to use the slicers like:\nwhere ID = 1 and customer like \u201cabc\u201d or customer like \u201cdef\u201d\n\nIs there any solution for this?", "author_fullname": "t2_aabuvwgh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualization tool with OR &amp; AND slicers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nknre", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684649147.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,\nSomeone knows a visualization tool with OR &amp;amp; AND slicers?\nI can\u2019t use the DAX solution because the customer want to use the slicers like:\nwhere ID = 1 and customer like \u201cabc\u201d or customer like \u201cdef\u201d&lt;/p&gt;\n\n&lt;p&gt;Is there any solution for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13nknre", "is_robot_indexable": true, "report_reasons": null, "author": "ImplementKind8414", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nknre/visualization_tool_with_or_and_slicers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nknre/visualization_tool_with_or_and_slicers/", "subreddit_subscribers": 106521, "created_utc": 1684649147.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi there!\n\nI'm having some trouble developing a Shiny dashboard for a client for whom I have no access to its data in a form of a DB connection, but I need to be able to stream it any time I needed it.\n\nDue to the industry, I know which columns I need in order to generate triggers, statics reports (done through Python and/or R), and dynamic ones.\n\nDo you know what are the possibilities of streaming data from a client to me?\n\nThanks", "author_fullname": "t2_3jpw1rxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting data from a client", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13nryyn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684672697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having some trouble developing a Shiny dashboard for a client for whom I have no access to its data in a form of a DB connection, but I need to be able to stream it any time I needed it.&lt;/p&gt;\n\n&lt;p&gt;Due to the industry, I know which columns I need in order to generate triggers, statics reports (done through Python and/or R), and dynamic ones.&lt;/p&gt;\n\n&lt;p&gt;Do you know what are the possibilities of streaming data from a client to me?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "13nryyn", "is_robot_indexable": true, "report_reasons": null, "author": "eviljia", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nryyn/getting_data_from_a_client/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nryyn/getting_data_from_a_client/", "subreddit_subscribers": 106521, "created_utc": 1684672697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to understand the thought process behind not having a valid to data field in a Data Vault Satellite. \n\nIt seems that given you generally have multiple satellites off a hub, you\u2019re going to be building a point in time table. A PIT is going to need either a valid to date in the satellites, else a LEAD window function on each satellite each time it loads. Surely the former is more performant?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No valid to data in DV2.0 Satellites. Why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13nnujn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684659656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to understand the thought process behind not having a valid to data field in a Data Vault Satellite. &lt;/p&gt;\n\n&lt;p&gt;It seems that given you generally have multiple satellites off a hub, you\u2019re going to be building a point in time table. A PIT is going to need either a valid to date in the satellites, else a LEAD window function on each satellite each time it loads. Surely the former is more performant?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13nnujn", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13nnujn/no_valid_to_data_in_dv20_satellites_why/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13nnujn/no_valid_to_data_in_dv20_satellites_why/", "subreddit_subscribers": 106521, "created_utc": 1684659656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Community, I'm tasked with extracting individual tables from a pdf as seen in the pdf screenshot, what is the best way to do it .?\n\nhttps://preview.redd.it/55fgklwpn31b1.jpg?width=908&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7ac5a475e24b00a1ba4025604b800759bb5790ba\n\nI want each table to be on a separate page ( it can be again written as a pdf file ). Have anyone worked on this before I would really appreciate any light on how I can achieve this", "author_fullname": "t2_6psngyc6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help to extract numerous tables from pdf.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "media_metadata": {"55fgklwpn31b1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 71, "x": 108, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10e435e2451d32a3158580ad8743a51a363630e5"}, {"y": 143, "x": 216, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3f8c1b56fa6c5066d984bd9db6dd18ca3ebf7c5d"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85c1be8dbebae82ee7f47530f61c4bc8dd6b1307"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6120a86f0945037dc94c2a03eafc8724736dbeae"}], "s": {"y": 605, "x": 908, "u": "https://preview.redd.it/55fgklwpn31b1.jpg?width=908&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7ac5a475e24b00a1ba4025604b800759bb5790ba"}, "id": "55fgklwpn31b1"}}, "name": "t3_13ngynt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/OuywNvamZrydEgnKYycxF8TpH_9opuxVrEduMrXz5bE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684637833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Community, I&amp;#39;m tasked with extracting individual tables from a pdf as seen in the pdf screenshot, what is the best way to do it .?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/55fgklwpn31b1.jpg?width=908&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7ac5a475e24b00a1ba4025604b800759bb5790ba\"&gt;https://preview.redd.it/55fgklwpn31b1.jpg?width=908&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7ac5a475e24b00a1ba4025604b800759bb5790ba&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I want each table to be on a separate page ( it can be again written as a pdf file ). Have anyone worked on this before I would really appreciate any light on how I can achieve this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13ngynt", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo_51799", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13ngynt/need_help_to_extract_numerous_tables_from_pdf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13ngynt/need_help_to_extract_numerous_tables_from_pdf/", "subreddit_subscribers": 106521, "created_utc": 1684637833.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks,\n\nI've recently joined a e-commere SaaS company that provides services for around 20k direct clients and around 100x indirect clients, and that has a weird data stack with tons of technical debt (absence of proper data modeling, broken pipelines, teams working in silos, bad usage of dbt, etc.).\n\nTo summarize things:  \n\n\n* We ingest some transactional data coming from MySQL, that's mainly composed by customer purchases and other interactions with the website. We also ingest some dimension tables coming from this MySQL.\n* We ingest a lot of transactional data (web events) coming directly from nginx and going to Clickhouse through Kafka, to be transformed later and ingested into a PostgreSQL (yes, we use it as DWH).\n* We ingest data coming from diferent apps (Hubspot, Odoo, etc.) into PostgreSQL, using NiFi.\n\nFor now, I've been thinking on using self-hosted Rudderstack both for ingestion and reverse ETL, [cube.dev](https://cube.dev) as the abstraction later for building webapps and providing catching for the BI layer, and dbt for transformations. But I have doubts with the following elements:  \n\n\n* Should I pick a single DWH, like Redshift, Snowflake, etc., or would it be better to go for a combination of solutions? My point is that our current bills coming from AWS are high, and even considering scaling capabilities, I'm tempted of storing some data in S3, some other in a 'normal' OLAP DWH, and using something like TimescaleDB / Clickhouse just for event analysis or CDS.\n\nThanks in advance!", "author_fullname": "t2_11ud20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice / Questions on Modern Data Stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13na68z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1684619361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently joined a e-commere SaaS company that provides services for around 20k direct clients and around 100x indirect clients, and that has a weird data stack with tons of technical debt (absence of proper data modeling, broken pipelines, teams working in silos, bad usage of dbt, etc.).&lt;/p&gt;\n\n&lt;p&gt;To summarize things:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;We ingest some transactional data coming from MySQL, that&amp;#39;s mainly composed by customer purchases and other interactions with the website. We also ingest some dimension tables coming from this MySQL.&lt;/li&gt;\n&lt;li&gt;We ingest a lot of transactional data (web events) coming directly from nginx and going to Clickhouse through Kafka, to be transformed later and ingested into a PostgreSQL (yes, we use it as DWH).&lt;/li&gt;\n&lt;li&gt;We ingest data coming from diferent apps (Hubspot, Odoo, etc.) into PostgreSQL, using NiFi.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For now, I&amp;#39;ve been thinking on using self-hosted Rudderstack both for ingestion and reverse ETL, &lt;a href=\"https://cube.dev\"&gt;cube.dev&lt;/a&gt; as the abstraction later for building webapps and providing catching for the BI layer, and dbt for transformations. But I have doubts with the following elements:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Should I pick a single DWH, like Redshift, Snowflake, etc., or would it be better to go for a combination of solutions? My point is that our current bills coming from AWS are high, and even considering scaling capabilities, I&amp;#39;m tempted of storing some data in S3, some other in a &amp;#39;normal&amp;#39; OLAP DWH, and using something like TimescaleDB / Clickhouse just for event analysis or CDS.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?auto=webp&amp;v=enabled&amp;s=4c2ee9ced32cf7f44c9acfadaf0fc6138d934235", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39fc2899acfe1fd7fec2ad6a9c6a16ed630cc31d", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb8a7f81c9f0c3327863c615505b600bdd32ead4", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0747c7e295fa50f4a169ff2c77b4e38dfe3c70c5", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44ddb5bc88a05e68b02981b71d6f63b8130ecb7a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd7a3df58c7294394cdfee68c359fedcde4abc6f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IkJBfNtzRzAGnHrD5WSLeOv32EOc9aOFFmrkE780nb8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=450ad0a42363c74fd12f5edf265b485734b70be3", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13na68z", "is_robot_indexable": true, "report_reasons": null, "author": "putokaos", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13na68z/advice_questions_on_modern_data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13na68z/advice_questions_on_modern_data_stack/", "subreddit_subscribers": 106521, "created_utc": 1684619361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On our client-facing UI, we have a few dashboards filled with normal analytics you'd expect: line/bar charts, data-tables, etc. Most of these we want to serve in a timeseries fashion e.g. for a given timeline (from/to dates or all time) we display data (bucketed per day/week/...). We also have some data tables which display an enriched view for some of these counters.\n\nRecently, we've been getting a lot of traffic so our analytics events table is growing quite rapidly. We expect this traffic to go up considerably in the next months. We're worried that our loading times will become worse as it can already take 10+ seconds for some of the queries (especially all-time).\n\nFor some context, our backend follows a semi \"event-driven\" paradigm. For instance, assume the following setup:\n\n1. Our clients can create different \"graphs\" which their users can enter/leave\n2. Each graph node can \"convert\" a user\n3. We persits \"events\" in our DB e.g. UserEnteredGraph, UserEnteredGraphNode, UserConverted (scoped appropriately per client, graph, etc)\n\nSo for the following rules, we want to export analytics. Note, all of these are scoped per client:\n\n1. How many unique users have entered a graph (sum across all graphs) per day/month/all-time\n2. How many unique users have entered a specific graph (per graph) per day/month/all-time\n3. How many unique users have reached a graph node (per all nodes in a graph) per day/month/all-time\n4. How many unique users have \"converted\" a graph node (per all nodes in a graph) ...\n5. Some data-tables which group users, say per \"Country\". For instance a table that shows top 10 countries with most user conversions. Here, we infer the information using the \\`analytics\\_events\\` table (count conversions) joined with our \\`users\\` table (enrich the country)\n\nAs you can see, we want to display these in a timeseries fashion (for a given timerange, bucketed per day/week/...). We initially decided to persist all of these events into a singular postgres table \\`analytics\\_events\\` which basically has:\n\n* `event_type` (UserEnteredGraph, UserConverted..)\n* `payload` JSONB column that stores any event specific data (which we use to query through a gin-index). Say for a \\`UserConverted\\` the payload column will contain the \\`nodeId\\` of the graph.\n* `inserted_at` timestamp\n\nTo serve our UI with timeseries (say 1day bucketed info for a from/to timeframe). To do so, we do some SQL acrobatics, including a `CROSS JOIN` with a `generate_series(?, ?, INTERVAL '1 day')`. Further, for some of the queries we use a gin-index to access data from the \\`payload\\` column.\n\nTo me, serving analytics like this seems like a solved problem. I suspect we're severely over-complicating and that instead of reinventing the wheel we can use a 3rd party to do all the heavy lifting.  I had a look at Redis time-series but it doesn't feel flexible enough for our data (query per client/graph/node/etc). Timescaledb was also another solution I had a look at, but unfortunately RDS does not support it.  What stack/tools/architectures would you recommend for this?", "author_fullname": "t2_xk16u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to store and serve client data analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13n93el", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684616993.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684616692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On our client-facing UI, we have a few dashboards filled with normal analytics you&amp;#39;d expect: line/bar charts, data-tables, etc. Most of these we want to serve in a timeseries fashion e.g. for a given timeline (from/to dates or all time) we display data (bucketed per day/week/...). We also have some data tables which display an enriched view for some of these counters.&lt;/p&gt;\n\n&lt;p&gt;Recently, we&amp;#39;ve been getting a lot of traffic so our analytics events table is growing quite rapidly. We expect this traffic to go up considerably in the next months. We&amp;#39;re worried that our loading times will become worse as it can already take 10+ seconds for some of the queries (especially all-time).&lt;/p&gt;\n\n&lt;p&gt;For some context, our backend follows a semi &amp;quot;event-driven&amp;quot; paradigm. For instance, assume the following setup:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Our clients can create different &amp;quot;graphs&amp;quot; which their users can enter/leave&lt;/li&gt;\n&lt;li&gt;Each graph node can &amp;quot;convert&amp;quot; a user&lt;/li&gt;\n&lt;li&gt;We persits &amp;quot;events&amp;quot; in our DB e.g. UserEnteredGraph, UserEnteredGraphNode, UserConverted (scoped appropriately per client, graph, etc)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So for the following rules, we want to export analytics. Note, all of these are scoped per client:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;How many unique users have entered a graph (sum across all graphs) per day/month/all-time&lt;/li&gt;\n&lt;li&gt;How many unique users have entered a specific graph (per graph) per day/month/all-time&lt;/li&gt;\n&lt;li&gt;How many unique users have reached a graph node (per all nodes in a graph) per day/month/all-time&lt;/li&gt;\n&lt;li&gt;How many unique users have &amp;quot;converted&amp;quot; a graph node (per all nodes in a graph) ...&lt;/li&gt;\n&lt;li&gt;Some data-tables which group users, say per &amp;quot;Country&amp;quot;. For instance a table that shows top 10 countries with most user conversions. Here, we infer the information using the `analytics_events` table (count conversions) joined with our `users` table (enrich the country)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;As you can see, we want to display these in a timeseries fashion (for a given timerange, bucketed per day/week/...). We initially decided to persist all of these events into a singular postgres table `analytics_events` which basically has:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;event_type&lt;/code&gt; (UserEnteredGraph, UserConverted..)&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;payload&lt;/code&gt; JSONB column that stores any event specific data (which we use to query through a gin-index). Say for a `UserConverted` the payload column will contain the `nodeId` of the graph.&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;inserted_at&lt;/code&gt; timestamp&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To serve our UI with timeseries (say 1day bucketed info for a from/to timeframe). To do so, we do some SQL acrobatics, including a &lt;code&gt;CROSS JOIN&lt;/code&gt; with a &lt;code&gt;generate_series(?, ?, INTERVAL &amp;#39;1 day&amp;#39;)&lt;/code&gt;. Further, for some of the queries we use a gin-index to access data from the `payload` column.&lt;/p&gt;\n\n&lt;p&gt;To me, serving analytics like this seems like a solved problem. I suspect we&amp;#39;re severely over-complicating and that instead of reinventing the wheel we can use a 3rd party to do all the heavy lifting.  I had a look at Redis time-series but it doesn&amp;#39;t feel flexible enough for our data (query per client/graph/node/etc). Timescaledb was also another solution I had a look at, but unfortunately RDS does not support it.  What stack/tools/architectures would you recommend for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13n93el", "is_robot_indexable": true, "report_reasons": null, "author": "TheWalkingFridge", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13n93el/how_to_store_and_serve_client_data_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13n93el/how_to_store_and_serve_client_data_analytics/", "subreddit_subscribers": 106521, "created_utc": 1684616692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm stuck in a new team full of SAP analysts and I've tried every possible way to explain what DE is and they think that DE is ML/Data Science. \n\nWhenever i try to explain to them the intricacies of ETL they behave like it's nothing significant to the business n just a tech spin off like ML or some short life spanned tech trend &amp; tells me that SAP does this better. They don't even understand how fast spark is when compared to their Hana.\n \nSome of these sap folks are too dumb i feel that they don't even know the importance of DE in 2023.\n\nHow can I help them understand what DE is and the value it adds to the business? \n\nI wanted to give them a 100ton heavy reply next time they daunt me.\n\n(My stack is databricks, azure and SQL!)", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to explain DE to a bunch of SAP boomers!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13mzoka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.39, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1684614111.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1684600739.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m stuck in a new team full of SAP analysts and I&amp;#39;ve tried every possible way to explain what DE is and they think that DE is ML/Data Science. &lt;/p&gt;\n\n&lt;p&gt;Whenever i try to explain to them the intricacies of ETL they behave like it&amp;#39;s nothing significant to the business n just a tech spin off like ML or some short life spanned tech trend &amp;amp; tells me that SAP does this better. They don&amp;#39;t even understand how fast spark is when compared to their Hana.&lt;/p&gt;\n\n&lt;p&gt;Some of these sap folks are too dumb i feel that they don&amp;#39;t even know the importance of DE in 2023.&lt;/p&gt;\n\n&lt;p&gt;How can I help them understand what DE is and the value it adds to the business? &lt;/p&gt;\n\n&lt;p&gt;I wanted to give them a 100ton heavy reply next time they daunt me.&lt;/p&gt;\n\n&lt;p&gt;(My stack is databricks, azure and SQL!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "13mzoka", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13mzoka/how_to_explain_de_to_a_bunch_of_sap_boomers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/13mzoka/how_to_explain_de_to_a_bunch_of_sap_boomers/", "subreddit_subscribers": 106521, "created_utc": 1684600739.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}