{"kind": "Listing", "data": {"after": "t3_17rs5t8", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Western Digital Won\u2019t Make SSDs Anymore", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17rekge", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 212, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 212, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/VCvibPAbgf6RGKdxE43x0TXrf8AScS3SuxTyjvhuzfk.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699541256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "howtogeek.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.howtogeek.com/western-digital-company-split-news/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/caBqF8kHPzevxEv-QCHsAdEAK2hCmd6xnHqzbq-78E8.jpg?auto=webp&amp;s=0c9fec8d2bd3bcac1ce312e13597849e9c3e5909", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/caBqF8kHPzevxEv-QCHsAdEAK2hCmd6xnHqzbq-78E8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7bacab30d9573e7df97bccfc4bf3be98c6e3fa87", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/caBqF8kHPzevxEv-QCHsAdEAK2hCmd6xnHqzbq-78E8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c64617108b368761a6d3824a0c08bb9e033634c7", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/caBqF8kHPzevxEv-QCHsAdEAK2hCmd6xnHqzbq-78E8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d8102d6cfdb34c9e3e1e2f7d518b548ab597029c", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/caBqF8kHPzevxEv-QCHsAdEAK2hCmd6xnHqzbq-78E8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=97b3ffa80b23fc72ff6fb8d34bf23073c49c2481", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/caBqF8kHPzevxEv-QCHsAdEAK2hCmd6xnHqzbq-78E8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=def85e7e37f1fbfa11cf8ce9ffc89e5b295b01e1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/caBqF8kHPzevxEv-QCHsAdEAK2hCmd6xnHqzbq-78E8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=68320fb6b9ce8a12a911e3fd54329b43de4e31dd", "width": 1080, "height": 607}], "variants": {}, "id": "kxWVjKJZ8m4J4WK5ND1wgKpOME8kEN5m5DaSTebJLmA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rekge", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17rekge/western_digital_wont_make_ssds_anymore/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.howtogeek.com/western-digital-company-split-news/", "subreddit_subscribers": 711362, "created_utc": 1699541256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Obligatory \"asking for a friend\". But no, like, if enemies of your state were possibly going to overrun a data center you rent a spot at but you still have some time, what's the quickest way to, remotely, absolutely ruin any of the data on the disks? I can remember seeing old IBM \"Death\"Stars with the magnetic medium stripped clean. Don't necessarily need that blatant a result, but say you didn't have enough time to know if a basic `dd /dev/{zero,random,urandom} /dev/mraid0` could run long enough to overwrite everything even just once, let alone a few times.\n\nAlternatively, is there a file system which is purpose built for this sort of thing, where you send the array some sort of panic code \u2013 and I'm not considering drive encryption with a hardware or software key, whereas even if they were in place they are external to the disks themselves and thus at risk from external factors, rather than there being some command I can send to, well, if not crash the heads, then otherwise completely unravel the data structure in an irretrievable way with just one or two commands?", "author_fullname": "t2_92oly", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Maybe this is not the precise purview of this sub, but is there a way, in software, to cause a headcrash on an array of HDDs if you needed to destroy a hoard imminently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17r5w10", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699507455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obligatory &amp;quot;asking for a friend&amp;quot;. But no, like, if enemies of your state were possibly going to overrun a data center you rent a spot at but you still have some time, what&amp;#39;s the quickest way to, remotely, absolutely ruin any of the data on the disks? I can remember seeing old IBM &amp;quot;Death&amp;quot;Stars with the magnetic medium stripped clean. Don&amp;#39;t necessarily need that blatant a result, but say you didn&amp;#39;t have enough time to know if a basic &lt;code&gt;dd /dev/{zero,random,urandom} /dev/mraid0&lt;/code&gt; could run long enough to overwrite everything even just once, let alone a few times.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, is there a file system which is purpose built for this sort of thing, where you send the array some sort of panic code \u2013 and I&amp;#39;m not considering drive encryption with a hardware or software key, whereas even if they were in place they are external to the disks themselves and thus at risk from external factors, rather than there being some command I can send to, well, if not crash the heads, then otherwise completely unravel the data structure in an irretrievable way with just one or two commands?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17r5w10", "is_robot_indexable": true, "report_reasons": null, "author": "dwkindig", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17r5w10/maybe_this_is_not_the_precise_purview_of_this_sub/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17r5w10/maybe_this_is_not_the_precise_purview_of_this_sub/", "subreddit_subscribers": 711362, "created_utc": 1699507455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am archiving a vast amount of media files that are rarely accessed. I'm writing large sequential files, at peaks of about 100MB/s. \n\nI want to maximise storage space primarily; I have 20x 18TB HDDs.\n\nI've been told that large (e.g. 20 disk) vdevs are bad because resilvers will take a very long time, which creates higher risk of pool failure. How bad of an idea is this?", "author_fullname": "t2_fmblw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a 20 disk raidz3 really a bad idea, for media archiving use cases?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rf03y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699542459.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am archiving a vast amount of media files that are rarely accessed. I&amp;#39;m writing large sequential files, at peaks of about 100MB/s. &lt;/p&gt;\n\n&lt;p&gt;I want to maximise storage space primarily; I have 20x 18TB HDDs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been told that large (e.g. 20 disk) vdevs are bad because resilvers will take a very long time, which creates higher risk of pool failure. How bad of an idea is this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rf03y", "is_robot_indexable": true, "report_reasons": null, "author": "goldcakes", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rf03y/is_a_20_disk_raidz3_really_a_bad_idea_for_media/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rf03y/is_a_20_disk_raidz3_really_a_bad_idea_for_media/", "subreddit_subscribers": 711362, "created_utc": 1699542459.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone, my easystore 14tb I use as my NAS backup is no longer copying, so I'm looking to replace it. I saw costco is having a sale on the Seagate 14TB Expansion Desktop drives starting on Nov 21 for $149 down from $199. I have been very weary of Seagate for a while due to negative personal experience and other reports of drives failing. Is this still the case with these seagate drives? Does anyone know if these drives are slow? I have seen other capacities on this sub from other stores being reported as Ironwolf Pro drives, perhaps that's what this is? At $149, this seems extremely hard to resist. Thanks", "author_fullname": "t2_ptv4d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I consider purchasing this Seagate 14TB drive from costco?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rkpo1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699557635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, my easystore 14tb I use as my NAS backup is no longer copying, so I&amp;#39;m looking to replace it. I saw costco is having a sale on the Seagate 14TB Expansion Desktop drives starting on Nov 21 for $149 down from $199. I have been very weary of Seagate for a while due to negative personal experience and other reports of drives failing. Is this still the case with these seagate drives? Does anyone know if these drives are slow? I have seen other capacities on this sub from other stores being reported as Ironwolf Pro drives, perhaps that&amp;#39;s what this is? At $149, this seems extremely hard to resist. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rkpo1", "is_robot_indexable": true, "report_reasons": null, "author": "scandalous_lime", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rkpo1/should_i_consider_purchasing_this_seagate_14tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rkpo1/should_i_consider_purchasing_this_seagate_14tb/", "subreddit_subscribers": 711362, "created_utc": 1699557635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I have a 1 file that I would like to store for a long time (Offline)  \nI have put it on magnetic tape and in an sealed enclosure, but time will destroy everything, so was wondering if there is a way to like zip the file to make it able to get restored if/when the corruption happens? *much like a raid but just for one file*", "author_fullname": "t2_foibre3a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How would you ensure a file with high level of error corrections/corruptions resistant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rfysi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699545061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I have a 1 file that I would like to store for a long time (Offline)&lt;br/&gt;\nI have put it on magnetic tape and in an sealed enclosure, but time will destroy everything, so was wondering if there is a way to like zip the file to make it able to get restored if/when the corruption happens? &lt;em&gt;much like a raid but just for one file&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rfysi", "is_robot_indexable": true, "report_reasons": null, "author": "SadZookeeper9398", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rfysi/how_would_you_ensure_a_file_with_high_level_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rfysi/how_would_you_ensure_a_file_with_high_level_of/", "subreddit_subscribers": 711362, "created_utc": 1699545061.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm toying with the idea of grabbing a 12G trimode 8i card and possibly expanding out the future. Lets say a 9400-8i. Drives will be split between enterprise and consumer SSD's.\n\nAside from bandwidth on older expanders being 6G and possibly implying pre-trimode era hardware (for arguments sake a LENOVO 03X3834 [https://www.newegg.com/p/14G-000U-000K2](https://www.newegg.com/p/14G-000U-000K2) ), are there any gotcha's with using older expanders with newer HBA's like SCSI unmap and whatever else is a \"trimode\" feature?\n\nmy guess is there's a controller under the expanders heatsink which would not pass through the newer instructions.", "author_fullname": "t2_k1ak0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "are sas expander features passed through from controller?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17remxi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699541455.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m toying with the idea of grabbing a 12G trimode 8i card and possibly expanding out the future. Lets say a 9400-8i. Drives will be split between enterprise and consumer SSD&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;Aside from bandwidth on older expanders being 6G and possibly implying pre-trimode era hardware (for arguments sake a LENOVO 03X3834 &lt;a href=\"https://www.newegg.com/p/14G-000U-000K2\"&gt;https://www.newegg.com/p/14G-000U-000K2&lt;/a&gt; ), are there any gotcha&amp;#39;s with using older expanders with newer HBA&amp;#39;s like SCSI unmap and whatever else is a &amp;quot;trimode&amp;quot; feature?&lt;/p&gt;\n\n&lt;p&gt;my guess is there&amp;#39;s a controller under the expanders heatsink which would not pass through the newer instructions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17remxi", "is_robot_indexable": true, "report_reasons": null, "author": "veehexx", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17remxi/are_sas_expander_features_passed_through_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17remxi/are_sas_expander_features_passed_through_from/", "subreddit_subscribers": 711362, "created_utc": 1699541455.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all\n\nLong time reader, first time poster. Hoping for some help from the community\n\nTL;dr: My Windows 11 install on my boot SSD corrupted, re-installed Windows 10, now storage spaces parity pool does not appear. Can I restore or do I need to reformat?\n\nMore details:\n\n4x4TB Samsung 980 Evo SSDs, originally configured in parity in Windows 11 Storage Spaces\n\nWindows 11 install was irretrievably corrupted. I had to format and do a fresh install of Windows. As I'd had nothing but problems with Windows 11 I decided to go back to Windows 10\n\nIn Windows 10: get-physicaldisk shows the drives are all stuck at {starting, OK}. \n\nThe storage pool members appear separately in Disk Management\n\nWindows Explorer does not show storage pool\n\nReclaiMe can still see the data on each disk\n\nI also still have my original 4x4TB WD Red Pro RAID5 array with data intact that was the source of the data for the 4x4TB Evo 870 storage space\n\n&amp;#x200B;\n\nAfter reading up online, the only thing it seems that I can do is: \n\n1) reset-physicaldisk to wipe each 4TB 870 Evo drive and then:\n\nA) either make another storage pool or go back to motherboard RAID5\n\nor\n\n2) possibly install Windows 11 on another SSD to \"see\" the pool (assuming it's an OS Build incompatibility), copy the data, then A)\n\nIs there an option 3) where I can just restore the storage pool in Windows 10? \n\nThanks in advance for any replies ", "author_fullname": "t2_151ezv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces pool not appearing after fresh install of Windows 10 (downgrade from Windows 11)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rb8we", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699530665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;Long time reader, first time poster. Hoping for some help from the community&lt;/p&gt;\n\n&lt;p&gt;TL;dr: My Windows 11 install on my boot SSD corrupted, re-installed Windows 10, now storage spaces parity pool does not appear. Can I restore or do I need to reformat?&lt;/p&gt;\n\n&lt;p&gt;More details:&lt;/p&gt;\n\n&lt;p&gt;4x4TB Samsung 980 Evo SSDs, originally configured in parity in Windows 11 Storage Spaces&lt;/p&gt;\n\n&lt;p&gt;Windows 11 install was irretrievably corrupted. I had to format and do a fresh install of Windows. As I&amp;#39;d had nothing but problems with Windows 11 I decided to go back to Windows 10&lt;/p&gt;\n\n&lt;p&gt;In Windows 10: get-physicaldisk shows the drives are all stuck at {starting, OK}. &lt;/p&gt;\n\n&lt;p&gt;The storage pool members appear separately in Disk Management&lt;/p&gt;\n\n&lt;p&gt;Windows Explorer does not show storage pool&lt;/p&gt;\n\n&lt;p&gt;ReclaiMe can still see the data on each disk&lt;/p&gt;\n\n&lt;p&gt;I also still have my original 4x4TB WD Red Pro RAID5 array with data intact that was the source of the data for the 4x4TB Evo 870 storage space&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;After reading up online, the only thing it seems that I can do is: &lt;/p&gt;\n\n&lt;p&gt;1) reset-physicaldisk to wipe each 4TB 870 Evo drive and then:&lt;/p&gt;\n\n&lt;p&gt;A) either make another storage pool or go back to motherboard RAID5&lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;2) possibly install Windows 11 on another SSD to &amp;quot;see&amp;quot; the pool (assuming it&amp;#39;s an OS Build incompatibility), copy the data, then A)&lt;/p&gt;\n\n&lt;p&gt;Is there an option 3) where I can just restore the storage pool in Windows 10? &lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any replies &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rb8we", "is_robot_indexable": true, "report_reasons": null, "author": "robodan918", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rb8we/storage_spaces_pool_not_appearing_after_fresh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rb8we/storage_spaces_pool_not_appearing_after_fresh/", "subreddit_subscribers": 711362, "created_utc": 1699530665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_12i7n4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Just bought a Dell Exos X18 (18TB). Does this look normal to you guys for it's first block test?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 91, "top_awarded_type": null, "hide_score": true, "name": "t3_17rsyxg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mIpvV3NGMBX9N-9ehPv3IbbUCyvSzHISwkAMHEiFIxE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699580434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/rr56figxcfzb1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/rr56figxcfzb1.png?auto=webp&amp;s=cb04680954f9244514ecd3b8b2730639ec2a4571", "width": 715, "height": 469}, "resolutions": [{"url": "https://preview.redd.it/rr56figxcfzb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2c5fa595c56ad95e28462ed7474077cf6abcde86", "width": 108, "height": 70}, {"url": "https://preview.redd.it/rr56figxcfzb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ff66c63488c8cf9fcf4c84212754282ca222722", "width": 216, "height": 141}, {"url": "https://preview.redd.it/rr56figxcfzb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7cd748c8a4a9dbc9139064a912c58e91376c248", "width": 320, "height": 209}, {"url": "https://preview.redd.it/rr56figxcfzb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d893de594a95645f5a01ce56be974c7bacbcdae3", "width": 640, "height": 419}], "variants": {}, "id": "L0oNJvIP5g8VUO-AOsikD3K_IS_POBohWbv0rsR1NXA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rsyxg", "is_robot_indexable": true, "report_reasons": null, "author": "Superbroom", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rsyxg/just_bought_a_dell_exos_x18_18tb_does_this_look/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/rr56figxcfzb1.png", "subreddit_subscribers": 711362, "created_utc": 1699580434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "SSD prices have come down a lot, so I thought it would be a good time to replace my external 2.5\" drive with something lighter and smaller. I settled on the Sabrent EC-SNVE enclosure (USB 3.2 to NVMe or SATA) and the WD SN850X 2 TB drive. One of my computers (a ThinkPad T440s) only has USB 3.0 ports, so I thought I'd also get a USB 3.0 type A to type C adapter.\n\nHowever, I'm having doubts about power. Apparently, a regular USB 3.0 port can only do 4.5 W. In the combination I picked, the SSD alone uses up to 6.8 W.\n\nMy question is, what's going to happen when I plug this combination into the old laptop? Is the drive going to automatically limit its power usage in some way, or am I going to risk losing data or frying a USB port? Is there a way to make it work reliably?", "author_fullname": "t2_n0p7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USB 3.2 NVMe enclosure in a USB 3.0 port?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rm0l0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699561152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SSD prices have come down a lot, so I thought it would be a good time to replace my external 2.5&amp;quot; drive with something lighter and smaller. I settled on the Sabrent EC-SNVE enclosure (USB 3.2 to NVMe or SATA) and the WD SN850X 2 TB drive. One of my computers (a ThinkPad T440s) only has USB 3.0 ports, so I thought I&amp;#39;d also get a USB 3.0 type A to type C adapter.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m having doubts about power. Apparently, a regular USB 3.0 port can only do 4.5 W. In the combination I picked, the SSD alone uses up to 6.8 W.&lt;/p&gt;\n\n&lt;p&gt;My question is, what&amp;#39;s going to happen when I plug this combination into the old laptop? Is the drive going to automatically limit its power usage in some way, or am I going to risk losing data or frying a USB port? Is there a way to make it work reliably?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rm0l0", "is_robot_indexable": true, "report_reasons": null, "author": "pkkm", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rm0l0/usb_32_nvme_enclosure_in_a_usb_30_port/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rm0l0/usb_32_nvme_enclosure_in_a_usb_30_port/", "subreddit_subscribers": 711362, "created_utc": 1699561152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Firstly, I know storage spaces aren't ideal, but for what I'm using, a not-always-on NAS for storage, it had worked fine for me. My biggest gripe with it would honestly be it allows you to add more data to your pool without telling you.. and that has come to bite me.\n\nI have three drives in a parity, and I made the mistake (or windows allowed me to..) of entirely filling up the space. I was xfering files to it over my network, so this might had bypassed any kind of stops on adding more data, but I digress. \n\nThree of my drives are 99.9% full. Storage spaces says that the pool is inaccessible and to check the physical drives section. The physical drives section shows no errors. Trying Get-VirtualDisk, Get-StoragePool, it shows that the pool is detached, and unhealthy. I saw comments that there was a Repair-VirtualDisk command, but I can't seem to get that to work.\n\nI tried to add another like-size drive to the pool and optimize, but optimizing never happens.\n\nShort of trying to add three more disks and rebuild, or connecting the disks to another machine (the one I have my stuff in right now was a good desktop 15 yr ago but now, not so much), I'm not sure what else I could do.\n\nAnd yes, I know storage spaces isn't ideal, and once I can recover this I'm really considering buying a modern lower power motherboard and cpu and going to truenas or something... But not until I can secure my stuff. Thankfully, nothing of true irreplaceable value is lost, but it's still a bad time.", "author_fullname": "t2_x6c8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces - Unhealthy, inaccessible", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rizmu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699553055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Firstly, I know storage spaces aren&amp;#39;t ideal, but for what I&amp;#39;m using, a not-always-on NAS for storage, it had worked fine for me. My biggest gripe with it would honestly be it allows you to add more data to your pool without telling you.. and that has come to bite me.&lt;/p&gt;\n\n&lt;p&gt;I have three drives in a parity, and I made the mistake (or windows allowed me to..) of entirely filling up the space. I was xfering files to it over my network, so this might had bypassed any kind of stops on adding more data, but I digress. &lt;/p&gt;\n\n&lt;p&gt;Three of my drives are 99.9% full. Storage spaces says that the pool is inaccessible and to check the physical drives section. The physical drives section shows no errors. Trying Get-VirtualDisk, Get-StoragePool, it shows that the pool is detached, and unhealthy. I saw comments that there was a Repair-VirtualDisk command, but I can&amp;#39;t seem to get that to work.&lt;/p&gt;\n\n&lt;p&gt;I tried to add another like-size drive to the pool and optimize, but optimizing never happens.&lt;/p&gt;\n\n&lt;p&gt;Short of trying to add three more disks and rebuild, or connecting the disks to another machine (the one I have my stuff in right now was a good desktop 15 yr ago but now, not so much), I&amp;#39;m not sure what else I could do.&lt;/p&gt;\n\n&lt;p&gt;And yes, I know storage spaces isn&amp;#39;t ideal, and once I can recover this I&amp;#39;m really considering buying a modern lower power motherboard and cpu and going to truenas or something... But not until I can secure my stuff. Thankfully, nothing of true irreplaceable value is lost, but it&amp;#39;s still a bad time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rizmu", "is_robot_indexable": true, "report_reasons": null, "author": "The_Wkwied", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rizmu/storage_spaces_unhealthy_inaccessible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rizmu/storage_spaces_unhealthy_inaccessible/", "subreddit_subscribers": 711362, "created_utc": 1699553055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for an off-the-shelf 4 or 6 bay NAS with upgradeable RAM that runs TrueNAS (for ZFS). The QNAP TS-664 looked very nice, but the price of the 4G version (with 2 RAM slots) is currently 25% more than the 8G version (with soldered RAM). I'd like to have 16 or better 32GB of RAM.\n\nIt is only used as file server and maybe 1-2 small vms for backup processes. Plex and other services will run on another device.\n\nDo you have any recommendations?", "author_fullname": "t2_lquq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS with upgradable RAM for TrueNAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rhapq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699549217.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699548604.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for an off-the-shelf 4 or 6 bay NAS with upgradeable RAM that runs TrueNAS (for ZFS). The QNAP TS-664 looked very nice, but the price of the 4G version (with 2 RAM slots) is currently 25% more than the 8G version (with soldered RAM). I&amp;#39;d like to have 16 or better 32GB of RAM.&lt;/p&gt;\n\n&lt;p&gt;It is only used as file server and maybe 1-2 small vms for backup processes. Plex and other services will run on another device.&lt;/p&gt;\n\n&lt;p&gt;Do you have any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rhapq", "is_robot_indexable": true, "report_reasons": null, "author": "madcook1", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rhapq/nas_with_upgradable_ram_for_truenas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rhapq/nas_with_upgradable_ram_for_truenas/", "subreddit_subscribers": 711362, "created_utc": 1699548604.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys,\n\nit's been years since i've bought a lacie 1TB external usb hard disk. I was wild, young and free.. and I was never afraid about backups.\n\nOne bad day the disk seemed to be broken. A red led turned on on the front panel and the sound was not so reassuring.\n\nI turned it off and opened it up. While opening it I damaged the electronic board (I was wild and young an free... and had no respect for my stuff). But I kept the two 500GB disks. On the board there was written \"JBOD\". At that time I didn't even know what was the meaning, but then i grew up.\n\nI've never touched those two disks till now, but I would like to retrieve stuff from there... mostly audio recording projects, that I would get even partially, and a 5GB file created with the good old truecrypt.\n\nAs far as I have seen there are no worries when I look into the SMART datas on the two disks.\n\nIf I attach the two disks in my home NAS, mounted them and joined them with MergerFS... would it be possible to retrieve something?\n\nIs there another way to rebuild the JBOD array (would it be any better)?\n\nI'd really appreciate some help. Thank you.  \n", "author_fullname": "t2_rlez2zua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "JBOD to MergerFS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rh8ih", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699548441.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;it&amp;#39;s been years since i&amp;#39;ve bought a lacie 1TB external usb hard disk. I was wild, young and free.. and I was never afraid about backups.&lt;/p&gt;\n\n&lt;p&gt;One bad day the disk seemed to be broken. A red led turned on on the front panel and the sound was not so reassuring.&lt;/p&gt;\n\n&lt;p&gt;I turned it off and opened it up. While opening it I damaged the electronic board (I was wild and young an free... and had no respect for my stuff). But I kept the two 500GB disks. On the board there was written &amp;quot;JBOD&amp;quot;. At that time I didn&amp;#39;t even know what was the meaning, but then i grew up.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve never touched those two disks till now, but I would like to retrieve stuff from there... mostly audio recording projects, that I would get even partially, and a 5GB file created with the good old truecrypt.&lt;/p&gt;\n\n&lt;p&gt;As far as I have seen there are no worries when I look into the SMART datas on the two disks.&lt;/p&gt;\n\n&lt;p&gt;If I attach the two disks in my home NAS, mounted them and joined them with MergerFS... would it be possible to retrieve something?&lt;/p&gt;\n\n&lt;p&gt;Is there another way to rebuild the JBOD array (would it be any better)?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d really appreciate some help. Thank you.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rh8ih", "is_robot_indexable": true, "report_reasons": null, "author": "asbruff", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rh8ih/jbod_to_mergerfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rh8ih/jbod_to_mergerfs/", "subreddit_subscribers": 711362, "created_utc": 1699548441.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was using robocopy to write to a freshly formatted 5TB Passport that's only a month old.  It was about 3.8 TB of data, mostly large files (about 300 MB to 1 GB each), but towards around the 3.5TB mark, it started writing very slow.  It started around 60 MB/s, then got down to around 30 MB/s (typical HDD stuff), but then it just sputters between 3 MB/s then goes back up to 30 MB/s for a second or two, then goes back to 3 MB/s.\n\nGive it a few more minutes and now it just won't go past 5-10 MB/s.  \n\nAfter I was done using robocopy, I have 700 GB free and I tried to copy some 2-4 GB files using Explorer and it starts out at 30 MB/s for the first few seconds, then drops to about 6-10 MB/s.  The drive utilization is at 100% and the average response times gets up to the 1200-1800ms range according to Task Manager.\n\nIs this normal for this drive?  According to WD Drive Utilities, SMART status passed, Quick Drive Test passed,  Working on a Complete Drive Test now.  Will take a long time according to the utility.  I'm already about an hour in and it's at 30% complete.  Chkdsk shows nothing wrong either.\n\nThis happened when it was brand new too but I shrugged it off because I just needed the data on there temporarily.  Yesterday I needed to clear out the drive and recopy the data since everything changed and figured I'd give it a fresh start by formatting (exFAT if it makes a difference).  Same behavior as before.  I recalled I verified the data just fine.  I don't recall if reads were slow though either.\n\nWaiting for the Complete Drive Test to finish on the WD Drive Utilities...  Is this an SMR drive?  If so, is this expected behavior?  Should I be worried?", "author_fullname": "t2_77vndzd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD 5TB Passport - slow writes? (5-10 MB/s)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rgtyz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699547387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was using robocopy to write to a freshly formatted 5TB Passport that&amp;#39;s only a month old.  It was about 3.8 TB of data, mostly large files (about 300 MB to 1 GB each), but towards around the 3.5TB mark, it started writing very slow.  It started around 60 MB/s, then got down to around 30 MB/s (typical HDD stuff), but then it just sputters between 3 MB/s then goes back up to 30 MB/s for a second or two, then goes back to 3 MB/s.&lt;/p&gt;\n\n&lt;p&gt;Give it a few more minutes and now it just won&amp;#39;t go past 5-10 MB/s.  &lt;/p&gt;\n\n&lt;p&gt;After I was done using robocopy, I have 700 GB free and I tried to copy some 2-4 GB files using Explorer and it starts out at 30 MB/s for the first few seconds, then drops to about 6-10 MB/s.  The drive utilization is at 100% and the average response times gets up to the 1200-1800ms range according to Task Manager.&lt;/p&gt;\n\n&lt;p&gt;Is this normal for this drive?  According to WD Drive Utilities, SMART status passed, Quick Drive Test passed,  Working on a Complete Drive Test now.  Will take a long time according to the utility.  I&amp;#39;m already about an hour in and it&amp;#39;s at 30% complete.  Chkdsk shows nothing wrong either.&lt;/p&gt;\n\n&lt;p&gt;This happened when it was brand new too but I shrugged it off because I just needed the data on there temporarily.  Yesterday I needed to clear out the drive and recopy the data since everything changed and figured I&amp;#39;d give it a fresh start by formatting (exFAT if it makes a difference).  Same behavior as before.  I recalled I verified the data just fine.  I don&amp;#39;t recall if reads were slow though either.&lt;/p&gt;\n\n&lt;p&gt;Waiting for the Complete Drive Test to finish on the WD Drive Utilities...  Is this an SMR drive?  If so, is this expected behavior?  Should I be worried?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rgtyz", "is_robot_indexable": true, "report_reasons": null, "author": "Zeddie-", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rgtyz/wd_5tb_passport_slow_writes_510_mbs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rgtyz/wd_5tb_passport_slow_writes_510_mbs/", "subreddit_subscribers": 711362, "created_utc": 1699547387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a way to back up **a specific tag** from a Tumblr blog. My Tumblr is 8 years old and has around 50,000 posts, but the content I really want to back up is only around 2,000 posts, so if I used any of the methods of backing up Tumblr I've managed to find so far, I would have to manually go through and delete the other 48,000 posts to have a useable archive.\n\nI've already found Tumbltwo/Tumblthree, which do allow tag filtering, but these only download images, and I'm trying to back up text posts.\n\nAlternatively:\n\n\\-If I could back up a specific tag from Tumblr as a whole, that would basically come to the same, since I use a unique tag for my original content. (Although, like the [tumblr.com/tagged/](https://tumblr.com/tagged/) pages, this would probably skip reblogs, and some of my original content is in additions to reblogs.)\n\n\\-If I could download my posts from a specific year, that would still leave me with a lot of pruning to do, but considerably less than downloading all eight years.\n\nAnyone have any idea? Thanks!", "author_fullname": "t2_e2xzqzdr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up Tumblr blog - But NOT the whole thing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rg3n0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699545422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a way to back up &lt;strong&gt;a specific tag&lt;/strong&gt; from a Tumblr blog. My Tumblr is 8 years old and has around 50,000 posts, but the content I really want to back up is only around 2,000 posts, so if I used any of the methods of backing up Tumblr I&amp;#39;ve managed to find so far, I would have to manually go through and delete the other 48,000 posts to have a useable archive.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve already found Tumbltwo/Tumblthree, which do allow tag filtering, but these only download images, and I&amp;#39;m trying to back up text posts.&lt;/p&gt;\n\n&lt;p&gt;Alternatively:&lt;/p&gt;\n\n&lt;p&gt;-If I could back up a specific tag from Tumblr as a whole, that would basically come to the same, since I use a unique tag for my original content. (Although, like the &lt;a href=\"https://tumblr.com/tagged/\"&gt;tumblr.com/tagged/&lt;/a&gt; pages, this would probably skip reblogs, and some of my original content is in additions to reblogs.)&lt;/p&gt;\n\n&lt;p&gt;-If I could download my posts from a specific year, that would still leave me with a lot of pruning to do, but considerably less than downloading all eight years.&lt;/p&gt;\n\n&lt;p&gt;Anyone have any idea? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rg3n0", "is_robot_indexable": true, "report_reasons": null, "author": "tumblrbackupq", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rg3n0/backing_up_tumblr_blog_but_not_the_whole_thing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rg3n0/backing_up_tumblr_blog_but_not_the_whole_thing/", "subreddit_subscribers": 711362, "created_utc": 1699545422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_nchiks84h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A tool I made to download all your Strava timeseries data to a single, large CSV file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17rfo2g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/L5BciwicR4y-rWg2z_rxBtAVg9ggsKy-LCQzJSaVpYg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699544277.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/aaron-schroeder/strava-timeseries-archive", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L8clojkjWiaDBiHJaN6_7C2nNO3h9vgP1tRSapqVDJY.jpg?auto=webp&amp;s=271b2117aebcba4948b4ed048712ffa69e097482", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/L8clojkjWiaDBiHJaN6_7C2nNO3h9vgP1tRSapqVDJY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=13f1fdc04c228205fc90f130d8e6ad4474a68602", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/L8clojkjWiaDBiHJaN6_7C2nNO3h9vgP1tRSapqVDJY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=489cbdc14b57333621dd20859464fa2156715eda", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/L8clojkjWiaDBiHJaN6_7C2nNO3h9vgP1tRSapqVDJY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=80d20d6bc99880553e3a447fd54435c472722b3f", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/L8clojkjWiaDBiHJaN6_7C2nNO3h9vgP1tRSapqVDJY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0872bf1ed663151931007e9f568cf93645db54a0", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/L8clojkjWiaDBiHJaN6_7C2nNO3h9vgP1tRSapqVDJY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cb16e8fd167bf99a9d7d69fef4e9e4dee063456", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/L8clojkjWiaDBiHJaN6_7C2nNO3h9vgP1tRSapqVDJY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8e2c5d58a9c7e51a160a9964dcaec6d12500b143", "width": 1080, "height": 540}], "variants": {}, "id": "jeAMLzfS4AAuMf1XC4ST1I01WCohBLhHlT0HfDuxrtk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rfo2g", "is_robot_indexable": true, "report_reasons": null, "author": "FoolsAaron", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rfo2g/a_tool_i_made_to_download_all_your_strava/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/aaron-schroeder/strava-timeseries-archive", "subreddit_subscribers": 711362, "created_utc": 1699544277.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a pretty large video and audio library and am running out of space. I was thinking about getting a DAS as I don\u2019t really want a nas. Just personal preference. But right now my storage is current in my gaming pc and thought maybe I should separate them. I have an Acer AC100 mini server with a 3770s, 16 gb of ram and it has 4 drive bays and supports raid which is something I definitely want. \nI like the ease of a DAS and don\u2019t necessarily want another machine but what are you guys thoughts on it? Qnap das or similar or just keep and setup the mini server?", "author_fullname": "t2_9pqc3aa9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice on expanding storage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rfcye", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699543430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a pretty large video and audio library and am running out of space. I was thinking about getting a DAS as I don\u2019t really want a nas. Just personal preference. But right now my storage is current in my gaming pc and thought maybe I should separate them. I have an Acer AC100 mini server with a 3770s, 16 gb of ram and it has 4 drive bays and supports raid which is something I definitely want. \nI like the ease of a DAS and don\u2019t necessarily want another machine but what are you guys thoughts on it? Qnap das or similar or just keep and setup the mini server?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rfcye", "is_robot_indexable": true, "report_reasons": null, "author": "Darklord97929", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rfcye/advice_on_expanding_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rfcye/advice_on_expanding_storage/", "subreddit_subscribers": 711362, "created_utc": 1699543430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I was wondering if some of you knew about a tool/website to download everything from a specific profile on [vk.com](https://vk.com).\n\nI tried with gallery-dl and ripme, but both can not handle profiles (only albums, etc)\n\nThanks in advance !", "author_fullname": "t2_h6pb6ppc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download a profile's content from VK", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17rezg9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699542406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I was wondering if some of you knew about a tool/website to download everything from a specific profile on &lt;a href=\"https://vk.com\"&gt;vk.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I tried with gallery-dl and ripme, but both can not handle profiles (only albums, etc)&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rezg9", "is_robot_indexable": true, "report_reasons": null, "author": "AntonioKarot", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rezg9/download_a_profiles_content_from_vk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rezg9/download_a_profiles_content_from_vk/", "subreddit_subscribers": 711362, "created_utc": 1699542406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a huge library of video files that I need to tag with the mpaa rating to make sorting kid friendly stuff from not kid friendly.  What is the best way to go about this.", "author_fullname": "t2_4bi2ialr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best free media tagger and sorter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17r9vud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699524959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a huge library of video files that I need to tag with the mpaa rating to make sorting kid friendly stuff from not kid friendly.  What is the best way to go about this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "52TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17r9vud", "is_robot_indexable": true, "report_reasons": null, "author": "Mbox97", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17r9vud/best_free_media_tagger_and_sorter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17r9vud/best_free_media_tagger_and_sorter/", "subreddit_subscribers": 711362, "created_utc": 1699524959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Game dev here and have over 3K pictures for concept art that I have to sort through. I have it organized through folders but this system is broken because no tagging. I tried DigiCam but after numerous attempts it crashes upon generating a database file with no exception messages. Since the developer doesn\u2019t care about being transparent I don\u2019t want to run into issues later on and fight with it so now I\u2019m on the fence if I should develop my own and open-source it or keep fighting with it.\n\nI was recommended to come here and ask because you all live and breathe this so really hoping to find something to get me back on track. FOSS only please!", "author_fullname": "t2_ix9qfiys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a good digital management program.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17r7s7s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699515361.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Game dev here and have over 3K pictures for concept art that I have to sort through. I have it organized through folders but this system is broken because no tagging. I tried DigiCam but after numerous attempts it crashes upon generating a database file with no exception messages. Since the developer doesn\u2019t care about being transparent I don\u2019t want to run into issues later on and fight with it so now I\u2019m on the fence if I should develop my own and open-source it or keep fighting with it.&lt;/p&gt;\n\n&lt;p&gt;I was recommended to come here and ask because you all live and breathe this so really hoping to find something to get me back on track. FOSS only please!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17r7s7s", "is_robot_indexable": true, "report_reasons": null, "author": "BL1NDX3N0N", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17r7s7s/looking_for_a_good_digital_management_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17r7s7s/looking_for_a_good_digital_management_program/", "subreddit_subscribers": 711362, "created_utc": 1699515361.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is anyone archiving special features? Or is it possible that this media is so ignored that it could actually get lost?\n\n&amp;#x200B;\n\nI gathered a pretty big dvd collection back in the day and now I'm moving so I've been thinking about getting rid of it. I transferred most of it to a digital collection, but now I'm realizing that a lot of the DVDs have special features that I'd miss out on if I get rid of them.", "author_fullname": "t2_ztcuh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DVD special features?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17r74bs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699512417.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is anyone archiving special features? Or is it possible that this media is so ignored that it could actually get lost?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I gathered a pretty big dvd collection back in the day and now I&amp;#39;m moving so I&amp;#39;ve been thinking about getting rid of it. I transferred most of it to a digital collection, but now I&amp;#39;m realizing that a lot of the DVDs have special features that I&amp;#39;d miss out on if I get rid of them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17r74bs", "is_robot_indexable": true, "report_reasons": null, "author": "Oldkingcole225", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17r74bs/dvd_special_features/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17r74bs/dvd_special_features/", "subreddit_subscribers": 711362, "created_utc": 1699512417.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My data hoarding has to do with gathering production assets for a game I'm working on and keeping them organized. For example, I collect thousands and thousands of icon assets and vfx assets and music assets and create libraries I can search through, that way I can grab the perfect asset when I'm making something for the game.\n\nIn this case I'm looking for a software to tag all my music. I need to be able to make up tags like \"sad\" and \"epic\" etc that way I can search those tags when I'm editing a video to pluck a track I need. I don't care about the file metadata or anything about the actual track (such as the artist, genre, etc). \n\nI notice a lot of the suggested software here like Mediamonkey and Picard etc are centered around managing metadata. Is there a simple software that just lets me tag tracks with custom tags and search across them? Like Eagle ([https://en.eagle.cool/](https://en.eagle.cool/)) but for music?", "author_fullname": "t2_7r7o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a music manager for an atypical use case and I figured you guys would be the best to ask", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17r5eaq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699505612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My data hoarding has to do with gathering production assets for a game I&amp;#39;m working on and keeping them organized. For example, I collect thousands and thousands of icon assets and vfx assets and music assets and create libraries I can search through, that way I can grab the perfect asset when I&amp;#39;m making something for the game.&lt;/p&gt;\n\n&lt;p&gt;In this case I&amp;#39;m looking for a software to tag all my music. I need to be able to make up tags like &amp;quot;sad&amp;quot; and &amp;quot;epic&amp;quot; etc that way I can search those tags when I&amp;#39;m editing a video to pluck a track I need. I don&amp;#39;t care about the file metadata or anything about the actual track (such as the artist, genre, etc). &lt;/p&gt;\n\n&lt;p&gt;I notice a lot of the suggested software here like Mediamonkey and Picard etc are centered around managing metadata. Is there a simple software that just lets me tag tracks with custom tags and search across them? Like Eagle (&lt;a href=\"https://en.eagle.cool/\"&gt;https://en.eagle.cool/&lt;/a&gt;) but for music?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XcQJkAJVH-mM1C5DjR7J3GNcHXfQBrjH8aR89-_P0p0.jpg?auto=webp&amp;s=9d452566e6873937a58d779770a93a0d3de0f739", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/XcQJkAJVH-mM1C5DjR7J3GNcHXfQBrjH8aR89-_P0p0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8359702cb078eb2ef571382803201cf7be843e7f", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/XcQJkAJVH-mM1C5DjR7J3GNcHXfQBrjH8aR89-_P0p0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=70698e8f4a65115908ba9272e68a81071b888abd", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/XcQJkAJVH-mM1C5DjR7J3GNcHXfQBrjH8aR89-_P0p0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1cb9951af2441d184ce3714ae8fa1ee5b036b539", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/XcQJkAJVH-mM1C5DjR7J3GNcHXfQBrjH8aR89-_P0p0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bfe7dc5dbf5ca1c12c135e3ba3a17cb9e6d32641", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/XcQJkAJVH-mM1C5DjR7J3GNcHXfQBrjH8aR89-_P0p0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1c7bfa9a9a814536ee7885b7b8afa25f8b9e1fa", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/XcQJkAJVH-mM1C5DjR7J3GNcHXfQBrjH8aR89-_P0p0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=13a2e89d789f71debdf3dfe05f4044aeb1294d4e", "width": 1080, "height": 567}], "variants": {}, "id": "Vi3qAC_LpGdu7ZcSoQUZENGt_bdoSvnAYVqTHBqtkdk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17r5eaq", "is_robot_indexable": true, "report_reasons": null, "author": "mccoypauley", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17r5eaq/looking_for_a_music_manager_for_an_atypical_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17r5eaq/looking_for_a_music_manager_for_an_atypical_use/", "subreddit_subscribers": 711362, "created_utc": 1699505612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using Windows HashCheck Shell Extension and it works by right clicking a file/folder and creating a checksum file for the entire file. The issue with this is this is manually done and that if I try to make a checksum for a large number of files at once, it lumps it all together into ONE checksum file whereas I would like multiple smaller checksum files for every folder (so checking also doesn't take forever and that I dont have to create new checksum files since more files within the checksum means changes more likely to happen and a new file is needed to be made).", "author_fullname": "t2_woqd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checksum file for every folder/file automatically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17rsyr4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699580422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Windows HashCheck Shell Extension and it works by right clicking a file/folder and creating a checksum file for the entire file. The issue with this is this is manually done and that if I try to make a checksum for a large number of files at once, it lumps it all together into ONE checksum file whereas I would like multiple smaller checksum files for every folder (so checking also doesn&amp;#39;t take forever and that I dont have to create new checksum files since more files within the checksum means changes more likely to happen and a new file is needed to be made).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rsyr4", "is_robot_indexable": true, "report_reasons": null, "author": "slaiyfer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rsyr4/checksum_file_for_every_folderfile_automatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rsyr4/checksum_file_for_every_folderfile_automatically/", "subreddit_subscribers": 711362, "created_utc": 1699580422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm using Windows HashCheck Shell Extension and it works by right clicking a file/folder and creating a checksum file for the entire file. The issue with this is this is manually done and that if I try to make a checksum for a large number of files at once, it lumps it all together into ONE checksum file whereas I would like multiple smaller checksum files for every folder (so checking also doesn't take forever and that I dont have to create new checksum files since more files within the checksum means changes more likely to happen and a new file is needed to be made).", "author_fullname": "t2_woqd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checksum file for every folder/file automatically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17rsyq9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699580419.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Windows HashCheck Shell Extension and it works by right clicking a file/folder and creating a checksum file for the entire file. The issue with this is this is manually done and that if I try to make a checksum for a large number of files at once, it lumps it all together into ONE checksum file whereas I would like multiple smaller checksum files for every folder (so checking also doesn&amp;#39;t take forever and that I dont have to create new checksum files since more files within the checksum means changes more likely to happen and a new file is needed to be made).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rsyq9", "is_robot_indexable": true, "report_reasons": null, "author": "slaiyfer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rsyq9/checksum_file_for_every_folderfile_automatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rsyq9/checksum_file_for_every_folderfile_automatically/", "subreddit_subscribers": 711362, "created_utc": 1699580419.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everybody, \n\nI'm trying to download a BBC Sound podcast, but I don't know how. I've tried with get\\_iplayer, but it didn't work (maybe my fault). \n\nDoes anyone knows how to do it. \n\nlink: [https://www.bbc.co.uk/sounds/play/p00fpvjf](https://www.bbc.co.uk/sounds/play/p00fpvjf)", "author_fullname": "t2_gtej2kux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Download a BBC Sounds podcast", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17rsjbb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699579109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everybody, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to download a BBC Sound podcast, but I don&amp;#39;t know how. I&amp;#39;ve tried with get_iplayer, but it didn&amp;#39;t work (maybe my fault). &lt;/p&gt;\n\n&lt;p&gt;Does anyone knows how to do it. &lt;/p&gt;\n\n&lt;p&gt;link: &lt;a href=\"https://www.bbc.co.uk/sounds/play/p00fpvjf\"&gt;https://www.bbc.co.uk/sounds/play/p00fpvjf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/FfchtmzSCXyUVlhcHi2VnefXya6kbunGUSAGsUvOZmY.jpg?auto=webp&amp;s=d0240a5e7af192462c16bca7877290e5cb52e915", "width": 1024, "height": 576}, "resolutions": [{"url": "https://external-preview.redd.it/FfchtmzSCXyUVlhcHi2VnefXya6kbunGUSAGsUvOZmY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dc0644b2117059bdc07fb2df59383f234cd64b7", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/FfchtmzSCXyUVlhcHi2VnefXya6kbunGUSAGsUvOZmY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8820851fa0e1eaddf4c528220d627623c3c20061", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/FfchtmzSCXyUVlhcHi2VnefXya6kbunGUSAGsUvOZmY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d179a290f3761f2c9554981b022160416213cff", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/FfchtmzSCXyUVlhcHi2VnefXya6kbunGUSAGsUvOZmY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=73189c0d84e7e45d9e5977992729419f33c873e8", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/FfchtmzSCXyUVlhcHi2VnefXya6kbunGUSAGsUvOZmY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=bf6c687d7eacb1f4ca4106a6669f8cda5935c6e7", "width": 960, "height": 540}], "variants": {}, "id": "SbffBzbSJ4fG6oRT-3aoG6CbPipkFoGlA6IkthVAzh8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17rsjbb", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Crab9125", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rsjbb/download_a_bbc_sounds_podcast/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rsjbb/download_a_bbc_sounds_podcast/", "subreddit_subscribers": 711362, "created_utc": 1699579109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "These are currently on sale at $9/tb from SPD. They appear to be rebranded Seagate Exos 20s. From reading online, it appears that these are drives that failed enterprise tests and are being resold for consumers. That makes me a bit nervous, but the price is right for budget redundancy. Would appreciate any input.\n\nhttps://www.ebay.com/itm/305178477894", "author_fullname": "t2_il7djfpv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on White Label HDDs from ServerPartDeals?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17rs5t8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699578007.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;These are currently on sale at $9/tb from SPD. They appear to be rebranded Seagate Exos 20s. From reading online, it appears that these are drives that failed enterprise tests and are being resold for consumers. That makes me a bit nervous, but the price is right for budget redundancy. Would appreciate any input.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.ebay.com/itm/305178477894\"&gt;https://www.ebay.com/itm/305178477894&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/c3zJIMtMlNGhMAIe7XRWcQZNyNXumBLAQxA4f6w5EaA.jpg?auto=webp&amp;s=e7f1a4206f3e8b550fc60c1a7f670b6183a9d2c0", "width": 267, "height": 400}, "resolutions": [{"url": "https://external-preview.redd.it/c3zJIMtMlNGhMAIe7XRWcQZNyNXumBLAQxA4f6w5EaA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=42c29df2788e8f459de3b31d1a7d7db3f0cf44a5", "width": 108, "height": 161}, {"url": "https://external-preview.redd.it/c3zJIMtMlNGhMAIe7XRWcQZNyNXumBLAQxA4f6w5EaA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8435f730de2e1986a46f60ef14d1336e5e8874e2", "width": 216, "height": 323}], "variants": {}, "id": "M_NLzkcTKCIk_8s66u0_ofRwtwXKrrzvO8gLq95_RPE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "17rs5t8", "is_robot_indexable": true, "report_reasons": null, "author": "oran12390", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17rs5t8/thoughts_on_white_label_hdds_from_serverpartdeals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17rs5t8/thoughts_on_white_label_hdds_from_serverpartdeals/", "subreddit_subscribers": 711362, "created_utc": 1699578007.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}