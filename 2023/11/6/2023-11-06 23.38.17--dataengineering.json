{"kind": "Listing", "data": {"after": "t3_17paw3d", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "During my time in data engineering, I've noticed a lot of data engineers discount their own experience compared to software engineers who do not work in data.  Do a lot of data engineers not consider themselves a type of software engineer?\n\n\n\nI find that strange, because during my career I was able to do a lot of work in python, java, SQL, and Terraform.  I also have a lot of experience setting up CI/CD pipelines and building cloud infrastructure.  In many cases, I feel like our field overlaps a lot with backend engineering.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why don't a lot of data engineers consider themselves software engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17p20y6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 70, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 70, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699275064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;During my time in data engineering, I&amp;#39;ve noticed a lot of data engineers discount their own experience compared to software engineers who do not work in data.  Do a lot of data engineers not consider themselves a type of software engineer?&lt;/p&gt;\n\n&lt;p&gt;I find that strange, because during my career I was able to do a lot of work in python, java, SQL, and Terraform.  I also have a lot of experience setting up CI/CD pipelines and building cloud infrastructure.  In many cases, I feel like our field overlaps a lot with backend engineering.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17p20y6", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 89, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17p20y6/why_dont_a_lot_of_data_engineers_consider/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17p20y6/why_dont_a_lot_of_data_engineers_consider/", "subreddit_subscribers": 138252, "created_utc": 1699275064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pretty often while browsing this sub I see people put down 'low code/no code' data engineering solutions, and promoting ETL processes running on engines like Spark as opposed to transforming data in SQL. The reasons stated are usually for being easier to follow software engineering practices with 'real' code.\n\nHowever, it's undeniable that there exist cloud-based analytics warehouses that significantly outperform the traditional 'real code' frameworks when it comes to data transformation. I have transformations running in BigQuery where my testing has shown the costs of running on BQ to be about four or five times cheaper than with PySpark. I've read articles that back up this performance gap and it seems to be well accepted that tools like BigQuery significantly outperform traditional batch processing frameworks.\n\nOn a personal level I would prefer to be writing these transformations in Python - it's easier, it's cleaner, it's more versatile. With functions written in BigQuery we're usually ending up with long queries featuring a mess of joins that are hard to troubleshoot when things start going wrong.\n\nBut if I go to my manager and say \"I would rather write these transformations in Spark - it will be significantly more expensive but I will find it easier\" then I don't think it's going to go down well.\n\nSo I'd like to put it to this sub - if we have transformation processes that perform extremely well in BigQuery and our evidence shows that it would be significantly more expensive to move to other technologies, how could we justify moving away from a 'low code/no code' stack when it comes to transformations?", "author_fullname": "t2_2jmvd5bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shunning 'low code/no code' solutions in a world where analytics warehouses provide great performance.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ow2ku", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699249969.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty often while browsing this sub I see people put down &amp;#39;low code/no code&amp;#39; data engineering solutions, and promoting ETL processes running on engines like Spark as opposed to transforming data in SQL. The reasons stated are usually for being easier to follow software engineering practices with &amp;#39;real&amp;#39; code.&lt;/p&gt;\n\n&lt;p&gt;However, it&amp;#39;s undeniable that there exist cloud-based analytics warehouses that significantly outperform the traditional &amp;#39;real code&amp;#39; frameworks when it comes to data transformation. I have transformations running in BigQuery where my testing has shown the costs of running on BQ to be about four or five times cheaper than with PySpark. I&amp;#39;ve read articles that back up this performance gap and it seems to be well accepted that tools like BigQuery significantly outperform traditional batch processing frameworks.&lt;/p&gt;\n\n&lt;p&gt;On a personal level I would prefer to be writing these transformations in Python - it&amp;#39;s easier, it&amp;#39;s cleaner, it&amp;#39;s more versatile. With functions written in BigQuery we&amp;#39;re usually ending up with long queries featuring a mess of joins that are hard to troubleshoot when things start going wrong.&lt;/p&gt;\n\n&lt;p&gt;But if I go to my manager and say &amp;quot;I would rather write these transformations in Spark - it will be significantly more expensive but I will find it easier&amp;quot; then I don&amp;#39;t think it&amp;#39;s going to go down well.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;d like to put it to this sub - if we have transformation processes that perform extremely well in BigQuery and our evidence shows that it would be significantly more expensive to move to other technologies, how could we justify moving away from a &amp;#39;low code/no code&amp;#39; stack when it comes to transformations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ow2ku", "is_robot_indexable": true, "report_reasons": null, "author": "Objectionne", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ow2ku/shunning_low_codeno_code_solutions_in_a_world/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ow2ku/shunning_low_codeno_code_solutions_in_a_world/", "subreddit_subscribers": 138252, "created_utc": 1699249969.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious to learn about the landscape of data engineering certifications in 2023/24. \n\nIf you don't mind sharing your insights, I have some questions:\n\n1. In your opinion, which DE/ML certifications are highly regarded or considered valuable in the industry right now or will be in near future?\n2. What is your perspective on certifications from major cloud service providers like AWS, GCP, and Azure, in terms of their relevance and impact on a DE career? Which one would you recommend?\n3. Do niche-specific certifications (e.g., CDMP, Airflow, Snowflake, Databricks, etc) hold significant weight?\n4. Are there any other certifications or training programs you think are worth mentioning for those looking to advance in the field?\n\nThanks in advance for sharing your knowledge and experience!", "author_fullname": "t2_xf2t5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are useful data engineering certifications in 2023/24?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17paeyc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699297910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious to learn about the landscape of data engineering certifications in 2023/24. &lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t mind sharing your insights, I have some questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In your opinion, which DE/ML certifications are highly regarded or considered valuable in the industry right now or will be in near future?&lt;/li&gt;\n&lt;li&gt;What is your perspective on certifications from major cloud service providers like AWS, GCP, and Azure, in terms of their relevance and impact on a DE career? Which one would you recommend?&lt;/li&gt;\n&lt;li&gt;Do niche-specific certifications (e.g., CDMP, Airflow, Snowflake, Databricks, etc) hold significant weight?&lt;/li&gt;\n&lt;li&gt;Are there any other certifications or training programs you think are worth mentioning for those looking to advance in the field?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for sharing your knowledge and experience!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17paeyc", "is_robot_indexable": true, "report_reasons": null, "author": "luminoumen", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17paeyc/what_are_useful_data_engineering_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17paeyc/what_are_useful_data_engineering_certifications/", "subreddit_subscribers": 138252, "created_utc": 1699297910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! I am a business analyst and planning to have an initiative to have the business glossary and data dictionary, mostly my target audience will be the stakeholders, customers and non technical people.\n\nTo give you a background, my current project is a web based application, coded in java. So mostly the data validation is configured through java. In line with that, I want that to translate by creating the DD.\n\nData Dictionary that I'm planning to do is kind of web based application that connects to the git/java code, so every time that there are changes in field validations it automatically reflects in the DD.\n\nFor example;\nPersonal name : Varchar (20), minimum 10\n\nDo you have any recommendation tools that I can use?", "author_fullname": "t2_4z8wla8ty", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tool for creating Business Glossary and Data Dictionary", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17oygw9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699260371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I am a business analyst and planning to have an initiative to have the business glossary and data dictionary, mostly my target audience will be the stakeholders, customers and non technical people.&lt;/p&gt;\n\n&lt;p&gt;To give you a background, my current project is a web based application, coded in java. So mostly the data validation is configured through java. In line with that, I want that to translate by creating the DD.&lt;/p&gt;\n\n&lt;p&gt;Data Dictionary that I&amp;#39;m planning to do is kind of web based application that connects to the git/java code, so every time that there are changes in field validations it automatically reflects in the DD.&lt;/p&gt;\n\n&lt;p&gt;For example;\nPersonal name : Varchar (20), minimum 10&lt;/p&gt;\n\n&lt;p&gt;Do you have any recommendation tools that I can use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17oygw9", "is_robot_indexable": true, "report_reasons": null, "author": "Kizzmexoxo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17oygw9/best_tool_for_creating_business_glossary_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17oygw9/best_tool_for_creating_business_glossary_and_data/", "subreddit_subscribers": 138252, "created_utc": 1699260371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://blog.peerdb.io/building-a-streaming-platform-in-go-for-postgres](https://blog.peerdb.io/building-a-streaming-platform-in-go-for-postgres)  \n[PeerDB's](https://www.peerdb.io/) recent engineering blog on a design change that reduces replication latency/lag while streaming data from Postgres  from 30s to less than 5s.\n\nIf you are a Go u/golang developer you would find this intriguing. Would love to hear your feedback.", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Streaming Platform in Go for Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17papeg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699298679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://blog.peerdb.io/building-a-streaming-platform-in-go-for-postgres\"&gt;https://blog.peerdb.io/building-a-streaming-platform-in-go-for-postgres&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.peerdb.io/\"&gt;PeerDB&amp;#39;s&lt;/a&gt; recent engineering blog on a design change that reduces replication latency/lag while streaming data from Postgres  from 30s to less than 5s.&lt;/p&gt;\n\n&lt;p&gt;If you are a Go &lt;a href=\"/u/golang\"&gt;u/golang&lt;/a&gt; developer you would find this intriguing. Would love to hear your feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?auto=webp&amp;s=51e136315e28eae2332c64f9794907f59192e89a", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=013570b5f4313cd4e20708bd8e6ff743d645a35d", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8195282656de899f495044c52af47a835299b3ea", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=673beea3ef9d37b60709058fcc5b155096a9bb29", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bed1027ebb57891852ebc59bfffcd58f7db8477", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5337009152e44970850c78d69c1817b2ee0e8f2", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b82655640d39f01746c8924e49e16db89a4f2c0f", "width": 1080, "height": 720}], "variants": {}, "id": "jLylNeqzjTc-QQvl96AqP5DuMQdHOnWYRo1GHxGz9Ls"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17papeg", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17papeg/building_a_streaming_platform_in_go_for_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17papeg/building_a_streaming_platform_in_go_for_postgres/", "subreddit_subscribers": 138252, "created_utc": 1699298679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a data engineer for about 3 years now mainly with snowflake, azure ecosystem (azure sql and data factory), and databricks mostly building pipelines and ELTs. The companies I worked for were big enough to justify and afford these tools.\n\nI have a side business that offers parcel services. We use AWS lightsale to host our web app as well as our posgreSQL database. Some operational reporting is done directly through the app. So far the database is working with no issue. The biggest table we have has around 123k records.\n\nI have some experience with power bi and wanted to build some more strategic reports with more functionality and to also lift some weight from the developers having to develop all the reports. My idea was to create a new star schema within the same database. Power bi would point to this schema and it would refresh daily at a point in time where it's barely being used to avoid performance issues.\n\nI'm debating whether to just use views pointing to the existing tables or to create and load new tables. I've seen that this subreddit talks very well of dbt and I'm thinking of using it in this project to learn the tool and to maintain good practices.\n\nWhat are your thoughts on views vs tables?\n\nWould you recommend sharing the database for both transactional and analytical workloads if the business is small?\n\n At what point do you decide to separate the data warehouse?", "author_fullname": "t2_177meuof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At what point do you need a data warehouse for a small business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pahfg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699298103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a data engineer for about 3 years now mainly with snowflake, azure ecosystem (azure sql and data factory), and databricks mostly building pipelines and ELTs. The companies I worked for were big enough to justify and afford these tools.&lt;/p&gt;\n\n&lt;p&gt;I have a side business that offers parcel services. We use AWS lightsale to host our web app as well as our posgreSQL database. Some operational reporting is done directly through the app. So far the database is working with no issue. The biggest table we have has around 123k records.&lt;/p&gt;\n\n&lt;p&gt;I have some experience with power bi and wanted to build some more strategic reports with more functionality and to also lift some weight from the developers having to develop all the reports. My idea was to create a new star schema within the same database. Power bi would point to this schema and it would refresh daily at a point in time where it&amp;#39;s barely being used to avoid performance issues.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m debating whether to just use views pointing to the existing tables or to create and load new tables. I&amp;#39;ve seen that this subreddit talks very well of dbt and I&amp;#39;m thinking of using it in this project to learn the tool and to maintain good practices.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on views vs tables?&lt;/p&gt;\n\n&lt;p&gt;Would you recommend sharing the database for both transactional and analytical workloads if the business is small?&lt;/p&gt;\n\n&lt;p&gt;At what point do you decide to separate the data warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pahfg", "is_robot_indexable": true, "report_reasons": null, "author": "hershy08", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pahfg/at_what_point_do_you_need_a_data_warehouse_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pahfg/at_what_point_do_you_need_a_data_warehouse_for_a/", "subreddit_subscribers": 138252, "created_utc": 1699298103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've joined a small org as a BI analyst but have been asked to build a data platform with support from an external consultant.  \n\nConsultant wants to use Fabric for everything, I raised a concern (as advised by many here on r/DEng) that Fabric is not yet ready too use as a primary system. \n\nHe responded that only some of the components are new, and as many are rebranded (eg PowerBI is now under the fabric banner) and I shouldn't worry.\n\nSo which components of fabric (other than powerBI) are deployment ready?", "author_fullname": "t2_2s6myxsc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Fabric components are stable and deployment ready?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17oyhgt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699260450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve joined a small org as a BI analyst but have been asked to build a data platform with support from an external consultant.  &lt;/p&gt;\n\n&lt;p&gt;Consultant wants to use Fabric for everything, I raised a concern (as advised by many here on &lt;a href=\"/r/DEng\"&gt;r/DEng&lt;/a&gt;) that Fabric is not yet ready too use as a primary system. &lt;/p&gt;\n\n&lt;p&gt;He responded that only some of the components are new, and as many are rebranded (eg PowerBI is now under the fabric banner) and I shouldn&amp;#39;t worry.&lt;/p&gt;\n\n&lt;p&gt;So which components of fabric (other than powerBI) are deployment ready?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17oyhgt", "is_robot_indexable": true, "report_reasons": null, "author": "Dog_In_A_Human_Suit", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17oyhgt/which_fabric_components_are_stable_and_deployment/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17oyhgt/which_fabric_components_are_stable_and_deployment/", "subreddit_subscribers": 138252, "created_utc": 1699260450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a big fan and have been exploring adding it as our data warehouse given we're at a small scale. Does anyone use it at work/production environment? If so, how's your experience been with it?", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using duckDB at work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17pfhzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699310785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a big fan and have been exploring adding it as our data warehouse given we&amp;#39;re at a small scale. Does anyone use it at work/production environment? If so, how&amp;#39;s your experience been with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pfhzy", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pfhzy/is_anyone_using_duckdb_at_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pfhzy/is_anyone_using_duckdb_at_work/", "subreddit_subscribers": 138252, "created_utc": 1699310785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 3 years of experience in data engineering and soon going to start a job as a solutions architect.\n\nI know this is less experience and I see most of the jobs in solutions architecture require 7-8+ years of work experience. With that considered, what are some great books that I can read to increase my knowledge about solution architecture concepts?  \nAlso please feel free to post some things that I need to keep in mind to succeed.\n\n&amp;#x200B;\n\nThank you in advance", "author_fullname": "t2_8gpfnv45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for solutions architect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17p90ns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699294316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 years of experience in data engineering and soon going to start a job as a solutions architect.&lt;/p&gt;\n\n&lt;p&gt;I know this is less experience and I see most of the jobs in solutions architecture require 7-8+ years of work experience. With that considered, what are some great books that I can read to increase my knowledge about solution architecture concepts?&lt;br/&gt;\nAlso please feel free to post some things that I need to keep in mind to succeed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17p90ns", "is_robot_indexable": true, "report_reasons": null, "author": "ameya_b", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17p90ns/resources_for_solutions_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17p90ns/resources_for_solutions_architect/", "subreddit_subscribers": 138252, "created_utc": 1699294316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a junior data engineer and i perform an automated ETL system that extract data from multiple excel sheets, transform them and then load them into a destination database table, my problem is the excel sheets have different column names and different column orders from each other and from the destination database table, so i want to make the mapping process automated (not to create a mapping dictionary that specify the old column name with the new column name)", "author_fullname": "t2_g2wxrrado", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need solution", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17oxw83", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699257790.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a junior data engineer and i perform an automated ETL system that extract data from multiple excel sheets, transform them and then load them into a destination database table, my problem is the excel sheets have different column names and different column orders from each other and from the destination database table, so i want to make the mapping process automated (not to create a mapping dictionary that specify the old column name with the new column name)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17oxw83", "is_robot_indexable": true, "report_reasons": null, "author": "OtherCan6354", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17oxw83/need_solution/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17oxw83/need_solution/", "subreddit_subscribers": 138252, "created_utc": 1699257790.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible for a DE to transition into MLE? I have been learning about NLP and it\u2019s been so interesting, and I would really want to learn more about MLE works. Given that it\u2019s possible for a DE to transition into MLE, what topics should I study to get a shot?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE transition into MLE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ooyfj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699226980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible for a DE to transition into MLE? I have been learning about NLP and it\u2019s been so interesting, and I would really want to learn more about MLE works. Given that it\u2019s possible for a DE to transition into MLE, what topics should I study to get a shot?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ooyfj", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ooyfj/de_transition_into_mle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ooyfj/de_transition_into_mle/", "subreddit_subscribers": 138252, "created_utc": 1699226980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy y'all \ud83e\udd20\ud83d\udc4b  \n\n\nI'm an aspiring data engineer and have been self-teaching a number of skills such as SQL, python, and dbt. One of my main goals now is to build a custom ETL pipeline using python.   \n\n\nWhat are your recommendations for an orchestration tool? Looking for a package that can be used directly from python - trying to avoid a pre-package tool like Fivetran as my goal is to develop useful core DE skills.  \n\n\nI see Airflow mentioned frequently. I know there is a web-based GUI, but can it also be used entirely via python?", "author_fullname": "t2_y2dhb7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Recommendation for Orchestration Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pbgyn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699300662.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy y&amp;#39;all \ud83e\udd20\ud83d\udc4b  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m an aspiring data engineer and have been self-teaching a number of skills such as SQL, python, and dbt. One of my main goals now is to build a custom ETL pipeline using python.   &lt;/p&gt;\n\n&lt;p&gt;What are your recommendations for an orchestration tool? Looking for a package that can be used directly from python - trying to avoid a pre-package tool like Fivetran as my goal is to develop useful core DE skills.  &lt;/p&gt;\n\n&lt;p&gt;I see Airflow mentioned frequently. I know there is a web-based GUI, but can it also be used entirely via python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pbgyn", "is_robot_indexable": true, "report_reasons": null, "author": "creamycolslaw", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pbgyn/looking_for_recommendation_for_orchestration_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pbgyn/looking_for_recommendation_for_orchestration_tool/", "subreddit_subscribers": 138252, "created_utc": 1699300662.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_c75e0hjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RevenueCat's data-caching techniques for 1.2 billion daily API requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17pb8v6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zI2UAjcgjgXOnusnkYi3B4iriAw8k5_pb89HFqhubcs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699300066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "revenuecat.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.revenuecat.com/blog/engineering/data-caching-revenuecat/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?auto=webp&amp;s=43eab23811acd15223d93996fd35633e2fbede08", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd59eb2ec39c278d82b7ed31c8fbbb8056ae1ed8", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08b662c261dc255ff76779cca5853eb5bb50b9fd", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71261122fba8653aa61fcba55218e728e7a57b0b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=51f542c45d65577603a894b246f2a71c5246349f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd661d2bac40c183fbc2fa97fbcf4b67c4a0356d", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6c9eff7b95ec4134311407d4d018811e2e4d039", "width": 1080, "height": 567}], "variants": {}, "id": "YgEOOqqwm_nMXategwITapc1-ybJGxYTttj9Te9eN3k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17pb8v6", "is_robot_indexable": true, "report_reasons": null, "author": "PoolOpening6090", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pb8v6/revenuecats_datacaching_techniques_for_12_billion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.revenuecat.com/blog/engineering/data-caching-revenuecat/", "subreddit_subscribers": 138252, "created_utc": 1699300066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\n**TL;DR**\n\n*Looking for an ETL tool that bridges the gap between no-code prototyping and established engineering practices. Want it to auto-generate code from visual steps and run in a serverless context. Seeking input and opinions from the community* \n\nI've searched all over the internet but couldn't find a tool of the following kind (dagster + dbt is close though).\n\n**Problem**:\n\nI work with various clients that start in a no-code environment (Alteryx, Informatica, Knime, etc.), but then they want to move to something else, either because the data has grown in size or they want a more stable and customizable workflow. This could involve building a Python library with testable components hosted on their side or at least sql scripts.\n\nI was wondering if there is an ETL tool that could bridge the gap between quick prototyping and clicking around, and established engineering practices. For example, an analyst starts a project as a no-code one, but behind every click on a step (filter, group by, etc.), the platform generates actual .py files or .sql files and prepares/structures the project so that when a more technical person takes over, they can switch to seeing the generated files, folders, connections instead of the visual aspect of the project. The technical team can then work alongside the analyst and adding new files/folders will automatically render the steps, tables, and so on.\n\nIdeally, this service would be sold as a managed service on the customer's cloud (similar to Databricks) rather than on a separate platform. Each table would be one file executed in a serverless context with the following flow: send the code to the cloud container service, execute it, save the table on the blob, and proceed to the next step.\n\n**Issues with current tools**:\n\n* Dagster + dbt seem to be close to what I envision because of the simplicity of syntax and autogeneration of DAGs. The only issues are that it lacks the visual no-code part, and to set it up with serverless execution, you would need some configuring and self-host it on a K8s cluster or create your own step launcher.\n* Databricks covers the ease of getting started: add your service, create your workflow, choose your cluster, and run it. I understand that it's intended for big data, but I'm adding it as a perspective on the end-user experience. I would like, in addition to VM provisioning, to also be able to spin up bare containers without the Spark overhead.\n\nDoes this concept resonate with anyone and can recommend something on the market? Also, what are your personal thoughts on this idea? Am I overthinking it or do you think it may be useful? ", "author_fullname": "t2_4l3aonbn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serverless ETL tool as managed service on Azure/AWS/GCP - combination of no code solution, Dagster and Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17p9nba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699295942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Looking for an ETL tool that bridges the gap between no-code prototyping and established engineering practices. Want it to auto-generate code from visual steps and run in a serverless context. Seeking input and opinions from the community&lt;/em&gt; &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve searched all over the internet but couldn&amp;#39;t find a tool of the following kind (dagster + dbt is close though).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;I work with various clients that start in a no-code environment (Alteryx, Informatica, Knime, etc.), but then they want to move to something else, either because the data has grown in size or they want a more stable and customizable workflow. This could involve building a Python library with testable components hosted on their side or at least sql scripts.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there is an ETL tool that could bridge the gap between quick prototyping and clicking around, and established engineering practices. For example, an analyst starts a project as a no-code one, but behind every click on a step (filter, group by, etc.), the platform generates actual .py files or .sql files and prepares/structures the project so that when a more technical person takes over, they can switch to seeing the generated files, folders, connections instead of the visual aspect of the project. The technical team can then work alongside the analyst and adding new files/folders will automatically render the steps, tables, and so on.&lt;/p&gt;\n\n&lt;p&gt;Ideally, this service would be sold as a managed service on the customer&amp;#39;s cloud (similar to Databricks) rather than on a separate platform. Each table would be one file executed in a serverless context with the following flow: send the code to the cloud container service, execute it, save the table on the blob, and proceed to the next step.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Issues with current tools&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Dagster + dbt seem to be close to what I envision because of the simplicity of syntax and autogeneration of DAGs. The only issues are that it lacks the visual no-code part, and to set it up with serverless execution, you would need some configuring and self-host it on a K8s cluster or create your own step launcher.&lt;/li&gt;\n&lt;li&gt;Databricks covers the ease of getting started: add your service, create your workflow, choose your cluster, and run it. I understand that it&amp;#39;s intended for big data, but I&amp;#39;m adding it as a perspective on the end-user experience. I would like, in addition to VM provisioning, to also be able to spin up bare containers without the Spark overhead.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Does this concept resonate with anyone and can recommend something on the market? Also, what are your personal thoughts on this idea? Am I overthinking it or do you think it may be useful? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17p9nba", "is_robot_indexable": true, "report_reasons": null, "author": "_randomymous_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17p9nba/serverless_etl_tool_as_managed_service_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17p9nba/serverless_etl_tool_as_managed_service_on/", "subreddit_subscribers": 138252, "created_utc": 1699295942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, I was tasked with a certain requirement as the title suggests, I need to push/export the bigquery tables via API so the client can fetch the data via same API calls. I tried checking via official documentation but except granting permissions or exporting to GCS buckets I couldn\u2019t come up with anything. \n\nPlease guide me if I\u2019ve missed out something. Open to suggestions on the same.\n\nThank you", "author_fullname": "t2_3qc9b4bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exporting BigQuery Tables via API", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17owput", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699252592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I was tasked with a certain requirement as the title suggests, I need to push/export the bigquery tables via API so the client can fetch the data via same API calls. I tried checking via official documentation but except granting permissions or exporting to GCS buckets I couldn\u2019t come up with anything. &lt;/p&gt;\n\n&lt;p&gt;Please guide me if I\u2019ve missed out something. Open to suggestions on the same.&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17owput", "is_robot_indexable": true, "report_reasons": null, "author": "apache444", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17owput/exporting_bigquery_tables_via_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17owput/exporting_bigquery_tables_via_api/", "subreddit_subscribers": 138252, "created_utc": 1699252592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learning pyspark and would be great to know if I can optimise this even more further.\n\nI have followed this [S/O post](https://stackoverflow.com/questions/69002085/populate-a-column-based-on-previous-value-and-row-pyspark) and was able to optimize one of the process but still Im struggling to figure out is there a better way to solve the below scenario.  \n\n\nThe dataframe will be like this \n\n&amp;#x200B;\n\n|Date|Date 2|Partition|\n|:-|:-|:-|\n| 6 Nov   |6 Nov  |1|\n|6 Nov|7 Nov|2|\n|7 Nov|7 Nov|2|\n|6 Nov|8 Nov|3|\n|7 Nov|8 Nov|3|\n|8 Nov|8 Nov|3|\n\n&amp;#x200B;\n\nPartition here is just a group of data and whatever the calculation that is required to generate the columns will be based on from those current partition.\n\n&gt;while (!threshold) {  \n&gt;  \n&gt;price =+ 25;  \n&gt;  \n&gt;result = // consider an operation like the one mentioned in the previous S/O link is happening here  \n&gt;  \n&gt;// few more calculations which will generate few columns  \n&gt;  \n&gt;if (result &lt; someValue) {   \n&gt;  \n&gt;populate one column here          \n&gt;  \n&gt;} else {  \n&gt;  \n&gt;flips the threshold  \n}\n\nFrom the above while loop we are interested in the final price and the aggregated values and the generated values in the final iteration of the while loop all intermediate results are overwritten or not used.\n\nRight now I have implemented this with while loop in spark and used \\`\\`\\`\\`when / otherwise\\`\\`\\`\\` to do all those if conditions\n\n&amp;#x200B;\n\nI want to know is there any other better spark way to do this, since before knowing the attached S/O implementation my other operation was taking few minutes to complete but now its a breeze.\n\n&amp;#x200B;\n\nAn idea or some reference links to look or concepts to learn would be even more appreciated.\n\nThanks in advance.", "author_fullname": "t2_6oj5b6d9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a better way to write / optimize this loop in pyspark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pa515", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699297203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learning pyspark and would be great to know if I can optimise this even more further.&lt;/p&gt;\n\n&lt;p&gt;I have followed this &lt;a href=\"https://stackoverflow.com/questions/69002085/populate-a-column-based-on-previous-value-and-row-pyspark\"&gt;S/O post&lt;/a&gt; and was able to optimize one of the process but still Im struggling to figure out is there a better way to solve the below scenario.  &lt;/p&gt;\n\n&lt;p&gt;The dataframe will be like this &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Date&lt;/th&gt;\n&lt;th align=\"left\"&gt;Date 2&lt;/th&gt;\n&lt;th align=\"left\"&gt;Partition&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;6 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;7 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;7 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;8 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;8 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;8 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;8 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Partition here is just a group of data and whatever the calculation that is required to generate the columns will be based on from those current partition.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;while (!threshold) {  &lt;/p&gt;\n\n&lt;p&gt;price =+ 25;  &lt;/p&gt;\n\n&lt;p&gt;result = // consider an operation like the one mentioned in the previous S/O link is happening here  &lt;/p&gt;\n\n&lt;p&gt;// few more calculations which will generate few columns  &lt;/p&gt;\n\n&lt;p&gt;if (result &amp;lt; someValue) {   &lt;/p&gt;\n\n&lt;p&gt;populate one column here          &lt;/p&gt;\n\n&lt;p&gt;} else {  &lt;/p&gt;\n\n&lt;p&gt;flips the threshold&lt;br/&gt;\n}&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;From the above while loop we are interested in the final price and the aggregated values and the generated values in the final iteration of the while loop all intermediate results are overwritten or not used.&lt;/p&gt;\n\n&lt;p&gt;Right now I have implemented this with while loop in spark and used ````when / otherwise```` to do all those if conditions&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I want to know is there any other better spark way to do this, since before knowing the attached S/O implementation my other operation was taking few minutes to complete but now its a breeze.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;An idea or some reference links to look or concepts to learn would be even more appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?auto=webp&amp;s=a70d21ce9f01f64670d2200ca9fc3f39b94a7e48", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aad06750c23b98c9b7595343a8b54a42dc18851", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b66126834977e269be586d07464046049ed09138", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pa515", "is_robot_indexable": true, "report_reasons": null, "author": "zee_wild_runner", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pa515/is_there_a_better_way_to_write_optimize_this_loop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pa515/is_there_a_better_way_to_write_optimize_this_loop/", "subreddit_subscribers": 138252, "created_utc": 1699297203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,  \n\n\nI am working on a Pipeline in Azure Data Factory which gets the last modified date data from ADLG2 &amp; On-prem storage.  I have a situation where multiple files are uploaded to the container with a difference of seconds and minutes.  \n\n\nThe issue is all these files are meant to arrive at the same time so with the issue of different time upload and want to create a function that ignores the Timestamp usually generate from \"Last Modified\" date and just consider only the Date (yyyy-MM-dd).\n\nI would love if anyone can help me because this is really a big issue for me. Also links to materials will help.  \n\n\nAlso in a scenario where let say 5-10 files enters the same time using the Last Modified Date activity in ADF will it pick all files.  \n\n\nThanks SOS.", "author_fullname": "t2_6fwa4j9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get Date only using the Azure Data Factory Get Meta Data (Last Modified)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17p52e3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699283946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,  &lt;/p&gt;\n\n&lt;p&gt;I am working on a Pipeline in Azure Data Factory which gets the last modified date data from ADLG2 &amp;amp; On-prem storage.  I have a situation where multiple files are uploaded to the container with a difference of seconds and minutes.  &lt;/p&gt;\n\n&lt;p&gt;The issue is all these files are meant to arrive at the same time so with the issue of different time upload and want to create a function that ignores the Timestamp usually generate from &amp;quot;Last Modified&amp;quot; date and just consider only the Date (yyyy-MM-dd).&lt;/p&gt;\n\n&lt;p&gt;I would love if anyone can help me because this is really a big issue for me. Also links to materials will help.  &lt;/p&gt;\n\n&lt;p&gt;Also in a scenario where let say 5-10 files enters the same time using the Last Modified Date activity in ADF will it pick all files.  &lt;/p&gt;\n\n&lt;p&gt;Thanks SOS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17p52e3", "is_robot_indexable": true, "report_reasons": null, "author": "kiddojazz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17p52e3/how_to_get_date_only_using_the_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17p52e3/how_to_get_date_only_using_the_azure_data_factory/", "subreddit_subscribers": 138252, "created_utc": 1699283946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Build Data Products | Evolve: Part 4/4 Advanced SLOs, Feedback Loops, Optimised Data Product, and more!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17p21jc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/CPo-qUNcpXogigmEZh7oJ4UqfS2IXKs7kEMtJFETld0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699275121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/evolving-data-products", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Rop2sdhd_CVzU2H4lV1I2lzz8cC_jk7A9wp3KZeZnkc.jpg?auto=webp&amp;s=1221a1dce31f3afe76700003ecc0b1ddb6be9b66", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Rop2sdhd_CVzU2H4lV1I2lzz8cC_jk7A9wp3KZeZnkc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=65be0a310cfc5f7368c888c1638e9db48dc08a1f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Rop2sdhd_CVzU2H4lV1I2lzz8cC_jk7A9wp3KZeZnkc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=84058159ac22b55a3f202ff5dc450404dbb2c6b3", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Rop2sdhd_CVzU2H4lV1I2lzz8cC_jk7A9wp3KZeZnkc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b88b81f6bd64d16a333eaa562695fb58dd2d057", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Rop2sdhd_CVzU2H4lV1I2lzz8cC_jk7A9wp3KZeZnkc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d6bfb73a928498e52dc9fa0684118e28d75f4c2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Rop2sdhd_CVzU2H4lV1I2lzz8cC_jk7A9wp3KZeZnkc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8de6d0063f9da38114ffd20967e62af2a6f8111a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Rop2sdhd_CVzU2H4lV1I2lzz8cC_jk7A9wp3KZeZnkc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c6b4f396c1c77a4b36e6b36f63fb6a313090f653", "width": 1080, "height": 540}], "variants": {}, "id": "XJZ-DYAnwfbcnEScRfgAlIRNXlSes7wfrL03G0CXFcI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17p21jc", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17p21jc/how_to_build_data_products_evolve_part_44/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/evolving-data-products", "subreddit_subscribers": 138252, "created_utc": 1699275121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some of your favorite tools for making dbt Core development easier?", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are good tools for working with dbt Core?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17pfavc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699310276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some of your favorite tools for making dbt Core development easier?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pfavc", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pfavc/what_are_good_tools_for_working_with_dbt_core/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pfavc/what_are_good_tools_for_working_with_dbt_core/", "subreddit_subscribers": 138252, "created_utc": 1699310276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was often asked what columns mean and which tables the data lives in. I built a tool that integrates with your GitHub and DBs to automatically document and map all your data on a feature level for business stakeholders (and also scans code commits to re-update your data docs so docs are never outdated). \n\nAsking the community here for some feedback. Here's a first draft of what was built to map code to data tables. \n\nhttps://preview.redd.it/akb4p4waxsyb1.png?width=2680&amp;format=png&amp;auto=webp&amp;s=6600604dc3ecd3b743c02caae7ab39ff2726295b\n\nhttps://preview.redd.it/lsoy9hvfxsyb1.png?width=2572&amp;format=png&amp;auto=webp&amp;s=0347643958e185484087f072d842d814d0d01c46", "author_fullname": "t2_cqdfu7nf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Product Feedback: Automatic data documentation for business stakeholders (Pics attached)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 74, "top_awarded_type": null, "hide_score": true, "media_metadata": {"lsoy9hvfxsyb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/lsoy9hvfxsyb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=18cabd7bdef3abc887955ca36b99366e3828f81f"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/lsoy9hvfxsyb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=031a05d0ef23ca6282789409b3ebeaac002c0cea"}, {"y": 164, "x": 320, "u": "https://preview.redd.it/lsoy9hvfxsyb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f482e3301cb09c0f51b68897a1540712a2251ab1"}, {"y": 329, "x": 640, "u": "https://preview.redd.it/lsoy9hvfxsyb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=596a9a4eac27c7eb99eb9923beeb6e4eaf797463"}, {"y": 494, "x": 960, "u": "https://preview.redd.it/lsoy9hvfxsyb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=30711e746ce40b141ee9b018ae458ed0268a7f86"}, {"y": 556, "x": 1080, "u": "https://preview.redd.it/lsoy9hvfxsyb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35fbc99cd02d79bd832794e1506ea9c74b92f058"}], "s": {"y": 1326, "x": 2572, "u": "https://preview.redd.it/lsoy9hvfxsyb1.png?width=2572&amp;format=png&amp;auto=webp&amp;s=0347643958e185484087f072d842d814d0d01c46"}, "id": "lsoy9hvfxsyb1"}, "akb4p4waxsyb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 57, "x": 108, "u": "https://preview.redd.it/akb4p4waxsyb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6f2cf1855664ce752e3915a0d8cf06ee1a398d3d"}, {"y": 115, "x": 216, "u": "https://preview.redd.it/akb4p4waxsyb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9b93da1d536673ce43029a50c2e4207e1638ed2c"}, {"y": 170, "x": 320, "u": "https://preview.redd.it/akb4p4waxsyb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c8f81a4a2f19fe8555ed978b4d7e0d07a540260c"}, {"y": 341, "x": 640, "u": "https://preview.redd.it/akb4p4waxsyb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=70c6361f7bd0b72a5e6ec9eafbf535def404e568"}, {"y": 512, "x": 960, "u": "https://preview.redd.it/akb4p4waxsyb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a8f3b849108badf2e13a788dfa98ac274ba3bd05"}, {"y": 576, "x": 1080, "u": "https://preview.redd.it/akb4p4waxsyb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=21b8a4d5fbc16292b3a61eadf3d1885f50a35bd4"}], "s": {"y": 1430, "x": 2680, "u": "https://preview.redd.it/akb4p4waxsyb1.png?width=2680&amp;format=png&amp;auto=webp&amp;s=6600604dc3ecd3b743c02caae7ab39ff2726295b"}, "id": "akb4p4waxsyb1"}}, "name": "t3_17pesv5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/fsmmLM1HzNbNhpRDKwk3RFjYB8eEK4FyDkGPc5KvB74.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699309001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was often asked what columns mean and which tables the data lives in. I built a tool that integrates with your GitHub and DBs to automatically document and map all your data on a feature level for business stakeholders (and also scans code commits to re-update your data docs so docs are never outdated). &lt;/p&gt;\n\n&lt;p&gt;Asking the community here for some feedback. Here&amp;#39;s a first draft of what was built to map code to data tables. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/akb4p4waxsyb1.png?width=2680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6600604dc3ecd3b743c02caae7ab39ff2726295b\"&gt;https://preview.redd.it/akb4p4waxsyb1.png?width=2680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6600604dc3ecd3b743c02caae7ab39ff2726295b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/lsoy9hvfxsyb1.png?width=2572&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0347643958e185484087f072d842d814d0d01c46\"&gt;https://preview.redd.it/lsoy9hvfxsyb1.png?width=2572&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0347643958e185484087f072d842d814d0d01c46&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pesv5", "is_robot_indexable": true, "report_reasons": null, "author": "Different-General700", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pesv5/product_feedback_automatic_data_documentation_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pesv5/product_feedback_automatic_data_documentation_for/", "subreddit_subscribers": 138252, "created_utc": 1699309001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which would you say is the most marketable (and you could only choose one track)?\n\nI\u2019m trying to decide where to invest my time. I am familiar with Python, SQL, etc. But should I focus on learning Kubenetes, AWS, Azure, etc?\n\nThanks!", "author_fullname": "t2_cz3wp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Code vs. No Code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17peewm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699308051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which would you say is the most marketable (and you could only choose one track)?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to decide where to invest my time. I am familiar with Python, SQL, etc. But should I focus on learning Kubenetes, AWS, Azure, etc?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17peewm", "is_robot_indexable": true, "report_reasons": null, "author": "anewguy03", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17peewm/code_vs_no_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17peewm/code_vs_no_code/", "subreddit_subscribers": 138252, "created_utc": 1699308051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone here have a system for versioning database environments? I know there are some paid services out there for doing this which I am open to hearing about, but would need very strong conviction to actually propose them to the team. I have also seen some systems where they essentially tear down and remake the database functions/stored procs from scratch with every deployment-- which seems like a valid approach, if not a little overkill.\n\nEssentially the situation is this, we have several redshift severless workgroups corresponding to different environments local, dev, test, prod. These databases are connected to their upstream versioned corresponding ETL processes. Often, I want to implement a stored proc or user-defined function or something in these databases, but the issue is deploying and keeping everything in sync. The current \"deployment\" method is just running the create statement in those in all different environments. This a little tedious, doesn't enforce consistency across the environments, and just feels bad. \n\nIdeally, what I would like is a form of ci/cd + version control where I can make a change to the local database (e.g. define/change a stored proc) and then press a button and have that change replicated through the higher envs with some sort of logging in place. There is no current need to version data or data models.", "author_fullname": "t2_kdj5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Versioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17pe4up", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699307345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone here have a system for versioning database environments? I know there are some paid services out there for doing this which I am open to hearing about, but would need very strong conviction to actually propose them to the team. I have also seen some systems where they essentially tear down and remake the database functions/stored procs from scratch with every deployment-- which seems like a valid approach, if not a little overkill.&lt;/p&gt;\n\n&lt;p&gt;Essentially the situation is this, we have several redshift severless workgroups corresponding to different environments local, dev, test, prod. These databases are connected to their upstream versioned corresponding ETL processes. Often, I want to implement a stored proc or user-defined function or something in these databases, but the issue is deploying and keeping everything in sync. The current &amp;quot;deployment&amp;quot; method is just running the create statement in those in all different environments. This a little tedious, doesn&amp;#39;t enforce consistency across the environments, and just feels bad. &lt;/p&gt;\n\n&lt;p&gt;Ideally, what I would like is a form of ci/cd + version control where I can make a change to the local database (e.g. define/change a stored proc) and then press a button and have that change replicated through the higher envs with some sort of logging in place. There is no current need to version data or data models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pe4up", "is_robot_indexable": true, "report_reasons": null, "author": "Touvejs", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pe4up/database_versioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pe4up/database_versioning/", "subreddit_subscribers": 138252, "created_utc": 1699307345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a test tool that compares data stored in SQL tables from multiple sources, some on-premise, some in the cloud. The connection strings, test configurations and test results are all stored inside SQL tables. The tool consists of a backend, a frontend and several Python scripts for computation. The Python is called from a batch script arranged by the Windows scheduler look at a test configuration table every minute to run scheduled test suites at that minute.\n\nWhat would be the best service/approach to move this tool using Azure services? The main aim is just to escape from the Windows scheduler and make it into cloud.\n\nThank you!", "author_fullname": "t2_5eskxtx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating an automated web App with Python scripts and connection to on-prem DB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pdmin", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699306099.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a test tool that compares data stored in SQL tables from multiple sources, some on-premise, some in the cloud. The connection strings, test configurations and test results are all stored inside SQL tables. The tool consists of a backend, a frontend and several Python scripts for computation. The Python is called from a batch script arranged by the Windows scheduler look at a test configuration table every minute to run scheduled test suites at that minute.&lt;/p&gt;\n\n&lt;p&gt;What would be the best service/approach to move this tool using Azure services? The main aim is just to escape from the Windows scheduler and make it into cloud.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pdmin", "is_robot_indexable": true, "report_reasons": null, "author": "ashmapleleaf", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pdmin/migrating_an_automated_web_app_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pdmin/migrating_an_automated_web_app_with_python/", "subreddit_subscribers": 138252, "created_utc": 1699306099.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4679pe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Run dbt with GitHub Actions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17pbu4y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5afuFUdK5c65B9oJzw2zRWdRyMBcUeQlEb9VH1py1vo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699301599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dbtips.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dbtips.substack.com/p/run-dbt-with-github-actions", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/47GkapDxIYifZZ1DCYnmoZ_FPBYYdZ4EDBZOLVAarjY.jpg?auto=webp&amp;s=5135569cf181cc047a9f3d28b46b0ddb7ea93cff", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/47GkapDxIYifZZ1DCYnmoZ_FPBYYdZ4EDBZOLVAarjY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a863c143d47028c6b313a16896e25bc67e3d9a5", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/47GkapDxIYifZZ1DCYnmoZ_FPBYYdZ4EDBZOLVAarjY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1140e7575e00732f05a4119e5ade3621733d4a16", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/47GkapDxIYifZZ1DCYnmoZ_FPBYYdZ4EDBZOLVAarjY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8904499e68052a14172abd671852e100de0764fb", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/47GkapDxIYifZZ1DCYnmoZ_FPBYYdZ4EDBZOLVAarjY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a0a7a47ecaf36b56c3fe61efa32a655e03fe51a5", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/47GkapDxIYifZZ1DCYnmoZ_FPBYYdZ4EDBZOLVAarjY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4fe5ffdf861f47345ee5843245c9e8c7476e93cb", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/47GkapDxIYifZZ1DCYnmoZ_FPBYYdZ4EDBZOLVAarjY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8794ec66b041d42c0532de045f7942a195e1c332", "width": 1080, "height": 540}], "variants": {}, "id": "nUxSwewqJso6xGXS9UI25rIQNvyeiTNtwyr2jltJljw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17pbu4y", "is_robot_indexable": true, "report_reasons": null, "author": "oleg_agapov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pbu4y/run_dbt_with_github_actions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dbtips.substack.com/p/run-dbt-with-github-actions", "subreddit_subscribers": 138252, "created_utc": 1699301599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_s3s2xpr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatic database change tracking with Bemi", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17paw3d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xMyrMOtivegnpi9uXU43_U-Oy9_g12YdZEqf5mBnLuA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699299156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bemi.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://bemi.io", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/x8ukifSZdRByod-RJi3daE-L2Vnl3zeVJNjo3YkP_20.jpg?auto=webp&amp;s=7c6ab18148ce84fe336720c19f5ef4b9f5cc2aa5", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/x8ukifSZdRByod-RJi3daE-L2Vnl3zeVJNjo3YkP_20.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dd96790cf3fa2dff690037fe557eb3b37f4be69b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/x8ukifSZdRByod-RJi3daE-L2Vnl3zeVJNjo3YkP_20.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=90657527360b975cbea4f78cd951dd6920d075ae", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/x8ukifSZdRByod-RJi3daE-L2Vnl3zeVJNjo3YkP_20.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e7d8bfac4fd1d40e243b5d78bc0a6cc582ef06b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/x8ukifSZdRByod-RJi3daE-L2Vnl3zeVJNjo3YkP_20.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=93a8f5b4d74f31a3de081d0a32e6963ddde365b6", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/x8ukifSZdRByod-RJi3daE-L2Vnl3zeVJNjo3YkP_20.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a0faaf624d123e05106e09e396d43bafaeedcd44", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/x8ukifSZdRByod-RJi3daE-L2Vnl3zeVJNjo3YkP_20.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b9c9d7da04c8b8cfbce79a94cb15835e0eeb8c51", "width": 1080, "height": 567}], "variants": {}, "id": "9fzOuHSY0Ha5CBeS5O60tfgpNBCxukJVuCqR1TP0ZlU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17paw3d", "is_robot_indexable": true, "report_reasons": null, "author": "More_Champion_7731", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17paw3d/automatic_database_change_tracking_with_bemi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://bemi.io", "subreddit_subscribers": 138252, "created_utc": 1699299156.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}