{"kind": "Listing", "data": {"after": "t3_17u6us0", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically the title.\n\nCurrently I\u2019m an Azure ETL developer and want to break into real time streaming jobs. I need all your advices to transition from no code to software development jobs.", "author_fullname": "t2_n5fep10f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "People that transitioned for no-code/low code to databricks/spark/python sw development: how did you do it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uc34w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699886485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically the title.&lt;/p&gt;\n\n&lt;p&gt;Currently I\u2019m an Azure ETL developer and want to break into real time streaming jobs. I need all your advices to transition from no code to software development jobs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17uc34w", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous-Bat-7812", "discussion_type": null, "num_comments": 51, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uc34w/people_that_transitioned_for_nocodelow_code_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uc34w/people_that_transitioned_for_nocodelow_code_to/", "subreddit_subscribers": 139596, "created_utc": 1699886485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been in my role as a junior engineer for over a year now. It's dawned on me that coding/programming is not for me. But I'm stuck. I still want to be in tech, but not sure which areas I can go into. Project management type roles seem interesting, but I have no idea where to begin.\n\nWhich careers/roles, do you advise and how would navigate myself to get a job in that role? Thank you all!", "author_fullname": "t2_gf4hocwmk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering is not for me. Please advise!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17udkju", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699890644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been in my role as a junior engineer for over a year now. It&amp;#39;s dawned on me that coding/programming is not for me. But I&amp;#39;m stuck. I still want to be in tech, but not sure which areas I can go into. Project management type roles seem interesting, but I have no idea where to begin.&lt;/p&gt;\n\n&lt;p&gt;Which careers/roles, do you advise and how would navigate myself to get a job in that role? Thank you all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17udkju", "is_robot_indexable": true, "report_reasons": null, "author": "akhi960", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17udkju/data_engineering_is_not_for_me_please_advise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17udkju/data_engineering_is_not_for_me_please_advise/", "subreddit_subscribers": 139596, "created_utc": 1699890644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m looking for insight on the technical limits of doing aggregate queries in OLTP systems like MySQL or psql. Figuring an example might be easy to understand. Trying to go a little deeper than the marketing material. \n\nI know it\u2019s probably possible. But what makes OLAPs suited for the purpose? \n\nThanks for any pointers.", "author_fullname": "t2_2tu8n7l9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What sort of query is possible in redshift/BQ/ClickHouse that isn\u2019t feasible/cheap in OLTPs like PostgreSQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uay2v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699883098.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for insight on the technical limits of doing aggregate queries in OLTP systems like MySQL or psql. Figuring an example might be easy to understand. Trying to go a little deeper than the marketing material. &lt;/p&gt;\n\n&lt;p&gt;I know it\u2019s probably possible. But what makes OLAPs suited for the purpose? &lt;/p&gt;\n\n&lt;p&gt;Thanks for any pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17uay2v", "is_robot_indexable": true, "report_reasons": null, "author": "Firm_Bit", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uay2v/what_sort_of_query_is_possible_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uay2v/what_sort_of_query_is_possible_in/", "subreddit_subscribers": 139596, "created_utc": 1699883098.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey folks, I am the cofounder of [dlt](https://pypi.org/project/dlt/), the opensource data loading library.\n\nWe are looking into supporting databricks and we have some doubts about which loading method to best use.\n\ndlt can take semi structured data and structure it to parquet or sql with schema evolution. So we can already output parquet to s3\n\nWe are looking into what's the best way for loading into databricks ecosystem.\n\n**Question**: The docs talk about these modes: [COPY INTO](https://docs.databricks.com/en/ingestion/copy-into/index.html) or [AutoLoader](https://docs.databricks.com/en/ingestion/auto-loader/index.html). Which one do you use and why?  We could support any of the patterns, and we would like to understand which ones are used, useful and why.\n\nIf you want to give feedback directly in github, our ticket is here [https://github.com/dlt-hub/dlt/issues/762](https://github.com/dlt-hub/dlt/issues/762)", "author_fullname": "t2_uamr9xer", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you load to databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17u90tc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1699876481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I am the cofounder of &lt;a href=\"https://pypi.org/project/dlt/\"&gt;dlt&lt;/a&gt;, the opensource data loading library.&lt;/p&gt;\n\n&lt;p&gt;We are looking into supporting databricks and we have some doubts about which loading method to best use.&lt;/p&gt;\n\n&lt;p&gt;dlt can take semi structured data and structure it to parquet or sql with schema evolution. So we can already output parquet to s3&lt;/p&gt;\n\n&lt;p&gt;We are looking into what&amp;#39;s the best way for loading into databricks ecosystem.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: The docs talk about these modes: &lt;a href=\"https://docs.databricks.com/en/ingestion/copy-into/index.html\"&gt;COPY INTO&lt;/a&gt; or &lt;a href=\"https://docs.databricks.com/en/ingestion/auto-loader/index.html\"&gt;AutoLoader&lt;/a&gt;. Which one do you use and why?  We could support any of the patterns, and we would like to understand which ones are used, useful and why.&lt;/p&gt;\n\n&lt;p&gt;If you want to give feedback directly in github, our ticket is here &lt;a href=\"https://github.com/dlt-hub/dlt/issues/762\"&gt;https://github.com/dlt-hub/dlt/issues/762&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MoP6enMQ2Q6o4o23d5xCmvlBtpeCXWiqxc63UVCX5Rk.jpg?auto=webp&amp;s=85f19a22cbd85fa784cdb417359d8ff7cda9e394", "width": 300, "height": 300}, "resolutions": [{"url": "https://external-preview.redd.it/MoP6enMQ2Q6o4o23d5xCmvlBtpeCXWiqxc63UVCX5Rk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=46fa55dd1b1e587ab93bcbbdc6cb2de37b810bf3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/MoP6enMQ2Q6o4o23d5xCmvlBtpeCXWiqxc63UVCX5Rk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cfd7f76ac4c13cdc287edd9856ef0430dbc862a5", "width": 216, "height": 216}], "variants": {}, "id": "IUHM4ctLZQorzkPuYJ4IkGSag8BtaIqZoyqL1L53KuM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17u90tc", "is_robot_indexable": true, "report_reasons": null, "author": "Thinker_Assignment", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17u90tc/how_do_you_load_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17u90tc/how_do_you_load_to_databricks/", "subreddit_subscribers": 139596, "created_utc": 1699876481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am confused \ud83d\ude15 in choosing the right role in IT. I did my bachelor in electrical 1.5 years back but want to switch to IT. I did nothing in between tried doing business but failed miserably. Preparing for MS now. But confused.\nI know basic of SQL and Python and that's it.\n\nFirst i wanted to go into data science but I found out that entry level job is hard to get there. \nOther options for me is either data engineer,cloud engineer or software developer.\n\nI am looking For a role that has high salary and less competition, unlike data science.", "author_fullname": "t2_7vkbdldt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which role in IT has good salary and less compitition?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17urs2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699928288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am confused \ud83d\ude15 in choosing the right role in IT. I did my bachelor in electrical 1.5 years back but want to switch to IT. I did nothing in between tried doing business but failed miserably. Preparing for MS now. But confused.\nI know basic of SQL and Python and that&amp;#39;s it.&lt;/p&gt;\n\n&lt;p&gt;First i wanted to go into data science but I found out that entry level job is hard to get there. \nOther options for me is either data engineer,cloud engineer or software developer.&lt;/p&gt;\n\n&lt;p&gt;I am looking For a role that has high salary and less competition, unlike data science.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17urs2s", "is_robot_indexable": true, "report_reasons": null, "author": "Munib_raza_khan", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17urs2s/which_role_in_it_has_good_salary_and_less/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17urs2s/which_role_in_it_has_good_salary_and_less/", "subreddit_subscribers": 139596, "created_utc": 1699928288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How many data engineers work on the same project/team as you? Put another way, how many developers could conceivably pick up the same tickets you do?\n\nDo you feel your team/project is understaffed, overstaffed, or at just the right level in terms of data engineers?\n\nI\u2019ll start: I oversee a team of four data engineers spread across three data warehouses. Warehouse A and B have one dedicated engineer each. Warehouse C has 2. I work on tickets where needed in addition to people management duties. Org has roughly 6,000 employees.\n\nFeeling very understaffed. We can add new features/data elements but rarely have time for tackling the mountain of technical debt. System stability is starting to pay the price.", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE team size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17usal7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699929823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How many data engineers work on the same project/team as you? Put another way, how many developers could conceivably pick up the same tickets you do?&lt;/p&gt;\n\n&lt;p&gt;Do you feel your team/project is understaffed, overstaffed, or at just the right level in terms of data engineers?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll start: I oversee a team of four data engineers spread across three data warehouses. Warehouse A and B have one dedicated engineer each. Warehouse C has 2. I work on tickets where needed in addition to people management duties. Org has roughly 6,000 employees.&lt;/p&gt;\n\n&lt;p&gt;Feeling very understaffed. We can add new features/data elements but rarely have time for tackling the mountain of technical debt. System stability is starting to pay the price.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17usal7", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17usal7/de_team_size/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17usal7/de_team_size/", "subreddit_subscribers": 139596, "created_utc": 1699929823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_c8aduissw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Store Architecture and How to Build One", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17ukb75", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/fkCiScP-C6ydL7zxjfdarNgNuqedb5exUVI89-qVxa0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699908076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "qwak.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.qwak.com/post/feature-store-architecture", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?auto=webp&amp;s=7d9eaf1a5cd5f569c46a924ea7ca81796458801c", "width": 750, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea3b225d1746e14281b8db60527d6877cc6c0c3d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f10eb55913c4bb463375b74ac9bc2c316a3aff74", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f80167e7203cf794f704e5b12de413e4e6572f5", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e6cb856e207c5577b5311dc9094245010d93f3f7", "width": 640, "height": 358}], "variants": {}, "id": "3GbgzbX3nObzU-FjdQNKX4C4ATlvqRcklyEOluYv358"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ukb75", "is_robot_indexable": true, "report_reasons": null, "author": "Odd_Spite3834", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ukb75/feature_store_architecture_and_how_to_build_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.qwak.com/post/feature-store-architecture", "subreddit_subscribers": 139596, "created_utc": 1699908076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ll try to keep this concise without too much venting. Jump to TLDR at bottom if necessary.\n\nI\u2019m currently a DA excel monkey that has been teaching myself SQL and PYTHON, passing Azure Certs, and building my own DE projects in my free time all in hopes to get out of this dreaded DA function that\u2019s I\u2019ve been stuck in for the last 4 years.\n\nI\u2019d argue I\u2019m above decent at SQL, basic in Python, really good in excel,  passed the DP-203 and DP-900, built an end-to-end ETL pipeline using azure (ADF -&gt; Databricks -&gt; Synapse -&gt; BI) currently working on CI/CD project using AzureDevOps, Databricks, Azure, GitHub. \n\nAll of this learning started July of this year and I  have not slowed down one bit. So I guess my question is do I have enough to try and make a jump into DE? I plan to finish this CI/CD project but I absolutely can\u2019t see myself working as a freaking data analyst anymore, I can confidently say I hate this type of function: endless pointless meetings, endless excel tools that never freaking work, unrealistic expectations from managers (dropping a request on you that they need back in an hour wtf), so much dirty un-useable data I don\u2019t know how this company gets anything done ect.\n\nI\u2019m not saying that DE won\u2019t have its issues, but I believe me being this interested in DE work is enough for me to get passed any BS that would come along. I haven\u2019t studied this hard, going over content/certs/taking test since college 10 years ago.\n\nThanks for reading. \n\nTLDR: it\u2019s been 6 months, Do I have enough under my belt to realistically jump from DA to DE? \n\nCurrent Industry: Govt/Defense Contractor\nCurrent Salary: 85k\nCurrent position: Data Analyst (Excel Monkey)", "author_fullname": "t2_22q9pua0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ready to jump ship (Into DE)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uhp3y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699934564.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699901445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ll try to keep this concise without too much venting. Jump to TLDR at bottom if necessary.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently a DA excel monkey that has been teaching myself SQL and PYTHON, passing Azure Certs, and building my own DE projects in my free time all in hopes to get out of this dreaded DA function that\u2019s I\u2019ve been stuck in for the last 4 years.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d argue I\u2019m above decent at SQL, basic in Python, really good in excel,  passed the DP-203 and DP-900, built an end-to-end ETL pipeline using azure (ADF -&amp;gt; Databricks -&amp;gt; Synapse -&amp;gt; BI) currently working on CI/CD project using AzureDevOps, Databricks, Azure, GitHub. &lt;/p&gt;\n\n&lt;p&gt;All of this learning started July of this year and I  have not slowed down one bit. So I guess my question is do I have enough to try and make a jump into DE? I plan to finish this CI/CD project but I absolutely can\u2019t see myself working as a freaking data analyst anymore, I can confidently say I hate this type of function: endless pointless meetings, endless excel tools that never freaking work, unrealistic expectations from managers (dropping a request on you that they need back in an hour wtf), so much dirty un-useable data I don\u2019t know how this company gets anything done ect.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not saying that DE won\u2019t have its issues, but I believe me being this interested in DE work is enough for me to get passed any BS that would come along. I haven\u2019t studied this hard, going over content/certs/taking test since college 10 years ago.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading. &lt;/p&gt;\n\n&lt;p&gt;TLDR: it\u2019s been 6 months, Do I have enough under my belt to realistically jump from DA to DE? &lt;/p&gt;\n\n&lt;p&gt;Current Industry: Govt/Defense Contractor\nCurrent Salary: 85k\nCurrent position: Data Analyst (Excel Monkey)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17uhp3y", "is_robot_indexable": true, "report_reasons": null, "author": "PoloParachutes", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uhp3y/ready_to_jump_ship_into_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uhp3y/ready_to_jump_ship_into_de/", "subreddit_subscribers": 139596, "created_utc": 1699901445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently a data analyst considering going in a data engineering direction. However, I don\u2019t quite have the experience for it yet, and I plan to get some certifications for Snowflake, AWS, and Airflow (or if you have other recs, please let me know).\n\nI don\u2019t like my current manager and have an opportunity for a data governance analyst role. Would this help me at all in the future, or should I just stick it out in my current position?", "author_fullname": "t2_46o0lefg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would a data governance role help in a data engineering career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uje8i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699905771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently a data analyst considering going in a data engineering direction. However, I don\u2019t quite have the experience for it yet, and I plan to get some certifications for Snowflake, AWS, and Airflow (or if you have other recs, please let me know).&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t like my current manager and have an opportunity for a data governance analyst role. Would this help me at all in the future, or should I just stick it out in my current position?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17uje8i", "is_robot_indexable": true, "report_reasons": null, "author": "love2eatalot", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uje8i/would_a_data_governance_role_help_in_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uje8i/would_a_data_governance_role_help_in_a_data/", "subreddit_subscribers": 139596, "created_utc": 1699905771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI'm aware that images like this exist:\n\n[https://hub.docker.com/r/jupyter/pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook)\n\nAfter getting the image\n\n`docker pull jupyter/pyspark-notebook`\n\nWe can run it with\n\n`docker run -it --rm -p 8888:8888 jupyter/pyspark-notebook`\n\nand write/run code from the notebook.\n\nHowever, I'm completely lost as to how to run PySpark with docker (also locally) but *from an IDE* (in my case I'm using PyCharm).  Is there any guide, somewhere, that I could follow? I'm guessing that, somehow, I should be able to link an image such as this one: [https://hub.docker.com/r/apache/spark-p](https://hub.docker.com/r/apache/spark-p)", "author_fullname": "t2_nqspn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help running PySpark from a docker container with an IDE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17u97lr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699877189.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware that images like this exist:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://hub.docker.com/r/jupyter/pyspark-notebook\"&gt;https://hub.docker.com/r/jupyter/pyspark-notebook&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After getting the image&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;docker pull jupyter/pyspark-notebook&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;We can run it with&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;docker run -it --rm -p 8888:8888 jupyter/pyspark-notebook&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;and write/run code from the notebook.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;m completely lost as to how to run PySpark with docker (also locally) but &lt;em&gt;from an IDE&lt;/em&gt; (in my case I&amp;#39;m using PyCharm).  Is there any guide, somewhere, that I could follow? I&amp;#39;m guessing that, somehow, I should be able to link an image such as this one: &lt;a href=\"https://hub.docker.com/r/apache/spark-p\"&gt;https://hub.docker.com/r/apache/spark-p&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17u97lr", "is_robot_indexable": true, "report_reasons": null, "author": "polidrupa", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17u97lr/help_running_pyspark_from_a_docker_container_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17u97lr/help_running_pyspark_from_a_docker_container_with/", "subreddit_subscribers": 139596, "created_utc": 1699877189.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is struggling to get test data that behaves the same as production data but has obfuscated PCI and PII data. What is the best way to get good quality test data for both pipelines and Data Scientist to use in their testing?  ", "author_fullname": "t2_6lh4st48", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gathering or Generating Test Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ug9ms", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699897774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is struggling to get test data that behaves the same as production data but has obfuscated PCI and PII data. What is the best way to get good quality test data for both pipelines and Data Scientist to use in their testing?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ug9ms", "is_robot_indexable": true, "report_reasons": null, "author": "Equivalent_Bluebird7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ug9ms/gathering_or_generating_test_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ug9ms/gathering_or_generating_test_data/", "subreddit_subscribers": 139596, "created_utc": 1699897774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TLDR: I want to use airflow to post jobs in rabbitmq for a python environment to pick up and run.\n\nHey y'all I work at a teeny tiny company without any dedicated data engineers, so I really have close to zero knowledge of what a standard pipeline looks like. I'm just a junior-ish software dev who is somehow the most qualified person to manage this stuff. So I'm providing a ton of context in case anybody has ANY feedback on ANY of this lol.\n\nWe have a pretty basic pipeline that pulls in data from a few sources, normalizes it, and then inserts it into our database. The code is basic, but the quantity is large enough that things are getting difficult. \n\nWe used to have a few scripts that just ran on crons. We had aa handful of scripts that we would run for each client, some of them taking up to 40 minutes! It started to get unwieldy: trying to track how many scripts were running at a given time, prioritizing certain clients/scripts, making sure we didn't overload the server (the whole operation is running on one beefy VPS).\n\nSo, we got a recommendation to move the heavy lifting to airflow. We went with google cloud composer, because apparently managing airflow yourself can suck. I don't love airflow, but I guess it works well enough. It helps with the scheduling and prioritizing, but we're once again bumping up against our usage limit. Instead of being constrained by memory on a VPS, cloud composer has a limited number of concurrent jobs you can run. And they are NOT cheap!\n\nI've been doing some reading, and apparently airflow is better for orchestration of tasks than execution of tasks directly. We're running all of our data pulling and normalization directly in airflow DAGs with python. \n\nI've seen recommendations of python + celery + rabbtimq for async long-running jobs, and now I'm tempted to switch to that. Does it make any sense to use airflow for scheduling if all it's going to do is post rabbitmq jobs?", "author_fullname": "t2_8k5ls63w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would it be weird to use airflow AND a message broker like rabbitmq?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uayeu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699883123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: I want to use airflow to post jobs in rabbitmq for a python environment to pick up and run.&lt;/p&gt;\n\n&lt;p&gt;Hey y&amp;#39;all I work at a teeny tiny company without any dedicated data engineers, so I really have close to zero knowledge of what a standard pipeline looks like. I&amp;#39;m just a junior-ish software dev who is somehow the most qualified person to manage this stuff. So I&amp;#39;m providing a ton of context in case anybody has ANY feedback on ANY of this lol.&lt;/p&gt;\n\n&lt;p&gt;We have a pretty basic pipeline that pulls in data from a few sources, normalizes it, and then inserts it into our database. The code is basic, but the quantity is large enough that things are getting difficult. &lt;/p&gt;\n\n&lt;p&gt;We used to have a few scripts that just ran on crons. We had aa handful of scripts that we would run for each client, some of them taking up to 40 minutes! It started to get unwieldy: trying to track how many scripts were running at a given time, prioritizing certain clients/scripts, making sure we didn&amp;#39;t overload the server (the whole operation is running on one beefy VPS).&lt;/p&gt;\n\n&lt;p&gt;So, we got a recommendation to move the heavy lifting to airflow. We went with google cloud composer, because apparently managing airflow yourself can suck. I don&amp;#39;t love airflow, but I guess it works well enough. It helps with the scheduling and prioritizing, but we&amp;#39;re once again bumping up against our usage limit. Instead of being constrained by memory on a VPS, cloud composer has a limited number of concurrent jobs you can run. And they are NOT cheap!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been doing some reading, and apparently airflow is better for orchestration of tasks than execution of tasks directly. We&amp;#39;re running all of our data pulling and normalization directly in airflow DAGs with python. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen recommendations of python + celery + rabbtimq for async long-running jobs, and now I&amp;#39;m tempted to switch to that. Does it make any sense to use airflow for scheduling if all it&amp;#39;s going to do is post rabbitmq jobs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17uayeu", "is_robot_indexable": true, "report_reasons": null, "author": "chamomile-crumbs", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uayeu/would_it_be_weird_to_use_airflow_and_a_message/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uayeu/would_it_be_weird_to_use_airflow_and_a_message/", "subreddit_subscribers": 139596, "created_utc": 1699883123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I signed up for the GCP Professional Data Engineering exam at the end of this month. Got an email few weeks ago that they start the new format start Nov 13 2023. Wondering if anyone might have taken the new format and can provide some feedback?", "author_fullname": "t2_iu3ot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP Professional Data Engineering New Format Nov 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uspgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699931079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I signed up for the GCP Professional Data Engineering exam at the end of this month. Got an email few weeks ago that they start the new format start Nov 13 2023. Wondering if anyone might have taken the new format and can provide some feedback?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17uspgf", "is_robot_indexable": true, "report_reasons": null, "author": "Cerivitus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uspgf/gcp_professional_data_engineering_new_format_nov/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uspgf/gcp_professional_data_engineering_new_format_nov/", "subreddit_subscribers": 139596, "created_utc": 1699931079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically I need to EL from a REST API, which requires a startDate and endDate parameter for each call that can't be greater than a 2 week range (source SaaS platform sets the limit). How do you guys work around this? Do you connect to your table (in my case on Snowflake) and pull the max date and use that as your startDate for each execution of the code? Planning on using Dagster running python code - and I'm pretty new to python. I can input credentials - get the athorization token, and get the JSON loaded into a dataframe just fine, but as far as making this a scalable/automated pipeline I'm a bit stuck... I'm sure someone has run into this at some point lol.", "author_fullname": "t2_3ugqxzu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage/store date parameters within API integrations for incremental loads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uimxt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699903831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically I need to EL from a REST API, which requires a startDate and endDate parameter for each call that can&amp;#39;t be greater than a 2 week range (source SaaS platform sets the limit). How do you guys work around this? Do you connect to your table (in my case on Snowflake) and pull the max date and use that as your startDate for each execution of the code? Planning on using Dagster running python code - and I&amp;#39;m pretty new to python. I can input credentials - get the athorization token, and get the JSON loaded into a dataframe just fine, but as far as making this a scalable/automated pipeline I&amp;#39;m a bit stuck... I&amp;#39;m sure someone has run into this at some point lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17uimxt", "is_robot_indexable": true, "report_reasons": null, "author": "Casdom33", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uimxt/how_do_you_managestore_date_parameters_within_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uimxt/how_do_you_managestore_date_parameters_within_api/", "subreddit_subscribers": 139596, "created_utc": 1699903831.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I've been thinking about a problem for a long time, and I'd like your insights. I\u2019m trying to have a standard, mathematical approach to the late arriving facts problem.\n\nHere\u2019s where I\u2019m so far.\n\nSay you have a fact table, with a unique\\_key, some dimensions (date or else) and some metrics (numbers, you'll average or sum). We snapshot that table at date 1 and at date 2. We suppose there is no schema update. And we ignore the collection of new data between date 1 and date 2.\n\nThis data is non moving. It\u2019s our orders, our billings, our rides, our bookings, etc.\n\nIn a perfect world, snapshot1 and snapshot2 are equal. In our case they are not equal.\n\n# Technical approach vs real-world approach \n\nTechnically, the diff between snapshot1 and 2 is composed of:\n\n* Added lines\n* Deleted lines\n* Modified lines, being the sum of:\n   * Affected column name, before\\_value, after\\_value \n\nThe breakdown of this data is pretty easy to compute. But this is pretty useless, without a root cause.\n\nThe root cause, is in one of 3 category (the trichotomy):\n\n1. External to the company\n2. Internal to the company, but external to the data team\n3. Internal to the company, and internal to the data team\n\nDo you agree so far ?\n\n# Category - Response\n\nMy first assumption is that each category has this best specific response:\n\nFor *category 1* (external to the company e.g. refunded transaction, cancelled booking). This will happen consistently, you have to deal with it once and for all. Ex: there is estimated metric, and actual metric. You know how to compute \u201cthis late arriving fact\u201d impact (e.g. all cancelled bookings that happened between \u201cestimated\u201d and \u201cactual\u201d). And, normally, estimated + late arriving fact impact = actual metric. It is quite expensive.\n\nFor *category 2* (internal to the company, but outside the data team e.g. production database update, CRM updates). This is my favorite. What I have observed is, as data team member you have to prove as fast as possible that is comes from another team. And make it their problem. Because, you cannot handle it as a category 1 because it would be too expensive, and there is a new case every month so the generic case is not solvable. It\u2019s an internal process problem. I'd like to break it down into more specific problems\n\nFor *category 3* (internal to the company, inside the data team e.g. it either a bug, or a bug fix). In that case, your data consumer do not have anything to know about it, the bug and the bugfix should be as close as possible (aka fix the bug asap). And, for self improvement, you measure how many category 3 happened and how long it took to fix it.\n\nSo I'm [developing this tool](https://github.com/data-drift/data-drift) and there are these 2 challenges where I\u2019m stuck: \n\n* from the technical perspective (added, deleted, modified lines) is there a way to find the category ? I think it is possible, as code, with a user defined function that takes the added, deleted, modified lines, and decides if it is a known case of category 1, or not. But for category 2 or 3, I think it has to be a **human process**. Or maybe something about \"for that table, if there is more than 10 lines its most likely a bug category 3, other wise it is a human modification somewhere\"\n* What to do for category 2 ? We can report it and try reduce it. I don't know so I try to break the problem down to sub categories. My examples are \"bugs from software team\", \"adjusting the CRM/billing data\" . There is maybe something about, \"is it supposed to happen again ?\" and \"will we revert that change ?\", waiting for ideas :D \n\nThanks for reading ! ", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Late arriving facts: trichotomy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ufwv5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699896839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;ve been thinking about a problem for a long time, and I&amp;#39;d like your insights. I\u2019m trying to have a standard, mathematical approach to the late arriving facts problem.&lt;/p&gt;\n\n&lt;p&gt;Here\u2019s where I\u2019m so far.&lt;/p&gt;\n\n&lt;p&gt;Say you have a fact table, with a unique_key, some dimensions (date or else) and some metrics (numbers, you&amp;#39;ll average or sum). We snapshot that table at date 1 and at date 2. We suppose there is no schema update. And we ignore the collection of new data between date 1 and date 2.&lt;/p&gt;\n\n&lt;p&gt;This data is non moving. It\u2019s our orders, our billings, our rides, our bookings, etc.&lt;/p&gt;\n\n&lt;p&gt;In a perfect world, snapshot1 and snapshot2 are equal. In our case they are not equal.&lt;/p&gt;\n\n&lt;h1&gt;Technical approach vs real-world approach&lt;/h1&gt;\n\n&lt;p&gt;Technically, the diff between snapshot1 and 2 is composed of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Added lines&lt;/li&gt;\n&lt;li&gt;Deleted lines&lt;/li&gt;\n&lt;li&gt;Modified lines, being the sum of:\n\n&lt;ul&gt;\n&lt;li&gt;Affected column name, before_value, after_value &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The breakdown of this data is pretty easy to compute. But this is pretty useless, without a root cause.&lt;/p&gt;\n\n&lt;p&gt;The root cause, is in one of 3 category (the trichotomy):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;External to the company&lt;/li&gt;\n&lt;li&gt;Internal to the company, but external to the data team&lt;/li&gt;\n&lt;li&gt;Internal to the company, and internal to the data team&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Do you agree so far ?&lt;/p&gt;\n\n&lt;h1&gt;Category - Response&lt;/h1&gt;\n\n&lt;p&gt;My first assumption is that each category has this best specific response:&lt;/p&gt;\n\n&lt;p&gt;For &lt;em&gt;category 1&lt;/em&gt; (external to the company e.g. refunded transaction, cancelled booking). This will happen consistently, you have to deal with it once and for all. Ex: there is estimated metric, and actual metric. You know how to compute \u201cthis late arriving fact\u201d impact (e.g. all cancelled bookings that happened between \u201cestimated\u201d and \u201cactual\u201d). And, normally, estimated + late arriving fact impact = actual metric. It is quite expensive.&lt;/p&gt;\n\n&lt;p&gt;For &lt;em&gt;category 2&lt;/em&gt; (internal to the company, but outside the data team e.g. production database update, CRM updates). This is my favorite. What I have observed is, as data team member you have to prove as fast as possible that is comes from another team. And make it their problem. Because, you cannot handle it as a category 1 because it would be too expensive, and there is a new case every month so the generic case is not solvable. It\u2019s an internal process problem. I&amp;#39;d like to break it down into more specific problems&lt;/p&gt;\n\n&lt;p&gt;For &lt;em&gt;category 3&lt;/em&gt; (internal to the company, inside the data team e.g. it either a bug, or a bug fix). In that case, your data consumer do not have anything to know about it, the bug and the bugfix should be as close as possible (aka fix the bug asap). And, for self improvement, you measure how many category 3 happened and how long it took to fix it.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m &lt;a href=\"https://github.com/data-drift/data-drift\"&gt;developing this tool&lt;/a&gt; and there are these 2 challenges where I\u2019m stuck: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;from the technical perspective (added, deleted, modified lines) is there a way to find the category ? I think it is possible, as code, with a user defined function that takes the added, deleted, modified lines, and decides if it is a known case of category 1, or not. But for category 2 or 3, I think it has to be a &lt;strong&gt;human process&lt;/strong&gt;. Or maybe something about &amp;quot;for that table, if there is more than 10 lines its most likely a bug category 3, other wise it is a human modification somewhere&amp;quot;&lt;/li&gt;\n&lt;li&gt;What to do for category 2 ? We can report it and try reduce it. I don&amp;#39;t know so I try to break the problem down to sub categories. My examples are &amp;quot;bugs from software team&amp;quot;, &amp;quot;adjusting the CRM/billing data&amp;quot; . There is maybe something about, &amp;quot;is it supposed to happen again ?&amp;quot; and &amp;quot;will we revert that change ?&amp;quot;, waiting for ideas :D &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for reading ! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?auto=webp&amp;s=f5071d66ba016d02d16d8fe542a9d3a1362d5bf7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63aec543dc09e53536c18730de8b14feffd9616c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77721d607863f9dcc0b35ce27084cf2b324c0495", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=10f6776ecf4e47448be36f11ff95ea973265598d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=62fafe3a3592eb7c8d5c5a62769451734ab17e0d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a7b8452647be660d7b238d31a61a5c70c538f3d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73bf82c5fd0823a47c23d23ce9ced940a09a6407", "width": 1080, "height": 540}], "variants": {}, "id": "gaq4gpMffllPSr4TtkFHfZvBLuKF-ddT00PmCOIaLdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ufwv5", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ufwv5/late_arriving_facts_trichotomy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ufwv5/late_arriving_facts_trichotomy/", "subreddit_subscribers": 139596, "created_utc": 1699896839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Have new capabilities of lakehouse formats like Delta and Iceberg, or any new streaming ingestion capabilities in data warehouses, reduced this time? \n\nIt used to be ok for reporting queries in DWHs and DLs to be run against data that was around 18h - 24h behind live data but this seems to be changing? Is getting closer to live data access (say 1hr -&gt; 10mins -&gt; or less??) a requirement that is here to stay, or just a passing fad? \n\n&amp;#x200B;", "author_fullname": "t2_7spandv9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is ETL job execution time (e.g. in Spark, or in your DWH) one of the biggest factors when it comes to being able to query the latest data? Which other factors play a major role and why?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17u918f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699876534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have new capabilities of lakehouse formats like Delta and Iceberg, or any new streaming ingestion capabilities in data warehouses, reduced this time? &lt;/p&gt;\n\n&lt;p&gt;It used to be ok for reporting queries in DWHs and DLs to be run against data that was around 18h - 24h behind live data but this seems to be changing? Is getting closer to live data access (say 1hr -&amp;gt; 10mins -&amp;gt; or less??) a requirement that is here to stay, or just a passing fad? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17u918f", "is_robot_indexable": true, "report_reasons": null, "author": "brrdprrsn", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17u918f/is_etl_job_execution_time_eg_in_spark_or_in_your/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17u918f/is_etl_job_execution_time_eg_in_spark_or_in_your/", "subreddit_subscribers": 139596, "created_utc": 1699876534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all \n\nI have an idea for the project and would like to ask for advice in regards to the general plan, AWS services used etc.\n\nSo there is a CompanyA, and my data app would check the mentions about CompanyA from diffrent websites,social media pages etc. Essentially web monitoring for a company.\n\nFor some of the data sources batch load and notification would be fine, but for some i would aim for realtime.\n\nWhat would be the right tooling/approach of doing that ?\n\nThank You\n\nBR R\n\n&amp;#x200B;", "author_fullname": "t2_dgni919o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering project in AWS- Company WebMonitoring alert.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17uy92c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699952387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all &lt;/p&gt;\n\n&lt;p&gt;I have an idea for the project and would like to ask for advice in regards to the general plan, AWS services used etc.&lt;/p&gt;\n\n&lt;p&gt;So there is a CompanyA, and my data app would check the mentions about CompanyA from diffrent websites,social media pages etc. Essentially web monitoring for a company.&lt;/p&gt;\n\n&lt;p&gt;For some of the data sources batch load and notification would be fine, but for some i would aim for realtime.&lt;/p&gt;\n\n&lt;p&gt;What would be the right tooling/approach of doing that ?&lt;/p&gt;\n\n&lt;p&gt;Thank You&lt;/p&gt;\n\n&lt;p&gt;BR R&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17uy92c", "is_robot_indexable": true, "report_reasons": null, "author": "BidAcrobatic5039", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uy92c/data_engineering_project_in_aws_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uy92c/data_engineering_project_in_aws_company/", "subreddit_subscribers": 139596, "created_utc": 1699952387.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "To me, data modeling is a bit ambiguous. I think it depends on the situations and projects. Is there a way to explain \u201cI do data modeling this way\u201d?", "author_fullname": "t2_exfd2a9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you data model? Go scenarios and how you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uuimw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699937038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To me, data modeling is a bit ambiguous. I think it depends on the situations and projects. Is there a way to explain \u201cI do data modeling this way\u201d?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17uuimw", "is_robot_indexable": true, "report_reasons": null, "author": "GiantEarnings", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uuimw/how_do_you_data_model_go_scenarios_and_how_you_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uuimw/how_do_you_data_model_go_scenarios_and_how_you_do/", "subreddit_subscribers": 139596, "created_utc": 1699937038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone..\n\nI'm currently working on a stock/crypto scanner project and I'm curious about how platforms like TradingView efficiently handle real-time alerts for a myriad of user-defined conditions, like specific price points, technical indicators, or chart lines. These platforms seem to manage countless alert settings and deliver these alerts promptly to users, and I'm interested in understanding the underlying technology that makes this possible.\n\nMy current tech stack involves Python (Django frontend), websockets to Redpanda to PostgreSQL + TimescaleDB. However, I'm facing challenges with the speed of queries across multiple timeframes (15min, 30min, 60min, 4H, D, W, M, Q, Y), which are proving to be quite slow.\n\nTo address this, I'm exploring stream processing solutions like Quix ([https://github.com/quixio/quix-streams](https://github.com/quixio/quix-streams)) and Bytewax ([https://bytewax.io](https://bytewax.io/)). Additionally, I've briefly looked into other technologies like Beam, Flink, Druid, Pinot, and ClickHouse, but I'm still figuring out how they could fit into my setup. My goal is to avoid relying on a large loop with numerous database queries.\n\nI'd love to hear from anyone who has experience or insights into building similar systems. Specifically, I'm looking for:\n\n* Feedback on my current tech stack and its scalability for real-time alerting.\n* Suggestions on alternative technologies or architectures that could improve performance.\n* Insights into how high-performance platforms handle such tasks, particularly those that allow for numerous and varied user-defined alert conditions.\n\nAny advice or shared experiences would be greatly appreciated!", "author_fullname": "t2_131vjk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do Platforms Like TradingView Handle Real-Time Alerts on Various Price Points and Indicators?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17utom7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699934163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone..&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a stock/crypto scanner project and I&amp;#39;m curious about how platforms like TradingView efficiently handle real-time alerts for a myriad of user-defined conditions, like specific price points, technical indicators, or chart lines. These platforms seem to manage countless alert settings and deliver these alerts promptly to users, and I&amp;#39;m interested in understanding the underlying technology that makes this possible.&lt;/p&gt;\n\n&lt;p&gt;My current tech stack involves Python (Django frontend), websockets to Redpanda to PostgreSQL + TimescaleDB. However, I&amp;#39;m facing challenges with the speed of queries across multiple timeframes (15min, 30min, 60min, 4H, D, W, M, Q, Y), which are proving to be quite slow.&lt;/p&gt;\n\n&lt;p&gt;To address this, I&amp;#39;m exploring stream processing solutions like Quix (&lt;a href=\"https://github.com/quixio/quix-streams\"&gt;https://github.com/quixio/quix-streams&lt;/a&gt;) and Bytewax (&lt;a href=\"https://bytewax.io/\"&gt;https://bytewax.io&lt;/a&gt;). Additionally, I&amp;#39;ve briefly looked into other technologies like Beam, Flink, Druid, Pinot, and ClickHouse, but I&amp;#39;m still figuring out how they could fit into my setup. My goal is to avoid relying on a large loop with numerous database queries.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear from anyone who has experience or insights into building similar systems. Specifically, I&amp;#39;m looking for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Feedback on my current tech stack and its scalability for real-time alerting.&lt;/li&gt;\n&lt;li&gt;Suggestions on alternative technologies or architectures that could improve performance.&lt;/li&gt;\n&lt;li&gt;Insights into how high-performance platforms handle such tasks, particularly those that allow for numerous and varied user-defined alert conditions.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice or shared experiences would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?auto=webp&amp;s=abd59f721c35c521161d310a0af69b69ea1616c6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=74fa9f80c35a9f22499e8a3d7b6027720f7d424b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1dddc85f52d397cefa333dc4fe5ece988081644", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce582fd27bd3c8aff7824acb53831c8ef67a8a6a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f89c2a414d2f47a74f7b7d5f0f60f7779eabc2f4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cadabfec6a3d18c1d2d07e1b1e485370ac2eac68", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05198745e533b79b4f029ccad26e7e30f4adf560", "width": 1080, "height": 540}], "variants": {}, "id": "vpfV6_e6S-Wn-gmFhRT5batbQd1JHay180C8lHn6Xcg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17utom7", "is_robot_indexable": true, "report_reasons": null, "author": "tomk2020", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17utom7/how_do_platforms_like_tradingview_handle_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17utom7/how_do_platforms_like_tradingview_handle_realtime/", "subreddit_subscribers": 139596, "created_utc": 1699934163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started as a data analyst at a small ad tech firm a couple of years ago. This naturally morphed into a DE position thanks to the existing and barely maintained 10 year old data infrastructure. \n\nWe run ad campaigns on a DSP for our clients and offer them custom reporting. Each report can be delivered at any cadence, to any destination within reason, with custom datetime format, headers, file naming convention, file format (including Excel reports with multiple sheets), and so on. \n\nThe legacy system that handles report delivery was an unbelievable nightmare amalgam created by many different contractors over the years. One of my first big projects was building a replacement. The solution I came up with was very hacky and consists of:\n\n* 1 yaml file per report per client where all custom parameters related to its creation and delivery are specified\n* 1 Airflow DAG with 3 tasks per report per client (run query, build report, upload report)\n * DAG runs daily and scheduling is handled per task by checking config against execution date\n\nWe now have about 40 clients and most ask for several highly customized reports. As you can imagine the cracks are beginning to show and I spend the majority of my week duct taping it back together. I would greatly appreciate any guidance on building a system that follows common best practices. \n\nThanks", "author_fullname": "t2_a0gc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to manage automatic, customized reporting for clients?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ut96a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699932754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started as a data analyst at a small ad tech firm a couple of years ago. This naturally morphed into a DE position thanks to the existing and barely maintained 10 year old data infrastructure. &lt;/p&gt;\n\n&lt;p&gt;We run ad campaigns on a DSP for our clients and offer them custom reporting. Each report can be delivered at any cadence, to any destination within reason, with custom datetime format, headers, file naming convention, file format (including Excel reports with multiple sheets), and so on. &lt;/p&gt;\n\n&lt;p&gt;The legacy system that handles report delivery was an unbelievable nightmare amalgam created by many different contractors over the years. One of my first big projects was building a replacement. The solution I came up with was very hacky and consists of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1 yaml file per report per client where all custom parameters related to its creation and delivery are specified&lt;/li&gt;\n&lt;li&gt;1 Airflow DAG with 3 tasks per report per client (run query, build report, upload report)\n\n&lt;ul&gt;\n&lt;li&gt;DAG runs daily and scheduling is handled per task by checking config against execution date&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We now have about 40 clients and most ask for several highly customized reports. As you can imagine the cracks are beginning to show and I spend the majority of my week duct taping it back together. I would greatly appreciate any guidance on building a system that follows common best practices. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ut96a", "is_robot_indexable": true, "report_reasons": null, "author": "_nu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ut96a/what_is_the_best_way_to_manage_automatic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ut96a/what_is_the_best_way_to_manage_automatic/", "subreddit_subscribers": 139596, "created_utc": 1699932754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background - I have a background in data analytics with 4 years of experience. After my post graduate certificate I moved into data engineering. And, I am currently working in my first role as a dara engineer as a contractor with bank.\n\nLocation - Toronto\nSalary - CAD 70,000 per annum \n\nSituation\nWhen I joined, there were two more people who joined with me. Me, having done some courses and a portfolio project, had the most experience and ended up contributing upto 90 percent of the code written for my spark project, without 10,000+ lines of code.\n\nI am being paid the same as other two, and I am the one making most contribution as well as have activate participation in discussions.\n\nHow should I deal with it. Being on contract means I cannot negotiate much of salary (from what I have heard about the team's structure).\n\nI applied to other opportunities but with major experience in data analytics, I am just getting rejections.\n\nWhat to do. Please advise?", "author_fullname": "t2_5db267r6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Major contributor paid fresher salary", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17usm4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699930820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background - I have a background in data analytics with 4 years of experience. After my post graduate certificate I moved into data engineering. And, I am currently working in my first role as a dara engineer as a contractor with bank.&lt;/p&gt;\n\n&lt;p&gt;Location - Toronto\nSalary - CAD 70,000 per annum &lt;/p&gt;\n\n&lt;p&gt;Situation\nWhen I joined, there were two more people who joined with me. Me, having done some courses and a portfolio project, had the most experience and ended up contributing upto 90 percent of the code written for my spark project, without 10,000+ lines of code.&lt;/p&gt;\n\n&lt;p&gt;I am being paid the same as other two, and I am the one making most contribution as well as have activate participation in discussions.&lt;/p&gt;\n\n&lt;p&gt;How should I deal with it. Being on contract means I cannot negotiate much of salary (from what I have heard about the team&amp;#39;s structure).&lt;/p&gt;\n\n&lt;p&gt;I applied to other opportunities but with major experience in data analytics, I am just getting rejections.&lt;/p&gt;\n\n&lt;p&gt;What to do. Please advise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17usm4e", "is_robot_indexable": true, "report_reasons": null, "author": "pulkit_kapoor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17usm4e/major_contributor_paid_fresher_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17usm4e/major_contributor_paid_fresher_salary/", "subreddit_subscribers": 139596, "created_utc": 1699930820.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a company in a product quality team. This team is using excel as a database and I am currently trying to persuade the uppermanagement the need of implementing a data system that the inspecters are able to use to input the results. \n\n1. 2k units inspected per week\n2. So much data errors\n3. 8 separate excel sheets that needs to be manually aggregated\n\n Please help", "author_fullname": "t2_5uvrlw9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inspection data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uooz1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699919298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a company in a product quality team. This team is using excel as a database and I am currently trying to persuade the uppermanagement the need of implementing a data system that the inspecters are able to use to input the results. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;2k units inspected per week&lt;/li&gt;\n&lt;li&gt;So much data errors&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;8 separate excel sheets that needs to be manually aggregated&lt;/p&gt;\n\n&lt;p&gt;Please help&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17uooz1", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_Ball_58", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uooz1/inspection_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uooz1/inspection_data/", "subreddit_subscribers": 139596, "created_utc": 1699919298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI'm in search of an API that can efficiently extract tables from PDFs, including style details like font and color. While I've used Azure Form Recognizer, I'm facing some accuracy issues.\n\nDoes anyone have suggestions for an API that is particularly good at both data extraction and style retention?\n\nAppreciate your insights!", "author_fullname": "t2_ncbwzerb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PDF Table Extraction APIs with Style Preservation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ub6vq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699883870.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in search of an API that can efficiently extract tables from PDFs, including style details like font and color. While I&amp;#39;ve used Azure Form Recognizer, I&amp;#39;m facing some accuracy issues.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have suggestions for an API that is particularly good at both data extraction and style retention?&lt;/p&gt;\n\n&lt;p&gt;Appreciate your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ub6vq", "is_robot_indexable": true, "report_reasons": null, "author": "PlatypusPrudent3076", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ub6vq/pdf_table_extraction_apis_with_style_preservation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ub6vq/pdf_table_extraction_apis_with_style_preservation/", "subreddit_subscribers": 139596, "created_utc": 1699883870.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6y0b4txf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Epsio's Diff in the Streaming Landscape", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17u9lwe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/A0inDn2raTgxJiJfkThlVaW0NsOvc3GBVEesydz7LQ8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699878644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "epsio.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.epsio.io/blog/epsios-diff-in-the-streaming-landscape", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Kks7FhVsdV_NNcHIydfXmKPRDcDLNZpCzOrSbsF3a1k.jpg?auto=webp&amp;s=84fcf1023d3e51a80ce9f64a66cf708e81ba2062", "width": 3601, "height": 1882}, "resolutions": [{"url": "https://external-preview.redd.it/Kks7FhVsdV_NNcHIydfXmKPRDcDLNZpCzOrSbsF3a1k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=11439bae647f340ed0e2d99cdbff4753c9c844e5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Kks7FhVsdV_NNcHIydfXmKPRDcDLNZpCzOrSbsF3a1k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=146460672e058257974b1ce3964205c53c72160b", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/Kks7FhVsdV_NNcHIydfXmKPRDcDLNZpCzOrSbsF3a1k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=098aab610ac60cc7e31272006bb6713e379eeddb", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/Kks7FhVsdV_NNcHIydfXmKPRDcDLNZpCzOrSbsF3a1k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8881275e3140901452589cc54036d2f8b8ce950", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/Kks7FhVsdV_NNcHIydfXmKPRDcDLNZpCzOrSbsF3a1k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d5b08ce0050aa5891cda38a7e9ea20199c755d9", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/Kks7FhVsdV_NNcHIydfXmKPRDcDLNZpCzOrSbsF3a1k.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cd45d1f859abc146d7444c5071efdde81e18f6fc", "width": 1080, "height": 564}], "variants": {}, "id": "rdMzXwi9Lwr9oaQScYGaue8Xsrg6npq89RCo6pYwjjw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17u9lwe", "is_robot_indexable": true, "report_reasons": null, "author": "Giladkl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17u9lwe/epsios_diff_in_the_streaming_landscape/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.epsio.io/blog/epsios-diff-in-the-streaming-landscape", "subreddit_subscribers": 139596, "created_utc": 1699878644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, \n\ni\u2019m relatively new to this, and wondering if anyone with experience in consulting would be able to weigh in here. \n\nThe use case is building a green field data warehouse for reporting and building dashboards, so basically it will be for the BI. \n\nStack: \nGCP, BigQuery, DBT, DbVisualizer \n\nfor EL; Not quite sure I should go with Talend or Meltano \ud83d\ude13\nAlso, I\u2019ve been searching a lot lately and I came across Dagster.. will it be useful here? what is the use-cases? \n\nThank you in advance \ud83d\ude4f", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17u6us0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699866905.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt;\n\n&lt;p&gt;i\u2019m relatively new to this, and wondering if anyone with experience in consulting would be able to weigh in here. &lt;/p&gt;\n\n&lt;p&gt;The use case is building a green field data warehouse for reporting and building dashboards, so basically it will be for the BI. &lt;/p&gt;\n\n&lt;p&gt;Stack: \nGCP, BigQuery, DBT, DbVisualizer &lt;/p&gt;\n\n&lt;p&gt;for EL; Not quite sure I should go with Talend or Meltano \ud83d\ude13\nAlso, I\u2019ve been searching a lot lately and I came across Dagster.. will it be useful here? what is the use-cases? &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17u6us0", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17u6us0/data_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17u6us0/data_stack/", "subreddit_subscribers": 139596, "created_utc": 1699866905.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}