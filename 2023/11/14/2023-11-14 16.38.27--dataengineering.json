{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am confused \ud83d\ude15 in choosing the right role in IT. I did my bachelor in electrical 1.5 years back but want to switch to IT. I did nothing in between tried doing business but failed miserably. Preparing for MS now. But confused.\nI know basic of SQL and Python and that's it.\n\nFirst i wanted to go into data science but I found out that entry level job is hard to get there. \nOther options for me is either data engineer,cloud engineer or software developer.\n\nI am looking For a role that has high salary and less competition, unlike data science.", "author_fullname": "t2_7vkbdldt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which role in IT has good salary and less compitition?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17urs2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699928288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am confused \ud83d\ude15 in choosing the right role in IT. I did my bachelor in electrical 1.5 years back but want to switch to IT. I did nothing in between tried doing business but failed miserably. Preparing for MS now. But confused.\nI know basic of SQL and Python and that&amp;#39;s it.&lt;/p&gt;\n\n&lt;p&gt;First i wanted to go into data science but I found out that entry level job is hard to get there. \nOther options for me is either data engineer,cloud engineer or software developer.&lt;/p&gt;\n\n&lt;p&gt;I am looking For a role that has high salary and less competition, unlike data science.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17urs2s", "is_robot_indexable": true, "report_reasons": null, "author": "Munib_raza_khan", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17urs2s/which_role_in_it_has_good_salary_and_less/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17urs2s/which_role_in_it_has_good_salary_and_less/", "subreddit_subscribers": 139641, "created_utc": 1699928288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m after finishing the snowflake fundamentals course and want to pursue as many certifications as possible, any advice would be great, thank you.", "author_fullname": "t2_6j3548qy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best snowflake certifications/hands on essentials to have as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uzdy2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699957617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m after finishing the snowflake fundamentals course and want to pursue as many certifications as possible, any advice would be great, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17uzdy2", "is_robot_indexable": true, "report_reasons": null, "author": "Kokadoodles", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uzdy2/what_are_the_best_snowflake_certificationshands/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uzdy2/what_are_the_best_snowflake_certificationshands/", "subreddit_subscribers": 139641, "created_utc": 1699957617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How many data engineers work on the same project/team as you? Put another way, how many developers could conceivably pick up the same tickets you do?\n\nDo you feel your team/project is understaffed, overstaffed, or at just the right level in terms of data engineers?\n\nI\u2019ll start: I oversee a team of four data engineers spread across three data warehouses. Warehouse A and B have one dedicated engineer each. Warehouse C has 2. I work on tickets where needed in addition to people management duties. Org has roughly 6,000 employees.\n\nFeeling very understaffed. We can add new features/data elements but rarely have time for tackling the mountain of technical debt. System stability is starting to pay the price.", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE team size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17usal7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699929823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How many data engineers work on the same project/team as you? Put another way, how many developers could conceivably pick up the same tickets you do?&lt;/p&gt;\n\n&lt;p&gt;Do you feel your team/project is understaffed, overstaffed, or at just the right level in terms of data engineers?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll start: I oversee a team of four data engineers spread across three data warehouses. Warehouse A and B have one dedicated engineer each. Warehouse C has 2. I work on tickets where needed in addition to people management duties. Org has roughly 6,000 employees.&lt;/p&gt;\n\n&lt;p&gt;Feeling very understaffed. We can add new features/data elements but rarely have time for tackling the mountain of technical debt. System stability is starting to pay the price.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17usal7", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17usal7/de_team_size/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17usal7/de_team_size/", "subreddit_subscribers": 139641, "created_utc": 1699929823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_c8aduissw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Feature Store Architecture and How to Build One", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17ukb75", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/fkCiScP-C6ydL7zxjfdarNgNuqedb5exUVI89-qVxa0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699908076.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "qwak.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.qwak.com/post/feature-store-architecture", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?auto=webp&amp;s=7d9eaf1a5cd5f569c46a924ea7ca81796458801c", "width": 750, "height": 420}, "resolutions": [{"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ea3b225d1746e14281b8db60527d6877cc6c0c3d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f10eb55913c4bb463375b74ac9bc2c316a3aff74", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f80167e7203cf794f704e5b12de413e4e6572f5", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/EJQhy6o56UBnReETRcTWpiyvC2DBN-gLv1G910TWXpQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e6cb856e207c5577b5311dc9094245010d93f3f7", "width": 640, "height": 358}], "variants": {}, "id": "3GbgzbX3nObzU-FjdQNKX4C4ATlvqRcklyEOluYv358"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ukb75", "is_robot_indexable": true, "report_reasons": null, "author": "Odd_Spite3834", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ukb75/feature_store_architecture_and_how_to_build_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.qwak.com/post/feature-store-architecture", "subreddit_subscribers": 139641, "created_utc": 1699908076.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ll try to keep this concise without too much venting. Jump to TLDR at bottom if necessary.\n\nI\u2019m currently a DA excel monkey that has been teaching myself SQL and PYTHON, passing Azure Certs, and building my own DE projects in my free time all in hopes to get out of this dreaded DA function that\u2019s I\u2019ve been stuck in for the last 4 years.\n\nI\u2019d argue I\u2019m above decent at SQL, basic in Python, really good in excel,  passed the DP-203 and DP-900, built an end-to-end ETL pipeline using azure (ADF -&gt; Databricks -&gt; Synapse -&gt; BI) currently working on CI/CD project using AzureDevOps, Databricks, Azure, GitHub. \n\nAll of this learning started July of this year and I  have not slowed down one bit. So I guess my question is do I have enough to try and make a jump into DE? I plan to finish this CI/CD project but I absolutely can\u2019t see myself working as a freaking data analyst anymore, I can confidently say I hate this type of function: endless pointless meetings, endless excel tools that never freaking work, unrealistic expectations from managers (dropping a request on you that they need back in an hour wtf), so much dirty un-useable data I don\u2019t know how this company gets anything done ect.\n\nI\u2019m not saying that DE won\u2019t have its issues, but I believe me being this interested in DE work is enough for me to get passed any BS that would come along. I haven\u2019t studied this hard, going over content/certs/taking test since college 10 years ago.\n\nThanks for reading. \n\nTLDR: it\u2019s been 6 months, Do I have enough under my belt to realistically jump from DA to DE? \n\nCurrent Industry: Govt/Defense Contractor\nCurrent Salary: 85k\nCurrent position: Data Analyst (Excel Monkey)", "author_fullname": "t2_22q9pua0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ready to jump ship (Into DE)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uhp3y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699934564.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699901445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ll try to keep this concise without too much venting. Jump to TLDR at bottom if necessary.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently a DA excel monkey that has been teaching myself SQL and PYTHON, passing Azure Certs, and building my own DE projects in my free time all in hopes to get out of this dreaded DA function that\u2019s I\u2019ve been stuck in for the last 4 years.&lt;/p&gt;\n\n&lt;p&gt;I\u2019d argue I\u2019m above decent at SQL, basic in Python, really good in excel,  passed the DP-203 and DP-900, built an end-to-end ETL pipeline using azure (ADF -&amp;gt; Databricks -&amp;gt; Synapse -&amp;gt; BI) currently working on CI/CD project using AzureDevOps, Databricks, Azure, GitHub. &lt;/p&gt;\n\n&lt;p&gt;All of this learning started July of this year and I  have not slowed down one bit. So I guess my question is do I have enough to try and make a jump into DE? I plan to finish this CI/CD project but I absolutely can\u2019t see myself working as a freaking data analyst anymore, I can confidently say I hate this type of function: endless pointless meetings, endless excel tools that never freaking work, unrealistic expectations from managers (dropping a request on you that they need back in an hour wtf), so much dirty un-useable data I don\u2019t know how this company gets anything done ect.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not saying that DE won\u2019t have its issues, but I believe me being this interested in DE work is enough for me to get passed any BS that would come along. I haven\u2019t studied this hard, going over content/certs/taking test since college 10 years ago.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading. &lt;/p&gt;\n\n&lt;p&gt;TLDR: it\u2019s been 6 months, Do I have enough under my belt to realistically jump from DA to DE? &lt;/p&gt;\n\n&lt;p&gt;Current Industry: Govt/Defense Contractor\nCurrent Salary: 85k\nCurrent position: Data Analyst (Excel Monkey)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17uhp3y", "is_robot_indexable": true, "report_reasons": null, "author": "PoloParachutes", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uhp3y/ready_to_jump_ship_into_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uhp3y/ready_to_jump_ship_into_de/", "subreddit_subscribers": 139641, "created_utc": 1699901445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone..\n\nI'm currently working on a stock/crypto scanner project and I'm curious about how platforms like TradingView efficiently handle real-time alerts for a myriad of user-defined conditions, like specific price points, technical indicators, or chart lines. These platforms seem to manage countless alert settings and deliver these alerts promptly to users, and I'm interested in understanding the underlying technology that makes this possible.\n\nMy current tech stack involves Python (Django frontend), websockets to Redpanda to PostgreSQL + TimescaleDB. However, I'm facing challenges with the speed of queries across multiple timeframes (15min, 30min, 60min, 4H, D, W, M, Q, Y), which are proving to be quite slow.\n\nTo address this, I'm exploring stream processing solutions like Quix ([https://github.com/quixio/quix-streams](https://github.com/quixio/quix-streams)) and Bytewax ([https://bytewax.io](https://bytewax.io/)). Additionally, I've briefly looked into other technologies like Beam, Flink, Druid, Pinot, and ClickHouse, but I'm still figuring out how they could fit into my setup. My goal is to avoid relying on a large loop with numerous database queries.\n\nI'd love to hear from anyone who has experience or insights into building similar systems. Specifically, I'm looking for:\n\n* Feedback on my current tech stack and its scalability for real-time alerting.\n* Suggestions on alternative technologies or architectures that could improve performance.\n* Insights into how high-performance platforms handle such tasks, particularly those that allow for numerous and varied user-defined alert conditions.\n\nAny advice or shared experiences would be greatly appreciated!", "author_fullname": "t2_131vjk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do Platforms Like TradingView Handle Real-Time Alerts on Various Price Points and Indicators?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17utom7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699934163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone..&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a stock/crypto scanner project and I&amp;#39;m curious about how platforms like TradingView efficiently handle real-time alerts for a myriad of user-defined conditions, like specific price points, technical indicators, or chart lines. These platforms seem to manage countless alert settings and deliver these alerts promptly to users, and I&amp;#39;m interested in understanding the underlying technology that makes this possible.&lt;/p&gt;\n\n&lt;p&gt;My current tech stack involves Python (Django frontend), websockets to Redpanda to PostgreSQL + TimescaleDB. However, I&amp;#39;m facing challenges with the speed of queries across multiple timeframes (15min, 30min, 60min, 4H, D, W, M, Q, Y), which are proving to be quite slow.&lt;/p&gt;\n\n&lt;p&gt;To address this, I&amp;#39;m exploring stream processing solutions like Quix (&lt;a href=\"https://github.com/quixio/quix-streams\"&gt;https://github.com/quixio/quix-streams&lt;/a&gt;) and Bytewax (&lt;a href=\"https://bytewax.io/\"&gt;https://bytewax.io&lt;/a&gt;). Additionally, I&amp;#39;ve briefly looked into other technologies like Beam, Flink, Druid, Pinot, and ClickHouse, but I&amp;#39;m still figuring out how they could fit into my setup. My goal is to avoid relying on a large loop with numerous database queries.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear from anyone who has experience or insights into building similar systems. Specifically, I&amp;#39;m looking for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Feedback on my current tech stack and its scalability for real-time alerting.&lt;/li&gt;\n&lt;li&gt;Suggestions on alternative technologies or architectures that could improve performance.&lt;/li&gt;\n&lt;li&gt;Insights into how high-performance platforms handle such tasks, particularly those that allow for numerous and varied user-defined alert conditions.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice or shared experiences would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?auto=webp&amp;s=abd59f721c35c521161d310a0af69b69ea1616c6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=74fa9f80c35a9f22499e8a3d7b6027720f7d424b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1dddc85f52d397cefa333dc4fe5ece988081644", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce582fd27bd3c8aff7824acb53831c8ef67a8a6a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f89c2a414d2f47a74f7b7d5f0f60f7779eabc2f4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cadabfec6a3d18c1d2d07e1b1e485370ac2eac68", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05198745e533b79b4f029ccad26e7e30f4adf560", "width": 1080, "height": 540}], "variants": {}, "id": "vpfV6_e6S-Wn-gmFhRT5batbQd1JHay180C8lHn6Xcg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17utom7", "is_robot_indexable": true, "report_reasons": null, "author": "tomk2020", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17utom7/how_do_platforms_like_tradingview_handle_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17utom7/how_do_platforms_like_tradingview_handle_realtime/", "subreddit_subscribers": 139641, "created_utc": 1699934163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently a data analyst considering going in a data engineering direction. However, I don\u2019t quite have the experience for it yet, and I plan to get some certifications for Snowflake, AWS, and Airflow (or if you have other recs, please let me know).\n\nI don\u2019t like my current manager and have an opportunity for a data governance analyst role. Would this help me at all in the future, or should I just stick it out in my current position?", "author_fullname": "t2_46o0lefg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would a data governance role help in a data engineering career?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uje8i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699905771.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently a data analyst considering going in a data engineering direction. However, I don\u2019t quite have the experience for it yet, and I plan to get some certifications for Snowflake, AWS, and Airflow (or if you have other recs, please let me know).&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t like my current manager and have an opportunity for a data governance analyst role. Would this help me at all in the future, or should I just stick it out in my current position?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17uje8i", "is_robot_indexable": true, "report_reasons": null, "author": "love2eatalot", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uje8i/would_a_data_governance_role_help_in_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uje8i/would_a_data_governance_role_help_in_a_data/", "subreddit_subscribers": 139641, "created_utc": 1699905771.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "To me, data modeling is a bit ambiguous. I think it depends on the situations and projects. Is there a way to explain \u201cI do data modeling this way\u201d?", "author_fullname": "t2_exfd2a9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you data model? Go scenarios and how you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uuimw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699937038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To me, data modeling is a bit ambiguous. I think it depends on the situations and projects. Is there a way to explain \u201cI do data modeling this way\u201d?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17uuimw", "is_robot_indexable": true, "report_reasons": null, "author": "GiantEarnings", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uuimw/how_do_you_data_model_go_scenarios_and_how_you_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uuimw/how_do_you_data_model_go_scenarios_and_how_you_do/", "subreddit_subscribers": 139641, "created_utc": 1699937038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is struggling to get test data that behaves the same as production data but has obfuscated PCI and PII data. What is the best way to get good quality test data for both pipelines and Data Scientist to use in their testing?  ", "author_fullname": "t2_6lh4st48", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Gathering or Generating Test Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ug9ms", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699897774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is struggling to get test data that behaves the same as production data but has obfuscated PCI and PII data. What is the best way to get good quality test data for both pipelines and Data Scientist to use in their testing?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ug9ms", "is_robot_indexable": true, "report_reasons": null, "author": "Equivalent_Bluebird7", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ug9ms/gathering_or_generating_test_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ug9ms/gathering_or_generating_test_data/", "subreddit_subscribers": 139641, "created_utc": 1699897774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started as a data analyst at a small ad tech firm a couple of years ago. This naturally morphed into a DE position thanks to the existing and barely maintained 10 year old data infrastructure. \n\nWe run ad campaigns on a DSP for our clients and offer them custom reporting. Each report can be delivered at any cadence, to any destination within reason, with custom datetime format, headers, file naming convention, file format (including Excel reports with multiple sheets), and so on. \n\nThe legacy system that handles report delivery was an unbelievable nightmare amalgam created by many different contractors over the years. One of my first big projects was building a replacement. The solution I came up with was very hacky and consists of:\n\n* 1 yaml file per report per client where all custom parameters related to its creation and delivery are specified\n* 1 Airflow DAG with 3 tasks per report per client (run query, build report, upload report)\n * DAG runs daily and scheduling is handled per task by checking config against execution date\n\nWe now have about 40 clients and most ask for several highly customized reports. As you can imagine the cracks are beginning to show and I spend the majority of my week duct taping it back together. I would greatly appreciate any guidance on building a system that follows common best practices. \n\nThanks", "author_fullname": "t2_a0gc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to manage automatic, customized reporting for clients?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ut96a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699932754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started as a data analyst at a small ad tech firm a couple of years ago. This naturally morphed into a DE position thanks to the existing and barely maintained 10 year old data infrastructure. &lt;/p&gt;\n\n&lt;p&gt;We run ad campaigns on a DSP for our clients and offer them custom reporting. Each report can be delivered at any cadence, to any destination within reason, with custom datetime format, headers, file naming convention, file format (including Excel reports with multiple sheets), and so on. &lt;/p&gt;\n\n&lt;p&gt;The legacy system that handles report delivery was an unbelievable nightmare amalgam created by many different contractors over the years. One of my first big projects was building a replacement. The solution I came up with was very hacky and consists of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1 yaml file per report per client where all custom parameters related to its creation and delivery are specified&lt;/li&gt;\n&lt;li&gt;1 Airflow DAG with 3 tasks per report per client (run query, build report, upload report)\n\n&lt;ul&gt;\n&lt;li&gt;DAG runs daily and scheduling is handled per task by checking config against execution date&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We now have about 40 clients and most ask for several highly customized reports. As you can imagine the cracks are beginning to show and I spend the majority of my week duct taping it back together. I would greatly appreciate any guidance on building a system that follows common best practices. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ut96a", "is_robot_indexable": true, "report_reasons": null, "author": "_nu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ut96a/what_is_the_best_way_to_manage_automatic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ut96a/what_is_the_best_way_to_manage_automatic/", "subreddit_subscribers": 139641, "created_utc": 1699932754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I signed up for the GCP Professional Data Engineering exam at the end of this month. Got an email few weeks ago that they start the new format start Nov 13 2023. Wondering if anyone might have taken the new format and can provide some feedback?", "author_fullname": "t2_iu3ot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP Professional Data Engineering New Format Nov 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uspgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699931079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I signed up for the GCP Professional Data Engineering exam at the end of this month. Got an email few weeks ago that they start the new format start Nov 13 2023. Wondering if anyone might have taken the new format and can provide some feedback?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17uspgf", "is_robot_indexable": true, "report_reasons": null, "author": "Cerivitus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uspgf/gcp_professional_data_engineering_new_format_nov/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uspgf/gcp_professional_data_engineering_new_format_nov/", "subreddit_subscribers": 139641, "created_utc": 1699931079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Basically I need to EL from a REST API, which requires a startDate and endDate parameter for each call that can't be greater than a 2 week range (source SaaS platform sets the limit). How do you guys work around this? Do you connect to your table (in my case on Snowflake) and pull the max date and use that as your startDate for each execution of the code? Planning on using Dagster running python code - and I'm pretty new to python. I can input credentials - get the athorization token, and get the JSON loaded into a dataframe just fine, but as far as making this a scalable/automated pipeline I'm a bit stuck... I'm sure someone has run into this at some point lol.", "author_fullname": "t2_3ugqxzu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you manage/store date parameters within API integrations for incremental loads?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uimxt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699903831.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically I need to EL from a REST API, which requires a startDate and endDate parameter for each call that can&amp;#39;t be greater than a 2 week range (source SaaS platform sets the limit). How do you guys work around this? Do you connect to your table (in my case on Snowflake) and pull the max date and use that as your startDate for each execution of the code? Planning on using Dagster running python code - and I&amp;#39;m pretty new to python. I can input credentials - get the athorization token, and get the JSON loaded into a dataframe just fine, but as far as making this a scalable/automated pipeline I&amp;#39;m a bit stuck... I&amp;#39;m sure someone has run into this at some point lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17uimxt", "is_robot_indexable": true, "report_reasons": null, "author": "Casdom33", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uimxt/how_do_you_managestore_date_parameters_within_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uimxt/how_do_you_managestore_date_parameters_within_api/", "subreddit_subscribers": 139641, "created_utc": 1699903831.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I've been thinking about a problem for a long time, and I'd like your insights. I\u2019m trying to have a standard, mathematical approach to the late arriving facts problem.\n\nHere\u2019s where I\u2019m so far.\n\nSay you have a fact table, with a unique\\_key, some dimensions (date or else) and some metrics (numbers, you'll average or sum). We snapshot that table at date 1 and at date 2. We suppose there is no schema update. And we ignore the collection of new data between date 1 and date 2.\n\nThis data is non moving. It\u2019s our orders, our billings, our rides, our bookings, etc.\n\nIn a perfect world, snapshot1 and snapshot2 are equal. In our case they are not equal.\n\n# Technical approach vs real-world approach \n\nTechnically, the diff between snapshot1 and 2 is composed of:\n\n* Added lines\n* Deleted lines\n* Modified lines, being the sum of:\n   * Affected column name, before\\_value, after\\_value \n\nThe breakdown of this data is pretty easy to compute. But this is pretty useless, without a root cause.\n\nThe root cause, is in one of 3 category (the trichotomy):\n\n1. External to the company\n2. Internal to the company, but external to the data team\n3. Internal to the company, and internal to the data team\n\nDo you agree so far ?\n\n# Category - Response\n\nMy first assumption is that each category has this best specific response:\n\nFor *category 1* (external to the company e.g. refunded transaction, cancelled booking). This will happen consistently, you have to deal with it once and for all. Ex: there is estimated metric, and actual metric. You know how to compute \u201cthis late arriving fact\u201d impact (e.g. all cancelled bookings that happened between \u201cestimated\u201d and \u201cactual\u201d). And, normally, estimated + late arriving fact impact = actual metric. It is quite expensive.\n\nFor *category 2* (internal to the company, but outside the data team e.g. production database update, CRM updates). This is my favorite. What I have observed is, as data team member you have to prove as fast as possible that is comes from another team. And make it their problem. Because, you cannot handle it as a category 1 because it would be too expensive, and there is a new case every month so the generic case is not solvable. It\u2019s an internal process problem. I'd like to break it down into more specific problems\n\nFor *category 3* (internal to the company, inside the data team e.g. it either a bug, or a bug fix). In that case, your data consumer do not have anything to know about it, the bug and the bugfix should be as close as possible (aka fix the bug asap). And, for self improvement, you measure how many category 3 happened and how long it took to fix it.\n\nSo I'm [developing this tool](https://github.com/data-drift/data-drift) and there are these 2 challenges where I\u2019m stuck: \n\n* from the technical perspective (added, deleted, modified lines) is there a way to find the category ? I think it is possible, as code, with a user defined function that takes the added, deleted, modified lines, and decides if it is a known case of category 1, or not. But for category 2 or 3, I think it has to be a **human process**. Or maybe something about \"for that table, if there is more than 10 lines its most likely a bug category 3, other wise it is a human modification somewhere\"\n* What to do for category 2 ? We can report it and try reduce it. I don't know so I try to break the problem down to sub categories. My examples are \"bugs from software team\", \"adjusting the CRM/billing data\" . There is maybe something about, \"is it supposed to happen again ?\" and \"will we revert that change ?\", waiting for ideas :D \n\nThanks for reading ! ", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Late arriving facts: trichotomy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ufwv5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699896839.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;ve been thinking about a problem for a long time, and I&amp;#39;d like your insights. I\u2019m trying to have a standard, mathematical approach to the late arriving facts problem.&lt;/p&gt;\n\n&lt;p&gt;Here\u2019s where I\u2019m so far.&lt;/p&gt;\n\n&lt;p&gt;Say you have a fact table, with a unique_key, some dimensions (date or else) and some metrics (numbers, you&amp;#39;ll average or sum). We snapshot that table at date 1 and at date 2. We suppose there is no schema update. And we ignore the collection of new data between date 1 and date 2.&lt;/p&gt;\n\n&lt;p&gt;This data is non moving. It\u2019s our orders, our billings, our rides, our bookings, etc.&lt;/p&gt;\n\n&lt;p&gt;In a perfect world, snapshot1 and snapshot2 are equal. In our case they are not equal.&lt;/p&gt;\n\n&lt;h1&gt;Technical approach vs real-world approach&lt;/h1&gt;\n\n&lt;p&gt;Technically, the diff between snapshot1 and 2 is composed of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Added lines&lt;/li&gt;\n&lt;li&gt;Deleted lines&lt;/li&gt;\n&lt;li&gt;Modified lines, being the sum of:\n\n&lt;ul&gt;\n&lt;li&gt;Affected column name, before_value, after_value &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The breakdown of this data is pretty easy to compute. But this is pretty useless, without a root cause.&lt;/p&gt;\n\n&lt;p&gt;The root cause, is in one of 3 category (the trichotomy):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;External to the company&lt;/li&gt;\n&lt;li&gt;Internal to the company, but external to the data team&lt;/li&gt;\n&lt;li&gt;Internal to the company, and internal to the data team&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Do you agree so far ?&lt;/p&gt;\n\n&lt;h1&gt;Category - Response&lt;/h1&gt;\n\n&lt;p&gt;My first assumption is that each category has this best specific response:&lt;/p&gt;\n\n&lt;p&gt;For &lt;em&gt;category 1&lt;/em&gt; (external to the company e.g. refunded transaction, cancelled booking). This will happen consistently, you have to deal with it once and for all. Ex: there is estimated metric, and actual metric. You know how to compute \u201cthis late arriving fact\u201d impact (e.g. all cancelled bookings that happened between \u201cestimated\u201d and \u201cactual\u201d). And, normally, estimated + late arriving fact impact = actual metric. It is quite expensive.&lt;/p&gt;\n\n&lt;p&gt;For &lt;em&gt;category 2&lt;/em&gt; (internal to the company, but outside the data team e.g. production database update, CRM updates). This is my favorite. What I have observed is, as data team member you have to prove as fast as possible that is comes from another team. And make it their problem. Because, you cannot handle it as a category 1 because it would be too expensive, and there is a new case every month so the generic case is not solvable. It\u2019s an internal process problem. I&amp;#39;d like to break it down into more specific problems&lt;/p&gt;\n\n&lt;p&gt;For &lt;em&gt;category 3&lt;/em&gt; (internal to the company, inside the data team e.g. it either a bug, or a bug fix). In that case, your data consumer do not have anything to know about it, the bug and the bugfix should be as close as possible (aka fix the bug asap). And, for self improvement, you measure how many category 3 happened and how long it took to fix it.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m &lt;a href=\"https://github.com/data-drift/data-drift\"&gt;developing this tool&lt;/a&gt; and there are these 2 challenges where I\u2019m stuck: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;from the technical perspective (added, deleted, modified lines) is there a way to find the category ? I think it is possible, as code, with a user defined function that takes the added, deleted, modified lines, and decides if it is a known case of category 1, or not. But for category 2 or 3, I think it has to be a &lt;strong&gt;human process&lt;/strong&gt;. Or maybe something about &amp;quot;for that table, if there is more than 10 lines its most likely a bug category 3, other wise it is a human modification somewhere&amp;quot;&lt;/li&gt;\n&lt;li&gt;What to do for category 2 ? We can report it and try reduce it. I don&amp;#39;t know so I try to break the problem down to sub categories. My examples are &amp;quot;bugs from software team&amp;quot;, &amp;quot;adjusting the CRM/billing data&amp;quot; . There is maybe something about, &amp;quot;is it supposed to happen again ?&amp;quot; and &amp;quot;will we revert that change ?&amp;quot;, waiting for ideas :D &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for reading ! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?auto=webp&amp;s=f5071d66ba016d02d16d8fe542a9d3a1362d5bf7", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=63aec543dc09e53536c18730de8b14feffd9616c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=77721d607863f9dcc0b35ce27084cf2b324c0495", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=10f6776ecf4e47448be36f11ff95ea973265598d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=62fafe3a3592eb7c8d5c5a62769451734ab17e0d", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9a7b8452647be660d7b238d31a61a5c70c538f3d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/ZgqAxsNyaTJF1vvgaDOQkXcup_o7LohGKlA2FWW9vOo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73bf82c5fd0823a47c23d23ce9ced940a09a6407", "width": 1080, "height": 540}], "variants": {}, "id": "gaq4gpMffllPSr4TtkFHfZvBLuKF-ddT00PmCOIaLdo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17ufwv5", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ufwv5/late_arriving_facts_trichotomy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ufwv5/late_arriving_facts_trichotomy/", "subreddit_subscribers": 139641, "created_utc": 1699896839.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I need to ensure that the company where I work is capable of handling requests for personal data deletion (due to GDPR). I am looking for a solution that can centralize data from different data sources (MSSQS, MySql, etc.) so that I can identify where sensitive information is stored and anonymize it. Which solution can I use? I've researched and found Atlan, but I'm unsure if it's the perfect fit for my needs. ", "author_fullname": "t2_dat51pdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on data discovery solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17v49dz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699974609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to ensure that the company where I work is capable of handling requests for personal data deletion (due to GDPR). I am looking for a solution that can centralize data from different data sources (MSSQS, MySql, etc.) so that I can identify where sensitive information is stored and anonymize it. Which solution can I use? I&amp;#39;ve researched and found Atlan, but I&amp;#39;m unsure if it&amp;#39;s the perfect fit for my needs. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v49dz", "is_robot_indexable": true, "report_reasons": null, "author": "cyberporing", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v49dz/need_advice_on_data_discovery_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v49dz/need_advice_on_data_discovery_solutions/", "subreddit_subscribers": 139641, "created_utc": 1699974609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The title pretty much says it all. I\u2019m using Databricks Lakehouse as my warehouse in case that changes anything. We\u2019re using incremental materializations for our fact table data into Silver, but the architects want table materialization for dimension data as a whole, and fact data from Silver to Gold.", "author_fullname": "t2_xbe9keo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I materialize my models as tables in dbt but not actually drop my tables every time the model is run?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v1i0h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699966091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title pretty much says it all. I\u2019m using Databricks Lakehouse as my warehouse in case that changes anything. We\u2019re using incremental materializations for our fact table data into Silver, but the architects want table materialization for dimension data as a whole, and fact data from Silver to Gold.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v1i0h", "is_robot_indexable": true, "report_reasons": null, "author": "therealtonyryantime", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v1i0h/how_can_i_materialize_my_models_as_tables_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v1i0h/how_can_i_materialize_my_models_as_tables_in_dbt/", "subreddit_subscribers": 139641, "created_utc": 1699966091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m using fivetran as a connector but i could find Odoo to Bigquery \nAny idea how to replicate my data into my bigquery data warehouse?", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Odoo &lt;&gt; Bigquery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v10wf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699964313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m using fivetran as a connector but i could find Odoo to Bigquery \nAny idea how to replicate my data into my bigquery data warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v10wf", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v10wf/odoo_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v10wf/odoo_bigquery/", "subreddit_subscribers": 139641, "created_utc": 1699964313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a company in a product quality team. This team is using excel as a database and I am currently trying to persuade the uppermanagement the need of implementing a data system that the inspecters are able to use to input the results. \n\n1. 2k units inspected per week\n2. So much data errors\n3. 8 separate excel sheets that needs to be manually aggregated\n\n Please help", "author_fullname": "t2_5uvrlw9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inspection data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uooz1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699919298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a company in a product quality team. This team is using excel as a database and I am currently trying to persuade the uppermanagement the need of implementing a data system that the inspecters are able to use to input the results. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;2k units inspected per week&lt;/li&gt;\n&lt;li&gt;So much data errors&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;8 separate excel sheets that needs to be manually aggregated&lt;/p&gt;\n\n&lt;p&gt;Please help&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17uooz1", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_Ball_58", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uooz1/inspection_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uooz1/inspection_data/", "subreddit_subscribers": 139641, "created_utc": 1699919298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\nRecently I have written a blog on Spark Optimizations in Medium.., would highly appreciate your reviews..\n\nlink: [https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1](https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1)\n\nThank you...", "author_fullname": "t2_b9lhjoqu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Optimizations &amp; Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v19u6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699965242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;Recently I have written a blog on Spark Optimizations in Medium.., would highly appreciate your reviews..&lt;/p&gt;\n\n&lt;p&gt;link: &lt;a href=\"https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1\"&gt;https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?auto=webp&amp;s=e19c54812ba96b901b55cc127fa6ba6b84465164", "width": 1200, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc787dc826cc9ba7b2a0415ff7039dc8edfab4ba", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66c58e683324bef9e85c739f0ccd24d321cb6121", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=af8ee5a165e80af11111b8c168a15b4356fcd864", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7c652ea446c6cd7e3230f515d29026f1e893322", "width": 640, "height": 346}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a0cf82e63521d7f38b7ad13fa093a7a1f3de1fd", "width": 960, "height": 520}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f858035747215b6705bfbaf47797f9e986b68795", "width": 1080, "height": 585}], "variants": {}, "id": "RTzjCNOob_aJ6Ogb_CAni_XWChMnzTF_FIm-ifa23Zk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v19u6", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical_Duty8486", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v19u6/spark_optimizations_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v19u6/spark_optimizations_databricks/", "subreddit_subscribers": 139641, "created_utc": 1699965242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_c5qi5iukw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Personal Odyssey: \u201cTransforming Data Analytics with AI\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17uz7yw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2RbtNib7Tb1qf0rIeTxN2L6bMN_AI_cswEpRxweyMsk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699956835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/emrcv4gega0c1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/emrcv4gega0c1.png?auto=webp&amp;s=e641c6ce3bd8e8ca7a487d664c1c422cf3ed4af3", "width": 4184, "height": 2108}, "resolutions": [{"url": "https://preview.redd.it/emrcv4gega0c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e11c5e27db9ff611210473a5001e0d19408b2597", "width": 108, "height": 54}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ca8970619b3224854c0103fc4beda842e793eb2", "width": 216, "height": 108}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fdac6852afcd5eebfda79fcc184b4a41d7ba47b", "width": 320, "height": 161}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1e9ce5b0c691de0a595dbd3c0b6391ad79c6182", "width": 640, "height": 322}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08a4e3d1988c1dcff300989f8c94eeec2c8a6ff9", "width": 960, "height": 483}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9a7c5798c217403019734db5be074abfe116cecd", "width": 1080, "height": 544}], "variants": {}, "id": "9qbS3SOEIwlA1QHAaXh0VYAmEzHVCKByJTy0_rB_mAw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17uz7yw", "is_robot_indexable": true, "report_reasons": null, "author": "legoaitech", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uz7yw/a_personal_odyssey_transforming_data_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/emrcv4gega0c1.png", "subreddit_subscribers": 139641, "created_utc": 1699956835.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background - I have a background in data analytics with 4 years of experience. After my post graduate certificate I moved into data engineering. And, I am currently working in my first role as a dara engineer as a contractor with bank.\n\nLocation - Toronto\nSalary - CAD 70,000 per annum \n\nSituation\nWhen I joined, there were two more people who joined with me. Me, having done some courses and a portfolio project, had the most experience and ended up contributing upto 90 percent of the code written for my spark project, without 10,000+ lines of code.\n\nI am being paid the same as other two, and I am the one making most contribution as well as have activate participation in discussions.\n\nHow should I deal with it. Being on contract means I cannot negotiate much of salary (from what I have heard about the team's structure).\n\nI applied to other opportunities but with major experience in data analytics, I am just getting rejections.\n\nWhat to do. Please advise?", "author_fullname": "t2_5db267r6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Major contributor paid fresher salary", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17usm4e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699930820.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background - I have a background in data analytics with 4 years of experience. After my post graduate certificate I moved into data engineering. And, I am currently working in my first role as a dara engineer as a contractor with bank.&lt;/p&gt;\n\n&lt;p&gt;Location - Toronto\nSalary - CAD 70,000 per annum &lt;/p&gt;\n\n&lt;p&gt;Situation\nWhen I joined, there were two more people who joined with me. Me, having done some courses and a portfolio project, had the most experience and ended up contributing upto 90 percent of the code written for my spark project, without 10,000+ lines of code.&lt;/p&gt;\n\n&lt;p&gt;I am being paid the same as other two, and I am the one making most contribution as well as have activate participation in discussions.&lt;/p&gt;\n\n&lt;p&gt;How should I deal with it. Being on contract means I cannot negotiate much of salary (from what I have heard about the team&amp;#39;s structure).&lt;/p&gt;\n\n&lt;p&gt;I applied to other opportunities but with major experience in data analytics, I am just getting rejections.&lt;/p&gt;\n\n&lt;p&gt;What to do. Please advise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17usm4e", "is_robot_indexable": true, "report_reasons": null, "author": "pulkit_kapoor", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17usm4e/major_contributor_paid_fresher_salary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17usm4e/major_contributor_paid_fresher_salary/", "subreddit_subscribers": 139641, "created_utc": 1699930820.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}