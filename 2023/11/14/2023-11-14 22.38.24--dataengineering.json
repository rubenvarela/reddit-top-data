{"kind": "Listing", "data": {"after": "t3_17v19u6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am confused \ud83d\ude15 in choosing the right role in IT. I did my bachelor in electrical 1.5 years back but want to switch to IT. I did nothing in between tried doing business but failed miserably. Preparing for MS now. But confused.\nI know basic of SQL and Python and that's it.\n\nFirst i wanted to go into data science but I found out that entry level job is hard to get there. \nOther options for me is either data engineer,cloud engineer or software developer.\n\nI am looking For a role that has high salary and less competition, unlike data science.", "author_fullname": "t2_7vkbdldt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which role in IT has good salary and less compitition?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17urs2s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 32, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 32, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699928288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am confused \ud83d\ude15 in choosing the right role in IT. I did my bachelor in electrical 1.5 years back but want to switch to IT. I did nothing in between tried doing business but failed miserably. Preparing for MS now. But confused.\nI know basic of SQL and Python and that&amp;#39;s it.&lt;/p&gt;\n\n&lt;p&gt;First i wanted to go into data science but I found out that entry level job is hard to get there. \nOther options for me is either data engineer,cloud engineer or software developer.&lt;/p&gt;\n\n&lt;p&gt;I am looking For a role that has high salary and less competition, unlike data science.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17urs2s", "is_robot_indexable": true, "report_reasons": null, "author": "Munib_raza_khan", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17urs2s/which_role_in_it_has_good_salary_and_less/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17urs2s/which_role_in_it_has_good_salary_and_less/", "subreddit_subscribers": 139700, "created_utc": 1699928288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work at a well known tech company, with about 5 years of experience.\n\n\n\n\nSince the competition to get into other unicorn and top tech companies have been too much, I've been applying to mid-level and senior data engineering roles at startups to add some variety to my engineering experience.  I've honestly been surprised that I've never been able to get an offer.\n\n\n\n\nAre a lot of startups and smaller tech/non-tech companies now only considering very experienced senior/staff engineers with way more years of experience than me?  The tech screenings have been going pretty well, as I do a lot of Leetcode and system design review.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are most companies that are hiring only considering extremely qualified candidates?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v81vl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699984810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work at a well known tech company, with about 5 years of experience.&lt;/p&gt;\n\n&lt;p&gt;Since the competition to get into other unicorn and top tech companies have been too much, I&amp;#39;ve been applying to mid-level and senior data engineering roles at startups to add some variety to my engineering experience.  I&amp;#39;ve honestly been surprised that I&amp;#39;ve never been able to get an offer.&lt;/p&gt;\n\n&lt;p&gt;Are a lot of startups and smaller tech/non-tech companies now only considering very experienced senior/staff engineers with way more years of experience than me?  The tech screenings have been going pretty well, as I do a lot of Leetcode and system design review.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17v81vl", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17v81vl/are_most_companies_that_are_hiring_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v81vl/are_most_companies_that_are_hiring_only/", "subreddit_subscribers": 139700, "created_utc": 1699984810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m after finishing the snowflake fundamentals course and want to pursue as many certifications as possible, any advice would be great, thank you.", "author_fullname": "t2_6j3548qy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best snowflake certifications/hands on essentials to have as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uzdy2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699957617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m after finishing the snowflake fundamentals course and want to pursue as many certifications as possible, any advice would be great, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17uzdy2", "is_robot_indexable": true, "report_reasons": null, "author": "Kokadoodles", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uzdy2/what_are_the_best_snowflake_certificationshands/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uzdy2/what_are_the_best_snowflake_certificationshands/", "subreddit_subscribers": 139700, "created_utc": 1699957617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How many data engineers work on the same project/team as you? Put another way, how many developers could conceivably pick up the same tickets you do?\n\nDo you feel your team/project is understaffed, overstaffed, or at just the right level in terms of data engineers?\n\nI\u2019ll start: I oversee a team of four data engineers spread across three data warehouses. Warehouse A and B have one dedicated engineer each. Warehouse C has 2. I work on tickets where needed in addition to people management duties. Org has roughly 6,000 employees.\n\nFeeling very understaffed. We can add new features/data elements but rarely have time for tackling the mountain of technical debt. System stability is starting to pay the price.", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE team size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17usal7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699929823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How many data engineers work on the same project/team as you? Put another way, how many developers could conceivably pick up the same tickets you do?&lt;/p&gt;\n\n&lt;p&gt;Do you feel your team/project is understaffed, overstaffed, or at just the right level in terms of data engineers?&lt;/p&gt;\n\n&lt;p&gt;I\u2019ll start: I oversee a team of four data engineers spread across three data warehouses. Warehouse A and B have one dedicated engineer each. Warehouse C has 2. I work on tickets where needed in addition to people management duties. Org has roughly 6,000 employees.&lt;/p&gt;\n\n&lt;p&gt;Feeling very understaffed. We can add new features/data elements but rarely have time for tackling the mountain of technical debt. System stability is starting to pay the price.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17usal7", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17usal7/de_team_size/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17usal7/de_team_size/", "subreddit_subscribers": 139700, "created_utc": 1699929823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_cj9opj0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why Cloud Data Warehouses Are Too Expensive For Emerging Data Requirements", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17vacxc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CyxSMP3QptO0N4Hyv2xBxcbPUL6f9DCQaceKC3TvUzc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699990810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ensembleanalytics.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ensembleanalytics.io/blog/why-cloud-datawarehouses-too-expensive", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GQnxzYiJ4bjDawvr-CH8LxdtOud5qsIFYbHS50mMsNw.jpg?auto=webp&amp;s=837e7d40e8b7010a4c4532804d07796662a7243e", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/GQnxzYiJ4bjDawvr-CH8LxdtOud5qsIFYbHS50mMsNw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d9a978076bdae0123751520b4483d24d285c118", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/GQnxzYiJ4bjDawvr-CH8LxdtOud5qsIFYbHS50mMsNw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3a25c39d21e2e27c34076a99a22db43b733096c", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/GQnxzYiJ4bjDawvr-CH8LxdtOud5qsIFYbHS50mMsNw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d53efcf21821cb6532cc91cc67369fbfc21976fe", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/GQnxzYiJ4bjDawvr-CH8LxdtOud5qsIFYbHS50mMsNw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d3c98d4bd5dd0095a1eaa3c61f913df2f1a98cb", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/GQnxzYiJ4bjDawvr-CH8LxdtOud5qsIFYbHS50mMsNw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=acf832ad44e991fccb1d11d74b7e58ab8691651c", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/GQnxzYiJ4bjDawvr-CH8LxdtOud5qsIFYbHS50mMsNw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=81f1f85a5ec4a9442b2eed707aeee7e4dbc4a5df", "width": 1080, "height": 567}], "variants": {}, "id": "6kFBzs4cgQu_iPxWLdsYwK8w6us5SgIK5F0mp2x2ma0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vacxc", "is_robot_indexable": true, "report_reasons": null, "author": "benjaminwootton81", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vacxc/why_cloud_data_warehouses_are_too_expensive_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ensembleanalytics.io/blog/why-cloud-datawarehouses-too-expensive", "subreddit_subscribers": 139700, "created_utc": 1699990810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone..\n\nI'm currently working on a stock/crypto scanner project and I'm curious about how platforms like TradingView efficiently handle real-time alerts for a myriad of user-defined conditions, like specific price points, technical indicators, or chart lines. These platforms seem to manage countless alert settings and deliver these alerts promptly to users, and I'm interested in understanding the underlying technology that makes this possible.\n\nMy current tech stack involves Python (Django frontend), websockets to Redpanda to PostgreSQL + TimescaleDB. However, I'm facing challenges with the speed of queries across multiple timeframes (15min, 30min, 60min, 4H, D, W, M, Q, Y), which are proving to be quite slow.\n\nTo address this, I'm exploring stream processing solutions like Quix ([https://github.com/quixio/quix-streams](https://github.com/quixio/quix-streams)) and Bytewax ([https://bytewax.io](https://bytewax.io/)). Additionally, I've briefly looked into other technologies like Beam, Flink, Druid, Pinot, and ClickHouse, but I'm still figuring out how they could fit into my setup. My goal is to avoid relying on a large loop with numerous database queries.\n\nI'd love to hear from anyone who has experience or insights into building similar systems. Specifically, I'm looking for:\n\n* Feedback on my current tech stack and its scalability for real-time alerting.\n* Suggestions on alternative technologies or architectures that could improve performance.\n* Insights into how high-performance platforms handle such tasks, particularly those that allow for numerous and varied user-defined alert conditions.\n\nAny advice or shared experiences would be greatly appreciated!", "author_fullname": "t2_131vjk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How Do Platforms Like TradingView Handle Real-Time Alerts on Various Price Points and Indicators?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17utom7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699934163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone..&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a stock/crypto scanner project and I&amp;#39;m curious about how platforms like TradingView efficiently handle real-time alerts for a myriad of user-defined conditions, like specific price points, technical indicators, or chart lines. These platforms seem to manage countless alert settings and deliver these alerts promptly to users, and I&amp;#39;m interested in understanding the underlying technology that makes this possible.&lt;/p&gt;\n\n&lt;p&gt;My current tech stack involves Python (Django frontend), websockets to Redpanda to PostgreSQL + TimescaleDB. However, I&amp;#39;m facing challenges with the speed of queries across multiple timeframes (15min, 30min, 60min, 4H, D, W, M, Q, Y), which are proving to be quite slow.&lt;/p&gt;\n\n&lt;p&gt;To address this, I&amp;#39;m exploring stream processing solutions like Quix (&lt;a href=\"https://github.com/quixio/quix-streams\"&gt;https://github.com/quixio/quix-streams&lt;/a&gt;) and Bytewax (&lt;a href=\"https://bytewax.io/\"&gt;https://bytewax.io&lt;/a&gt;). Additionally, I&amp;#39;ve briefly looked into other technologies like Beam, Flink, Druid, Pinot, and ClickHouse, but I&amp;#39;m still figuring out how they could fit into my setup. My goal is to avoid relying on a large loop with numerous database queries.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear from anyone who has experience or insights into building similar systems. Specifically, I&amp;#39;m looking for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Feedback on my current tech stack and its scalability for real-time alerting.&lt;/li&gt;\n&lt;li&gt;Suggestions on alternative technologies or architectures that could improve performance.&lt;/li&gt;\n&lt;li&gt;Insights into how high-performance platforms handle such tasks, particularly those that allow for numerous and varied user-defined alert conditions.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice or shared experiences would be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?auto=webp&amp;s=abd59f721c35c521161d310a0af69b69ea1616c6", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=74fa9f80c35a9f22499e8a3d7b6027720f7d424b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1dddc85f52d397cefa333dc4fe5ece988081644", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ce582fd27bd3c8aff7824acb53831c8ef67a8a6a", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f89c2a414d2f47a74f7b7d5f0f60f7779eabc2f4", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cadabfec6a3d18c1d2d07e1b1e485370ac2eac68", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/EcgBLUGbhESs66kHXsarg-75oAI-8E8N8Ncvui6SEWs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05198745e533b79b4f029ccad26e7e30f4adf560", "width": 1080, "height": 540}], "variants": {}, "id": "vpfV6_e6S-Wn-gmFhRT5batbQd1JHay180C8lHn6Xcg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17utom7", "is_robot_indexable": true, "report_reasons": null, "author": "tomk2020", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17utom7/how_do_platforms_like_tradingview_handle_realtime/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17utom7/how_do_platforms_like_tradingview_handle_realtime/", "subreddit_subscribers": 139700, "created_utc": 1699934163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My colleague u/gunnarmorling and I have launched a monthly collection of read-worthy posts, events, announcements, and papers in the **data and stream processing space**.\n\n\ud83d\udcf0 The first edition is online now: [https://www.decodable.co/blog/checkpoint-chronicle-november-2023](https://www.decodable.co/blog/checkpoint-chronicle-november-2023)\n\nWould love to get any feedback (flamesuit at the ready ;) )", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checkpoint Chronicle - a monthly collection of interesting stuff in the data and stream processing space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v82or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699984858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My colleague &lt;a href=\"/u/gunnarmorling\"&gt;u/gunnarmorling&lt;/a&gt; and I have launched a monthly collection of read-worthy posts, events, announcements, and papers in the &lt;strong&gt;data and stream processing space&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcf0 The first edition is online now: &lt;a href=\"https://www.decodable.co/blog/checkpoint-chronicle-november-2023\"&gt;https://www.decodable.co/blog/checkpoint-chronicle-november-2023&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to get any feedback (flamesuit at the ready ;) )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?auto=webp&amp;s=a5142a62838c4edbf1c8b142600c82d4078d4b8f", "width": 640, "height": 402}, "resolutions": [{"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e83c3976ff24e6f20b66f959a4f87a8ccd221c9", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eeac073f1252f578b40279dffec10df8b889ae68", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84f7d88cecec068b380bfd91ff2a78bbf7f3624c", "width": 320, "height": 201}, {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3633d887d2af2d791833a95162b8cf03423b1c05", "width": 640, "height": 402}], "variants": {}, "id": "ozZsXqouUgAEnICZ6yJXC3C6GWE3PNs5b5Br2ZNTpuw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v82or", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v82or/checkpoint_chronicle_a_monthly_collection_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v82or/checkpoint_chronicle_a_monthly_collection_of/", "subreddit_subscribers": 139700, "created_utc": 1699984858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey fellow SQL monkeys, some of you may know that my team has been drowning in adhoc SQL requests, most of them are simple and should be automated. The time spent on context switching and dealing with the stakeholders could be better spent on modelling, cleaning up our dbt project, and improving our pipeline orchestration.\n\nSome background on me: I've been building and scaling data teams at startups the last 5 years. The startup I'm at now has about 300 employees with a 3 person data team including me. We use \"self-serve\" tools like Looker and hold periodic training sessions, but there's always data requests that can't be self-served and it blows up our backlog. Our team is stretched thin and we decided to deploy a LLM to automate these requests. It's been working very well, we're seeing most requests answered by the LLM, with only a few that we have to review. \n\nWe've been posting in a few data communities in Reddit over the past month and it seems like this isn't just an issue at my org, that's why we're hoping to get your feedback to make something awesome and give time back to your teams", "author_fullname": "t2_l386p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "looking for users to pilot tool to automate adhoc SQL requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v5yak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699979260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey fellow SQL monkeys, some of you may know that my team has been drowning in adhoc SQL requests, most of them are simple and should be automated. The time spent on context switching and dealing with the stakeholders could be better spent on modelling, cleaning up our dbt project, and improving our pipeline orchestration.&lt;/p&gt;\n\n&lt;p&gt;Some background on me: I&amp;#39;ve been building and scaling data teams at startups the last 5 years. The startup I&amp;#39;m at now has about 300 employees with a 3 person data team including me. We use &amp;quot;self-serve&amp;quot; tools like Looker and hold periodic training sessions, but there&amp;#39;s always data requests that can&amp;#39;t be self-served and it blows up our backlog. Our team is stretched thin and we decided to deploy a LLM to automate these requests. It&amp;#39;s been working very well, we&amp;#39;re seeing most requests answered by the LLM, with only a few that we have to review. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve been posting in a few data communities in Reddit over the past month and it seems like this isn&amp;#39;t just an issue at my org, that&amp;#39;s why we&amp;#39;re hoping to get your feedback to make something awesome and give time back to your teams&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v5yak", "is_robot_indexable": true, "report_reasons": null, "author": "ruckrawjers", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v5yak/looking_for_users_to_pilot_tool_to_automate_adhoc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v5yak/looking_for_users_to_pilot_tool_to_automate_adhoc/", "subreddit_subscribers": 139700, "created_utc": 1699979260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "To me, data modeling is a bit ambiguous. I think it depends on the situations and projects. Is there a way to explain \u201cI do data modeling this way\u201d?", "author_fullname": "t2_exfd2a9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you data model? Go scenarios and how you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uuimw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699937038.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;To me, data modeling is a bit ambiguous. I think it depends on the situations and projects. Is there a way to explain \u201cI do data modeling this way\u201d?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17uuimw", "is_robot_indexable": true, "report_reasons": null, "author": "GiantEarnings", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uuimw/how_do_you_data_model_go_scenarios_and_how_you_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uuimw/how_do_you_data_model_go_scenarios_and_how_you_do/", "subreddit_subscribers": 139700, "created_utc": 1699937038.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\u2b50Become a next level Senior Engineer\nIf you feel stuck and thinking ways to contribute then give this blog a read as explain ten ways to be a better Senior Engineer.\n\n\ud83d\ude80Help Setting up Team Goals\n\ud83d\udd13Unblocking the Team\n\ud83e\udd35\u200d\u2642\ufe0fLeading by Example\n\ud83e\uddf9Doing the Grungiest Work \n\ud83d\udd0aSharing Work Publicly\n\nAnd many more discussed in detail: https://www.junaideffendi.com/blog/next-level-senior-engineer/\n\nLet me know what elde Senior folks have been doing to stand out.", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "10 ways to be better Senior Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": true, "name": "t3_17vcv54", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/taGHryiwbenKnI4LiUGMEv3GfRXUbiIce_xkzA7-vi8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699997584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\u2b50Become a next level Senior Engineer\nIf you feel stuck and thinking ways to contribute then give this blog a read as explain ten ways to be a better Senior Engineer.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80Help Setting up Team Goals\n\ud83d\udd13Unblocking the Team\n\ud83e\udd35\u200d\u2642\ufe0fLeading by Example\n\ud83e\uddf9Doing the Grungiest Work \n\ud83d\udd0aSharing Work Publicly&lt;/p&gt;\n\n&lt;p&gt;And many more discussed in detail: &lt;a href=\"https://www.junaideffendi.com/blog/next-level-senior-engineer/\"&gt;https://www.junaideffendi.com/blog/next-level-senior-engineer/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what elde Senior folks have been doing to stand out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/lf7liybmtd0c1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/lf7liybmtd0c1.jpg?auto=webp&amp;s=c08a369b4f693385b6ff1848f6b6c8597ae34b58", "width": 5760, "height": 3240}, "resolutions": [{"url": "https://preview.redd.it/lf7liybmtd0c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f4620275c5835750915a0dce8ab52ed51ce3f76f", "width": 108, "height": 60}, {"url": "https://preview.redd.it/lf7liybmtd0c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3456c4b68a1a919e83d22b24763ab0b34e912b4c", "width": 216, "height": 121}, {"url": "https://preview.redd.it/lf7liybmtd0c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=01a0e7af56e14641a026d68efa2606c9d1dd7706", "width": 320, "height": 180}, {"url": "https://preview.redd.it/lf7liybmtd0c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=09b4d6c9e4ffe6c10f0f06c2f0cb1c85a607e37e", "width": 640, "height": 360}, {"url": "https://preview.redd.it/lf7liybmtd0c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3198e3ce2643a1c3b88ae91f76c406f72e0007c", "width": 960, "height": 540}, {"url": "https://preview.redd.it/lf7liybmtd0c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0711326b19c7519a6ec327fd2d8ade4ea1e27662", "width": 1080, "height": 607}], "variants": {}, "id": "IPEBNFAWDmfGp55Lt74Xa4yHTyoYm3mHHANYyvZu6W4"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vcv54", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vcv54/10_ways_to_be_better_senior_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/lf7liybmtd0c1.jpg", "subreddit_subscribers": 139700, "created_utc": 1699997584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have experience with building star schema data marts and small - single use case focused - data warehouses but I am wondering about an enterprise (spanning all verticals within the company) data warehouse design best practices.\n\nSome questions that come to mind:\n\n1. As a rule of thumb.  for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called \"stage\" tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on a larger scale too or if maybe you just let historical information sit in file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.\n2. Do you **track history** for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called \"stage\" tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on larger scale too or if maybe you just let historical information sitting in a file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.\n3. How do you deal with **sensitive data** (PII, etc), do you separate them into special tables? Do you tokenize potentially sensitive natural keys? Do you even let PII and such enter the warehouse or store it somewhere else (some kind of vault)?\n4. Do you try to build one \"**source of truth**\" set of tables, that models your whole business and that everybody else then builds on top of (e.g. departmental data marts), or do you just bring in the data, capture the history, and cleanse it and let the departmental experts figure out themselves how to combine 10 different versions of customer data (from different systems) into one customer dataset? If you do try to build a \"source of truth\" is the resulting model typically a Star/Galaxy schema, or data vault, or some kind of hybrid?\n5. Do you have a **centralized** team owning the data warehouses and building out all the tables/models or do you split it between different departmental experts (kinda what data mesh suggests)?\n\nI have an opinion on these things based on my smaller-scale experience but wonder what worked best for folks who actually designed/maintained enterprise-scale dwhs before. If it makes any difference, assume the use of modern cloud data stack technologies.", "author_fullname": "t2_gx2hs6l34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Ideal\" data model for an enterprise data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17vbmjo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699994398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have experience with building star schema data marts and small - single use case focused - data warehouses but I am wondering about an enterprise (spanning all verticals within the company) data warehouse design best practices.&lt;/p&gt;\n\n&lt;p&gt;Some questions that come to mind:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;As a rule of thumb.  for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called &amp;quot;stage&amp;quot; tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on a larger scale too or if maybe you just let historical information sit in file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.&lt;/li&gt;\n&lt;li&gt;Do you &lt;strong&gt;track history&lt;/strong&gt; for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called &amp;quot;stage&amp;quot; tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on larger scale too or if maybe you just let historical information sitting in a file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.&lt;/li&gt;\n&lt;li&gt;How do you deal with &lt;strong&gt;sensitive data&lt;/strong&gt; (PII, etc), do you separate them into special tables? Do you tokenize potentially sensitive natural keys? Do you even let PII and such enter the warehouse or store it somewhere else (some kind of vault)?&lt;/li&gt;\n&lt;li&gt;Do you try to build one &amp;quot;&lt;strong&gt;source of truth&lt;/strong&gt;&amp;quot; set of tables, that models your whole business and that everybody else then builds on top of (e.g. departmental data marts), or do you just bring in the data, capture the history, and cleanse it and let the departmental experts figure out themselves how to combine 10 different versions of customer data (from different systems) into one customer dataset? If you do try to build a &amp;quot;source of truth&amp;quot; is the resulting model typically a Star/Galaxy schema, or data vault, or some kind of hybrid?&lt;/li&gt;\n&lt;li&gt;Do you have a &lt;strong&gt;centralized&lt;/strong&gt; team owning the data warehouses and building out all the tables/models or do you split it between different departmental experts (kinda what data mesh suggests)?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have an opinion on these things based on my smaller-scale experience but wonder what worked best for folks who actually designed/maintained enterprise-scale dwhs before. If it makes any difference, assume the use of modern cloud data stack technologies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vbmjo", "is_robot_indexable": true, "report_reasons": null, "author": "InsightInk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vbmjo/ideal_data_model_for_an_enterprise_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vbmjo/ideal_data_model_for_an_enterprise_data_warehouse/", "subreddit_subscribers": 139700, "created_utc": 1699994398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A few hours? \n\nA few days? \n\nA sprint? More than 1 sprint? \n\n&amp;#x200B;\n\nWhat's a reasonable expectation? We often have an issue where business users expect a data warehouse to be a thing where you can just send data easily with a few clicks and then the users can query it. \n\nHowever we know it has to be ingested, tested, tables created, scheduled, upserted/merged, etc. But maybe our process is actually bad and slower than it needs to be. What is a reasonable expectation? ", "author_fullname": "t2_b3n9i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long should it take to integrate a new data source into the data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vapzq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699991785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few hours? &lt;/p&gt;\n\n&lt;p&gt;A few days? &lt;/p&gt;\n\n&lt;p&gt;A sprint? More than 1 sprint? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s a reasonable expectation? We often have an issue where business users expect a data warehouse to be a thing where you can just send data easily with a few clicks and then the users can query it. &lt;/p&gt;\n\n&lt;p&gt;However we know it has to be ingested, tested, tables created, scheduled, upserted/merged, etc. But maybe our process is actually bad and slower than it needs to be. What is a reasonable expectation? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vapzq", "is_robot_indexable": true, "report_reasons": null, "author": "third_dude", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vapzq/how_long_should_it_take_to_integrate_a_new_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vapzq/how_long_should_it_take_to_integrate_a_new_data/", "subreddit_subscribers": 139700, "created_utc": 1699991785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I recently joined a small healthcare as the first data analyst and soon after we purchased powerBI. I cirrently work on excel to clean data and powerbi to import and analyze, prepare dashboards/reports. While this is very simple to what I want to do as a data analyst (especially that we are expanding our services currently). While this also means more data and more complexity.\n\nCurrently I store data in a drive that is hosted by our IT department. \n\nI know basic SQL and want to keep that knowledge growing by using it at my work. I talked to my manager and he said if you think sql will help us any better than the regular drive where we store data, then we can get sql. \n\nWhats the best SQL thing (not a technical person here) that I can use in my company that will help me maintain a database, use sql commands frequently and what would be the benefits of using a sql database vs regular drive where I store data manually. Also is ms sql a good option or azure sql database? \n\nJust so you understand the nature of my work. We have 50+ employees submitting 50+ records everyday on google sheet. I take that data and paste in in excel, save the file with the employees name and date/year and store it in the drive. Every week, Im suppose to send a report saying how many records are submitting by each employee and whats the average, how many offices the reports were sent to and stuff like that. \n\nId appreciate a deep insight on what to use, how to convince manager and how to get sql in my organization", "author_fullname": "t2_83gq3oxts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS SQL / AZURE SQL data base?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v7i4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699983376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently joined a small healthcare as the first data analyst and soon after we purchased powerBI. I cirrently work on excel to clean data and powerbi to import and analyze, prepare dashboards/reports. While this is very simple to what I want to do as a data analyst (especially that we are expanding our services currently). While this also means more data and more complexity.&lt;/p&gt;\n\n&lt;p&gt;Currently I store data in a drive that is hosted by our IT department. &lt;/p&gt;\n\n&lt;p&gt;I know basic SQL and want to keep that knowledge growing by using it at my work. I talked to my manager and he said if you think sql will help us any better than the regular drive where we store data, then we can get sql. &lt;/p&gt;\n\n&lt;p&gt;Whats the best SQL thing (not a technical person here) that I can use in my company that will help me maintain a database, use sql commands frequently and what would be the benefits of using a sql database vs regular drive where I store data manually. Also is ms sql a good option or azure sql database? &lt;/p&gt;\n\n&lt;p&gt;Just so you understand the nature of my work. We have 50+ employees submitting 50+ records everyday on google sheet. I take that data and paste in in excel, save the file with the employees name and date/year and store it in the drive. Every week, Im suppose to send a report saying how many records are submitting by each employee and whats the average, how many offices the reports were sent to and stuff like that. &lt;/p&gt;\n\n&lt;p&gt;Id appreciate a deep insight on what to use, how to convince manager and how to get sql in my organization&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17v7i4n", "is_robot_indexable": true, "report_reasons": null, "author": "FigTraditional1201", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v7i4n/ms_sql_azure_sql_data_base/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v7i4n/ms_sql_azure_sql_data_base/", "subreddit_subscribers": 139700, "created_utc": 1699983376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using Meltano for data integration. I need to put data into an Iceberg data lake. Unfortunately, meltano doesn't have a target for Iceberg. Is there anyone who can point me in the right direction?", "author_fullname": "t2_1ajx7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone experienced with both Meltano and Iceberg?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v6jdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699985801.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699980834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Meltano for data integration. I need to put data into an Iceberg data lake. Unfortunately, meltano doesn&amp;#39;t have a target for Iceberg. Is there anyone who can point me in the right direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v6jdr", "is_robot_indexable": true, "report_reasons": null, "author": "TaeefNajib", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v6jdr/is_anyone_experienced_with_both_meltano_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v6jdr/is_anyone_experienced_with_both_meltano_and/", "subreddit_subscribers": 139700, "created_utc": 1699980834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I started as a data analyst at a small ad tech firm a couple of years ago. This naturally morphed into a DE position thanks to the existing and barely maintained 10 year old data infrastructure. \n\nWe run ad campaigns on a DSP for our clients and offer them custom reporting. Each report can be delivered at any cadence, to any destination within reason, with custom datetime format, headers, file naming convention, file format (including Excel reports with multiple sheets), and so on. \n\nThe legacy system that handles report delivery was an unbelievable nightmare amalgam created by many different contractors over the years. One of my first big projects was building a replacement. The solution I came up with was very hacky and consists of:\n\n* 1 yaml file per report per client where all custom parameters related to its creation and delivery are specified\n* 1 Airflow DAG with 3 tasks per report per client (run query, build report, upload report)\n * DAG runs daily and scheduling is handled per task by checking config against execution date\n\nWe now have about 40 clients and most ask for several highly customized reports. As you can imagine the cracks are beginning to show and I spend the majority of my week duct taping it back together. I would greatly appreciate any guidance on building a system that follows common best practices. \n\nThanks", "author_fullname": "t2_a0gc8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best way to manage automatic, customized reporting for clients?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ut96a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699932754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I started as a data analyst at a small ad tech firm a couple of years ago. This naturally morphed into a DE position thanks to the existing and barely maintained 10 year old data infrastructure. &lt;/p&gt;\n\n&lt;p&gt;We run ad campaigns on a DSP for our clients and offer them custom reporting. Each report can be delivered at any cadence, to any destination within reason, with custom datetime format, headers, file naming convention, file format (including Excel reports with multiple sheets), and so on. &lt;/p&gt;\n\n&lt;p&gt;The legacy system that handles report delivery was an unbelievable nightmare amalgam created by many different contractors over the years. One of my first big projects was building a replacement. The solution I came up with was very hacky and consists of:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;1 yaml file per report per client where all custom parameters related to its creation and delivery are specified&lt;/li&gt;\n&lt;li&gt;1 Airflow DAG with 3 tasks per report per client (run query, build report, upload report)\n\n&lt;ul&gt;\n&lt;li&gt;DAG runs daily and scheduling is handled per task by checking config against execution date&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We now have about 40 clients and most ask for several highly customized reports. As you can imagine the cracks are beginning to show and I spend the majority of my week duct taping it back together. I would greatly appreciate any guidance on building a system that follows common best practices. &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17ut96a", "is_robot_indexable": true, "report_reasons": null, "author": "_nu", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ut96a/what_is_the_best_way_to_manage_automatic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ut96a/what_is_the_best_way_to_manage_automatic/", "subreddit_subscribers": 139700, "created_utc": 1699932754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, I signed up for the GCP Professional Data Engineering exam at the end of this month. Got an email few weeks ago that they start the new format start Nov 13 2023. Wondering if anyone might have taken the new format and can provide some feedback?", "author_fullname": "t2_iu3ot", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP Professional Data Engineering New Format Nov 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uspgf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699931079.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, I signed up for the GCP Professional Data Engineering exam at the end of this month. Got an email few weeks ago that they start the new format start Nov 13 2023. Wondering if anyone might have taken the new format and can provide some feedback?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17uspgf", "is_robot_indexable": true, "report_reasons": null, "author": "Cerivitus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uspgf/gcp_professional_data_engineering_new_format_nov/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uspgf/gcp_professional_data_engineering_new_format_nov/", "subreddit_subscribers": 139700, "created_utc": 1699931079.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm currently working in somewhat of a data analytics role but am looking to transition into data engineering. In my current position, I have an idea to create my own project would be helpful to talk about in a portfolio.\n\nThe current process is simply to pull data from an integrated tool that our clients' use and export it into Excel to analyze and create a Power BI dashboard. The integrated tool that is used also has a reporting api that I have access to that can also provide more data than the default integrated tool. The api has a over a dozen endpoints that can be accessed.\n\nIt's not a huge amount of data that can be pulled at once (probably very low thousand rows and then a few hundred per day/week) as the tool has a data retention policy of a few months. So I was thinking of pulling the most data that I can within the retention range into some sort of database, then appending new data to it as time goes on.\n\nI started using Python and Pandas to create some dataframes with some of the most used endpoints and merge them together. But I don't think that would be the most efficient way.\n\nHow do you think I should approach idea to turn it into a nice DE project? What tools would be most efficient and beneficial to use?\n\nCheers", "author_fullname": "t2_6h4gboa8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions of creating a new data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vbfnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699993887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working in somewhat of a data analytics role but am looking to transition into data engineering. In my current position, I have an idea to create my own project would be helpful to talk about in a portfolio.&lt;/p&gt;\n\n&lt;p&gt;The current process is simply to pull data from an integrated tool that our clients&amp;#39; use and export it into Excel to analyze and create a Power BI dashboard. The integrated tool that is used also has a reporting api that I have access to that can also provide more data than the default integrated tool. The api has a over a dozen endpoints that can be accessed.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not a huge amount of data that can be pulled at once (probably very low thousand rows and then a few hundred per day/week) as the tool has a data retention policy of a few months. So I was thinking of pulling the most data that I can within the retention range into some sort of database, then appending new data to it as time goes on.&lt;/p&gt;\n\n&lt;p&gt;I started using Python and Pandas to create some dataframes with some of the most used endpoints and merge them together. But I don&amp;#39;t think that would be the most efficient way.&lt;/p&gt;\n\n&lt;p&gt;How do you think I should approach idea to turn it into a nice DE project? What tools would be most efficient and beneficial to use?&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vbfnf", "is_robot_indexable": true, "report_reasons": null, "author": "BBlluurrrryy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vbfnf/suggestions_of_creating_a_new_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vbfnf/suggestions_of_creating_a_new_data_pipeline/", "subreddit_subscribers": 139700, "created_utc": 1699993887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6f2li", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost-predictable logging at scale with ClickHouse, Grafana &amp; WarpStream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17v75ce", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/z_i73tkAespAEloVaohpZrJLTCPzT9EmEKh-iQhSKTs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699982427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "clickhouse.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://clickhouse.com/blog/cost-predictable-logging-with-clickhouse-vs-datadog-elastic-stack", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?auto=webp&amp;s=0f62866c7add760051b827fc27a17325fd41c075", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=62312bed324273672c1a6e3b3aefb564a0965ffc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=168b62593e8eb085ca5f1177be1e76f4e6cfe311", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c86ba87a4ea4caf45a4a4220a5398ac6019c256f", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eba908273861bcab4188d5f92fb246b3f917fd9a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=093b0fd61ea2f95e938cec95cc961c6a2c382665", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d22096c7b3883535e3492680d612b2b7281d8357", "width": 1080, "height": 567}], "variants": {}, "id": "rexK6oTJje2ZVAcaX2x42RXc0eqDADY0FOwezzCXrRg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v75ce", "is_robot_indexable": true, "report_reasons": null, "author": "kadermo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v75ce/costpredictable_logging_at_scale_with_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://clickhouse.com/blog/cost-predictable-logging-with-clickhouse-vs-datadog-elastic-stack", "subreddit_subscribers": 139700, "created_utc": 1699982427.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I need to ensure that the company where I work is capable of handling requests for personal data deletion (due to GDPR). I am looking for a solution that can centralize data from different data sources (MSSQS, MySql, etc.) so that I can identify where sensitive information is stored and anonymize it. Which solution can I use? I've researched and found Atlan, but I'm unsure if it's the perfect fit for my needs. ", "author_fullname": "t2_dat51pdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on data discovery solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v49dz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699974609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to ensure that the company where I work is capable of handling requests for personal data deletion (due to GDPR). I am looking for a solution that can centralize data from different data sources (MSSQS, MySql, etc.) so that I can identify where sensitive information is stored and anonymize it. Which solution can I use? I&amp;#39;ve researched and found Atlan, but I&amp;#39;m unsure if it&amp;#39;s the perfect fit for my needs. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v49dz", "is_robot_indexable": true, "report_reasons": null, "author": "cyberporing", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v49dz/need_advice_on_data_discovery_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v49dz/need_advice_on_data_discovery_solutions/", "subreddit_subscribers": 139700, "created_utc": 1699974609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The title pretty much says it all. I\u2019m using Databricks Lakehouse as my warehouse in case that changes anything. We\u2019re using incremental materializations for our fact table data into Silver, but the architects want table materialization for dimension data as a whole, and fact data from Silver to Gold.", "author_fullname": "t2_xbe9keo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I materialize my models as tables in dbt but not actually drop my tables every time the model is run?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v1i0h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699966091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title pretty much says it all. I\u2019m using Databricks Lakehouse as my warehouse in case that changes anything. We\u2019re using incremental materializations for our fact table data into Silver, but the architects want table materialization for dimension data as a whole, and fact data from Silver to Gold.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v1i0h", "is_robot_indexable": true, "report_reasons": null, "author": "therealtonyryantime", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v1i0h/how_can_i_materialize_my_models_as_tables_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v1i0h/how_can_i_materialize_my_models_as_tables_in_dbt/", "subreddit_subscribers": 139700, "created_utc": 1699966091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m using fivetran as a connector but i could find Odoo to Bigquery \nAny idea how to replicate my data into my bigquery data warehouse?", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Odoo &lt;&gt; Bigquery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v10wf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699964313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m using fivetran as a connector but i could find Odoo to Bigquery \nAny idea how to replicate my data into my bigquery data warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v10wf", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v10wf/odoo_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v10wf/odoo_bigquery/", "subreddit_subscribers": 139700, "created_utc": 1699964313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_c5qi5iukw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Personal Odyssey: \u201cTransforming Data Analytics with AI\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17uz7yw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2RbtNib7Tb1qf0rIeTxN2L6bMN_AI_cswEpRxweyMsk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699956835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/emrcv4gega0c1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/emrcv4gega0c1.png?auto=webp&amp;s=e641c6ce3bd8e8ca7a487d664c1c422cf3ed4af3", "width": 4184, "height": 2108}, "resolutions": [{"url": "https://preview.redd.it/emrcv4gega0c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e11c5e27db9ff611210473a5001e0d19408b2597", "width": 108, "height": 54}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ca8970619b3224854c0103fc4beda842e793eb2", "width": 216, "height": 108}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fdac6852afcd5eebfda79fcc184b4a41d7ba47b", "width": 320, "height": 161}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1e9ce5b0c691de0a595dbd3c0b6391ad79c6182", "width": 640, "height": 322}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08a4e3d1988c1dcff300989f8c94eeec2c8a6ff9", "width": 960, "height": 483}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9a7c5798c217403019734db5be074abfe116cecd", "width": 1080, "height": 544}], "variants": {}, "id": "9qbS3SOEIwlA1QHAaXh0VYAmEzHVCKByJTy0_rB_mAw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17uz7yw", "is_robot_indexable": true, "report_reasons": null, "author": "legoaitech", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uz7yw/a_personal_odyssey_transforming_data_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/emrcv4gega0c1.png", "subreddit_subscribers": 139700, "created_utc": 1699956835.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work in a company in a product quality team. This team is using excel as a database and I am currently trying to persuade the uppermanagement the need of implementing a data system that the inspecters are able to use to input the results. \n\n1. 2k units inspected per week\n2. So much data errors\n3. 8 separate excel sheets that needs to be manually aggregated\n\n Please help", "author_fullname": "t2_5uvrlw9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Inspection data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uooz1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699919298.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work in a company in a product quality team. This team is using excel as a database and I am currently trying to persuade the uppermanagement the need of implementing a data system that the inspecters are able to use to input the results. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;2k units inspected per week&lt;/li&gt;\n&lt;li&gt;So much data errors&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;8 separate excel sheets that needs to be manually aggregated&lt;/p&gt;\n\n&lt;p&gt;Please help&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17uooz1", "is_robot_indexable": true, "report_reasons": null, "author": "Professional_Ball_58", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uooz1/inspection_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uooz1/inspection_data/", "subreddit_subscribers": 139700, "created_utc": 1699919298.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI\u2019m a very new data engineer at a company who\u2019s wanting me to figure out if they should use DBT or not. Currently using GCP BigQuery but am trying to see if the built in data governance tools in Dataplex are sufficient or if DBT will be useful. \n\nWhat are the main differences in using Attribute Store, Profile, and Data Quality in Dataplex vs starting to use DBT? Mostly everything is coded in python.\n\n\nThanks", "author_fullname": "t2_l2ggjmpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I suggest DBT to my company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v6kpk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699980935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m a very new data engineer at a company who\u2019s wanting me to figure out if they should use DBT or not. Currently using GCP BigQuery but am trying to see if the built in data governance tools in Dataplex are sufficient or if DBT will be useful. &lt;/p&gt;\n\n&lt;p&gt;What are the main differences in using Attribute Store, Profile, and Data Quality in Dataplex vs starting to use DBT? Mostly everything is coded in python.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v6kpk", "is_robot_indexable": true, "report_reasons": null, "author": "Prestigious-Egg-2582", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v6kpk/should_i_suggest_dbt_to_my_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v6kpk/should_i_suggest_dbt_to_my_company/", "subreddit_subscribers": 139700, "created_utc": 1699980935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\nRecently I have written a blog on Spark Optimizations in Medium.., would highly appreciate your reviews..\n\nlink: [https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1](https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1)\n\nThank you...", "author_fullname": "t2_b9lhjoqu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Optimizations &amp; Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v19u6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699965242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;Recently I have written a blog on Spark Optimizations in Medium.., would highly appreciate your reviews..&lt;/p&gt;\n\n&lt;p&gt;link: &lt;a href=\"https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1\"&gt;https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?auto=webp&amp;s=e19c54812ba96b901b55cc127fa6ba6b84465164", "width": 1200, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc787dc826cc9ba7b2a0415ff7039dc8edfab4ba", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66c58e683324bef9e85c739f0ccd24d321cb6121", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=af8ee5a165e80af11111b8c168a15b4356fcd864", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7c652ea446c6cd7e3230f515d29026f1e893322", "width": 640, "height": 346}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a0cf82e63521d7f38b7ad13fa093a7a1f3de1fd", "width": 960, "height": 520}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f858035747215b6705bfbaf47797f9e986b68795", "width": 1080, "height": 585}], "variants": {}, "id": "RTzjCNOob_aJ6Ogb_CAni_XWChMnzTF_FIm-ifa23Zk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v19u6", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical_Duty8486", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v19u6/spark_optimizations_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v19u6/spark_optimizations_databricks/", "subreddit_subscribers": 139700, "created_utc": 1699965242.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}