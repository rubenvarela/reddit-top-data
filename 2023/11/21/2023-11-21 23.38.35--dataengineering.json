{"kind": "Listing", "data": {"after": "t3_180cwkp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I'm currently learning about orchestrating data pipeline with Airflow. With all the free resources I have seen, it seems that a majority of the demos are showing developers to save the python code in the `dags/` folder and then call PythonOperator to run the task. Note that the python code would not only include the Dags and tasks code, but they also have the actual ETL codes in there. To be honest, I am not usually comfortable in jamming codes for different purpose in one .py file because I prefer separating codes for ETL from codes that defines a DAG and tasks in other places.\n\nSo, some questions I have, maybe a stupid question, are:\n\nIn real practice, are we really saving the codes in the `dags` folder?  \n\nSay that I have a function that does simple data cleansing with pandas. in real practice, should I just define a function in a .py and then also define the dags and tasks in the same python file? Would the `dags` folder be flooded? Is there any ways to just save the dags and tasks codes in the dags folder, but import the actual ETL codes from other .py files saved in another folder?\n\nI feel the question may be leaning towards a 'no', but what I'm looking to learn is the \\`why\\` and \\`how is that being done in real practice\\`.\n\n&amp;#x200B;\n\nThanks!!!", "author_fullname": "t2_8vauvmym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is Airflow really used in real practice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180lef1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700585257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;m currently learning about orchestrating data pipeline with Airflow. With all the free resources I have seen, it seems that a majority of the demos are showing developers to save the python code in the &lt;code&gt;dags/&lt;/code&gt; folder and then call PythonOperator to run the task. Note that the python code would not only include the Dags and tasks code, but they also have the actual ETL codes in there. To be honest, I am not usually comfortable in jamming codes for different purpose in one .py file because I prefer separating codes for ETL from codes that defines a DAG and tasks in other places.&lt;/p&gt;\n\n&lt;p&gt;So, some questions I have, maybe a stupid question, are:&lt;/p&gt;\n\n&lt;p&gt;In real practice, are we really saving the codes in the &lt;code&gt;dags&lt;/code&gt; folder?  &lt;/p&gt;\n\n&lt;p&gt;Say that I have a function that does simple data cleansing with pandas. in real practice, should I just define a function in a .py and then also define the dags and tasks in the same python file? Would the &lt;code&gt;dags&lt;/code&gt; folder be flooded? Is there any ways to just save the dags and tasks codes in the dags folder, but import the actual ETL codes from other .py files saved in another folder?&lt;/p&gt;\n\n&lt;p&gt;I feel the question may be leaning towards a &amp;#39;no&amp;#39;, but what I&amp;#39;m looking to learn is the `why` and `how is that being done in real practice`.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180lef1", "is_robot_indexable": true, "report_reasons": null, "author": "sevkw", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180lef1/how_is_airflow_really_used_in_real_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180lef1/how_is_airflow_really_used_in_real_practice/", "subreddit_subscribers": 141006, "created_utc": 1700585257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a personal project where I\u2019m pulling NBA data from a scraper and putting it into a database. This scraping obviously needs to be automated. Obviously going to need to automate my data cleaning as well. A little bit further down the line I\u2019m going to want to automate the process of pulling data from my database to visualize it on a website. Not sure if this calls for Airflow, I have zero data engineer experience.\n\nThanks!", "author_fullname": "t2_kc7mg3akr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I use Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1808qro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700541639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a personal project where I\u2019m pulling NBA data from a scraper and putting it into a database. This scraping obviously needs to be automated. Obviously going to need to automate my data cleaning as well. A little bit further down the line I\u2019m going to want to automate the process of pulling data from my database to visualize it on a website. Not sure if this calls for Airflow, I have zero data engineer experience.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1808qro", "is_robot_indexable": true, "report_reasons": null, "author": "Brief-Union-3493", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1808qro/should_i_use_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1808qro/should_i_use_airflow/", "subreddit_subscribers": 141006, "created_utc": 1700541639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building a pipeline that will read a very huge data (700M rows) from postgres and then load into a delta table. I have databricks spark at my disposal to get the job done, the ask is to perform the activity as quick as possible.\n\nI tried executing read from postgres using 30 parallel connections and then writing it to an external delta table (S3) but it just takes infinite amount of time.\n\nDatabricks is hosted on AWS and I'm using r5d.4xLarge instances with auto scaling, but stats tell me this is an overkill. My cluster constantly sizes up and down which I think is adding to the delays\n\nAny suggestions on how I approach the pipleine with any custom spark configs will be greatly appreciated. Data is not skewed, none of the tasks have been completed. Thank you!", "author_fullname": "t2_tacjangv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting huge dataset from postgres to databricks delta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180cq2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700556958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a pipeline that will read a very huge data (700M rows) from postgres and then load into a delta table. I have databricks spark at my disposal to get the job done, the ask is to perform the activity as quick as possible.&lt;/p&gt;\n\n&lt;p&gt;I tried executing read from postgres using 30 parallel connections and then writing it to an external delta table (S3) but it just takes infinite amount of time.&lt;/p&gt;\n\n&lt;p&gt;Databricks is hosted on AWS and I&amp;#39;m using r5d.4xLarge instances with auto scaling, but stats tell me this is an overkill. My cluster constantly sizes up and down which I think is adding to the delays&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on how I approach the pipleine with any custom spark configs will be greatly appreciated. Data is not skewed, none of the tasks have been completed. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180cq2j", "is_robot_indexable": true, "report_reasons": null, "author": "reflectico", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180cq2j/ingesting_huge_dataset_from_postgres_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180cq2j/ingesting_huge_dataset_from_postgres_to/", "subreddit_subscribers": 141006, "created_utc": 1700556958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am currently working as a data analyst and was interested to know if someone has given the new data engineering exam from AWS (DEA-C01). It would be great if the experts/individuals can share their experiences and study pattern for the same since I am planning to give the same.", "author_fullname": "t2_a06jzj8ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Data Engineering exam( DEA-C01)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1804yfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700529816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am currently working as a data analyst and was interested to know if someone has given the new data engineering exam from AWS (DEA-C01). It would be great if the experts/individuals can share their experiences and study pattern for the same since I am planning to give the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1804yfc", "is_robot_indexable": true, "report_reasons": null, "author": "ravipar", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1804yfc/aws_data_engineering_exam_deac01/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1804yfc/aws_data_engineering_exam_deac01/", "subreddit_subscribers": 141006, "created_utc": 1700529816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you have an API serving layer enabled directly from the lake? \nWhat tech stack do you use?", "author_fullname": "t2_4ncjzv2a6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API serving layer over data lake tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1809l87", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700544591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have an API serving layer enabled directly from the lake? \nWhat tech stack do you use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1809l87", "is_robot_indexable": true, "report_reasons": null, "author": "ImpactOk7137", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1809l87/api_serving_layer_over_data_lake_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1809l87/api_serving_layer_over_data_lake_tables/", "subreddit_subscribers": 141006, "created_utc": 1700544591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI\u00b4m currently recieving JSON files from a source system. I process the json files and first store them as  nested delta tables. After that I flatten the nested data with pyspark. At the moment I pretty much wrote the flattening code by hand without any recursive logic. However since the number of live markets is increasing (&gt;20 already) and the schemas differ for each market this is not acceptable anymore.\n\nTherefore I would like to implement a scalable solution that takes the following things into account:- use pyspark- split arrays into seperate dataframes- automatically create technical ids that can be used to join the between the flattened tables (Not used at the moment but yields to some problems if not used)\n\nThis was inspired by the following package: [https://github.com/kindly/flatterer](https://github.com/kindly/flatterer)However the package can only be used with python and raw json files.\n\n**Help needed**\n\n\\- Are there any pypspark packages that can be used to flatten nested data and take into considerations the creation of technical ids?- If no package are available. Has anyone experience with flattening in pyspark and can share code?\n\n**Example with current approach**\n\nThe following example shows how arrays should be split in seperate tables. It also shows some of the current problems when not using technical ids.\n\n    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n    import pyspark.sql.functions as F\n    \n    # lets create a nested schema\n    schema = StructType([\n        StructField(\"some_business_id\", StringType(), True),\n        StructField(\"some_field\", StringType(), True),\n        StructField(\"businesspartners\", ArrayType(\n            StructType([\n                StructField(\"name\", StringType(), True),\n                StructField(\"profession\", StringType(), True),\n                StructField(\"addresses\", ArrayType(\n                    StructType([\n                        StructField(\"street\", StringType(), True),\n                        StructField(\"city\", StringType(), True)\n                    ])\n                ), True)\n            ])\n        ), True)\n    ])\n    \n    # 2 dummy entries for the schema\n    data = [\n        (\"1\", \"value1\", [\n            {\"name\": \"claus\", \"profession\": \"data engineer\", \"addresses\": [{\"street\": \"Main St\", \"city\": \"Metropolis\"}]},\n            {\"name\": \"joern\", \"profession\": \"data engineer\", \"addresses\": [{\"street\": \"Side St\", \"city\": \"Gotham\"}]}\n        ]),\n        (\"2\", \"value2\", [\n            {\"name\": \"claus\", \"profession\": \"data scientist\", \"addresses\": [{\"street\": \"Sixt St\", \"city\": \"Manhatten\"}]},\n            {\"name\": \"joern\", \"profession\": \"Football player\", \"addresses\": [{\"street\": \"Baker St\", \"city\": \"Fullham\"}]}\n        ]),\n    \n    ]\n    \n    # the initial nested data to flatten\n    df = spark.createDataFrame(data, schema=schema)\n\nThe schema includes 2 nested arrays. Therefore the flattening process should result in 3 tables:- main table- businesspartners table- businesspartner\\_addresses table\n\n    # create the top level table. 'some_business_id' is unique\n    main_table = df.select(\"some_business_id\", \"some_field\")\n\n&amp;#x200B;\n\n[main\\_table.show\\(\\)](https://preview.redd.it/6381cnkmnn1c1.png?width=305&amp;format=png&amp;auto=webp&amp;s=7a73e8b08b5c77aca9119a18936c7c964e5f48ee)\n\n    businesspartner_table = (df\n                             .withColumn(\"businesspartners\", F.explode(F.col(\"businesspartners\")))\n                             .select(\n                                 \"some_business_id\", # added for joins\n                                 \"businesspartners.name\", \n                                 \"businesspartners.profession\"\n                                 ))\n\n&amp;#x200B;\n\n[businesspartner\\_table.show\\(\\)](https://preview.redd.it/6s0hfegsnn1c1.png?width=412&amp;format=png&amp;auto=webp&amp;s=c4d83ebb51bb6a7bbd70e26ebb0a3e12cfebaec7)\n\n\"some\\_business\\_id\" is added in order to allow joins between main and businesspartner table. However having a unique field is not always true. The businesspartner table itself does not have any unique column. This yields to problems when creating the businesspartner address table:\n\n    businesspartner_adress_table = (df\n                             .withColumn(\"businesspartners\", F.explode(F.col(\"businesspartners\")))\n                             .withColumn(\"addresses\", F.explode(F.col(\"businesspartners.addresses\")))\n                             .select(\n                                 \"some_business_id\",\n                                 \"businesspartners.name\",\n                                 \"businesspartners.profession\",\n                                 \"addresses.street\",\n                                 \"addresses.city\",\n                                 )\n                             )\n\nI added name and profession which in combination may be unique. However uniqueness is not guaranteed. Therefore I think technical ids are required.", "author_fullname": "t2_rydqu8m3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unnest nested data recursively with technical ids (pyspark)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"6s0hfegsnn1c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/6s0hfegsnn1c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cad01d32137d89a5397484f56a609e3719eb5bbf"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/6s0hfegsnn1c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c0a243c78eac42f691720c6d77e9009eb6145846"}, {"y": 164, "x": 320, "u": "https://preview.redd.it/6s0hfegsnn1c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3811f7daca31a165aeeb2f368498f05c829c2c07"}], "s": {"y": 212, "x": 412, "u": "https://preview.redd.it/6s0hfegsnn1c1.png?width=412&amp;format=png&amp;auto=webp&amp;s=c4d83ebb51bb6a7bbd70e26ebb0a3e12cfebaec7"}, "id": "6s0hfegsnn1c1"}, "6381cnkmnn1c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 57, "x": 108, "u": "https://preview.redd.it/6381cnkmnn1c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=95ec5b146e378d76c100c1ba16c64234e24cb460"}, {"y": 115, "x": 216, "u": "https://preview.redd.it/6381cnkmnn1c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ac2fc22f74f45793b22f7ed7ecc6c6cc858422b"}], "s": {"y": 163, "x": 305, "u": "https://preview.redd.it/6381cnkmnn1c1.png?width=305&amp;format=png&amp;auto=webp&amp;s=7a73e8b08b5c77aca9119a18936c7c964e5f48ee"}, "id": "6381cnkmnn1c1"}}, "name": "t3_180c326", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KDO7xlB9iRmLtxV3RoqxzT2n5MZKAjya9DfLGAy_TSw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1700554252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I\u00b4m currently recieving JSON files from a source system. I process the json files and first store them as  nested delta tables. After that I flatten the nested data with pyspark. At the moment I pretty much wrote the flattening code by hand without any recursive logic. However since the number of live markets is increasing (&amp;gt;20 already) and the schemas differ for each market this is not acceptable anymore.&lt;/p&gt;\n\n&lt;p&gt;Therefore I would like to implement a scalable solution that takes the following things into account:- use pyspark- split arrays into seperate dataframes- automatically create technical ids that can be used to join the between the flattened tables (Not used at the moment but yields to some problems if not used)&lt;/p&gt;\n\n&lt;p&gt;This was inspired by the following package: &lt;a href=\"https://github.com/kindly/flatterer\"&gt;https://github.com/kindly/flatterer&lt;/a&gt;However the package can only be used with python and raw json files.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Help needed&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Are there any pypspark packages that can be used to flatten nested data and take into considerations the creation of technical ids?- If no package are available. Has anyone experience with flattening in pyspark and can share code?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Example with current approach&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The following example shows how arrays should be split in seperate tables. It also shows some of the current problems when not using technical ids.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\nimport pyspark.sql.functions as F\n\n# lets create a nested schema\nschema = StructType([\n    StructField(&amp;quot;some_business_id&amp;quot;, StringType(), True),\n    StructField(&amp;quot;some_field&amp;quot;, StringType(), True),\n    StructField(&amp;quot;businesspartners&amp;quot;, ArrayType(\n        StructType([\n            StructField(&amp;quot;name&amp;quot;, StringType(), True),\n            StructField(&amp;quot;profession&amp;quot;, StringType(), True),\n            StructField(&amp;quot;addresses&amp;quot;, ArrayType(\n                StructType([\n                    StructField(&amp;quot;street&amp;quot;, StringType(), True),\n                    StructField(&amp;quot;city&amp;quot;, StringType(), True)\n                ])\n            ), True)\n        ])\n    ), True)\n])\n\n# 2 dummy entries for the schema\ndata = [\n    (&amp;quot;1&amp;quot;, &amp;quot;value1&amp;quot;, [\n        {&amp;quot;name&amp;quot;: &amp;quot;claus&amp;quot;, &amp;quot;profession&amp;quot;: &amp;quot;data engineer&amp;quot;, &amp;quot;addresses&amp;quot;: [{&amp;quot;street&amp;quot;: &amp;quot;Main St&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Metropolis&amp;quot;}]},\n        {&amp;quot;name&amp;quot;: &amp;quot;joern&amp;quot;, &amp;quot;profession&amp;quot;: &amp;quot;data engineer&amp;quot;, &amp;quot;addresses&amp;quot;: [{&amp;quot;street&amp;quot;: &amp;quot;Side St&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Gotham&amp;quot;}]}\n    ]),\n    (&amp;quot;2&amp;quot;, &amp;quot;value2&amp;quot;, [\n        {&amp;quot;name&amp;quot;: &amp;quot;claus&amp;quot;, &amp;quot;profession&amp;quot;: &amp;quot;data scientist&amp;quot;, &amp;quot;addresses&amp;quot;: [{&amp;quot;street&amp;quot;: &amp;quot;Sixt St&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Manhatten&amp;quot;}]},\n        {&amp;quot;name&amp;quot;: &amp;quot;joern&amp;quot;, &amp;quot;profession&amp;quot;: &amp;quot;Football player&amp;quot;, &amp;quot;addresses&amp;quot;: [{&amp;quot;street&amp;quot;: &amp;quot;Baker St&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Fullham&amp;quot;}]}\n    ]),\n\n]\n\n# the initial nested data to flatten\ndf = spark.createDataFrame(data, schema=schema)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The schema includes 2 nested arrays. Therefore the flattening process should result in 3 tables:- main table- businesspartners table- businesspartner_addresses table&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# create the top level table. &amp;#39;some_business_id&amp;#39; is unique\nmain_table = df.select(&amp;quot;some_business_id&amp;quot;, &amp;quot;some_field&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6381cnkmnn1c1.png?width=305&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a73e8b08b5c77aca9119a18936c7c964e5f48ee\"&gt;main_table.show()&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;businesspartner_table = (df\n                         .withColumn(&amp;quot;businesspartners&amp;quot;, F.explode(F.col(&amp;quot;businesspartners&amp;quot;)))\n                         .select(\n                             &amp;quot;some_business_id&amp;quot;, # added for joins\n                             &amp;quot;businesspartners.name&amp;quot;, \n                             &amp;quot;businesspartners.profession&amp;quot;\n                             ))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6s0hfegsnn1c1.png?width=412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4d83ebb51bb6a7bbd70e26ebb0a3e12cfebaec7\"&gt;businesspartner_table.show()&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;some_business_id&amp;quot; is added in order to allow joins between main and businesspartner table. However having a unique field is not always true. The businesspartner table itself does not have any unique column. This yields to problems when creating the businesspartner address table:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;businesspartner_adress_table = (df\n                         .withColumn(&amp;quot;businesspartners&amp;quot;, F.explode(F.col(&amp;quot;businesspartners&amp;quot;)))\n                         .withColumn(&amp;quot;addresses&amp;quot;, F.explode(F.col(&amp;quot;businesspartners.addresses&amp;quot;)))\n                         .select(\n                             &amp;quot;some_business_id&amp;quot;,\n                             &amp;quot;businesspartners.name&amp;quot;,\n                             &amp;quot;businesspartners.profession&amp;quot;,\n                             &amp;quot;addresses.street&amp;quot;,\n                             &amp;quot;addresses.city&amp;quot;,\n                             )\n                         )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I added name and profession which in combination may be unique. However uniqueness is not guaranteed. Therefore I think technical ids are required.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?auto=webp&amp;s=c280e9e1b1e5bf7504be8b5ad2b81b1971a2c277", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bcc6c241e48f2ba73b3f5129e3661109ab75dab8", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=802e9ff3de8c8129bcfc9d9f2772ad52320f1b5b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=151c37f4872eaca074c65ed6a13635455d905480", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=341e0a0bb001f848ffdbe1d6c800a195acf588e2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=78949e161f10fe0f9c51a285824b8be5375814a4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35df9b70a49221e7a9ed7806e0c5bb218a6c45cb", "width": 1080, "height": 540}], "variants": {}, "id": "hyFi5jUHPZCRVg_vPNhPPj2NjjHzUPV_O3g8U3Oi8Lo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180c326", "is_robot_indexable": true, "report_reasons": null, "author": "DecisionAgile7326", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/180c326/unnest_nested_data_recursively_with_technical_ids/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180c326/unnest_nested_data_recursively_with_technical_ids/", "subreddit_subscribers": 141006, "created_utc": 1700554252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I'm about 4 years removed from college at my second job, both have been relatively early startups. My first title was DE Engineer and now I am an ML engineer. Neither title really encapsulates what I do as I've been involved in architecting two end-to-end data pipelines, building labeling tools, integrating ml models into the production code base, A LOT of web scraping (node, puppeteer), various IaC projects (terraform for aws resources, databricks, etc.).\n\nI'm looking for a new job, but really just don't feel like I belong in any of the positions I see. For example, I mostly do software engineering, but don't go as low level as a lot of the dedicated SE engineers at the company. I work day to day on the data science team, but don't actually do any modeling (mostly handle the data, retrain pipelines, labeling tools, implementation, liaison between teams, etc.). I feel like a competent high level data engineer because of what I have built top to bottom (data lake, aws dms, data warehouse, delta lake, etc), but really don't touch these tech stacks on a day to day anymore. As you can see I am having a hard time figuring out where/what to apply to because I've become very much a jack of all trades and master of none (which I enjoy a lot).\n\nHave any of you gone through this experience and have any advice. While I like having been able to try so many new things, I worry my lack of specializing in any one thing is hindering my career advancement.", "author_fullname": "t2_2nfde5yl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not sure what I should apply for", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18070j2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700536041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;m about 4 years removed from college at my second job, both have been relatively early startups. My first title was DE Engineer and now I am an ML engineer. Neither title really encapsulates what I do as I&amp;#39;ve been involved in architecting two end-to-end data pipelines, building labeling tools, integrating ml models into the production code base, A LOT of web scraping (node, puppeteer), various IaC projects (terraform for aws resources, databricks, etc.).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a new job, but really just don&amp;#39;t feel like I belong in any of the positions I see. For example, I mostly do software engineering, but don&amp;#39;t go as low level as a lot of the dedicated SE engineers at the company. I work day to day on the data science team, but don&amp;#39;t actually do any modeling (mostly handle the data, retrain pipelines, labeling tools, implementation, liaison between teams, etc.). I feel like a competent high level data engineer because of what I have built top to bottom (data lake, aws dms, data warehouse, delta lake, etc), but really don&amp;#39;t touch these tech stacks on a day to day anymore. As you can see I am having a hard time figuring out where/what to apply to because I&amp;#39;ve become very much a jack of all trades and master of none (which I enjoy a lot).&lt;/p&gt;\n\n&lt;p&gt;Have any of you gone through this experience and have any advice. While I like having been able to try so many new things, I worry my lack of specializing in any one thing is hindering my career advancement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18070j2", "is_robot_indexable": true, "report_reasons": null, "author": "pmarct", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18070j2/not_sure_what_i_should_apply_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18070j2/not_sure_what_i_should_apply_for/", "subreddit_subscribers": 141006, "created_utc": 1700536041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some background first: I work as a data engineer in the BI department of a medium sized e-commerce firm. Our BI architecture is currently 100% on-premise. We use a data virtualization platform as a middleware between source systems and a data warehouse (postgres). For log and web tracking analytics we also use Clickhouse. This setup is probably not state of the art but fits our current needs quiet well. \n\nThe server hosting both the dwh and the middleware is going to be renewed and management decided it has to be something cloud. I'm struggeling a bit about the right course of action. Our data warehouse is currently around 2 TB in size. Most SQL queries run within an acceptable time frame. Some large queries could be faster but we are currently shifting some of these workloads to Clickhouse, with great results. \n\nMy approach is to keep the overall setup but separate BI middleware from the postgres database by bringing the latter to the cloud. I was thinking about an AWS EC2 instance like r5b to host postgres. Budget is always tight so RDS is probably not an option. I expect an increase in query performance due to better hardware and dedicated server. But is it really recommened to hold onto a relational database for mostly analytical purposes? Should we better switch to Redshift/BigQuery instead? It would be more expensive and an even bigger migration project (no manpower lol) and maybe absolutely oversized for our needs... Anyone has any thoughts on this? I'd really appreciate it. Thank you", "author_fullname": "t2_ramk8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating from on-premise to cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1803jav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700525734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some background first: I work as a data engineer in the BI department of a medium sized e-commerce firm. Our BI architecture is currently 100% on-premise. We use a data virtualization platform as a middleware between source systems and a data warehouse (postgres). For log and web tracking analytics we also use Clickhouse. This setup is probably not state of the art but fits our current needs quiet well. &lt;/p&gt;\n\n&lt;p&gt;The server hosting both the dwh and the middleware is going to be renewed and management decided it has to be something cloud. I&amp;#39;m struggeling a bit about the right course of action. Our data warehouse is currently around 2 TB in size. Most SQL queries run within an acceptable time frame. Some large queries could be faster but we are currently shifting some of these workloads to Clickhouse, with great results. &lt;/p&gt;\n\n&lt;p&gt;My approach is to keep the overall setup but separate BI middleware from the postgres database by bringing the latter to the cloud. I was thinking about an AWS EC2 instance like r5b to host postgres. Budget is always tight so RDS is probably not an option. I expect an increase in query performance due to better hardware and dedicated server. But is it really recommened to hold onto a relational database for mostly analytical purposes? Should we better switch to Redshift/BigQuery instead? It would be more expensive and an even bigger migration project (no manpower lol) and maybe absolutely oversized for our needs... Anyone has any thoughts on this? I&amp;#39;d really appreciate it. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1803jav", "is_robot_indexable": true, "report_reasons": null, "author": "knabbels", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1803jav/migrating_from_onpremise_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1803jav/migrating_from_onpremise_to_cloud/", "subreddit_subscribers": 141006, "created_utc": 1700525734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently when we merge into our protected `development` or `production` branches, our CI/CD pipeline gets triggered and it builds one or more docker containers and pushes them to a repo where our run tasks pick them up. At each task invocation dbt starts by parsing the entire project that is contained within that particular image. \n\nA few weeks ago I passed some non-existent tags to our task runner, which is all in all 8 different dbt operations, and it took about 7 minutes to complete. All of that time is spent spinning up the task, parsing, finding no matches for these tags, and shutting down. \n\nI would like to incorporate parsing into our CI/CD pipelines so that parsing happens only when it's required, which is when new code is merged in, and otherwise successive operations on unchanged codebases just reference the same saved manifest to save some time during runs. Even if is just a couple of minutes.  \n\nBut I haven't come up with a clean way to do it. I could include `dbt parse` as a `RUN` command in each project's Dockerfile. But then I need to get the database credentials to the docker build, as passing a large number of arguments seems clunky. Additionally, I then need to include the database credentials as CI/CD variables in my pipeline which will make key rotation difficult. \n\nHas anyone found a clean way to do this?", "author_fullname": "t2_6nf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone had success incorporating dbt's parse step into your CI/CD pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180ml58", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700588268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently when we merge into our protected &lt;code&gt;development&lt;/code&gt; or &lt;code&gt;production&lt;/code&gt; branches, our CI/CD pipeline gets triggered and it builds one or more docker containers and pushes them to a repo where our run tasks pick them up. At each task invocation dbt starts by parsing the entire project that is contained within that particular image. &lt;/p&gt;\n\n&lt;p&gt;A few weeks ago I passed some non-existent tags to our task runner, which is all in all 8 different dbt operations, and it took about 7 minutes to complete. All of that time is spent spinning up the task, parsing, finding no matches for these tags, and shutting down. &lt;/p&gt;\n\n&lt;p&gt;I would like to incorporate parsing into our CI/CD pipelines so that parsing happens only when it&amp;#39;s required, which is when new code is merged in, and otherwise successive operations on unchanged codebases just reference the same saved manifest to save some time during runs. Even if is just a couple of minutes.  &lt;/p&gt;\n\n&lt;p&gt;But I haven&amp;#39;t come up with a clean way to do it. I could include &lt;code&gt;dbt parse&lt;/code&gt; as a &lt;code&gt;RUN&lt;/code&gt; command in each project&amp;#39;s Dockerfile. But then I need to get the database credentials to the docker build, as passing a large number of arguments seems clunky. Additionally, I then need to include the database credentials as CI/CD variables in my pipeline which will make key rotation difficult. &lt;/p&gt;\n\n&lt;p&gt;Has anyone found a clean way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180ml58", "is_robot_indexable": true, "report_reasons": null, "author": "radil", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180ml58/has_anyone_had_success_incorporating_dbts_parse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180ml58/has_anyone_had_success_incorporating_dbts_parse/", "subreddit_subscribers": 141006, "created_utc": 1700588268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 5 years of json application log files stored in 60 second folder partitions with one json log file per event in S3. So a single 60 second partition could have 100\u2019s of json files. This is obviously unusable for analytics. \n\nI need to consolidate the log data and repartition into nested year/month/day partitions with one json log file per day\n\nAny thoughts on the best way to do this within AWS or outside of AWS, that\u2019s not I/O bound? ", "author_fullname": "t2_r168sf7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to repartition json data in s3 bucket", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180q34j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700597454.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700597255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 5 years of json application log files stored in 60 second folder partitions with one json log file per event in S3. So a single 60 second partition could have 100\u2019s of json files. This is obviously unusable for analytics. &lt;/p&gt;\n\n&lt;p&gt;I need to consolidate the log data and repartition into nested year/month/day partitions with one json log file per day&lt;/p&gt;\n\n&lt;p&gt;Any thoughts on the best way to do this within AWS or outside of AWS, that\u2019s not I/O bound? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180q34j", "is_robot_indexable": true, "report_reasons": null, "author": "johncena9519", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180q34j/best_way_to_repartition_json_data_in_s3_bucket/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180q34j/best_way_to_repartition_json_data_in_s3_bucket/", "subreddit_subscribers": 141006, "created_utc": 1700597255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bxjjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Change Data Capture Breaks Encapsulation\". Does it, though?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "name": "t3_180m4ib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lb_31UM3cfi5BJLJ7V8rI9LuJZDwEVlI8UMRcWYEUxM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700587093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "decodable.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.decodable.co/blog/change-data-capture-breaks-encapsulation-does-it-though", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?auto=webp&amp;s=76d0fb3735486ab22191f7d87d4f31f534c0b3ba", "width": 1898, "height": 1233}, "resolutions": [{"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c620453ffb0a56ab41a2188e1956d8a52f4c078", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ca2423f02ac40f19b1b79f2731eb418ff11ec83", "width": 216, "height": 140}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09eaca410b99c679520de72384272bfa3a935393", "width": 320, "height": 207}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=298d58ab70c8baa397e0e3f20b55b4c8d35ae7c5", "width": 640, "height": 415}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7542d65a374213316e86de0f856f5f789cec9f69", "width": 960, "height": 623}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=22b0a0db31c57353d318821141190a39ae8c5b96", "width": 1080, "height": 701}], "variants": {}, "id": "fZR5RCGRbhiP6OBRB8mmdV7xnByAN1Vs7fs2gFHquvU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "180m4ib", "is_robot_indexable": true, "report_reasons": null, "author": "gunnarmorling", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180m4ib/change_data_capture_breaks_encapsulation_does_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.decodable.co/blog/change-data-capture-breaks-encapsulation-does-it-though", "subreddit_subscribers": 141006, "created_utc": 1700587093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short I\u2019m an eLearning Developer wanting to turn into a Data Engineer. I have 1+ years of SQL experience. I\u2019m finishing up a Data Engineering course on Datacamp.\nI\u2019m looking for interview prep questions so that I can crack this asap. Thanks in advance!", "author_fullname": "t2_yoget", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep for DE roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180gajt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700571170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I\u2019m an eLearning Developer wanting to turn into a Data Engineer. I have 1+ years of SQL experience. I\u2019m finishing up a Data Engineering course on Datacamp.\nI\u2019m looking for interview prep questions so that I can crack this asap. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "180gajt", "is_robot_indexable": true, "report_reasons": null, "author": "Drrazor", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180gajt/interview_prep_for_de_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180gajt/interview_prep_for_de_roles/", "subreddit_subscribers": 141006, "created_utc": 1700571170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI want to use dbt with databricks.If you use dbt cloud, it provides you with online IDE and GUI. The integration with databricks works fine.\n\nWhat if I dont want to use dbt cloud, but dbt core? I know that you can connect dbt core to databricks and start developing in local IDE (ex: VS code). ( github + dbt core + databricks)\n\n* github: for version control\n* dbt core: for dbt project management\n* databricks: for deployment environment + data storage (unity catalog)\n\nIn a nutshell, my question is, can I use databricks as an IDE to develop dbt projects? (instead of developing in local)", "author_fullname": "t2_2x81kpwx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Databricks as IDE for dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180d7cm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700559855.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700559097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I want to use dbt with databricks.If you use dbt cloud, it provides you with online IDE and GUI. The integration with databricks works fine.&lt;/p&gt;\n\n&lt;p&gt;What if I dont want to use dbt cloud, but dbt core? I know that you can connect dbt core to databricks and start developing in local IDE (ex: VS code). ( github + dbt core + databricks)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;github: for version control&lt;/li&gt;\n&lt;li&gt;dbt core: for dbt project management&lt;/li&gt;\n&lt;li&gt;databricks: for deployment environment + data storage (unity catalog)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In a nutshell, my question is, can I use databricks as an IDE to develop dbt projects? (instead of developing in local)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180d7cm", "is_robot_indexable": true, "report_reasons": null, "author": "Disastrous-State-503", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180d7cm/using_databricks_as_ide_for_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180d7cm/using_databricks_as_ide_for_dbt/", "subreddit_subscribers": 141006, "created_utc": 1700559097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI've been learning by Data Engr skills slowly but surely, and the last/current major subject I've done the least on is SQL/DWH. I have nearly completed an \"Analytics Engineering\" course, which does (from what I can tell) go over a fair bit of the data modelling terms/practices, but then the implementation is all done in DBT...\n\nThere seems to be very mixed opinions on DBT in this subreddit and it seems to highly vary by circumstance of the organization. The main insight given this, is that it is probably not good to just rely on DBT for everything it handles behind the scenes, because once I work somewhere that doesn't use DBT, I am way behind.\n\nWith all that said, can anyone recommend some courses that would be good for learning the above?\n\nThanks in advance ", "author_fullname": "t2_8chdw7c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Raw SQL for DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1803j7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700525727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been learning by Data Engr skills slowly but surely, and the last/current major subject I&amp;#39;ve done the least on is SQL/DWH. I have nearly completed an &amp;quot;Analytics Engineering&amp;quot; course, which does (from what I can tell) go over a fair bit of the data modelling terms/practices, but then the implementation is all done in DBT...&lt;/p&gt;\n\n&lt;p&gt;There seems to be very mixed opinions on DBT in this subreddit and it seems to highly vary by circumstance of the organization. The main insight given this, is that it is probably not good to just rely on DBT for everything it handles behind the scenes, because once I work somewhere that doesn&amp;#39;t use DBT, I am way behind.&lt;/p&gt;\n\n&lt;p&gt;With all that said, can anyone recommend some courses that would be good for learning the above?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1803j7c", "is_robot_indexable": true, "report_reasons": null, "author": "pdxtechnologist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1803j7c/raw_sql_for_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1803j7c/raw_sql_for_dwh/", "subreddit_subscribers": 141006, "created_utc": 1700525727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone working with databricks, and have implemented CI/CD? \nI'm struggling to come up with an good approach that covers development of Notebooks, DLT workflows and jobs. Things seem to separated atm. Any tips or advice? \n\nCheers", "author_fullname": "t2_7p1fczdj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD in Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_180u635", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700607538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone working with databricks, and have implemented CI/CD? \nI&amp;#39;m struggling to come up with an good approach that covers development of Notebooks, DLT workflows and jobs. Things seem to separated atm. Any tips or advice? &lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180u635", "is_robot_indexable": true, "report_reasons": null, "author": "Far-Inspection9930", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180u635/cicd_in_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180u635/cicd_in_databricks/", "subreddit_subscribers": 141006, "created_utc": 1700607538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a background in Python script development and eight months of experience building I/O pipelines using SQL Alchemy, SQL Connector, pandas, NumPy, fastAPI, and Django, I am eager to apply my skills and knowledge to contribute to real-world projects.\n\nUnfortunately, unforeseen circumstances have placed me in a situation where I urgently need to secure a stable job to support myself. While I am actively working on building a comprehensive portfolio to showcase my full-stack development capabilities, the immediate need for an income source has prompted me to seek entry-level data engineering positions.\n\nDespite the lack of a formal portfolio at this stage, I am confident in my ability to learn quickly, adapt to new challenges, and contribute meaningfully to a team of talented data professionals. I am a team player, always willing to go the extra mile to achieve project goals, and possess a strong work ethic and dedication to continuous learning.\n\nI am aware that my situation may seem unconventional, but I assure you that my passion for data engineering and my determination to succeed in this field will overcome any temporary setbacks. I am committed to expanding my skillset to include the latest data engineering tools and technologies, and I am eager to apply my existing knowledge to make a positive impact on any organization that values innovation and continuous growth.\n\nI would be grateful for any opportunities to connect with data engineering professionals who could provide guidance or mentorship as I embark on this journey. I am open to exploring entry-level positions, internships, or freelance projects that would allow me to gain hands-on experience and contribute to the success of a team.\n\nThank you for taking the time to read this. Please share your opinions on me.", "author_fullname": "t2_vffigorh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on How to Get Entry-Level Data Engineering Role for Skill Advancement and Career Growth", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_180tqka", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700606413.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a background in Python script development and eight months of experience building I/O pipelines using SQL Alchemy, SQL Connector, pandas, NumPy, fastAPI, and Django, I am eager to apply my skills and knowledge to contribute to real-world projects.&lt;/p&gt;\n\n&lt;p&gt;Unfortunately, unforeseen circumstances have placed me in a situation where I urgently need to secure a stable job to support myself. While I am actively working on building a comprehensive portfolio to showcase my full-stack development capabilities, the immediate need for an income source has prompted me to seek entry-level data engineering positions.&lt;/p&gt;\n\n&lt;p&gt;Despite the lack of a formal portfolio at this stage, I am confident in my ability to learn quickly, adapt to new challenges, and contribute meaningfully to a team of talented data professionals. I am a team player, always willing to go the extra mile to achieve project goals, and possess a strong work ethic and dedication to continuous learning.&lt;/p&gt;\n\n&lt;p&gt;I am aware that my situation may seem unconventional, but I assure you that my passion for data engineering and my determination to succeed in this field will overcome any temporary setbacks. I am committed to expanding my skillset to include the latest data engineering tools and technologies, and I am eager to apply my existing knowledge to make a positive impact on any organization that values innovation and continuous growth.&lt;/p&gt;\n\n&lt;p&gt;I would be grateful for any opportunities to connect with data engineering professionals who could provide guidance or mentorship as I embark on this journey. I am open to exploring entry-level positions, internships, or freelance projects that would allow me to gain hands-on experience and contribute to the success of a team.&lt;/p&gt;\n\n&lt;p&gt;Thank you for taking the time to read this. Please share your opinions on me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "180tqka", "is_robot_indexable": true, "report_reasons": null, "author": "guvavava", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180tqka/seeking_advice_on_how_to_get_entrylevel_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180tqka/seeking_advice_on_how_to_get_entrylevel_data/", "subreddit_subscribers": 141006, "created_utc": 1700606413.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone tried any AI tools to apply for jobs ?", "author_fullname": "t2_3x0urbjs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ai tools to apply for jobs ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_180sqzb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700603896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone tried any AI tools to apply for jobs ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180sqzb", "is_robot_indexable": true, "report_reasons": null, "author": "educationruinedme1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180sqzb/ai_tools_to_apply_for_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180sqzb/ai_tools_to_apply_for_jobs/", "subreddit_subscribers": 141006, "created_utc": 1700603896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe have a Databricks env where we have been following the documented Databricks model, where we extract data from a relational data source into our bronze table.   We then do additional processing to create the silver table for the appropriate use case.  We extract the data from a read replica to ensure that we don't impact our transactional work.  This is a beginner question, but what are the main reasons to use the bronze table when doing processing for our silver table versus just pulling directly from the read replica?  Thanks! ", "author_fullname": "t2_b3ncdhop6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Silver from bronze vs Silver pulling directly from read replicas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180rzhz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700602027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We have a Databricks env where we have been following the documented Databricks model, where we extract data from a relational data source into our bronze table.   We then do additional processing to create the silver table for the appropriate use case.  We extract the data from a read replica to ensure that we don&amp;#39;t impact our transactional work.  This is a beginner question, but what are the main reasons to use the bronze table when doing processing for our silver table versus just pulling directly from the read replica?  Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180rzhz", "is_robot_indexable": true, "report_reasons": null, "author": "Annual-Scallion-4888", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180rzhz/databricks_silver_from_bronze_vs_silver_pulling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180rzhz/databricks_silver_from_bronze_vs_silver_pulling/", "subreddit_subscribers": 141006, "created_utc": 1700602027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks, I had some thoughts about what are the quality indicators of a data team. In software engineering, it's all about DORA. It's not purely software engineering, but more about devops. And there was a large study to correlate success of companies to those indicators (not sure it would apply to data :D). \n\nI found this thread [https://www.reddit.com/r/dataengineering/comments/17e377n/tech\\_kpis/](https://www.reddit.com/r/dataengineering/comments/17e377n/tech_kpis/) but I did not like the \"Number of user per report\", because the data team cannot really directly impact this.\n\nMy naive thoughts of a parallel of DORA are these, let us consider a report:\n\n* Deploy frequency =&gt; Data freshness\n* Lead time for change =&gt; Cutoff time\n* Change Failure Rate =&gt; Stability of your metric\n* Mean time to recovery =&gt; Meantime between data issue introduction and resolution \n\nData freshness is straightforward, I just need a updated\\_at column, or a refreshed\\_at\n\nLead time for change =&gt; the \\*cutoff time\\*, like the time between the end of the month, and the day the report is up to date (We wait \"3 day\" to have the data), not sure how to get that from a table.\n\nChange Failure Rate =&gt; Stability of your metric: when the report says \"january 100\u20ac\", but then after some weeks  \"january 99\" (because some fix or data update). In this case the data has some kind of volatility of 1%.\n\nMeantime to recovery =&gt; Say you have a drop in a column but it takes a week to see it and solve it, recovery is 1w.\n\n&amp;#x200B;\n\nI think that to compute those indicators I need a regular snapshot of our report, so we can detect how long before the new data is append, we could also see the a problem in the report (like a big drop in a metric some day, then a big pump, the time in between would be the recovery time) and the stability.  \n\n\nDo any of you do something like this ? Snapshoting reports ? Or maybe computing those indicators somehow ?", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Performance indicators of a data team ? Can we compute some DORA metrics ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180l3o9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700584480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks, I had some thoughts about what are the quality indicators of a data team. In software engineering, it&amp;#39;s all about DORA. It&amp;#39;s not purely software engineering, but more about devops. And there was a large study to correlate success of companies to those indicators (not sure it would apply to data :D). &lt;/p&gt;\n\n&lt;p&gt;I found this thread &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/17e377n/tech_kpis/\"&gt;https://www.reddit.com/r/dataengineering/comments/17e377n/tech_kpis/&lt;/a&gt; but I did not like the &amp;quot;Number of user per report&amp;quot;, because the data team cannot really directly impact this.&lt;/p&gt;\n\n&lt;p&gt;My naive thoughts of a parallel of DORA are these, let us consider a report:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Deploy frequency =&amp;gt; Data freshness&lt;/li&gt;\n&lt;li&gt;Lead time for change =&amp;gt; Cutoff time&lt;/li&gt;\n&lt;li&gt;Change Failure Rate =&amp;gt; Stability of your metric&lt;/li&gt;\n&lt;li&gt;Mean time to recovery =&amp;gt; Meantime between data issue introduction and resolution &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Data freshness is straightforward, I just need a updated_at column, or a refreshed_at&lt;/p&gt;\n\n&lt;p&gt;Lead time for change =&amp;gt; the *cutoff time*, like the time between the end of the month, and the day the report is up to date (We wait &amp;quot;3 day&amp;quot; to have the data), not sure how to get that from a table.&lt;/p&gt;\n\n&lt;p&gt;Change Failure Rate =&amp;gt; Stability of your metric: when the report says &amp;quot;january 100\u20ac&amp;quot;, but then after some weeks  &amp;quot;january 99&amp;quot; (because some fix or data update). In this case the data has some kind of volatility of 1%.&lt;/p&gt;\n\n&lt;p&gt;Meantime to recovery =&amp;gt; Say you have a drop in a column but it takes a week to see it and solve it, recovery is 1w.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I think that to compute those indicators I need a regular snapshot of our report, so we can detect how long before the new data is append, we could also see the a problem in the report (like a big drop in a metric some day, then a big pump, the time in between would be the recovery time) and the stability.  &lt;/p&gt;\n\n&lt;p&gt;Do any of you do something like this ? Snapshoting reports ? Or maybe computing those indicators somehow ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180l3o9", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180l3o9/performance_indicators_of_a_data_team_can_we/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180l3o9/performance_indicators_of_a_data_team_can_we/", "subreddit_subscribers": 141006, "created_utc": 1700584480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Edit: the code is already in bq syntax. \n\nHi all, just moved to bigquery, I am wondering if there is a tool like dbt-core or dataform that requires little to no refactoring? None of the existing code to build datamarts is in CTEs and there are multiple DDL statements where we store intermediate results in temp tables.\n\nI could write a novel explaining how terrible the current process to execute and promote code is but suffice to say it's not good at all and I am desperate to find something better.", "author_fullname": "t2_39nrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Something like dbt/dataform that requires minimum refactoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180kvqa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700588601.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700583928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit: the code is already in bq syntax. &lt;/p&gt;\n\n&lt;p&gt;Hi all, just moved to bigquery, I am wondering if there is a tool like dbt-core or dataform that requires little to no refactoring? None of the existing code to build datamarts is in CTEs and there are multiple DDL statements where we store intermediate results in temp tables.&lt;/p&gt;\n\n&lt;p&gt;I could write a novel explaining how terrible the current process to execute and promote code is but suffice to say it&amp;#39;s not good at all and I am desperate to find something better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180kvqa", "is_robot_indexable": true, "report_reasons": null, "author": "arborealguy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180kvqa/something_like_dbtdataform_that_requires_minimum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180kvqa/something_like_dbtdataform_that_requires_minimum/", "subreddit_subscribers": 141006, "created_utc": 1700583928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello data engineers!\n\nI am in my first DE project and I will be giving my fellow PBI Devs survey data. My question is: how to serve it properly, looking at big picture and\n\n\\&amp;#x200B;\n\n&amp;#x200B;\n\nThe raw data is in xlsx format, with two worksheets. One with numerical data, and one with descriptions what is hidden behind the numbers as well as Question's\n\n&amp;#x200B;\n\nThis is sample of the data\n\n| RespID | Age | Loc | Gen | PRD | BRD | Q1 | Q2 | Q3 | Q4 | Q5 |\n|--------|-----|-----|-----|-----|-----|----|----|----|----|----|\n| 1      | 1   | 1   | 1   | 4   | 3   | 4  | 1  | 6  | 8  | 5  |\n| 2      | 1   | 2   | 2   | 4   | 1   | 1  | 4  | 9  | 9  | 7  |\n| 3      | 4   | 3   | 2   | 1   | 1   | 3  | 4  | 6  | 9  | 1  |\n| 4      | 1   | 2   | 2   | 4   | 5   | 7  | 4  | 3  | 8  | 6  |\n| 5      | 2   | 4   | 1   | 4   | 5   | 1  | 4  | 3  | 2  | 6  |\n| 6      | 1   | 4   | 2   | 2   | 7   | 2  | 4  | 2  | 1  | 5  |\n| 7      | 2   | 2   | 2   | 3   | 5   | 7  | 2  | 10 | 2  | 6  |\n| 8      | 4   | 2   | 1   | 4   | 1   | 4  | 2  | 6  | 5  | 4  |\n| 9      | 3   | 1   | 2   | 2   | 6   | 4  | 3  | 9  | 4  | 6  |\n| 10     | 1   | 4   | 1   | 4   | 1   | 7  | 7  | 7  | 7  | 6  |\n\nand in the second worksheet i'd have something like\n\nRespID - RespondentID\n\nAge - Age of respondent\n\nLoc - Location of respondent\n\nPRD - Product\n\n..\n\nQ1 - In a scale of 1-7 how do you like XX\n\nthen values\n\nage: 1 - '19-24' .. 4 - '60+'\n\ngender: 1-'female',2-'male'\n\netc\n\nMy considerations:\n\nThis is for one country and all countries will be there. So how can I distinguish then countries? Lets say RespID = 1 from that survey (GB) will not equal to RespID = 1 of USA survey. Also the age buckets could be different, questions etc. Should I append everything and create a new ID for each respondent?\n\nIn the future, data from other department will need to be attached there. So how can I proceed right now, bring some artificial key and let them attach it to the files they want to merge with these surveys?\n\nShould I keep numerical answers and have some sort of dictionary in separate table or should I run python dict over it so all numbers become answers?\n\nThank you for any tips", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Properly delivering for BI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180gwoz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700573097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello data engineers!&lt;/p&gt;\n\n&lt;p&gt;I am in my first DE project and I will be giving my fellow PBI Devs survey data. My question is: how to serve it properly, looking at big picture and&lt;/p&gt;\n\n&lt;p&gt;&amp;amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The raw data is in xlsx format, with two worksheets. One with numerical data, and one with descriptions what is hidden behind the numbers as well as Question&amp;#39;s&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This is sample of the data&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;RespID&lt;/th&gt;\n&lt;th&gt;Age&lt;/th&gt;\n&lt;th&gt;Loc&lt;/th&gt;\n&lt;th&gt;Gen&lt;/th&gt;\n&lt;th&gt;PRD&lt;/th&gt;\n&lt;th&gt;BRD&lt;/th&gt;\n&lt;th&gt;Q1&lt;/th&gt;\n&lt;th&gt;Q2&lt;/th&gt;\n&lt;th&gt;Q3&lt;/th&gt;\n&lt;th&gt;Q4&lt;/th&gt;\n&lt;th&gt;Q5&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;8&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;8&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;10&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;8&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;10&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;and in the second worksheet i&amp;#39;d have something like&lt;/p&gt;\n\n&lt;p&gt;RespID - RespondentID&lt;/p&gt;\n\n&lt;p&gt;Age - Age of respondent&lt;/p&gt;\n\n&lt;p&gt;Loc - Location of respondent&lt;/p&gt;\n\n&lt;p&gt;PRD - Product&lt;/p&gt;\n\n&lt;p&gt;..&lt;/p&gt;\n\n&lt;p&gt;Q1 - In a scale of 1-7 how do you like XX&lt;/p&gt;\n\n&lt;p&gt;then values&lt;/p&gt;\n\n&lt;p&gt;age: 1 - &amp;#39;19-24&amp;#39; .. 4 - &amp;#39;60+&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;gender: 1-&amp;#39;female&amp;#39;,2-&amp;#39;male&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;etc&lt;/p&gt;\n\n&lt;p&gt;My considerations:&lt;/p&gt;\n\n&lt;p&gt;This is for one country and all countries will be there. So how can I distinguish then countries? Lets say RespID = 1 from that survey (GB) will not equal to RespID = 1 of USA survey. Also the age buckets could be different, questions etc. Should I append everything and create a new ID for each respondent?&lt;/p&gt;\n\n&lt;p&gt;In the future, data from other department will need to be attached there. So how can I proceed right now, bring some artificial key and let them attach it to the files they want to merge with these surveys?&lt;/p&gt;\n\n&lt;p&gt;Should I keep numerical answers and have some sort of dictionary in separate table or should I run python dict over it so all numbers become answers?&lt;/p&gt;\n\n&lt;p&gt;Thank you for any tips&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180gwoz", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180gwoz/properly_delivering_for_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180gwoz/properly_delivering_for_bi/", "subreddit_subscribers": 141006, "created_utc": 1700573097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/180fwa8)", "author_fullname": "t2_d9zudt5m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will LLMs replace the data transformation layer when building ETL pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180fwa8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700569896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/180fwa8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180fwa8", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Berry", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1701174696530, "options": [{"text": "Yes (Full automation)", "id": "26096551"}, {"text": "Yes (Partial automation with some configuration required)", "id": "26096552"}, {"text": "No", "id": "26096553"}, {"text": "Don't know", "id": "26096554"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 33, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180fwa8/will_llms_replace_the_data_transformation_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/180fwa8/will_llms_replace_the_data_transformation_layer/", "subreddit_subscribers": 141006, "created_utc": 1700569896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I work in data for an organization with about 100 people in a two man data team. I and my college have the same job title of business intelligence specialist. We are both industrial engineers who specialized in data and data analysis and have the same responsibilities. Those are :\n\n1. Deploy and maintain data pipelines from various systems to the data warehouse\n2. Design, build and maintain data warehouse\n3. Design, build and maintain Power BI datasets and reports\n4. Perform statistical analysis on data and calculate KPI's\n5. Find ideas for creating value with mathematical/machine learning models, build and execute them\n6. Various other tasks related to data and tech such as automation of work flows, building power apps for data entry and other purposes and more\n\nFrom any resources about job titles and data teams I find it seems we fill every position in the data team structure as all purpose data specialists. The broad meaning of business intelligence specialist is one who analyses data and provides solutions for business operations, which, in our case, catches our role of statistical analysis and Power BI building in my opinion. The rest of our responsibilities, who account for at least 60% of our work, are not defined in the job title, and the pay is not reflective of our work either but that's another story... So my question is, what job title would be descriptive of all the different activities a data team performs?\n\nMany thanks in advance.", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive job title", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180enru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700565354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I work in data for an organization with about 100 people in a two man data team. I and my college have the same job title of business intelligence specialist. We are both industrial engineers who specialized in data and data analysis and have the same responsibilities. Those are :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Deploy and maintain data pipelines from various systems to the data warehouse&lt;/li&gt;\n&lt;li&gt;Design, build and maintain data warehouse&lt;/li&gt;\n&lt;li&gt;Design, build and maintain Power BI datasets and reports&lt;/li&gt;\n&lt;li&gt;Perform statistical analysis on data and calculate KPI&amp;#39;s&lt;/li&gt;\n&lt;li&gt;Find ideas for creating value with mathematical/machine learning models, build and execute them&lt;/li&gt;\n&lt;li&gt;Various other tasks related to data and tech such as automation of work flows, building power apps for data entry and other purposes and more&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;From any resources about job titles and data teams I find it seems we fill every position in the data team structure as all purpose data specialists. The broad meaning of business intelligence specialist is one who analyses data and provides solutions for business operations, which, in our case, catches our role of statistical analysis and Power BI building in my opinion. The rest of our responsibilities, who account for at least 60% of our work, are not defined in the job title, and the pay is not reflective of our work either but that&amp;#39;s another story... So my question is, what job title would be descriptive of all the different activities a data team performs?&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "180enru", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180enru/comprehensive_job_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180enru/comprehensive_job_title/", "subreddit_subscribers": 141006, "created_utc": 1700565354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a couple of stored procedures that re-compute daily. These contain aggregate functions like max, distinct, window functions, etc. I know this is quite vague, but is there a way to structure your queries/models to enable you to run incremental refreshes?", "author_fullname": "t2_c2f6kxll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental aggregations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180e2pn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700562913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple of stored procedures that re-compute daily. These contain aggregate functions like max, distinct, window functions, etc. I know this is quite vague, but is there a way to structure your queries/models to enable you to run incremental refreshes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180e2pn", "is_robot_indexable": true, "report_reasons": null, "author": "unstable_label", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180e2pn/incremental_aggregations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180e2pn/incremental_aggregations/", "subreddit_subscribers": 141006, "created_utc": 1700562913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At an organization where the data is significantly small, all the data we have doesnt sum up to 15GB, and that is on a dry run, I already have method implemented to only move updates and inserts on a daily basis.\n\nHow bad is it to use SPs (routines on BigQuery) to do the ETL jobs, moving from landing to staging to the DW.\n\nJobs will be scheduled using Composer, will write my workflow logic in airflow", "author_fullname": "t2_8lszlkal", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on using Stored Procedures as an ETL tool on BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180cwkp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700557740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At an organization where the data is significantly small, all the data we have doesnt sum up to 15GB, and that is on a dry run, I already have method implemented to only move updates and inserts on a daily basis.&lt;/p&gt;\n\n&lt;p&gt;How bad is it to use SPs (routines on BigQuery) to do the ETL jobs, moving from landing to staging to the DW.&lt;/p&gt;\n\n&lt;p&gt;Jobs will be scheduled using Composer, will write my workflow logic in airflow&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180cwkp", "is_robot_indexable": true, "report_reasons": null, "author": "70sechoes", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180cwkp/thoughts_on_using_stored_procedures_as_an_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180cwkp/thoughts_on_using_stored_procedures_as_an_etl/", "subreddit_subscribers": 141006, "created_utc": 1700557740.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}