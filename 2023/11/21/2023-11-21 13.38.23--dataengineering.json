{"kind": "Listing", "data": {"after": "t3_17zs8at", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I'm a newer DE tasked with building our DW architecture. We decided to go with Snowflake for our warehouse and dbt for our 'T'. We're primarily an Azure shop - and I'm really stuck on WHERE to execute my python code. There is so so much noise on the topic and so many options that I'm honestly feeling overwhelmed. It doesnt seem like people like ADF and say it comes with a lot of headache (I dont know how true this is), some people don't seem to like Airflow (Idk if this is bc they're using it wrong, whatever), Dagster seems to be a good option - but their managed serverless compute tier is probably overkill right now. Right now I'm thinking of Dagster/Airflow orchestrating and calling Docker containers that have my pthon code, and then running that on like Azure Kubernetes Service? What about running those docker containers on a VM and then shipping them to something more heavy duty later? Would this be preferred to azure functions or azure durable functions? Is ADF really that bad? What do you guys think has a feasible learning curve (assuming the solution isn't some managed service) while still serving as a scalable and/or cost effective solution? \n\nHopefully vendors don't infiltrate this conversation too much lol", "author_fullname": "t2_3ugqxzu6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing our 'EL' stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17zxmev", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700510932.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m a newer DE tasked with building our DW architecture. We decided to go with Snowflake for our warehouse and dbt for our &amp;#39;T&amp;#39;. We&amp;#39;re primarily an Azure shop - and I&amp;#39;m really stuck on WHERE to execute my python code. There is so so much noise on the topic and so many options that I&amp;#39;m honestly feeling overwhelmed. It doesnt seem like people like ADF and say it comes with a lot of headache (I dont know how true this is), some people don&amp;#39;t seem to like Airflow (Idk if this is bc they&amp;#39;re using it wrong, whatever), Dagster seems to be a good option - but their managed serverless compute tier is probably overkill right now. Right now I&amp;#39;m thinking of Dagster/Airflow orchestrating and calling Docker containers that have my pthon code, and then running that on like Azure Kubernetes Service? What about running those docker containers on a VM and then shipping them to something more heavy duty later? Would this be preferred to azure functions or azure durable functions? Is ADF really that bad? What do you guys think has a feasible learning curve (assuming the solution isn&amp;#39;t some managed service) while still serving as a scalable and/or cost effective solution? &lt;/p&gt;\n\n&lt;p&gt;Hopefully vendors don&amp;#39;t infiltrate this conversation too much lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17zxmev", "is_robot_indexable": true, "report_reasons": null, "author": "Casdom33", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17zxmev/choosing_our_el_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17zxmev/choosing_our_el_stack/", "subreddit_subscribers": 140902, "created_utc": 1700510932.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on a personal project where I\u2019m pulling NBA data from a scraper and putting it into a database. This scraping obviously needs to be automated. Obviously going to need to automate my data cleaning as well. A little bit further down the line I\u2019m going to want to automate the process of pulling data from my database to visualize it on a website. Not sure if this calls for Airflow, I have zero data engineer experience.\n\nThanks!", "author_fullname": "t2_kc7mg3akr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I use Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1808qro", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700541639.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a personal project where I\u2019m pulling NBA data from a scraper and putting it into a database. This scraping obviously needs to be automated. Obviously going to need to automate my data cleaning as well. A little bit further down the line I\u2019m going to want to automate the process of pulling data from my database to visualize it on a website. Not sure if this calls for Airflow, I have zero data engineer experience.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1808qro", "is_robot_indexable": true, "report_reasons": null, "author": "Brief-Union-3493", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1808qro/should_i_use_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1808qro/should_i_use_airflow/", "subreddit_subscribers": 140902, "created_utc": 1700541639.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am currently working as a data analyst and was interested to know if someone has given the new data engineering exam from AWS (DEA-C01). It would be great if the experts/individuals can share their experiences and study pattern for the same since I am planning to give the same.", "author_fullname": "t2_a06jzj8ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Data Engineering exam( DEA-C01)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1804yfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700529816.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am currently working as a data analyst and was interested to know if someone has given the new data engineering exam from AWS (DEA-C01). It would be great if the experts/individuals can share their experiences and study pattern for the same since I am planning to give the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1804yfc", "is_robot_indexable": true, "report_reasons": null, "author": "ravipar", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1804yfc/aws_data_engineering_exam_deac01/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1804yfc/aws_data_engineering_exam_deac01/", "subreddit_subscribers": 140902, "created_utc": 1700529816.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today, I'm just a backend web developer with a lot of SQL experience wrapping up my first semester in a Masters of CS program. I took: Advanced Operating Systems, Computer Security, Database Architecture, and Software Engineering Course. \n\nWhat sort of courses would you recommend to break into Data Engineering? I'm currently eyeing a Machine Learning Fundamentals, Data Visualization, and Distributed Systems course.", "author_fullname": "t2_anqr7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Presently in an MSCS Program -- What courses would you recommend for Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17zy8sc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700512502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today, I&amp;#39;m just a backend web developer with a lot of SQL experience wrapping up my first semester in a Masters of CS program. I took: Advanced Operating Systems, Computer Security, Database Architecture, and Software Engineering Course. &lt;/p&gt;\n\n&lt;p&gt;What sort of courses would you recommend to break into Data Engineering? I&amp;#39;m currently eyeing a Machine Learning Fundamentals, Data Visualization, and Distributed Systems course.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17zy8sc", "is_robot_indexable": true, "report_reasons": null, "author": "KillerSmalls", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17zy8sc/presently_in_an_mscs_program_what_courses_would/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17zy8sc/presently_in_an_mscs_program_what_courses_would/", "subreddit_subscribers": 140902, "created_utc": 1700512502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you have an API serving layer enabled directly from the lake? \nWhat tech stack do you use?", "author_fullname": "t2_4ncjzv2a6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "API serving layer over data lake tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1809l87", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700544591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have an API serving layer enabled directly from the lake? \nWhat tech stack do you use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1809l87", "is_robot_indexable": true, "report_reasons": null, "author": "ImpactOk7137", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1809l87/api_serving_layer_over_data_lake_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1809l87/api_serving_layer_over_data_lake_tables/", "subreddit_subscribers": 140902, "created_utc": 1700544591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Some background first: I work as a data engineer in the BI department of a medium sized e-commerce firm. Our BI architecture is currently 100% on-premise. We use a data virtualization platform as a middleware between source systems and a data warehouse (postgres). For log and web tracking analytics we also use Clickhouse. This setup is probably not state of the art but fits our current needs quiet well. \n\nThe server hosting both the dwh and the middleware is going to be renewed and management decided it has to be something cloud. I'm struggeling a bit about the right course of action. Our data warehouse is currently around 2 TB in size. Most SQL queries run within an acceptable time frame. Some large queries could be faster but we are currently shifting some of these workloads to Clickhouse, with great results. \n\nMy approach is to keep the overall setup but separate BI middleware from the postgres database by bringing the latter to the cloud. I was thinking about an AWS EC2 instance like r5b to host postgres. Budget is always tight so RDS is probably not an option. I expect an increase in query performance due to better hardware and dedicated server. But is it really recommened to hold onto a relational database for mostly analytical purposes? Should we better switch to Redshift/BigQuery instead? It would be more expensive and an even bigger migration project (no manpower lol) and maybe absolutely oversized for our needs... Anyone has any thoughts on this? I'd really appreciate it. Thank you", "author_fullname": "t2_ramk8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating from on-premise to cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1803jav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700525734.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Some background first: I work as a data engineer in the BI department of a medium sized e-commerce firm. Our BI architecture is currently 100% on-premise. We use a data virtualization platform as a middleware between source systems and a data warehouse (postgres). For log and web tracking analytics we also use Clickhouse. This setup is probably not state of the art but fits our current needs quiet well. &lt;/p&gt;\n\n&lt;p&gt;The server hosting both the dwh and the middleware is going to be renewed and management decided it has to be something cloud. I&amp;#39;m struggeling a bit about the right course of action. Our data warehouse is currently around 2 TB in size. Most SQL queries run within an acceptable time frame. Some large queries could be faster but we are currently shifting some of these workloads to Clickhouse, with great results. &lt;/p&gt;\n\n&lt;p&gt;My approach is to keep the overall setup but separate BI middleware from the postgres database by bringing the latter to the cloud. I was thinking about an AWS EC2 instance like r5b to host postgres. Budget is always tight so RDS is probably not an option. I expect an increase in query performance due to better hardware and dedicated server. But is it really recommened to hold onto a relational database for mostly analytical purposes? Should we better switch to Redshift/BigQuery instead? It would be more expensive and an even bigger migration project (no manpower lol) and maybe absolutely oversized for our needs... Anyone has any thoughts on this? I&amp;#39;d really appreciate it. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1803jav", "is_robot_indexable": true, "report_reasons": null, "author": "knabbels", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1803jav/migrating_from_onpremise_to_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1803jav/migrating_from_onpremise_to_cloud/", "subreddit_subscribers": 140902, "created_utc": 1700525734.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My goal is training a machine learning model using SageMaker. I have preprocessed my data using pyspark in Amazon EMR, and I want to save my processed data in S3 and read it from SageMaker to use Random Cut Forest.  \nIs this the best way to do it or I should change something? Should I save processed data as parquet or another type?  \nNote that I would like to train the model using pipe mode", "author_fullname": "t2_7zvlhn0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon EMR and Sagemaker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17zz1ez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700514519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My goal is training a machine learning model using SageMaker. I have preprocessed my data using pyspark in Amazon EMR, and I want to save my processed data in S3 and read it from SageMaker to use Random Cut Forest.&lt;br/&gt;\nIs this the best way to do it or I should change something? Should I save processed data as parquet or another type?&lt;br/&gt;\nNote that I would like to train the model using pipe mode&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17zz1ez", "is_robot_indexable": true, "report_reasons": null, "author": "Omart__", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17zz1ez/amazon_emr_and_sagemaker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17zz1ez/amazon_emr_and_sagemaker/", "subreddit_subscribers": 140902, "created_utc": 1700514519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm building a pipeline that will read a very huge data (700M rows) from postgres and then load into a delta table. I have databricks spark at my disposal to get the job done, the ask is to perform the activity as quick as possible.\n\nI tried executing read from postgres using 30 parallel connections and then writing it to an external delta table (S3) but it just takes infinite amount of time.\n\nDatabricks is hosted on AWS and I'm using r5d.4xLarge instances with auto scaling, but stats tell me this is an overkill. My cluster constantly sizes up and down which I think is adding to the delays\n\nAny suggestions on how I approach the pipleine with any custom spark configs will be greatly appreciated. Data is not skewed, none of the tasks have been completed. Thank you!", "author_fullname": "t2_tacjangv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting huge dataset from postgres to databricks delta", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180cq2j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700556958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m building a pipeline that will read a very huge data (700M rows) from postgres and then load into a delta table. I have databricks spark at my disposal to get the job done, the ask is to perform the activity as quick as possible.&lt;/p&gt;\n\n&lt;p&gt;I tried executing read from postgres using 30 parallel connections and then writing it to an external delta table (S3) but it just takes infinite amount of time.&lt;/p&gt;\n\n&lt;p&gt;Databricks is hosted on AWS and I&amp;#39;m using r5d.4xLarge instances with auto scaling, but stats tell me this is an overkill. My cluster constantly sizes up and down which I think is adding to the delays&lt;/p&gt;\n\n&lt;p&gt;Any suggestions on how I approach the pipleine with any custom spark configs will be greatly appreciated. Data is not skewed, none of the tasks have been completed. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180cq2j", "is_robot_indexable": true, "report_reasons": null, "author": "reflectico", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180cq2j/ingesting_huge_dataset_from_postgres_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180cq2j/ingesting_huge_dataset_from_postgres_to/", "subreddit_subscribers": 140902, "created_utc": 1700556958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI\u00b4m currently recieving JSON files from a source system. I process the json files and first store them as  nested delta tables. After that I flatten the nested data with pyspark. At the moment I pretty much wrote the flattening code by hand without any recursive logic. However since the number of live markets is increasing (&gt;20 already) and the schemas differ for each market this is not acceptable anymore.\n\nTherefore I would like to implement a scalable solution that takes the following things into account:- use pyspark- split arrays into seperate dataframes- automatically create technical ids that can be used to join the between the flattened tables (Not used at the moment but yields to some problems if not used)\n\nThis was inspired by the following package: [https://github.com/kindly/flatterer](https://github.com/kindly/flatterer)However the package can only be used with python and raw json files.\n\n**Help needed**\n\n\\- Are there any pypspark packages that can be used to flatten nested data and take into considerations the creation of technical ids?- If no package are available. Has anyone experience with flattening in pyspark and can share code?\n\n**Example with current approach**\n\nThe following example shows how arrays should be split in seperate tables. It also shows some of the current problems when not using technical ids.\n\n    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n    import pyspark.sql.functions as F\n    \n    # lets create a nested schema\n    schema = StructType([\n        StructField(\"some_business_id\", StringType(), True),\n        StructField(\"some_field\", StringType(), True),\n        StructField(\"businesspartners\", ArrayType(\n            StructType([\n                StructField(\"name\", StringType(), True),\n                StructField(\"profession\", StringType(), True),\n                StructField(\"addresses\", ArrayType(\n                    StructType([\n                        StructField(\"street\", StringType(), True),\n                        StructField(\"city\", StringType(), True)\n                    ])\n                ), True)\n            ])\n        ), True)\n    ])\n    \n    # 2 dummy entries for the schema\n    data = [\n        (\"1\", \"value1\", [\n            {\"name\": \"claus\", \"profession\": \"data engineer\", \"addresses\": [{\"street\": \"Main St\", \"city\": \"Metropolis\"}]},\n            {\"name\": \"joern\", \"profession\": \"data engineer\", \"addresses\": [{\"street\": \"Side St\", \"city\": \"Gotham\"}]}\n        ]),\n        (\"2\", \"value2\", [\n            {\"name\": \"claus\", \"profession\": \"data scientist\", \"addresses\": [{\"street\": \"Sixt St\", \"city\": \"Manhatten\"}]},\n            {\"name\": \"joern\", \"profession\": \"Football player\", \"addresses\": [{\"street\": \"Baker St\", \"city\": \"Fullham\"}]}\n        ]),\n    \n    ]\n    \n    # the initial nested data to flatten\n    df = spark.createDataFrame(data, schema=schema)\n\nThe schema includes 2 nested arrays. Therefore the flattening process should result in 3 tables:- main table- businesspartners table- businesspartner\\_addresses table\n\n    # create the top level table. 'some_business_id' is unique\n    main_table = df.select(\"some_business_id\", \"some_field\")\n\n&amp;#x200B;\n\n[main\\_table.show\\(\\)](https://preview.redd.it/6381cnkmnn1c1.png?width=305&amp;format=png&amp;auto=webp&amp;s=7a73e8b08b5c77aca9119a18936c7c964e5f48ee)\n\n    businesspartner_table = (df\n                             .withColumn(\"businesspartners\", F.explode(F.col(\"businesspartners\")))\n                             .select(\n                                 \"some_business_id\", # added for joins\n                                 \"businesspartners.name\", \n                                 \"businesspartners.profession\"\n                                 ))\n\n&amp;#x200B;\n\n[businesspartner\\_table.show\\(\\)](https://preview.redd.it/6s0hfegsnn1c1.png?width=412&amp;format=png&amp;auto=webp&amp;s=c4d83ebb51bb6a7bbd70e26ebb0a3e12cfebaec7)\n\n\"some\\_business\\_id\" is added in order to allow joins between main and businesspartner table. However having a unique field is not always true. The businesspartner table itself does not have any unique column. This yields to problems when creating the businesspartner address table:\n\n    businesspartner_adress_table = (df\n                             .withColumn(\"businesspartners\", F.explode(F.col(\"businesspartners\")))\n                             .withColumn(\"addresses\", F.explode(F.col(\"businesspartners.addresses\")))\n                             .select(\n                                 \"some_business_id\",\n                                 \"businesspartners.name\",\n                                 \"businesspartners.profession\",\n                                 \"addresses.street\",\n                                 \"addresses.city\",\n                                 )\n                             )\n\nI added name and profession which in combination may be unique. However uniqueness is not guaranteed. Therefore I think technical ids are required.", "author_fullname": "t2_rydqu8m3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unnest nested data recursively with technical ids (pyspark)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"6s0hfegsnn1c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 55, "x": 108, "u": "https://preview.redd.it/6s0hfegsnn1c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cad01d32137d89a5397484f56a609e3719eb5bbf"}, {"y": 111, "x": 216, "u": "https://preview.redd.it/6s0hfegsnn1c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c0a243c78eac42f691720c6d77e9009eb6145846"}, {"y": 164, "x": 320, "u": "https://preview.redd.it/6s0hfegsnn1c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3811f7daca31a165aeeb2f368498f05c829c2c07"}], "s": {"y": 212, "x": 412, "u": "https://preview.redd.it/6s0hfegsnn1c1.png?width=412&amp;format=png&amp;auto=webp&amp;s=c4d83ebb51bb6a7bbd70e26ebb0a3e12cfebaec7"}, "id": "6s0hfegsnn1c1"}, "6381cnkmnn1c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 57, "x": 108, "u": "https://preview.redd.it/6381cnkmnn1c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=95ec5b146e378d76c100c1ba16c64234e24cb460"}, {"y": 115, "x": 216, "u": "https://preview.redd.it/6381cnkmnn1c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ac2fc22f74f45793b22f7ed7ecc6c6cc858422b"}], "s": {"y": 163, "x": 305, "u": "https://preview.redd.it/6381cnkmnn1c1.png?width=305&amp;format=png&amp;auto=webp&amp;s=7a73e8b08b5c77aca9119a18936c7c964e5f48ee"}, "id": "6381cnkmnn1c1"}}, "name": "t3_180c326", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": "transparent", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/KDO7xlB9iRmLtxV3RoqxzT2n5MZKAjya9DfLGAy_TSw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1700554252.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I\u00b4m currently recieving JSON files from a source system. I process the json files and first store them as  nested delta tables. After that I flatten the nested data with pyspark. At the moment I pretty much wrote the flattening code by hand without any recursive logic. However since the number of live markets is increasing (&amp;gt;20 already) and the schemas differ for each market this is not acceptable anymore.&lt;/p&gt;\n\n&lt;p&gt;Therefore I would like to implement a scalable solution that takes the following things into account:- use pyspark- split arrays into seperate dataframes- automatically create technical ids that can be used to join the between the flattened tables (Not used at the moment but yields to some problems if not used)&lt;/p&gt;\n\n&lt;p&gt;This was inspired by the following package: &lt;a href=\"https://github.com/kindly/flatterer\"&gt;https://github.com/kindly/flatterer&lt;/a&gt;However the package can only be used with python and raw json files.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Help needed&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;- Are there any pypspark packages that can be used to flatten nested data and take into considerations the creation of technical ids?- If no package are available. Has anyone experience with flattening in pyspark and can share code?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Example with current approach&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The following example shows how arrays should be split in seperate tables. It also shows some of the current problems when not using technical ids.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\nimport pyspark.sql.functions as F\n\n# lets create a nested schema\nschema = StructType([\n    StructField(&amp;quot;some_business_id&amp;quot;, StringType(), True),\n    StructField(&amp;quot;some_field&amp;quot;, StringType(), True),\n    StructField(&amp;quot;businesspartners&amp;quot;, ArrayType(\n        StructType([\n            StructField(&amp;quot;name&amp;quot;, StringType(), True),\n            StructField(&amp;quot;profession&amp;quot;, StringType(), True),\n            StructField(&amp;quot;addresses&amp;quot;, ArrayType(\n                StructType([\n                    StructField(&amp;quot;street&amp;quot;, StringType(), True),\n                    StructField(&amp;quot;city&amp;quot;, StringType(), True)\n                ])\n            ), True)\n        ])\n    ), True)\n])\n\n# 2 dummy entries for the schema\ndata = [\n    (&amp;quot;1&amp;quot;, &amp;quot;value1&amp;quot;, [\n        {&amp;quot;name&amp;quot;: &amp;quot;claus&amp;quot;, &amp;quot;profession&amp;quot;: &amp;quot;data engineer&amp;quot;, &amp;quot;addresses&amp;quot;: [{&amp;quot;street&amp;quot;: &amp;quot;Main St&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Metropolis&amp;quot;}]},\n        {&amp;quot;name&amp;quot;: &amp;quot;joern&amp;quot;, &amp;quot;profession&amp;quot;: &amp;quot;data engineer&amp;quot;, &amp;quot;addresses&amp;quot;: [{&amp;quot;street&amp;quot;: &amp;quot;Side St&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Gotham&amp;quot;}]}\n    ]),\n    (&amp;quot;2&amp;quot;, &amp;quot;value2&amp;quot;, [\n        {&amp;quot;name&amp;quot;: &amp;quot;claus&amp;quot;, &amp;quot;profession&amp;quot;: &amp;quot;data scientist&amp;quot;, &amp;quot;addresses&amp;quot;: [{&amp;quot;street&amp;quot;: &amp;quot;Sixt St&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Manhatten&amp;quot;}]},\n        {&amp;quot;name&amp;quot;: &amp;quot;joern&amp;quot;, &amp;quot;profession&amp;quot;: &amp;quot;Football player&amp;quot;, &amp;quot;addresses&amp;quot;: [{&amp;quot;street&amp;quot;: &amp;quot;Baker St&amp;quot;, &amp;quot;city&amp;quot;: &amp;quot;Fullham&amp;quot;}]}\n    ]),\n\n]\n\n# the initial nested data to flatten\ndf = spark.createDataFrame(data, schema=schema)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The schema includes 2 nested arrays. Therefore the flattening process should result in 3 tables:- main table- businesspartners table- businesspartner_addresses table&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# create the top level table. &amp;#39;some_business_id&amp;#39; is unique\nmain_table = df.select(&amp;quot;some_business_id&amp;quot;, &amp;quot;some_field&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6381cnkmnn1c1.png?width=305&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7a73e8b08b5c77aca9119a18936c7c964e5f48ee\"&gt;main_table.show()&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;businesspartner_table = (df\n                         .withColumn(&amp;quot;businesspartners&amp;quot;, F.explode(F.col(&amp;quot;businesspartners&amp;quot;)))\n                         .select(\n                             &amp;quot;some_business_id&amp;quot;, # added for joins\n                             &amp;quot;businesspartners.name&amp;quot;, \n                             &amp;quot;businesspartners.profession&amp;quot;\n                             ))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6s0hfegsnn1c1.png?width=412&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4d83ebb51bb6a7bbd70e26ebb0a3e12cfebaec7\"&gt;businesspartner_table.show()&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;some_business_id&amp;quot; is added in order to allow joins between main and businesspartner table. However having a unique field is not always true. The businesspartner table itself does not have any unique column. This yields to problems when creating the businesspartner address table:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;businesspartner_adress_table = (df\n                         .withColumn(&amp;quot;businesspartners&amp;quot;, F.explode(F.col(&amp;quot;businesspartners&amp;quot;)))\n                         .withColumn(&amp;quot;addresses&amp;quot;, F.explode(F.col(&amp;quot;businesspartners.addresses&amp;quot;)))\n                         .select(\n                             &amp;quot;some_business_id&amp;quot;,\n                             &amp;quot;businesspartners.name&amp;quot;,\n                             &amp;quot;businesspartners.profession&amp;quot;,\n                             &amp;quot;addresses.street&amp;quot;,\n                             &amp;quot;addresses.city&amp;quot;,\n                             )\n                         )\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I added name and profession which in combination may be unique. However uniqueness is not guaranteed. Therefore I think technical ids are required.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?auto=webp&amp;s=c280e9e1b1e5bf7504be8b5ad2b81b1971a2c277", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bcc6c241e48f2ba73b3f5129e3661109ab75dab8", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=802e9ff3de8c8129bcfc9d9f2772ad52320f1b5b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=151c37f4872eaca074c65ed6a13635455d905480", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=341e0a0bb001f848ffdbe1d6c800a195acf588e2", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=78949e161f10fe0f9c51a285824b8be5375814a4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/tI4t8pg0mYBbPz89vWDSOR0izvZSRTy11TmYS502GVQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=35df9b70a49221e7a9ed7806e0c5bb218a6c45cb", "width": 1080, "height": 540}], "variants": {}, "id": "hyFi5jUHPZCRVg_vPNhPPj2NjjHzUPV_O3g8U3Oi8Lo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180c326", "is_robot_indexable": true, "report_reasons": null, "author": "DecisionAgile7326", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/180c326/unnest_nested_data_recursively_with_technical_ids/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180c326/unnest_nested_data_recursively_with_technical_ids/", "subreddit_subscribers": 140902, "created_utc": 1700554252.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I'm about 4 years removed from college at my second job, both have been relatively early startups. My first title was DE Engineer and now I am an ML engineer. Neither title really encapsulates what I do as I've been involved in architecting two end-to-end data pipelines, building labeling tools, integrating ml models into the production code base, A LOT of web scraping (node, puppeteer), various IaC projects (terraform for aws resources, databricks, etc.).\n\nI'm looking for a new job, but really just don't feel like I belong in any of the positions I see. For example, I mostly do software engineering, but don't go as low level as a lot of the dedicated SE engineers at the company. I work day to day on the data science team, but don't actually do any modeling (mostly handle the data, retrain pipelines, labeling tools, implementation, liaison between teams, etc.). I feel like a competent high level data engineer because of what I have built top to bottom (data lake, aws dms, data warehouse, delta lake, etc), but really don't touch these tech stacks on a day to day anymore. As you can see I am having a hard time figuring out where/what to apply to because I've become very much a jack of all trades and master of none (which I enjoy a lot).\n\nHave any of you gone through this experience and have any advice. While I like having been able to try so many new things, I worry my lack of specializing in any one thing is hindering my career advancement.", "author_fullname": "t2_2nfde5yl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not sure what I should apply for", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18070j2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700536041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I&amp;#39;m about 4 years removed from college at my second job, both have been relatively early startups. My first title was DE Engineer and now I am an ML engineer. Neither title really encapsulates what I do as I&amp;#39;ve been involved in architecting two end-to-end data pipelines, building labeling tools, integrating ml models into the production code base, A LOT of web scraping (node, puppeteer), various IaC projects (terraform for aws resources, databricks, etc.).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a new job, but really just don&amp;#39;t feel like I belong in any of the positions I see. For example, I mostly do software engineering, but don&amp;#39;t go as low level as a lot of the dedicated SE engineers at the company. I work day to day on the data science team, but don&amp;#39;t actually do any modeling (mostly handle the data, retrain pipelines, labeling tools, implementation, liaison between teams, etc.). I feel like a competent high level data engineer because of what I have built top to bottom (data lake, aws dms, data warehouse, delta lake, etc), but really don&amp;#39;t touch these tech stacks on a day to day anymore. As you can see I am having a hard time figuring out where/what to apply to because I&amp;#39;ve become very much a jack of all trades and master of none (which I enjoy a lot).&lt;/p&gt;\n\n&lt;p&gt;Have any of you gone through this experience and have any advice. While I like having been able to try so many new things, I worry my lack of specializing in any one thing is hindering my career advancement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "18070j2", "is_robot_indexable": true, "report_reasons": null, "author": "pmarct", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18070j2/not_sure_what_i_should_apply_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18070j2/not_sure_what_i_should_apply_for/", "subreddit_subscribers": 140902, "created_utc": 1700536041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "what are your experiences with their security team? There is some accusations on their leadership and inaccurate communication with customers.\n\nhttps://www.teamblind.com/post/politcs-favoritism-and-unethical-databricks-security-org-xcLNEu6A", "author_fullname": "t2_w39c9q7z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spicy blind thread on Databricks security", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1801xn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700521490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;what are your experiences with their security team? There is some accusations on their leadership and inaccurate communication with customers.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.teamblind.com/post/politcs-favoritism-and-unethical-databricks-security-org-xcLNEu6A\"&gt;https://www.teamblind.com/post/politcs-favoritism-and-unethical-databricks-security-org-xcLNEu6A&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Nny4vUCELn0RRwLgCmVpejUljcO_TNFbIi3MdWBV8e0.jpg?auto=webp&amp;s=494fb506600a9642f30deafe2b5e369e54bcf7ba", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Nny4vUCELn0RRwLgCmVpejUljcO_TNFbIi3MdWBV8e0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6c8a8cafa32470600d154c74aa7c3ad02694788", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Nny4vUCELn0RRwLgCmVpejUljcO_TNFbIi3MdWBV8e0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4778554fc065e788daf9d7482a11fec1cd8ef528", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Nny4vUCELn0RRwLgCmVpejUljcO_TNFbIi3MdWBV8e0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0759d085fb9739f3cc852b7290c58bfea4059ed4", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Nny4vUCELn0RRwLgCmVpejUljcO_TNFbIi3MdWBV8e0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=803999044d2dd22a6db4e5788613504e1eb23f1f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Nny4vUCELn0RRwLgCmVpejUljcO_TNFbIi3MdWBV8e0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3f23c2e17c4ebbf757c9dff81ba2dd0bb119bdd5", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Nny4vUCELn0RRwLgCmVpejUljcO_TNFbIi3MdWBV8e0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1f5932b4e7861f1ebfe1d5704e0dd7e838575417", "width": 1080, "height": 567}], "variants": {}, "id": "Gk6YolC49N131jjEu6ojWAA-u6goSZJNsu8wwnZLzKg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1801xn0", "is_robot_indexable": true, "report_reasons": null, "author": "data-ai-nerd", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1801xn0/spicy_blind_thread_on_databricks_security/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1801xn0/spicy_blind_thread_on_databricks_security/", "subreddit_subscribers": 140902, "created_utc": 1700521490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After reading a bit on the internet (and even asking about it here) it seems the majority opinion is that airflow is better for task scheduling and management than it is for actual task execution. \n\nBasically: you're whole pipeline shouldn't be a shit ton of python written directly into airflow DAGs, airflow should just be triggering workflows on other computers (lambdas, cloud run, etc).\n\nBut I'm having a hard time with some of the specifics of this. The nice thing about airflow is how it lets you describe the order of DAGs. Like I can run my \"get data from someone else's API\" DAG, and once that's done, it'll run the \"normalize that data and put it in a database\" DAG. It's easy peasy to set up those relationships, and manage the failure states and stuff.\n\nWhen airflow is just triggering jobs to run elsewhere, how do you keep this nice control flow of DAGs? Do you make an intermediate DAG that uses a reschedule-mode sensor? Or is that a weird hack?", "author_fullname": "t2_8k5ls63w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused about the role of airflow in distributed environments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17zxaby", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700510100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After reading a bit on the internet (and even asking about it here) it seems the majority opinion is that airflow is better for task scheduling and management than it is for actual task execution. &lt;/p&gt;\n\n&lt;p&gt;Basically: you&amp;#39;re whole pipeline shouldn&amp;#39;t be a shit ton of python written directly into airflow DAGs, airflow should just be triggering workflows on other computers (lambdas, cloud run, etc).&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m having a hard time with some of the specifics of this. The nice thing about airflow is how it lets you describe the order of DAGs. Like I can run my &amp;quot;get data from someone else&amp;#39;s API&amp;quot; DAG, and once that&amp;#39;s done, it&amp;#39;ll run the &amp;quot;normalize that data and put it in a database&amp;quot; DAG. It&amp;#39;s easy peasy to set up those relationships, and manage the failure states and stuff.&lt;/p&gt;\n\n&lt;p&gt;When airflow is just triggering jobs to run elsewhere, how do you keep this nice control flow of DAGs? Do you make an intermediate DAG that uses a reschedule-mode sensor? Or is that a weird hack?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17zxaby", "is_robot_indexable": true, "report_reasons": null, "author": "chamomile-crumbs", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17zxaby/confused_about_the_role_of_airflow_in_distributed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17zxaby/confused_about_the_role_of_airflow_in_distributed/", "subreddit_subscribers": 140902, "created_utc": 1700510100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you manage a Ceph cluster for data storage and you are using Kubernetes, you might be interested in our newest software, the Koor Data Control Center. We are in beta and would love to get your feedback about what features would help you most with Ceph storage.\n\nThis describes what we have built so far [https://about.koor.tech/product](https://about.koor.tech/product) Anyone can install a free trial for up to 4 storage nodes. No time limits on the trial. If you want to contact us to give feedback or discuss your situation, here's our contact form: [https://about.koor.tech/contact](https://about.koor.tech/contact)\n\nAbove all, we like to be helpful. We will take any questions about data storage.\n\n&amp;#x200B;", "author_fullname": "t2_d321jhgdp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for feedback on our beta product", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ztwel", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700501502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you manage a Ceph cluster for data storage and you are using Kubernetes, you might be interested in our newest software, the Koor Data Control Center. We are in beta and would love to get your feedback about what features would help you most with Ceph storage.&lt;/p&gt;\n\n&lt;p&gt;This describes what we have built so far &lt;a href=\"https://about.koor.tech/product\"&gt;https://about.koor.tech/product&lt;/a&gt; Anyone can install a free trial for up to 4 storage nodes. No time limits on the trial. If you want to contact us to give feedback or discuss your situation, here&amp;#39;s our contact form: &lt;a href=\"https://about.koor.tech/contact\"&gt;https://about.koor.tech/contact&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Above all, we like to be helpful. We will take any questions about data storage.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pXsV2SXS9SbquPI5gahzjKefpG8-btfZnjO0uPAPCNA.jpg?auto=webp&amp;s=70f74026d1c62617c1774fde3e71a198fe95429d", "width": 1917, "height": 1128}, "resolutions": [{"url": "https://external-preview.redd.it/pXsV2SXS9SbquPI5gahzjKefpG8-btfZnjO0uPAPCNA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8b71fdef9d0fabb53d097600bc3ab7d7f6f565df", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/pXsV2SXS9SbquPI5gahzjKefpG8-btfZnjO0uPAPCNA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0051a2251507bf3c8f3d3a918df32a6b3ee5eaa2", "width": 216, "height": 127}, {"url": "https://external-preview.redd.it/pXsV2SXS9SbquPI5gahzjKefpG8-btfZnjO0uPAPCNA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea722a7eb0e15c6607621bd2291d974d79fea9ec", "width": 320, "height": 188}, {"url": "https://external-preview.redd.it/pXsV2SXS9SbquPI5gahzjKefpG8-btfZnjO0uPAPCNA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=db500642a1e74c7bb9dd71b6259f3ccd448390dd", "width": 640, "height": 376}, {"url": "https://external-preview.redd.it/pXsV2SXS9SbquPI5gahzjKefpG8-btfZnjO0uPAPCNA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c95104e6468850127d68f25b4f6be57fc808672d", "width": 960, "height": 564}, {"url": "https://external-preview.redd.it/pXsV2SXS9SbquPI5gahzjKefpG8-btfZnjO0uPAPCNA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=767cdbd3ee81ba9aa47d29696b97de45fd694765", "width": 1080, "height": 635}], "variants": {}, "id": "W8LZN3dTBlBXOohUIv0rRWpk-U5tJSdTcj0Vl3FmcVI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17ztwel", "is_robot_indexable": true, "report_reasons": null, "author": "Dave-at-Koor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ztwel/looking_for_feedback_on_our_beta_product/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ztwel/looking_for_feedback_on_our_beta_product/", "subreddit_subscribers": 140902, "created_utc": 1700501502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nI've been learning by Data Engr skills slowly but surely, and the last/current major subject I've done the least on is SQL/DWH. I have nearly completed an \"Analytics Engineering\" course, which does (from what I can tell) go over a fair bit of the data modelling terms/practices, but then the implementation is all done in DBT...\n\nThere seems to be very mixed opinions on DBT in this subreddit and it seems to highly vary by circumstance of the organization. The main insight given this, is that it is probably not good to just rely on DBT for everything it handles behind the scenes, because once I work somewhere that doesn't use DBT, I am way behind.\n\nWith all that said, can anyone recommend some courses that would be good for learning the above?\n\nThanks in advance ", "author_fullname": "t2_8chdw7c4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Raw SQL for DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1803j7c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700525727.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been learning by Data Engr skills slowly but surely, and the last/current major subject I&amp;#39;ve done the least on is SQL/DWH. I have nearly completed an &amp;quot;Analytics Engineering&amp;quot; course, which does (from what I can tell) go over a fair bit of the data modelling terms/practices, but then the implementation is all done in DBT...&lt;/p&gt;\n\n&lt;p&gt;There seems to be very mixed opinions on DBT in this subreddit and it seems to highly vary by circumstance of the organization. The main insight given this, is that it is probably not good to just rely on DBT for everything it handles behind the scenes, because once I work somewhere that doesn&amp;#39;t use DBT, I am way behind.&lt;/p&gt;\n\n&lt;p&gt;With all that said, can anyone recommend some courses that would be good for learning the above?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1803j7c", "is_robot_indexable": true, "report_reasons": null, "author": "pdxtechnologist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1803j7c/raw_sql_for_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1803j7c/raw_sql_for_dwh/", "subreddit_subscribers": 140902, "created_utc": 1700525727.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m currently researching on orchestrating   databricks workflows with airflow, and was just wondering what resources do i need and the cost implications of deploying airflow on azure for production", "author_fullname": "t2_jettu57pn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost of implementing airflow in azure for production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17zr1db", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700494124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m currently researching on orchestrating   databricks workflows with airflow, and was just wondering what resources do i need and the cost implications of deploying airflow on azure for production&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17zr1db", "is_robot_indexable": true, "report_reasons": null, "author": "Slight_Award8187", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17zr1db/cost_of_implementing_airflow_in_azure_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17zr1db/cost_of_implementing_airflow_in_azure_for/", "subreddit_subscribers": 140902, "created_utc": 1700494124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short I\u2019m an eLearning Developer wanting to turn into a Data Engineer. I have 1+ years of SQL experience. I\u2019m finishing up a Data Engineering course on Datacamp.\nI\u2019m looking for interview prep questions so that I can crack this asap. Thanks in advance!", "author_fullname": "t2_yoget", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep for DE roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_180gajt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700571170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I\u2019m an eLearning Developer wanting to turn into a Data Engineer. I have 1+ years of SQL experience. I\u2019m finishing up a Data Engineering course on Datacamp.\nI\u2019m looking for interview prep questions so that I can crack this asap. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "180gajt", "is_robot_indexable": true, "report_reasons": null, "author": "Drrazor", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180gajt/interview_prep_for_de_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180gajt/interview_prep_for_de_roles/", "subreddit_subscribers": 140902, "created_utc": 1700571170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I work in data for an organization with about 100 people in a two man data team. I and my college have the same job title of business intelligence specialist. We are both industrial engineers who specialized in data and data analysis and have the same responsibilities. Those are :\n\n1. Deploy and maintain data pipelines from various systems to the data warehouse\n2. Design, build and maintain data warehouse\n3. Design, build and maintain Power BI datasets and reports\n4. Perform statistical analysis on data and calculate KPI's\n5. Find ideas for creating value with mathematical/machine learning models, build and execute them\n6. Various other tasks related to data and tech such as automation of work flows, building power apps for data entry and other purposes and more\n\nFrom any resources about job titles and data teams I find it seems we fill every position in the data team structure as all purpose data specialists. The broad meaning of business intelligence specialist is one who analyses data and provides solutions for business operations, which, in our case, catches our role of statistical analysis and Power BI building in my opinion. The rest of our responsibilities, who account for at least 60% of our work, are not defined in the job title, and the pay is not reflective of our work either but that's another story... So my question is, what job title would be descriptive of all the different activities a data team performs?\n\nMany thanks in advance.", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive job title", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180enru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700565354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I work in data for an organization with about 100 people in a two man data team. I and my college have the same job title of business intelligence specialist. We are both industrial engineers who specialized in data and data analysis and have the same responsibilities. Those are :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Deploy and maintain data pipelines from various systems to the data warehouse&lt;/li&gt;\n&lt;li&gt;Design, build and maintain data warehouse&lt;/li&gt;\n&lt;li&gt;Design, build and maintain Power BI datasets and reports&lt;/li&gt;\n&lt;li&gt;Perform statistical analysis on data and calculate KPI&amp;#39;s&lt;/li&gt;\n&lt;li&gt;Find ideas for creating value with mathematical/machine learning models, build and execute them&lt;/li&gt;\n&lt;li&gt;Various other tasks related to data and tech such as automation of work flows, building power apps for data entry and other purposes and more&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;From any resources about job titles and data teams I find it seems we fill every position in the data team structure as all purpose data specialists. The broad meaning of business intelligence specialist is one who analyses data and provides solutions for business operations, which, in our case, catches our role of statistical analysis and Power BI building in my opinion. The rest of our responsibilities, who account for at least 60% of our work, are not defined in the job title, and the pay is not reflective of our work either but that&amp;#39;s another story... So my question is, what job title would be descriptive of all the different activities a data team performs?&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "180enru", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180enru/comprehensive_job_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180enru/comprehensive_job_title/", "subreddit_subscribers": 140902, "created_utc": 1700565354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a couple of stored procedures that re-compute daily. These contain aggregate functions like max, distinct, window functions, etc. I know this is quite vague, but is there a way to structure your queries/models to enable you to run incremental refreshes?", "author_fullname": "t2_c2f6kxll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental aggregations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180e2pn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700562913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple of stored procedures that re-compute daily. These contain aggregate functions like max, distinct, window functions, etc. I know this is quite vague, but is there a way to structure your queries/models to enable you to run incremental refreshes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180e2pn", "is_robot_indexable": true, "report_reasons": null, "author": "unstable_label", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180e2pn/incremental_aggregations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180e2pn/incremental_aggregations/", "subreddit_subscribers": 140902, "created_utc": 1700562913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI want to use dbt with databricks.If you use dbt cloud, it provides you with online IDE and GUI. The integration with databricks works fine.\n\nWhat if I dont want to use dbt cloud, but dbt core? I know that you can connect dbt core to databricks and start developing in local IDE (ex: VS code). ( github + dbt core + databricks)\n\n* github: for version control\n* dbt core: for dbt project management\n* databricks: for deployment environment + data storage (unity catalog)\n\nIn a nutshell, my question is, can I use databricks as an IDE to develop dbt projects? (instead of developing in local)", "author_fullname": "t2_2x81kpwx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Databricks as IDE for dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180d7cm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700559855.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700559097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I want to use dbt with databricks.If you use dbt cloud, it provides you with online IDE and GUI. The integration with databricks works fine.&lt;/p&gt;\n\n&lt;p&gt;What if I dont want to use dbt cloud, but dbt core? I know that you can connect dbt core to databricks and start developing in local IDE (ex: VS code). ( github + dbt core + databricks)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;github: for version control&lt;/li&gt;\n&lt;li&gt;dbt core: for dbt project management&lt;/li&gt;\n&lt;li&gt;databricks: for deployment environment + data storage (unity catalog)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In a nutshell, my question is, can I use databricks as an IDE to develop dbt projects? (instead of developing in local)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180d7cm", "is_robot_indexable": true, "report_reasons": null, "author": "Disastrous-State-503", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180d7cm/using_databricks_as_ide_for_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180d7cm/using_databricks_as_ide_for_dbt/", "subreddit_subscribers": 140902, "created_utc": 1700559097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "At an organization where the data is significantly small, all the data we have doesnt sum up to 15GB, and that is on a dry run, I already have method implemented to only move updates and inserts on a daily basis.\n\nHow bad is it to use SPs (routines on BigQuery) to do the ETL jobs, moving from landing to staging to the DW.\n\nJobs will be scheduled using Composer, will write my workflow logic in airflow", "author_fullname": "t2_8lszlkal", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on using Stored Procedures as an ETL tool on BigQuery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180cwkp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700557740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At an organization where the data is significantly small, all the data we have doesnt sum up to 15GB, and that is on a dry run, I already have method implemented to only move updates and inserts on a daily basis.&lt;/p&gt;\n\n&lt;p&gt;How bad is it to use SPs (routines on BigQuery) to do the ETL jobs, moving from landing to staging to the DW.&lt;/p&gt;\n\n&lt;p&gt;Jobs will be scheduled using Composer, will write my workflow logic in airflow&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180cwkp", "is_robot_indexable": true, "report_reasons": null, "author": "70sechoes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180cwkp/thoughts_on_using_stored_procedures_as_an_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180cwkp/thoughts_on_using_stored_procedures_as_an_etl/", "subreddit_subscribers": 140902, "created_utc": 1700557740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm a Data Engineer working at a small startup, where I am currently the only data engineer. As we're noticing an increase in our monthly RDS costs, I have a question regarding our current data pipeline setup, which involves transferring data from an AWS RDS PostgreSQL (operational database) to Databricks for analysis and reporting. To minimize any potential impact on the performance of our production environment,  we've opted to use a cloned instance of our production RDS as our data source. This clone is integrated into Databricks through a notebook job, where we handle the ELT. \n\nI'm exploring the most efficient and viable approaches for this task and would appreciate any insights or recommendations from the community. Here are some specific areas where I'm seeking advice:\n\n* Considering that I don't have much influence on our dev team to implement data dumping or queue services, my access is limited to the RDS prod instance, is using an RDS clone the optimal approach, or are there more efficient alternatives?\n* If real-time data refresh isn't a current requirement but might be in the future, what would be the best strategy to adopt now to accommodate potential future needs?\n* I'm considering the following alternative solutions:\n\n1. Periodic snapshots and export to S3.\n2. Using AWS Kinesis.\n3. Using AWS Data Migration Service (DMS).\n4. Continuing with the current method - creating an RDS clone instance for Databricks connection.\n5. Does any other way come to your mind?\n\nThanks in advance for your help.", "author_fullname": "t2_nzi7c4l6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to transfer data from rds to Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1806uqj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700535566.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m a Data Engineer working at a small startup, where I am currently the only data engineer. As we&amp;#39;re noticing an increase in our monthly RDS costs, I have a question regarding our current data pipeline setup, which involves transferring data from an AWS RDS PostgreSQL (operational database) to Databricks for analysis and reporting. To minimize any potential impact on the performance of our production environment,  we&amp;#39;ve opted to use a cloned instance of our production RDS as our data source. This clone is integrated into Databricks through a notebook job, where we handle the ELT. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m exploring the most efficient and viable approaches for this task and would appreciate any insights or recommendations from the community. Here are some specific areas where I&amp;#39;m seeking advice:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Considering that I don&amp;#39;t have much influence on our dev team to implement data dumping or queue services, my access is limited to the RDS prod instance, is using an RDS clone the optimal approach, or are there more efficient alternatives?&lt;/li&gt;\n&lt;li&gt;If real-time data refresh isn&amp;#39;t a current requirement but might be in the future, what would be the best strategy to adopt now to accommodate potential future needs?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m considering the following alternative solutions:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Periodic snapshots and export to S3.&lt;/li&gt;\n&lt;li&gt;Using AWS Kinesis.&lt;/li&gt;\n&lt;li&gt;Using AWS Data Migration Service (DMS).&lt;/li&gt;\n&lt;li&gt;Continuing with the current method - creating an RDS clone instance for Databricks connection.&lt;/li&gt;\n&lt;li&gt;Does any other way come to your mind?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1806uqj", "is_robot_indexable": true, "report_reasons": null, "author": "next_helicopter2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1806uqj/best_way_to_transfer_data_from_rds_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1806uqj/best_way_to_transfer_data_from_rds_to_databricks/", "subreddit_subscribers": 140902, "created_utc": 1700535566.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\n&amp;#x200B;\n\nMy current org. is looking to implement a central data repository (data warehouse/lake/lakehouse). Being a municipal govt., we reached out to our counterparts who have implemented DW. Some have steered away from lake/lakehouse due to security concerns. I've done some researching and unable to determine what might be the best approach. We've accounting and financial, data in a database, payroll data coming from a cloud data lake. Then we've in house and cloud databases as well serving various divisions. Probably in the the future we might also incorporate sensors, images, videos and documents. But that is unclear if it needs to be or not.\n\nI don't know if DW is getting old and if there are newer or better systems in place. As we don't have this system yet, we do not have data engineers/architects.\n\n&amp;#x200B;\n\nIs there some questionnaire or some guide that can help choose the right architecture or systems. All I know is we need a central data repository to build an analytics platform for dashboards and reports and get insights from the data and solution must be scalable", "author_fullname": "t2_np3evoin", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Central Data Repository", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17zzfgd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700516037.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700515464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My current org. is looking to implement a central data repository (data warehouse/lake/lakehouse). Being a municipal govt., we reached out to our counterparts who have implemented DW. Some have steered away from lake/lakehouse due to security concerns. I&amp;#39;ve done some researching and unable to determine what might be the best approach. We&amp;#39;ve accounting and financial, data in a database, payroll data coming from a cloud data lake. Then we&amp;#39;ve in house and cloud databases as well serving various divisions. Probably in the the future we might also incorporate sensors, images, videos and documents. But that is unclear if it needs to be or not.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know if DW is getting old and if there are newer or better systems in place. As we don&amp;#39;t have this system yet, we do not have data engineers/architects.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there some questionnaire or some guide that can help choose the right architecture or systems. All I know is we need a central data repository to build an analytics platform for dashboards and reports and get insights from the data and solution must be scalable&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17zzfgd", "is_robot_indexable": true, "report_reasons": null, "author": "naruzum", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17zzfgd/central_data_repository/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17zzfgd/central_data_repository/", "subreddit_subscribers": 140902, "created_utc": 1700515464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone hope you are doing fine, currently I\u2019m using elastic search for aggregations and I\u2019m really new to this data engineering and I want to know what are the available alternatives to elastic search aggregation i don\u2019t use elastic search for search needs but only for aggregation need, since our dashboard is used by vast number of users I want a alternative which can support high throughput like elastic searches kindly advise me on what are can be alternatives", "author_fullname": "t2_g66aigmk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative for elastic search aggregation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ztrzu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700501198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone hope you are doing fine, currently I\u2019m using elastic search for aggregations and I\u2019m really new to this data engineering and I want to know what are the available alternatives to elastic search aggregation i don\u2019t use elastic search for search needs but only for aggregation need, since our dashboard is used by vast number of users I want a alternative which can support high throughput like elastic searches kindly advise me on what are can be alternatives&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17ztrzu", "is_robot_indexable": true, "report_reasons": null, "author": "Numerous-Bug8381", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ztrzu/alternative_for_elastic_search_aggregation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ztrzu/alternative_for_elastic_search_aggregation/", "subreddit_subscribers": 140902, "created_utc": 1700501198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A nice tutorial about how to handle big data datasets efficiently when building dataflows. It uses two open source Python libraries and the tutorial builds a data pipeline to analyze customer data.\n\n[https://medium.com/@marine.gosselin/big-data-models-vs-computer-memory-b345814ece9f](https://medium.com/@marine.gosselin/big-data-models-vs-computer-memory-b345814ece9f)\n\nI wanted your thoughts: what libraries do you use when typically faced with managing large datasets in back-end creation?\n\nEnjoy!", "author_fullname": "t2_tfe7ylgn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Learn how to build dataflows with larger than memory datasets.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17zsyw6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700499200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A nice tutorial about how to handle big data datasets efficiently when building dataflows. It uses two open source Python libraries and the tutorial builds a data pipeline to analyze customer data.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@marine.gosselin/big-data-models-vs-computer-memory-b345814ece9f\"&gt;https://medium.com/@marine.gosselin/big-data-models-vs-computer-memory-b345814ece9f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I wanted your thoughts: what libraries do you use when typically faced with managing large datasets in back-end creation?&lt;/p&gt;\n\n&lt;p&gt;Enjoy!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/eDCKNhUuH6cQH47OKAtUEw6f8qHgkXSvBsUjXqP-Bd4.jpg?auto=webp&amp;s=3b5e410f440e95a52a35878f677e9603657b64a7", "width": 800, "height": 333}, "resolutions": [{"url": "https://external-preview.redd.it/eDCKNhUuH6cQH47OKAtUEw6f8qHgkXSvBsUjXqP-Bd4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=554f3f06f4a57f57464438e79c0017dfc3de512c", "width": 108, "height": 44}, {"url": "https://external-preview.redd.it/eDCKNhUuH6cQH47OKAtUEw6f8qHgkXSvBsUjXqP-Bd4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=128b12489ea024c1b350a98d2d4a95e05b071e2d", "width": 216, "height": 89}, {"url": "https://external-preview.redd.it/eDCKNhUuH6cQH47OKAtUEw6f8qHgkXSvBsUjXqP-Bd4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9bfadd51c7dbc9524d7b5103c028ef37c492339", "width": 320, "height": 133}, {"url": "https://external-preview.redd.it/eDCKNhUuH6cQH47OKAtUEw6f8qHgkXSvBsUjXqP-Bd4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ebd738d6ced9cee838c6f9d82ef7a8a155c8298", "width": 640, "height": 266}], "variants": {}, "id": "HGI-vRG7wujMoeTFgVEM9JPE4QYiHX-A8ny5bWr0L4s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17zsyw6", "is_robot_indexable": true, "report_reasons": null, "author": "quicklyalienated76", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17zsyw6/learn_how_to_build_dataflows_with_larger_than/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17zsyw6/learn_how_to_build_dataflows_with_larger_than/", "subreddit_subscribers": 140902, "created_utc": 1700499200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've never used Databricks or Spark in a production environment and have been thrown onto a project that is heavily using it. I'm coming from a rdbms-heavy background, so I approach things with a SQL-like approach and have done a decent amount of query optimization.\n\nHowever I'm a little lost on deciphering Spark logical plans using the output of `df.explain`. I've tried `df.explain(True)` and I can't even figure out the difference between the Parsed Logical Plan, Analyzed Logical Plan, Optimized Logical Plan, and Physical Plan.\n\nWhat are some things I should try to avoid? From what I've learned, I should avoid shuffling, avoid skewed joins, and optimize my partitions. But I'm not even doing any joins in a notebook I'm working with. I'd appreciate suggestions, best practices, or anything to tweak my notebooks based on what I see in a physical plan.\n\nAlso, does anyone have suggestions on how to read a large data source and quickly filter it down? I'm trying to test some code but it's insanely slow (taking &gt; 1 hour) even when I try to filter it down and limit it as early as possible.", "author_fullname": "t2_2pyy4c8f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to Databricks (and Spark in general) - how do I decipher logical plans and update my notebook/script accordingly?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17zs8at", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700497270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve never used Databricks or Spark in a production environment and have been thrown onto a project that is heavily using it. I&amp;#39;m coming from a rdbms-heavy background, so I approach things with a SQL-like approach and have done a decent amount of query optimization.&lt;/p&gt;\n\n&lt;p&gt;However I&amp;#39;m a little lost on deciphering Spark logical plans using the output of &lt;code&gt;df.explain&lt;/code&gt;. I&amp;#39;ve tried &lt;code&gt;df.explain(True)&lt;/code&gt; and I can&amp;#39;t even figure out the difference between the Parsed Logical Plan, Analyzed Logical Plan, Optimized Logical Plan, and Physical Plan.&lt;/p&gt;\n\n&lt;p&gt;What are some things I should try to avoid? From what I&amp;#39;ve learned, I should avoid shuffling, avoid skewed joins, and optimize my partitions. But I&amp;#39;m not even doing any joins in a notebook I&amp;#39;m working with. I&amp;#39;d appreciate suggestions, best practices, or anything to tweak my notebooks based on what I see in a physical plan.&lt;/p&gt;\n\n&lt;p&gt;Also, does anyone have suggestions on how to read a large data source and quickly filter it down? I&amp;#39;m trying to test some code but it&amp;#39;s insanely slow (taking &amp;gt; 1 hour) even when I try to filter it down and limit it as early as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17zs8at", "is_robot_indexable": true, "report_reasons": null, "author": "opabm", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17zs8at/new_to_databricks_and_spark_in_general_how_do_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17zs8at/new_to_databricks_and_spark_in_general_how_do_i/", "subreddit_subscribers": 140902, "created_utc": 1700497270.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}