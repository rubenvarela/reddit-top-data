{"kind": "Listing", "data": {"after": null, "dist": 18, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_5x2az", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "burning libraries, less than 1 month until the Tokyo Lab archive is destroyed, 10,000+ masters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17qbhp1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 224, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Less than 1 month before Tokyo Lab&amp;#39;s closure.&lt;br&gt;- More than 10 000 masters (including animes) remain unclaimed by rights holders (many of whom are unknown today) &amp;amp; will probably be destroyed...&lt;br&gt;- Paying to continue storing is not possible...&lt;br&gt;Via &lt;a href=\"https://twitter.com/zkurishi?ref_src=twsrc%5Etfw\"&gt;@zkurishi&lt;/a&gt;: &lt;a href=\"https://t.co/Yk7kCYv6oZ\"&gt;https://t.co/Yk7kCYv6oZ&lt;/a&gt; &lt;a href=\"https://t.co/6k50Ytwhj8\"&gt;https://t.co/6k50Ytwhj8&lt;/a&gt; &lt;a href=\"https://t.co/mK7Rofj1x1\"&gt;pic.twitter.com/mK7Rofj1x1&lt;/a&gt;&lt;/p&gt;&amp;mdash; Catsuka (@catsuka) &lt;a href=\"https://twitter.com/catsuka/status/1721882224549929359?ref_src=twsrc%5Etfw\"&gt;November 7, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/catsuka/status/1721882224549929359", "author_name": "Catsuka", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Less than 1 month before Tokyo Lab&amp;#39;s closure.&lt;br&gt;- More than 10 000 masters (including animes) remain unclaimed by rights holders (many of whom are unknown today) &amp;amp; will probably be destroyed...&lt;br&gt;- Paying to continue storing is not possible...&lt;br&gt;Via &lt;a href=\"https://twitter.com/zkurishi?ref_src=twsrc%5Etfw\"&gt;@zkurishi&lt;/a&gt;: &lt;a href=\"https://t.co/Yk7kCYv6oZ\"&gt;https://t.co/Yk7kCYv6oZ&lt;/a&gt; &lt;a href=\"https://t.co/6k50Ytwhj8\"&gt;https://t.co/6k50Ytwhj8&lt;/a&gt; &lt;a href=\"https://t.co/mK7Rofj1x1\"&gt;pic.twitter.com/mK7Rofj1x1&lt;/a&gt;&lt;/p&gt;&amp;mdash; Catsuka (@catsuka) &lt;a href=\"https://twitter.com/catsuka/status/1721882224549929359?ref_src=twsrc%5Etfw\"&gt;November 7, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/catsuka", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Less than 1 month before Tokyo Lab&amp;#39;s closure.&lt;br&gt;- More than 10 000 masters (including animes) remain unclaimed by rights holders (many of whom are unknown today) &amp;amp; will probably be destroyed...&lt;br&gt;- Paying to continue storing is not possible...&lt;br&gt;Via &lt;a href=\"https://twitter.com/zkurishi?ref_src=twsrc%5Etfw\"&gt;@zkurishi&lt;/a&gt;: &lt;a href=\"https://t.co/Yk7kCYv6oZ\"&gt;https://t.co/Yk7kCYv6oZ&lt;/a&gt; &lt;a href=\"https://t.co/6k50Ytwhj8\"&gt;https://t.co/6k50Ytwhj8&lt;/a&gt; &lt;a href=\"https://t.co/mK7Rofj1x1\"&gt;pic.twitter.com/mK7Rofj1x1&lt;/a&gt;&lt;/p&gt;&amp;mdash; Catsuka (@catsuka) &lt;a href=\"https://twitter.com/catsuka/status/1721882224549929359?ref_src=twsrc%5Etfw\"&gt;November 7, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17qbhp1", "height": 200}, "link_flair_text": "News", "can_mod_post": false, "score": 224, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vGP-a23YGietpwrcLlVji82RYn0-okZegxx3DoTgE8w.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699410327.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/catsuka/status/1721882224549929359", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oOpjfWaDuMUXQX-buFt-3XbOQUiX270cnbvEdXs_eQw.jpg?auto=webp&amp;s=ae5cec0857da42cf7880bd4371bc3838467d4350", "width": 140, "height": 93}, "resolutions": [{"url": "https://external-preview.redd.it/oOpjfWaDuMUXQX-buFt-3XbOQUiX270cnbvEdXs_eQw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c57156f19efd869c3c5e4c3f7157e23cd22aee4a", "width": 108, "height": 71}], "variants": {}, "id": "eRx8QfseGN7GV_8sMc7fnJzT2FZLO6HRhn8rHZuitBg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17qbhp1", "is_robot_indexable": true, "report_reasons": null, "author": "mutsuto", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17qbhp1/burning_libraries_less_than_1_month_until_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/catsuka/status/1721882224549929359", "subreddit_subscribers": 711123, "created_utc": 1699410327.0, "num_crossposts": 0, "media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/catsuka/status/1721882224549929359", "author_name": "Catsuka", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Less than 1 month before Tokyo Lab&amp;#39;s closure.&lt;br&gt;- More than 10 000 masters (including animes) remain unclaimed by rights holders (many of whom are unknown today) &amp;amp; will probably be destroyed...&lt;br&gt;- Paying to continue storing is not possible...&lt;br&gt;Via &lt;a href=\"https://twitter.com/zkurishi?ref_src=twsrc%5Etfw\"&gt;@zkurishi&lt;/a&gt;: &lt;a href=\"https://t.co/Yk7kCYv6oZ\"&gt;https://t.co/Yk7kCYv6oZ&lt;/a&gt; &lt;a href=\"https://t.co/6k50Ytwhj8\"&gt;https://t.co/6k50Ytwhj8&lt;/a&gt; &lt;a href=\"https://t.co/mK7Rofj1x1\"&gt;pic.twitter.com/mK7Rofj1x1&lt;/a&gt;&lt;/p&gt;&amp;mdash; Catsuka (@catsuka) &lt;a href=\"https://twitter.com/catsuka/status/1721882224549929359?ref_src=twsrc%5Etfw\"&gt;November 7, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n", "author_url": "https://twitter.com/catsuka", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Newbie here. I have read different definitions of JBOD on the internet:\n\n1. Treats every disk as separate block device. Data is NOT written across disks. Failure of one disk won't affect other disks\n2. Combine all disks into one block device. Data is NOT written across disks. Whole array fails if one single disk fails\n3. Combine all disks into one block device. Data is written across disks. Whole array fails if one single disk fails (isn't this just RAID 0?)\n4. Some other definitions I am not aware of...\n\nWhat is the correct definition of JBOD? Do NAS manufacturers have different definitions of it?\n\n(Note: I don't use JBOD. Just got curious)\n\nEdit: Thanks for all the answers! ", "author_fullname": "t2_q5egv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What exactly is JBOD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17px2h7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699409444.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699371642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Newbie here. I have read different definitions of JBOD on the internet:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Treats every disk as separate block device. Data is NOT written across disks. Failure of one disk won&amp;#39;t affect other disks&lt;/li&gt;\n&lt;li&gt;Combine all disks into one block device. Data is NOT written across disks. Whole array fails if one single disk fails&lt;/li&gt;\n&lt;li&gt;Combine all disks into one block device. Data is written across disks. Whole array fails if one single disk fails (isn&amp;#39;t this just RAID 0?)&lt;/li&gt;\n&lt;li&gt;Some other definitions I am not aware of...&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What is the correct definition of JBOD? Do NAS manufacturers have different definitions of it?&lt;/p&gt;\n\n&lt;p&gt;(Note: I don&amp;#39;t use JBOD. Just got curious)&lt;/p&gt;\n\n&lt;p&gt;Edit: Thanks for all the answers! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17px2h7", "is_robot_indexable": true, "report_reasons": null, "author": "regunakyle", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17px2h7/what_exactly_is_jbod/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17px2h7/what_exactly_is_jbod/", "subreddit_subscribers": 711123, "created_utc": 1699371642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm working on shutting down a business that has significant digital assets that they'd like to store for several years post-shutdown. Does anyone know what the most reasonable approach would be for storing \\~10TB somewhere and *prepaying* for 2-3? Egress is unlikely, so high egress fees are fine \u2013\u00a0this is more of a CYA for them.\n\n&amp;#x200B;\n\nNone of the pricing pages out there really seem to cover this.", "author_fullname": "t2_4i5l4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best place to store ~10TB that lets me prepay for several years?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17q6dn6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699395915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on shutting down a business that has significant digital assets that they&amp;#39;d like to store for several years post-shutdown. Does anyone know what the most reasonable approach would be for storing ~10TB somewhere and &lt;em&gt;prepaying&lt;/em&gt; for 2-3? Egress is unlikely, so high egress fees are fine \u2013\u00a0this is more of a CYA for them.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;None of the pricing pages out there really seem to cover this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17q6dn6", "is_robot_indexable": true, "report_reasons": null, "author": "cscpru", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17q6dn6/best_place_to_store_10tb_that_lets_me_prepay_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17q6dn6/best_place_to_store_10tb_that_lets_me_prepay_for/", "subreddit_subscribers": 711123, "created_utc": 1699395915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nSo I\u2019ve been looking to expand storage in my NAS and was looking at some 20TB drives on serverpartdeals that were hanging around $210 for a couple months.  I decided to hold off on purchasing thinking perhaps i couldn\u2019t get them even cheaper on Black Friday.  However, the price has been going up every few days for the past couple weeks and is now at $240.  \n\nAre prices going up in general?  Or are they simply inflating the price now to offer a discount on Black Friday (that\u2019s not really a discount)?\n\nUltimately, I\u2019m wondering if i should pounce on the drives now before they go up even more or if i should be patient and wait to get them on \u201csale\u201d around Black Friday?", "author_fullname": "t2_h9avq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ServerPartDeals price increases in advance of Black Friday \u201cSales\u201d?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17q5cee", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699393302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I\u2019ve been looking to expand storage in my NAS and was looking at some 20TB drives on serverpartdeals that were hanging around $210 for a couple months.  I decided to hold off on purchasing thinking perhaps i couldn\u2019t get them even cheaper on Black Friday.  However, the price has been going up every few days for the past couple weeks and is now at $240.  &lt;/p&gt;\n\n&lt;p&gt;Are prices going up in general?  Or are they simply inflating the price now to offer a discount on Black Friday (that\u2019s not really a discount)?&lt;/p&gt;\n\n&lt;p&gt;Ultimately, I\u2019m wondering if i should pounce on the drives now before they go up even more or if i should be patient and wait to get them on \u201csale\u201d around Black Friday?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17q5cee", "is_robot_indexable": true, "report_reasons": null, "author": "Bighairedaristocrat", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17q5cee/serverpartdeals_price_increases_in_advance_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17q5cee/serverpartdeals_price_increases_in_advance_of/", "subreddit_subscribers": 711123, "created_utc": 1699393302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all,\n\nI'm looking for something that I'm not quite sure it exists.\n\nWe  are looking for a cloud storage solution that is able to encrypt our  files, but also have a feature that in order to retrieve them, at least  two persons or more must agree and enter a valid code/password.\n\nImagine those missile launch key pairs.\n\nMy searches haven't been very conclusive. Does anyone know if such thing exists?\n\nBest regards", "author_fullname": "t2_ujtxn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Encrypted Cloud Data Storage with a twist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pxszp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699373631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for something that I&amp;#39;m not quite sure it exists.&lt;/p&gt;\n\n&lt;p&gt;We  are looking for a cloud storage solution that is able to encrypt our  files, but also have a feature that in order to retrieve them, at least  two persons or more must agree and enter a valid code/password.&lt;/p&gt;\n\n&lt;p&gt;Imagine those missile launch key pairs.&lt;/p&gt;\n\n&lt;p&gt;My searches haven&amp;#39;t been very conclusive. Does anyone know if such thing exists?&lt;/p&gt;\n\n&lt;p&gt;Best regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17pxszp", "is_robot_indexable": true, "report_reasons": null, "author": "c0v3n4n7", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17pxszp/encrypted_cloud_data_storage_with_a_twist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17pxszp/encrypted_cloud_data_storage_with_a_twist/", "subreddit_subscribers": 711123, "created_utc": 1699373631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a server which we'll call bigdata, it's got a lot of hard disks with the majority of the data on it unused and I want to power it down to save power. So I start looking at spinning down hard drives, which seems it will work but then opens up another can of worms, like reducing drive lifespans. Right now the approach I've settled on is to have the server rarely powered on and on a schedule, such as only weekends or several days a month to reduce this mechanical wear.\n\nI want to set up another low-powered always-on server (littledata) with SSDs or minimal HDDs. I could use syncthing and choose certain folders to mirror, and changes would replicate when bigdata is up and that would be the end of the discussion. But it seems a little bit clunky, and I wouldn't be able to see what data is on the powered-down server.\n\nReally at the end of the day all I want is to be able to access specified content when bigdata is off (I can already do this with syncthing) but also be able to have a virtual catalogue of the files on littledata (Even something that mirrors the folder structure with 0b files is OK as long as I don't have to do any housekeeping to keep it in sync. Of course it would be better if it's more intelligent).\n\nAre there solutions that could do any of the following (not necessarily all at once)\n\n- mixture between real files and \"virtual\" files, where virtual ones will sync when bigdata is online (kind of like how cloud programs on the desktop do it\n- some kind of \"checkout system\" so I \"register\" that I need certain files and the software \"fetches\" them the next time they're accessible\n- some smart caching policy that automatically determines what I may need and caches them on littledata without any action needed\n\nThe key is the files are stored on ZFS but shared with samba and accessed with various OS's, so I'm not accessing them directly.", "author_fullname": "t2_h6ftzl0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to cache/sync a server's files onto another one so I can shut it down periodically", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17qgffx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699428423.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a server which we&amp;#39;ll call bigdata, it&amp;#39;s got a lot of hard disks with the majority of the data on it unused and I want to power it down to save power. So I start looking at spinning down hard drives, which seems it will work but then opens up another can of worms, like reducing drive lifespans. Right now the approach I&amp;#39;ve settled on is to have the server rarely powered on and on a schedule, such as only weekends or several days a month to reduce this mechanical wear.&lt;/p&gt;\n\n&lt;p&gt;I want to set up another low-powered always-on server (littledata) with SSDs or minimal HDDs. I could use syncthing and choose certain folders to mirror, and changes would replicate when bigdata is up and that would be the end of the discussion. But it seems a little bit clunky, and I wouldn&amp;#39;t be able to see what data is on the powered-down server.&lt;/p&gt;\n\n&lt;p&gt;Really at the end of the day all I want is to be able to access specified content when bigdata is off (I can already do this with syncthing) but also be able to have a virtual catalogue of the files on littledata (Even something that mirrors the folder structure with 0b files is OK as long as I don&amp;#39;t have to do any housekeeping to keep it in sync. Of course it would be better if it&amp;#39;s more intelligent).&lt;/p&gt;\n\n&lt;p&gt;Are there solutions that could do any of the following (not necessarily all at once)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;mixture between real files and &amp;quot;virtual&amp;quot; files, where virtual ones will sync when bigdata is online (kind of like how cloud programs on the desktop do it&lt;/li&gt;\n&lt;li&gt;some kind of &amp;quot;checkout system&amp;quot; so I &amp;quot;register&amp;quot; that I need certain files and the software &amp;quot;fetches&amp;quot; them the next time they&amp;#39;re accessible&lt;/li&gt;\n&lt;li&gt;some smart caching policy that automatically determines what I may need and caches them on littledata without any action needed&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The key is the files are stored on ZFS but shared with samba and accessed with various OS&amp;#39;s, so I&amp;#39;m not accessing them directly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17qgffx", "is_robot_indexable": true, "report_reasons": null, "author": "Van_Curious", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17qgffx/i_want_to_cachesync_a_servers_files_onto_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17qgffx/i_want_to_cachesync_a_servers_files_onto_another/", "subreddit_subscribers": 711123, "created_utc": 1699428423.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Now that 4TB NVMe SSDs are getting into a somewhat more affordable price range I'm thinking of upgrading my PC, I have an old 512GB and a 1TB one but constantly struggling with free space on them.    \n  \nOn my own accord I would go Samsung 990 pro or Sabrent but honestly don't know much about NVMes and especially not much about reliabilty. My samsungs are 100% fine and had no issues ever, but I'm thinking maybe larger 4TB ones are different.  \n  \nWhich models would you go for if reliability was the most important aspect (still talking consumer-grade products for fairly lightweight work, definitely not terabytes of read-write per day).   \nMany thanks!", "author_fullname": "t2_l3upv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "4TB NVMe recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pvu4k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699368262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Now that 4TB NVMe SSDs are getting into a somewhat more affordable price range I&amp;#39;m thinking of upgrading my PC, I have an old 512GB and a 1TB one but constantly struggling with free space on them.    &lt;/p&gt;\n\n&lt;p&gt;On my own accord I would go Samsung 990 pro or Sabrent but honestly don&amp;#39;t know much about NVMes and especially not much about reliabilty. My samsungs are 100% fine and had no issues ever, but I&amp;#39;m thinking maybe larger 4TB ones are different.  &lt;/p&gt;\n\n&lt;p&gt;Which models would you go for if reliability was the most important aspect (still talking consumer-grade products for fairly lightweight work, definitely not terabytes of read-write per day).&lt;br/&gt;\nMany thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17pvu4k", "is_robot_indexable": true, "report_reasons": null, "author": "gt_kenny", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17pvu4k/4tb_nvme_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17pvu4k/4tb_nvme_recommendations/", "subreddit_subscribers": 711123, "created_utc": 1699368262.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "All of them are saved as .jpg, containing document images, family, pets memes and I want to separate out the scanned pages.", "author_fullname": "t2_60r15so6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way I can filter out work/document related Images from a folder of 6000+ images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pv4tq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699366237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;All of them are saved as .jpg, containing document images, family, pets memes and I want to separate out the scanned pages.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17pv4tq", "is_robot_indexable": true, "report_reasons": null, "author": "NXS_GLITCH", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17pv4tq/is_there_a_way_i_can_filter_out_workdocument/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17pv4tq/is_there_a_way_i_can_filter_out_workdocument/", "subreddit_subscribers": 711123, "created_utc": 1699366237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I asked this question in /r/librarians but didn't get a response.  I kind of think this might be the better subreddit to get an answer.  If anyone  has a suggestion for a better subreddit to answer the question then  please chime in.\n\nI have a bunch of  family photos that I'm converting to digital.  I've got a good quality  scanner and I'm scanning into a lossless .PNG format.  But I'm  interested in storing the biographical meta data with the photo as well.   Things like a scan/image of the back of the photo where something was  written, who is in the photo, where was the photo taken, what is the  story behind the photo--e.g. \"This is John Smith boarding the train to  go to boot camp before the war\".  Etc.\n\nI  know that EXIF data tags can capture some of this information, but  there seems to be significant gaps what I want to store.  EXIF data is focus more on the technical  aspects of the photo, like camera settings.  Also EXIF doesn't seem to be well  supported by software applications.\n\nI'm  also aware that apps like Photos on mac can add a significant amount of  meta data to an image, however it isn't portable--It gets stored in a  data base and if you email or export a photo out of the app, the metadata doesn't go along  with it.\n\nWhat are the best  practices around capturing and archiving photos in this way.  Is there  any file format or structure that supports what I'm looking for?", "author_fullname": "t2_gevig", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archival format for historic photos and other media", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17qm9ow", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699452071.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I asked this question in &lt;a href=\"/r/librarians\"&gt;/r/librarians&lt;/a&gt; but didn&amp;#39;t get a response.  I kind of think this might be the better subreddit to get an answer.  If anyone  has a suggestion for a better subreddit to answer the question then  please chime in.&lt;/p&gt;\n\n&lt;p&gt;I have a bunch of  family photos that I&amp;#39;m converting to digital.  I&amp;#39;ve got a good quality  scanner and I&amp;#39;m scanning into a lossless .PNG format.  But I&amp;#39;m  interested in storing the biographical meta data with the photo as well.   Things like a scan/image of the back of the photo where something was  written, who is in the photo, where was the photo taken, what is the  story behind the photo--e.g. &amp;quot;This is John Smith boarding the train to  go to boot camp before the war&amp;quot;.  Etc.&lt;/p&gt;\n\n&lt;p&gt;I  know that EXIF data tags can capture some of this information, but  there seems to be significant gaps what I want to store.  EXIF data is focus more on the technical  aspects of the photo, like camera settings.  Also EXIF doesn&amp;#39;t seem to be well  supported by software applications.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m  also aware that apps like Photos on mac can add a significant amount of  meta data to an image, however it isn&amp;#39;t portable--It gets stored in a  data base and if you email or export a photo out of the app, the metadata doesn&amp;#39;t go along  with it.&lt;/p&gt;\n\n&lt;p&gt;What are the best  practices around capturing and archiving photos in this way.  Is there  any file format or structure that supports what I&amp;#39;m looking for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17qm9ow", "is_robot_indexable": true, "report_reasons": null, "author": "cthulhu944", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17qm9ow/archival_format_for_historic_photos_and_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17qm9ow/archival_format_for_historic_photos_and_other/", "subreddit_subscribers": 711123, "created_utc": 1699452071.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So i wanted to re-format my first drive. already backed up the data on cloud, but still feel nostalgic when i see the file dates on the drive. First I wanted to make/use a script to make a file tree with with og dates+ there on the tree, even named it Project Epic Tree. But.. you know.. sigh!!! So, any such script there? or better options to archive the files with the dates? General archive mods the ctime.  \nor ig i'll have to make the script in the end!  \nThanks for reading, any advice would really be helpful!\n\nEdit: I forgot to mention, i'll be backing up on cloud, so local backup with dates might not be helpful, so, either tree or archive with metadata.Ik it might be being kinda greedy, but u know, as the sub says, it's a digital disease!!", "author_fullname": "t2_505ff6ed", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any way to backup files with the original file dates' metadata intact?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17qlney", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699452847.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699450124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i wanted to re-format my first drive. already backed up the data on cloud, but still feel nostalgic when i see the file dates on the drive. First I wanted to make/use a script to make a file tree with with og dates+ there on the tree, even named it Project Epic Tree. But.. you know.. sigh!!! So, any such script there? or better options to archive the files with the dates? General archive mods the ctime.&lt;br/&gt;\nor ig i&amp;#39;ll have to make the script in the end!&lt;br/&gt;\nThanks for reading, any advice would really be helpful!&lt;/p&gt;\n\n&lt;p&gt;Edit: I forgot to mention, i&amp;#39;ll be backing up on cloud, so local backup with dates might not be helpful, so, either tree or archive with metadata.Ik it might be being kinda greedy, but u know, as the sub says, it&amp;#39;s a digital disease!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17qlney", "is_robot_indexable": true, "report_reasons": null, "author": "NoobSvCy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17qlney/any_way_to_backup_files_with_the_original_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17qlney/any_way_to_backup_files_with_the_original_file/", "subreddit_subscribers": 711123, "created_utc": 1699450124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all, apologies if there is a better thread for this. A friend brought his 1TB external USB disk saying it died. After connecting it, it does eventually show up. However when I view it in Gparted it says \"unknown file system\". Seems all partition info is corrupted. Trying to recover files with Gparted and it's been scanning the past 2 hours. Presume I will get a fail and thinking to try the SystemRescue after. Now obviously, my friend did not back up the long bitlocker code... Am wondering if anything can be recovered without it? Any tips for software to recover corrupted partition table info? (What do you think about easeus partition recovery?) Any help will be greatly appreciated!", "author_fullname": "t2_ahziux", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "External bitlockered disk partition recovery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17qkpvh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699447062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, apologies if there is a better thread for this. A friend brought his 1TB external USB disk saying it died. After connecting it, it does eventually show up. However when I view it in Gparted it says &amp;quot;unknown file system&amp;quot;. Seems all partition info is corrupted. Trying to recover files with Gparted and it&amp;#39;s been scanning the past 2 hours. Presume I will get a fail and thinking to try the SystemRescue after. Now obviously, my friend did not back up the long bitlocker code... Am wondering if anything can be recovered without it? Any tips for software to recover corrupted partition table info? (What do you think about easeus partition recovery?) Any help will be greatly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17qkpvh", "is_robot_indexable": true, "report_reasons": null, "author": "kosh_neranek", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17qkpvh/external_bitlockered_disk_partition_recovery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17qkpvh/external_bitlockered_disk_partition_recovery/", "subreddit_subscribers": 711123, "created_utc": 1699447062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nI'm dealing with a block-level deduplication issue for my Fedora system with Btrfs subvolumes. These volumes contains largely similar VM images\u2014QCOW2 and raw\u2014, often differing only in filesystem alignment. \n\nThis misalignment across images renders typical block hashing dedupe tools ineffective, as identical files may start on different logical blocks, leading to different hashes and missed deduplication.\n\nI'm seeking a deduplication tool or strategy that can either align these files within the images or dedupe while considering a file-level delta offset. The goal is to deduplicate data at the host level efficiently, without the misalignment throwing off missed hashes of the dedupe process.\n\nIs anyone savvy enough to overcome this?\nI\u2019d appreciate any guidance or recommendations.", "author_fullname": "t2_eai7yd0l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deduplication on VM Images with Block-Level Misalignment Issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17qgki7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699443490.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699429051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m dealing with a block-level deduplication issue for my Fedora system with Btrfs subvolumes. These volumes contains largely similar VM images\u2014QCOW2 and raw\u2014, often differing only in filesystem alignment. &lt;/p&gt;\n\n&lt;p&gt;This misalignment across images renders typical block hashing dedupe tools ineffective, as identical files may start on different logical blocks, leading to different hashes and missed deduplication.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m seeking a deduplication tool or strategy that can either align these files within the images or dedupe while considering a file-level delta offset. The goal is to deduplicate data at the host level efficiently, without the misalignment throwing off missed hashes of the dedupe process.&lt;/p&gt;\n\n&lt;p&gt;Is anyone savvy enough to overcome this?\nI\u2019d appreciate any guidance or recommendations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17qgki7", "is_robot_indexable": true, "report_reasons": null, "author": "JuIi0", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17qgki7/deduplication_on_vm_images_with_blocklevel/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17qgki7/deduplication_on_vm_images_with_blocklevel/", "subreddit_subscribers": 711123, "created_utc": 1699429051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://malls.fandom.com/wiki/Special:Statistics](https://malls.fandom.com/wiki/Special:Statistics)\n\nWhen I click the current pages link at the bottom, I get the below webpage:\n\n    This XML file does not appear to have any style information associated with it. The document tree is shown below.\n    &lt;Error&gt;\n    &lt;Code&gt;InvalidObjectState&lt;/Code&gt;\n    &lt;Message&gt;The operation is not valid for the object's storage class&lt;/Message&gt;\n    &lt;StorageClass&gt;DEEP_ARCHIVE&lt;/StorageClass&gt;\n    &lt;RequestId&gt;9BX9QSXPM7P9ER2P&lt;/RequestId&gt;\n    &lt;HostId&gt;IaTMBnPl/JKVn5tsxwRRKDcFjd5WDEZLt8lONd8Xd0/uB6Gjuol27ueMkjM9BG4ixwc2GJA38JQ=&lt;/HostId&gt;\n    &lt;/Error&gt;\n\nThis happens on multiple browsers and I haven't been able to get wget to download it either, so not sure what's up.", "author_fullname": "t2_ay7tp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trouble downloading fandom wiki database dumps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17q44ji", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699390267.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://malls.fandom.com/wiki/Special:Statistics\"&gt;https://malls.fandom.com/wiki/Special:Statistics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;When I click the current pages link at the bottom, I get the below webpage:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;This XML file does not appear to have any style information associated with it. The document tree is shown below.\n&amp;lt;Error&amp;gt;\n&amp;lt;Code&amp;gt;InvalidObjectState&amp;lt;/Code&amp;gt;\n&amp;lt;Message&amp;gt;The operation is not valid for the object&amp;#39;s storage class&amp;lt;/Message&amp;gt;\n&amp;lt;StorageClass&amp;gt;DEEP_ARCHIVE&amp;lt;/StorageClass&amp;gt;\n&amp;lt;RequestId&amp;gt;9BX9QSXPM7P9ER2P&amp;lt;/RequestId&amp;gt;\n&amp;lt;HostId&amp;gt;IaTMBnPl/JKVn5tsxwRRKDcFjd5WDEZLt8lONd8Xd0/uB6Gjuol27ueMkjM9BG4ixwc2GJA38JQ=&amp;lt;/HostId&amp;gt;\n&amp;lt;/Error&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This happens on multiple browsers and I haven&amp;#39;t been able to get wget to download it either, so not sure what&amp;#39;s up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "244TB ZFS and Synology", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17q44ji", "is_robot_indexable": true, "report_reasons": null, "author": "erik530195", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17q44ji/trouble_downloading_fandom_wiki_database_dumps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17q44ji/trouble_downloading_fandom_wiki_database_dumps/", "subreddit_subscribers": 711123, "created_utc": 1699390267.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am considering getting a Seagate Exos x20 to move most of my media into a single drive and get rid of all the 4 and 5 TB portable USB hard drives. I know this means that if the drive fails then all of the 20 TB will be lost, rather than 4 or 5 TB for a smaller drive. But I can live with it, or at least offload the data to another drive if I get some early warnings. Based on this plan, which would be the more reliable drive to begin with. I will have to ship it to the other side of the globe.\n\nPrice on Newegg incl. International Shipping: **$308 (New)**\n\nPrice on ServerPartDeals incl. International Shipping: **$280 (Manufacturer Recertified) + cost of 3.5\" enclosure.**\n\n&amp;#x200B;\n\nWhat do you reckon is the safer option to get a drive that will not have any issues at least in the beginning? I will obviously test the drive thoroughly before moving data into it, but I want to minimize the hassle of having to return DoA or faulty drives.", "author_fullname": "t2_dkjyr9vo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New vs. Recertified drives for international shipping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17q197g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699382733.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am considering getting a Seagate Exos x20 to move most of my media into a single drive and get rid of all the 4 and 5 TB portable USB hard drives. I know this means that if the drive fails then all of the 20 TB will be lost, rather than 4 or 5 TB for a smaller drive. But I can live with it, or at least offload the data to another drive if I get some early warnings. Based on this plan, which would be the more reliable drive to begin with. I will have to ship it to the other side of the globe.&lt;/p&gt;\n\n&lt;p&gt;Price on Newegg incl. International Shipping: &lt;strong&gt;$308 (New)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Price on ServerPartDeals incl. International Shipping: &lt;strong&gt;$280 (Manufacturer Recertified) + cost of 3.5&amp;quot; enclosure.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What do you reckon is the safer option to get a drive that will not have any issues at least in the beginning? I will obviously test the drive thoroughly before moving data into it, but I want to minimize the hassle of having to return DoA or faulty drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17q197g", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Earth_394", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17q197g/new_vs_recertified_drives_for_international/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17q197g/new_vs_recertified_drives_for_international/", "subreddit_subscribers": 711123, "created_utc": 1699382733.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently I have about 60TB of data on my main server which has 8 bays (84tb total). I've been eyeing the new ZimaCube with 6(?) bays, and would need to get drives for it (thinking of going full hog with 20tb drives). I grabbed 2(!!) LTO-7 tape drives from the recycling bin at work, which I was thinking I could sell towards the ZimaCube, but maybe I should just invest in some tapes instead?\n\nWhat would you do?", "author_fullname": "t2_56bs9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LTO-7 vs separate backup server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17qda3y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699415903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I have about 60TB of data on my main server which has 8 bays (84tb total). I&amp;#39;ve been eyeing the new ZimaCube with 6(?) bays, and would need to get drives for it (thinking of going full hog with 20tb drives). I grabbed 2(!!) LTO-7 tape drives from the recycling bin at work, which I was thinking I could sell towards the ZimaCube, but maybe I should just invest in some tapes instead?&lt;/p&gt;\n\n&lt;p&gt;What would you do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "100tb", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17qda3y", "is_robot_indexable": true, "report_reasons": null, "author": "19wolf", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17qda3y/lto7_vs_separate_backup_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17qda3y/lto7_vs_separate_backup_server/", "subreddit_subscribers": 711123, "created_utc": 1699415903.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure if this is the right place to ask this. On an album I really like, one song was released only as an \"iTunes deluxe pre-order bonus\" which is as stupid as it sounds. [Recordings exist](https://www.youtube.com/watch?v=LJIXm9sQoQc), but they're pretty low quality as unauthorized uploads to YouTube as 2009 go. What's really frustrating about this is that a SoundCloud page exists for the song, but displays the message \"This track is not available in the United States.\" All the SoundCloud downloaders I've tried haven't worked, so I'm curious how to proceed to get a higher quality version of the song. Here's the link if anyone can help out: \n\n[https://soundcloud.com/reginaspektor/riot-gear?in=reginaspektor/sets/far-24&amp;utm\\_source=clipboard&amp;utm\\_medium=text&amp;utm\\_campaign=social\\_sharing](https://soundcloud.com/reginaspektor/riot-gear?in=reginaspektor/sets/far-24&amp;utm_source=clipboard&amp;utm_medium=text&amp;utm_campaign=social_sharing)", "author_fullname": "t2_5illsm5b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Request] Help downloading unavailable SoundCloud song", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17q00vq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699379523.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if this is the right place to ask this. On an album I really like, one song was released only as an &amp;quot;iTunes deluxe pre-order bonus&amp;quot; which is as stupid as it sounds. &lt;a href=\"https://www.youtube.com/watch?v=LJIXm9sQoQc\"&gt;Recordings exist&lt;/a&gt;, but they&amp;#39;re pretty low quality as unauthorized uploads to YouTube as 2009 go. What&amp;#39;s really frustrating about this is that a SoundCloud page exists for the song, but displays the message &amp;quot;This track is not available in the United States.&amp;quot; All the SoundCloud downloaders I&amp;#39;ve tried haven&amp;#39;t worked, so I&amp;#39;m curious how to proceed to get a higher quality version of the song. Here&amp;#39;s the link if anyone can help out: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://soundcloud.com/reginaspektor/riot-gear?in=reginaspektor/sets/far-24&amp;amp;utm_source=clipboard&amp;amp;utm_medium=text&amp;amp;utm_campaign=social_sharing\"&gt;https://soundcloud.com/reginaspektor/riot-gear?in=reginaspektor/sets/far-24&amp;amp;utm_source=clipboard&amp;amp;utm_medium=text&amp;amp;utm_campaign=social_sharing&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cI3csrjSclkRYcdVJHswOmwJbGnPSVU5mTaUfhB9Va8.jpg?auto=webp&amp;s=dfc119e209566d82f4a36d642f88274aaf399654", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/cI3csrjSclkRYcdVJHswOmwJbGnPSVU5mTaUfhB9Va8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c0eb25ebb193afd0ba1dfe2e996475cac9c06c32", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/cI3csrjSclkRYcdVJHswOmwJbGnPSVU5mTaUfhB9Va8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1ee3b15ad9406e77d28f21784331f7f6913a5d15", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/cI3csrjSclkRYcdVJHswOmwJbGnPSVU5mTaUfhB9Va8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d763c8e97c10b56ffe56f868dce617c3a365df30", "width": 320, "height": 240}], "variants": {}, "id": "r1gkbdX2wu2_o5gjq8YqMsPHJ7-dEI-rreP9phgVqlc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17q00vq", "is_robot_indexable": true, "report_reasons": null, "author": "cb-fan", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17q00vq/request_help_downloading_unavailable_soundcloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17q00vq/request_help_downloading_unavailable_soundcloud/", "subreddit_subscribers": 711123, "created_utc": 1699379523.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I usually buy western digitals.\n\nI thought I'd take a chance a year or two ago on a seagate ironwolf drive for a media machine, rationalizing that if it failed I could just reload the files. I wanted to see if current seagate models were more reliable. Well, its kinda holding a bunch of files temporarily while I setup a dedicated storage machine.\n\nYesterday and today while accessing a large media file my computer hiccupped, beeped loudly, and the actuator arm made a loud click noise.\n\nBoys, I don't actually know what that means. But years of data hoarding have taught me that when HDDS do anything but hum away quietly and invisibly in the case, that death/data loss is imminent. So uh...yeah.", "author_fullname": "t2_vivjs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Iron wolf: Maybe not the best.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17qfnly", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.39, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699424963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I usually buy western digitals.&lt;/p&gt;\n\n&lt;p&gt;I thought I&amp;#39;d take a chance a year or two ago on a seagate ironwolf drive for a media machine, rationalizing that if it failed I could just reload the files. I wanted to see if current seagate models were more reliable. Well, its kinda holding a bunch of files temporarily while I setup a dedicated storage machine.&lt;/p&gt;\n\n&lt;p&gt;Yesterday and today while accessing a large media file my computer hiccupped, beeped loudly, and the actuator arm made a loud click noise.&lt;/p&gt;\n\n&lt;p&gt;Boys, I don&amp;#39;t actually know what that means. But years of data hoarding have taught me that when HDDS do anything but hum away quietly and invisibly in the case, that death/data loss is imminent. So uh...yeah.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17qfnly", "is_robot_indexable": true, "report_reasons": null, "author": "Captain_Starkiller", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17qfnly/seagate_iron_wolf_maybe_not_the_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17qfnly/seagate_iron_wolf_maybe_not_the_best/", "subreddit_subscribers": 711123, "created_utc": 1699424963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "We all know that advertised space is always more than actual space.\n\nBut I thought all drives were consistently cheating. But interesting some of my 2tb drives show total space of 1863gb and some show 1907.71gb in windows. \n\n1863: 2.5\u201d crucial mx500\n1863: 2280 Samsung 970 evo plus\n\n1907: 2230 western digital sn740\n1907: 2280 intel 660p\n\nThat 44gb difference is big for me. Anyone know the reason some of the drives have 1863 and others have 1907?", "author_fullname": "t2_e32nu7yd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Actual space is less than 2tb", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17q46vv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.3, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699390443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We all know that advertised space is always more than actual space.&lt;/p&gt;\n\n&lt;p&gt;But I thought all drives were consistently cheating. But interesting some of my 2tb drives show total space of 1863gb and some show 1907.71gb in windows. &lt;/p&gt;\n\n&lt;p&gt;1863: 2.5\u201d crucial mx500\n1863: 2280 Samsung 970 evo plus&lt;/p&gt;\n\n&lt;p&gt;1907: 2230 western digital sn740\n1907: 2280 intel 660p&lt;/p&gt;\n\n&lt;p&gt;That 44gb difference is big for me. Anyone know the reason some of the drives have 1863 and others have 1907?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17q46vv", "is_robot_indexable": true, "report_reasons": null, "author": "ChillCaptain", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17q46vv/actual_space_is_less_than_2tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17q46vv/actual_space_is_less_than_2tb/", "subreddit_subscribers": 711123, "created_utc": 1699390443.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}