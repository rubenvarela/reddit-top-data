{"kind": "Listing", "data": {"after": "t3_17pvhbv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a big fan and have been exploring adding it as our data warehouse given we're at a small scale. Does anyone use it at work/production environment? If so, how's your experience been with it?", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using duckDB at work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pfhzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699310785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a big fan and have been exploring adding it as our data warehouse given we&amp;#39;re at a small scale. Does anyone use it at work/production environment? If so, how&amp;#39;s your experience been with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pfhzy", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pfhzy/is_anyone_using_duckdb_at_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pfhzy/is_anyone_using_duckdb_at_work/", "subreddit_subscribers": 138355, "created_utc": 1699310785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm curious to learn about the landscape of data engineering certifications in 2023/24. \n\nIf you don't mind sharing your insights, I have some questions:\n\n1. In your opinion, which DE/ML certifications are highly regarded or considered valuable in the industry right now or will be in near future?\n2. What is your perspective on certifications from major cloud service providers like AWS, GCP, and Azure, in terms of their relevance and impact on a DE career? Which one would you recommend?\n3. Do niche-specific certifications (e.g., CDMP, Airflow, Snowflake, Databricks, etc) hold significant weight?\n4. Are there any other certifications or training programs you think are worth mentioning for those looking to advance in the field?\n\nThanks in advance for sharing your knowledge and experience!", "author_fullname": "t2_xf2t5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are useful data engineering certifications in 2023/24?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17paeyc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699297910.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious to learn about the landscape of data engineering certifications in 2023/24. &lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t mind sharing your insights, I have some questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;In your opinion, which DE/ML certifications are highly regarded or considered valuable in the industry right now or will be in near future?&lt;/li&gt;\n&lt;li&gt;What is your perspective on certifications from major cloud service providers like AWS, GCP, and Azure, in terms of their relevance and impact on a DE career? Which one would you recommend?&lt;/li&gt;\n&lt;li&gt;Do niche-specific certifications (e.g., CDMP, Airflow, Snowflake, Databricks, etc) hold significant weight?&lt;/li&gt;\n&lt;li&gt;Are there any other certifications or training programs you think are worth mentioning for those looking to advance in the field?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance for sharing your knowledge and experience!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17paeyc", "is_robot_indexable": true, "report_reasons": null, "author": "luminoumen", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17paeyc/what_are_useful_data_engineering_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17paeyc/what_are_useful_data_engineering_certifications/", "subreddit_subscribers": 138355, "created_utc": 1699297910.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI recently completed a personal project, and I am eager to receive feedback. Any suggestions for improvement would be greatly appreciated. Additionally, as a recent graduate, I'm thinking whether this project would be a good fit to include on my resume. Your insights on this matter would be very helpful.\n\n&amp;#x200B;\n\nThe architecture is:\n\nhttps://preview.redd.it/h1j2kx65bvyb1.png?width=3582&amp;format=png&amp;auto=webp&amp;s=a7ecb570b0a95dad3f25d8fef196068ab53a6d72\n\nThe dashboard for the project is: [https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD](https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3w4gqbn8bvyb1.png?width=2482&amp;format=png&amp;auto=webp&amp;s=9d3e65c593dcb3c2155af6b46c058911d1117b1a\n\nGithub repo: [https://github.com/Zzdragon66/ucla-reddit-dahsboard-public](https://github.com/Zzdragon66/ucla-reddit-dahsboard-public)", "author_fullname": "t2_5igde9z6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Project of End-End ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3w4gqbn8bvyb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 82, "x": 108, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f6402e5d8012a1e837fa81ef3ff45ab96a8056d"}, {"y": 165, "x": 216, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01f12fa8d7a1a69ca487c7d3b3d13644ef5f8cee"}, {"y": 245, "x": 320, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cfd705831ac7d643e23d89f33f02f1e7666b5e98"}, {"y": 490, "x": 640, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=167d14f9eef1b9949f53ebd87b02e60e58edea5b"}, {"y": 735, "x": 960, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=12e8b58f57495af10ae16f05528a4bfe9b5a7db8"}, {"y": 827, "x": 1080, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=592d1036e4a9435c2cb10c898f06423ac42e4bc7"}], "s": {"y": 1902, "x": 2482, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=2482&amp;format=png&amp;auto=webp&amp;s=9d3e65c593dcb3c2155af6b46c058911d1117b1a"}, "id": "3w4gqbn8bvyb1"}, "h1j2kx65bvyb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 46, "x": 108, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ab70939f11be44c99dd4e130ea771a081f6bf57"}, {"y": 92, "x": 216, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=29b3e6f32fc6d7259929edb6e056b165f008179d"}, {"y": 137, "x": 320, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d0b206325d48c4f9b0f0b07c8b08e50aee5d22b"}, {"y": 274, "x": 640, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3ff76f20d77691e39f0d465c2f92b970dd6f83a"}, {"y": 412, "x": 960, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2407495da6a1b63ae5d484163db0faaad6ecd38c"}, {"y": 464, "x": 1080, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12c6f445b85babe8b7632787426c7675fd7cc1e0"}], "s": {"y": 1539, "x": 3582, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=3582&amp;format=png&amp;auto=webp&amp;s=a7ecb570b0a95dad3f25d8fef196068ab53a6d72"}, "id": "h1j2kx65bvyb1"}}, "name": "t3_17po009", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fn5_tH4Xl1xvVNvAovmyl8tYc2ILptSU-3I3EfOfAZs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1699337786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently completed a personal project, and I am eager to receive feedback. Any suggestions for improvement would be greatly appreciated. Additionally, as a recent graduate, I&amp;#39;m thinking whether this project would be a good fit to include on my resume. Your insights on this matter would be very helpful.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The architecture is:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/h1j2kx65bvyb1.png?width=3582&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7ecb570b0a95dad3f25d8fef196068ab53a6d72\"&gt;https://preview.redd.it/h1j2kx65bvyb1.png?width=3582&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7ecb570b0a95dad3f25d8fef196068ab53a6d72&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The dashboard for the project is: &lt;a href=\"https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD\"&gt;https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3w4gqbn8bvyb1.png?width=2482&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d3e65c593dcb3c2155af6b46c058911d1117b1a\"&gt;https://preview.redd.it/3w4gqbn8bvyb1.png?width=2482&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d3e65c593dcb3c2155af6b46c058911d1117b1a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Github repo: &lt;a href=\"https://github.com/Zzdragon66/ucla-reddit-dahsboard-public\"&gt;https://github.com/Zzdragon66/ucla-reddit-dahsboard-public&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Dt_WDsRInDtBEaAMeDYG27iXGkTTGHX32oVedSsPqyo.jpg?auto=webp&amp;s=76dfa394f4787ae8cb10b052e4dc8c1a5dc1a0f8", "width": 320, "height": 240}, "resolutions": [{"url": "https://external-preview.redd.it/Dt_WDsRInDtBEaAMeDYG27iXGkTTGHX32oVedSsPqyo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12da9bc423fa95bffe58b4b9fd0bd99e715055c2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Dt_WDsRInDtBEaAMeDYG27iXGkTTGHX32oVedSsPqyo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3a871ed67d9cef591a15b3877b42a1c7783a546", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Dt_WDsRInDtBEaAMeDYG27iXGkTTGHX32oVedSsPqyo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=45bc45dbfadc1ed4e05ce2f4e3fb34e5224a29c7", "width": 320, "height": 240}], "variants": {}, "id": "q2RV6VdBViSPw6O7YLf2kDPldgE-_Rn0cHvVhCZZlp0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "17po009", "is_robot_indexable": true, "report_reasons": null, "author": "AffectionateEmu8146", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17po009/personal_project_of_endend_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17po009/personal_project_of_endend_etl/", "subreddit_subscribers": 138355, "created_utc": 1699337786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a data engineer for about 3 years now mainly with snowflake, azure ecosystem (azure sql and data factory), and databricks mostly building pipelines and ELTs. The companies I worked for were big enough to justify and afford these tools.\n\nI have a side business that offers parcel services. We use AWS lightsale to host our web app as well as our posgreSQL database. Some operational reporting is done directly through the app. So far the database is working with no issue. The biggest table we have has around 123k records.\n\nI have some experience with power bi and wanted to build some more strategic reports with more functionality and to also lift some weight from the developers having to develop all the reports. My idea was to create a new star schema within the same database. Power bi would point to this schema and it would refresh daily at a point in time where it's barely being used to avoid performance issues.\n\nI'm debating whether to just use views pointing to the existing tables or to create and load new tables. I've seen that this subreddit talks very well of dbt and I'm thinking of using it in this project to learn the tool and to maintain good practices.\n\nWhat are your thoughts on views vs tables?\n\nWould you recommend sharing the database for both transactional and analytical workloads if the business is small?\n\n At what point do you decide to separate the data warehouse?", "author_fullname": "t2_177meuof", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At what point do you need a data warehouse for a small business?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pahfg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699298103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a data engineer for about 3 years now mainly with snowflake, azure ecosystem (azure sql and data factory), and databricks mostly building pipelines and ELTs. The companies I worked for were big enough to justify and afford these tools.&lt;/p&gt;\n\n&lt;p&gt;I have a side business that offers parcel services. We use AWS lightsale to host our web app as well as our posgreSQL database. Some operational reporting is done directly through the app. So far the database is working with no issue. The biggest table we have has around 123k records.&lt;/p&gt;\n\n&lt;p&gt;I have some experience with power bi and wanted to build some more strategic reports with more functionality and to also lift some weight from the developers having to develop all the reports. My idea was to create a new star schema within the same database. Power bi would point to this schema and it would refresh daily at a point in time where it&amp;#39;s barely being used to avoid performance issues.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m debating whether to just use views pointing to the existing tables or to create and load new tables. I&amp;#39;ve seen that this subreddit talks very well of dbt and I&amp;#39;m thinking of using it in this project to learn the tool and to maintain good practices.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on views vs tables?&lt;/p&gt;\n\n&lt;p&gt;Would you recommend sharing the database for both transactional and analytical workloads if the business is small?&lt;/p&gt;\n\n&lt;p&gt;At what point do you decide to separate the data warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pahfg", "is_robot_indexable": true, "report_reasons": null, "author": "hershy08", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pahfg/at_what_point_do_you_need_a_data_warehouse_for_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pahfg/at_what_point_do_you_need_a_data_warehouse_for_a/", "subreddit_subscribers": 138355, "created_utc": 1699298103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://blog.peerdb.io/building-a-streaming-platform-in-go-for-postgres](https://blog.peerdb.io/building-a-streaming-platform-in-go-for-postgres)  \n[PeerDB's](https://www.peerdb.io/) recent engineering blog on a design change that reduces replication latency/lag while streaming data from Postgres  from 30s to less than 5s.\n\nIf you are a Go u/golang developer you would find this intriguing. Would love to hear your feedback.", "author_fullname": "t2_simedz82", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building a Streaming Platform in Go for Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17papeg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699298679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://blog.peerdb.io/building-a-streaming-platform-in-go-for-postgres\"&gt;https://blog.peerdb.io/building-a-streaming-platform-in-go-for-postgres&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.peerdb.io/\"&gt;PeerDB&amp;#39;s&lt;/a&gt; recent engineering blog on a design change that reduces replication latency/lag while streaming data from Postgres  from 30s to less than 5s.&lt;/p&gt;\n\n&lt;p&gt;If you are a Go &lt;a href=\"/u/golang\"&gt;u/golang&lt;/a&gt; developer you would find this intriguing. Would love to hear your feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?auto=webp&amp;s=51e136315e28eae2332c64f9794907f59192e89a", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=013570b5f4313cd4e20708bd8e6ff743d645a35d", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8195282656de899f495044c52af47a835299b3ea", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=673beea3ef9d37b60709058fcc5b155096a9bb29", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bed1027ebb57891852ebc59bfffcd58f7db8477", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5337009152e44970850c78d69c1817b2ee0e8f2", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/DDjncTXWggiNzdVYG9e0CLj8gWIQqqkxXUTaR5rozTo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b82655640d39f01746c8924e49e16db89a4f2c0f", "width": 1080, "height": 720}], "variants": {}, "id": "jLylNeqzjTc-QQvl96AqP5DuMQdHOnWYRo1GHxGz9Ls"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17papeg", "is_robot_indexable": true, "report_reasons": null, "author": "saipeerdb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17papeg/building_a_streaming_platform_in_go_for_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17papeg/building_a_streaming_platform_in_go_for_postgres/", "subreddit_subscribers": 138355, "created_utc": 1699298679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I gave this talk at PyData NYC last week.  It was fun working with devs from various projects (Dask, Arrow, Polars, Spark) in the week leading up to the event.  Thought I'd share a re-recording of it here\n\n[https://youtu.be/wKH0-zs2g\\_U](https://youtu.be/wKH0-zs2g_U)\n\nThis is the result of a couple weeks of work comparing large data frameworks on benchmarks ranging in size 10GB to 10TB.  No project wins.  It's really interesting analyzing results though.\n\nDuckDB and Dask are the only projects that reliably finish things (although possibly Dask's success here has to do with me knowing Dask better than the others).  DuckDB is way faster at small scale (along with Polars).  Dask and Spark are generally more robust and performant at large scale, mostly because they're able to parallelize S3 access.  Really-good-S3 access seems to be the way you win at real-world cloud performance.\n\nLooking more deeply at Dask results, we're wildly inefficient.  There's at least a 2x-5x performance increase to be had here.    Given that Dask does about as well as any other project on cloud this really means that \\*no one\\* has optimized cloud well yet.\n\nThis talk also goes into how we attempted to address bias (super hard to do in benchmarks).  We had active collaborations with Polars and Spark people (made Polars quite a bit faster during this process actually).  See [https://matthewrocklin.com/biased-benchmarks.html](https://matthewrocklin.com/biased-benchmarks.html) for more thoughts.\n\n&amp;#x200B;\n\nThis also shows the improvement Dask made in the last six months.    Dask used to suck at benchmarks.  Now it doesn't win, but reliably places among the top.  This is due to ... \n\n1. Arrow strings\n2. New shuffling algorithms\n3. Query optimization  \n\nThere's a lot of work for projects like Dask and Polars to fix themselves up in this space.  They're both moving pretty fast right now.  I'm curious to see how they progress in the next few months.\n\nFor future work I'd like to expand this out a bit beyond TPC-H.  TPC-H is great because they're fairly serious queries (lots of tables, lots of joins) and not micro-benchmarks.  We could use broader coverage though.  Any ideas?", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark, Dask, DuckDB, Polars: TPC-H Benchmarks at Scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17puedp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699364059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I gave this talk at PyData NYC last week.  It was fun working with devs from various projects (Dask, Arrow, Polars, Spark) in the week leading up to the event.  Thought I&amp;#39;d share a re-recording of it here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/wKH0-zs2g_U\"&gt;https://youtu.be/wKH0-zs2g_U&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the result of a couple weeks of work comparing large data frameworks on benchmarks ranging in size 10GB to 10TB.  No project wins.  It&amp;#39;s really interesting analyzing results though.&lt;/p&gt;\n\n&lt;p&gt;DuckDB and Dask are the only projects that reliably finish things (although possibly Dask&amp;#39;s success here has to do with me knowing Dask better than the others).  DuckDB is way faster at small scale (along with Polars).  Dask and Spark are generally more robust and performant at large scale, mostly because they&amp;#39;re able to parallelize S3 access.  Really-good-S3 access seems to be the way you win at real-world cloud performance.&lt;/p&gt;\n\n&lt;p&gt;Looking more deeply at Dask results, we&amp;#39;re wildly inefficient.  There&amp;#39;s at least a 2x-5x performance increase to be had here.    Given that Dask does about as well as any other project on cloud this really means that *no one* has optimized cloud well yet.&lt;/p&gt;\n\n&lt;p&gt;This talk also goes into how we attempted to address bias (super hard to do in benchmarks).  We had active collaborations with Polars and Spark people (made Polars quite a bit faster during this process actually).  See &lt;a href=\"https://matthewrocklin.com/biased-benchmarks.html\"&gt;https://matthewrocklin.com/biased-benchmarks.html&lt;/a&gt; for more thoughts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This also shows the improvement Dask made in the last six months.    Dask used to suck at benchmarks.  Now it doesn&amp;#39;t win, but reliably places among the top.  This is due to ... &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Arrow strings&lt;/li&gt;\n&lt;li&gt;New shuffling algorithms&lt;/li&gt;\n&lt;li&gt;Query optimization&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of work for projects like Dask and Polars to fix themselves up in this space.  They&amp;#39;re both moving pretty fast right now.  I&amp;#39;m curious to see how they progress in the next few months.&lt;/p&gt;\n\n&lt;p&gt;For future work I&amp;#39;d like to expand this out a bit beyond TPC-H.  TPC-H is great because they&amp;#39;re fairly serious queries (lots of tables, lots of joins) and not micro-benchmarks.  We could use broader coverage though.  Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GKIJFAXC5MxDvqoiFAe_JuBjYW_-SN5PG5BvuQzJVFc.jpg?auto=webp&amp;s=da7d0b2e1dddb9a541c5e00e32b0c0febbc324d4", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GKIJFAXC5MxDvqoiFAe_JuBjYW_-SN5PG5BvuQzJVFc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=599f0b55752f80a746571a6d0466f0cdf6c55888", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GKIJFAXC5MxDvqoiFAe_JuBjYW_-SN5PG5BvuQzJVFc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c171018f1df6b821ac69934e98ca441dddb8e196", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GKIJFAXC5MxDvqoiFAe_JuBjYW_-SN5PG5BvuQzJVFc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f10e3936368e5aa15f9ccbf80970e40ff6a6857", "width": 320, "height": 240}], "variants": {}, "id": "NXJ6LdhKF_lKsbQSg6ourXQ88ir9X8cDbT37Oe33T6o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17puedp", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17puedp/spark_dask_duckdb_polars_tpch_benchmarks_at_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17puedp/spark_dask_duckdb_polars_tpch_benchmarks_at_scale/", "subreddit_subscribers": 138355, "created_utc": 1699364059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working in the data engineering/science space for 3 years at a large government organisation where I\u2019m working in Azure and Databricks on the daily. I also have a computer science degree and a couple Microsoft certs but otherwise I am only 2 years out of uni.\n\nI recently started looking for my next role and have several offers including a contracting role that is paying a substantial daily rate. \n\nBut I can\u2019t help but think I am not experienced enough for this but they seem very keen on me. \n\nIs my imposter syndrome on overdrive or could it be too early in my career to go for a role like this?", "author_fullname": "t2_qc6h84ym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it ever too early to start contracting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ppziq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699346834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working in the data engineering/science space for 3 years at a large government organisation where I\u2019m working in Azure and Databricks on the daily. I also have a computer science degree and a couple Microsoft certs but otherwise I am only 2 years out of uni.&lt;/p&gt;\n\n&lt;p&gt;I recently started looking for my next role and have several offers including a contracting role that is paying a substantial daily rate. &lt;/p&gt;\n\n&lt;p&gt;But I can\u2019t help but think I am not experienced enough for this but they seem very keen on me. &lt;/p&gt;\n\n&lt;p&gt;Is my imposter syndrome on overdrive or could it be too early in my career to go for a role like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ppziq", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated-Western1788", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ppziq/is_it_ever_too_early_to_start_contracting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ppziq/is_it_ever_too_early_to_start_contracting/", "subreddit_subscribers": 138355, "created_utc": 1699346834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which would you say is the most marketable (and you could only choose one track)?\n\nI\u2019m trying to decide where to invest my time. I am familiar with Python, SQL, etc. But should I focus on learning Kubenetes, AWS, Azure, etc?\n\nThanks!", "author_fullname": "t2_cz3wp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Code vs. No Code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17peewm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699308051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which would you say is the most marketable (and you could only choose one track)?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to decide where to invest my time. I am familiar with Python, SQL, etc. But should I focus on learning Kubenetes, AWS, Azure, etc?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17peewm", "is_robot_indexable": true, "report_reasons": null, "author": "anewguy03", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17peewm/code_vs_no_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17peewm/code_vs_no_code/", "subreddit_subscribers": 138355, "created_utc": 1699308051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been brought onto a team to help them sort out some challenges they are having orchestrating their data pipeline. They are a group of non-technical BI developers and data analysts and so I've been told multiple times that if at all possible I should aim to have a solution be as close to no-code / low-code as possible. I was also told that they are/want to leverage step functions for this effort.\n\nTheir data pipelines involve a series of steps in sequence (Glue Jobs). These are bundled into step functions. The main challenges are:\n\n* There are other step functions that require that multiple other step functions are completed before executing (e.g. Step Function C needs Step Function A and B to be complete)\n* The input step functions are not idempotent and are the inputs/dependencies to multiple other step functions (a 1 to many relationship)\n\nCurrently I'm considering the following:\n\n* Implement a master orchestration step function that encapsulates all the step functions. In order to prevent issues with rerunning the same dependency multiple times I'm planning to setup a little sentinel in state machine that will just check and verify that the state function dependencies had succeeded in the last N hours (where N is the SLA) and wait if not.\n* Setup a combination of EventBridge + SQS + Lambda to basically setup a small service that records when step functions are completed and then use to this as a hub/trigger for step functions that have those dependencies.\n\nI'm currently leaning towards the first option be it is low code, and easier to work within a single DAG (e.g. don't have to go hunting through events to debug the data pipeline).\n\nAm I missing anything obvious? Has anyone done something like this (e.g. Step Function -&gt; Step Function orchestration).", "author_fullname": "t2_6d49ikq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Step Functions as an Orchestration Tool and how to handle multiple dependencies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pih9x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699319121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been brought onto a team to help them sort out some challenges they are having orchestrating their data pipeline. They are a group of non-technical BI developers and data analysts and so I&amp;#39;ve been told multiple times that if at all possible I should aim to have a solution be as close to no-code / low-code as possible. I was also told that they are/want to leverage step functions for this effort.&lt;/p&gt;\n\n&lt;p&gt;Their data pipelines involve a series of steps in sequence (Glue Jobs). These are bundled into step functions. The main challenges are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;There are other step functions that require that multiple other step functions are completed before executing (e.g. Step Function C needs Step Function A and B to be complete)&lt;/li&gt;\n&lt;li&gt;The input step functions are not idempotent and are the inputs/dependencies to multiple other step functions (a 1 to many relationship)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Currently I&amp;#39;m considering the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implement a master orchestration step function that encapsulates all the step functions. In order to prevent issues with rerunning the same dependency multiple times I&amp;#39;m planning to setup a little sentinel in state machine that will just check and verify that the state function dependencies had succeeded in the last N hours (where N is the SLA) and wait if not.&lt;/li&gt;\n&lt;li&gt;Setup a combination of EventBridge + SQS + Lambda to basically setup a small service that records when step functions are completed and then use to this as a hub/trigger for step functions that have those dependencies.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m currently leaning towards the first option be it is low code, and easier to work within a single DAG (e.g. don&amp;#39;t have to go hunting through events to debug the data pipeline).&lt;/p&gt;\n\n&lt;p&gt;Am I missing anything obvious? Has anyone done something like this (e.g. Step Function -&amp;gt; Step Function orchestration).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pih9x", "is_robot_indexable": true, "report_reasons": null, "author": "poppinstacks", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pih9x/aws_step_functions_as_an_orchestration_tool_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pih9x/aws_step_functions_as_an_orchestration_tool_and/", "subreddit_subscribers": 138355, "created_utc": 1699319121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone here have a system for versioning database environments? I know there are some paid services out there for doing this which I am open to hearing about, but would need very strong conviction to actually propose them to the team. I have also seen some systems where they essentially tear down and remake the database functions/stored procs from scratch with every deployment-- which seems like a valid approach, if not a little overkill.\n\nEssentially the situation is this, we have several redshift severless workgroups corresponding to different environments local, dev, test, prod. These databases are connected to their upstream versioned corresponding ETL processes. Often, I want to implement a stored proc or user-defined function or something in these databases, but the issue is deploying and keeping everything in sync. The current \"deployment\" method is just running the create statement in those in all different environments. This a little tedious, doesn't enforce consistency across the environments, and just feels bad. \n\nIdeally, what I would like is a form of ci/cd + version control where I can make a change to the local database (e.g. define/change a stored proc) and then press a button and have that change replicated through the higher envs with some sort of logging in place. There is no current need to version data or data models.", "author_fullname": "t2_kdj5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Versioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pe4up", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699307345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone here have a system for versioning database environments? I know there are some paid services out there for doing this which I am open to hearing about, but would need very strong conviction to actually propose them to the team. I have also seen some systems where they essentially tear down and remake the database functions/stored procs from scratch with every deployment-- which seems like a valid approach, if not a little overkill.&lt;/p&gt;\n\n&lt;p&gt;Essentially the situation is this, we have several redshift severless workgroups corresponding to different environments local, dev, test, prod. These databases are connected to their upstream versioned corresponding ETL processes. Often, I want to implement a stored proc or user-defined function or something in these databases, but the issue is deploying and keeping everything in sync. The current &amp;quot;deployment&amp;quot; method is just running the create statement in those in all different environments. This a little tedious, doesn&amp;#39;t enforce consistency across the environments, and just feels bad. &lt;/p&gt;\n\n&lt;p&gt;Ideally, what I would like is a form of ci/cd + version control where I can make a change to the local database (e.g. define/change a stored proc) and then press a button and have that change replicated through the higher envs with some sort of logging in place. There is no current need to version data or data models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pe4up", "is_robot_indexable": true, "report_reasons": null, "author": "Touvejs", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pe4up/database_versioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pe4up/database_versioning/", "subreddit_subscribers": 138355, "created_utc": 1699307345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 3 years of experience in data engineering and soon going to start a job as a solutions architect.\n\nI know this is less experience and I see most of the jobs in solutions architecture require 7-8+ years of work experience. With that considered, what are some great books that I can read to increase my knowledge about solution architecture concepts?  \nAlso please feel free to post some things that I need to keep in mind to succeed.\n\n&amp;#x200B;\n\nThank you in advance", "author_fullname": "t2_8gpfnv45", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for solutions architect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17p90ns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699294316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3 years of experience in data engineering and soon going to start a job as a solutions architect.&lt;/p&gt;\n\n&lt;p&gt;I know this is less experience and I see most of the jobs in solutions architecture require 7-8+ years of work experience. With that considered, what are some great books that I can read to increase my knowledge about solution architecture concepts?&lt;br/&gt;\nAlso please feel free to post some things that I need to keep in mind to succeed.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17p90ns", "is_robot_indexable": true, "report_reasons": null, "author": "ameya_b", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17p90ns/resources_for_solutions_architect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17p90ns/resources_for_solutions_architect/", "subreddit_subscribers": 138355, "created_utc": 1699294316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_c75e0hjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RevenueCat's data-caching techniques for 1.2 billion daily API requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17pb8v6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/zI2UAjcgjgXOnusnkYi3B4iriAw8k5_pb89HFqhubcs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699300066.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "revenuecat.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.revenuecat.com/blog/engineering/data-caching-revenuecat/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?auto=webp&amp;s=43eab23811acd15223d93996fd35633e2fbede08", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd59eb2ec39c278d82b7ed31c8fbbb8056ae1ed8", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08b662c261dc255ff76779cca5853eb5bb50b9fd", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=71261122fba8653aa61fcba55218e728e7a57b0b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=51f542c45d65577603a894b246f2a71c5246349f", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cd661d2bac40c183fbc2fa97fbcf4b67c4a0356d", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fTpx0SpHtgvUWceCFNqdjsCzD4U9nnFSSAPJwUbl7xc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6c9eff7b95ec4134311407d4d018811e2e4d039", "width": 1080, "height": 567}], "variants": {}, "id": "YgEOOqqwm_nMXategwITapc1-ybJGxYTttj9Te9eN3k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17pb8v6", "is_robot_indexable": true, "report_reasons": null, "author": "PoolOpening6090", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pb8v6/revenuecats_datacaching_techniques_for_12_billion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.revenuecat.com/blog/engineering/data-caching-revenuecat/", "subreddit_subscribers": 138355, "created_utc": 1699300066.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I gave this talk at PyData NYC last week.  It was fun working with devs from various projects (Dask, Arrow, Polars, Spark) in the week leading up to the event.  Thought I'd share a re-recording of it here\n\n[https://youtu.be/wKH0-zs2g\\_U](https://youtu.be/wKH0-zs2g_U)\n\nThis is the result of a couple weeks of work comparing large data frameworks on benchmarks ranging in size 10GB to 10TB.  No project wins.  It's really interesting analyzing results though.\n\nDuckDB and Dask are the only projects that reliably finish things (although possibly Dask's success here has to do with me knowing Dask better than the others).  DuckDB is way faster at small scale (along with Polars).  Dask and Spark are generally more robust and performant at large scale, mostly because they're able to parallelize S3 access.  Really-good-S3 access seems to be the way you win at real-world cloud performance.\n\nLooking more deeply at Dask results, we're wildly inefficient.  There's at least a 2x-5x performance increase to be had here.    Given that Dask does about as well as any other project on cloud this really means that \\*no one\\* has optimized cloud well yet.\n\nThis talk also goes into how we attempted to address bias (super hard to do in benchmarks).  We had active collaborations with Polars and Spark people (made Polars quite a bit faster during this process actually).  See [https://matthewrocklin.com/biased-benchmarks.html](https://matthewrocklin.com/biased-benchmarks.html) for more thoughts.\n\n&amp;#x200B;\n\nThis also shows the improvement Dask made in the last six months.    Dask used to suck at benchmarks.  Now it doesn't win, but reliably places among the top.  This is due to ... \n\n1. Arrow strings\n2. New shuffling algorithms\n3. Query optimization  \n\nThere's a lot of work for projects like Dask and Polars to fix themselves up in this space.  They're both moving pretty fast right now.  I'm curious to see how they progress in the next few months.\n\nFor future work I'd like to expand this out a bit beyond TPC-H.  TPC-H is great because they're fairly serious queries (lots of tables, lots of joins) and not micro-benchmarks.  We could use broader coverage though.  Any ideas?", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark, Dask, DuckDB, Polars: TPC-H Benchmarks at Scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pueb4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699364053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I gave this talk at PyData NYC last week.  It was fun working with devs from various projects (Dask, Arrow, Polars, Spark) in the week leading up to the event.  Thought I&amp;#39;d share a re-recording of it here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/wKH0-zs2g_U\"&gt;https://youtu.be/wKH0-zs2g_U&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the result of a couple weeks of work comparing large data frameworks on benchmarks ranging in size 10GB to 10TB.  No project wins.  It&amp;#39;s really interesting analyzing results though.&lt;/p&gt;\n\n&lt;p&gt;DuckDB and Dask are the only projects that reliably finish things (although possibly Dask&amp;#39;s success here has to do with me knowing Dask better than the others).  DuckDB is way faster at small scale (along with Polars).  Dask and Spark are generally more robust and performant at large scale, mostly because they&amp;#39;re able to parallelize S3 access.  Really-good-S3 access seems to be the way you win at real-world cloud performance.&lt;/p&gt;\n\n&lt;p&gt;Looking more deeply at Dask results, we&amp;#39;re wildly inefficient.  There&amp;#39;s at least a 2x-5x performance increase to be had here.    Given that Dask does about as well as any other project on cloud this really means that *no one* has optimized cloud well yet.&lt;/p&gt;\n\n&lt;p&gt;This talk also goes into how we attempted to address bias (super hard to do in benchmarks).  We had active collaborations with Polars and Spark people (made Polars quite a bit faster during this process actually).  See &lt;a href=\"https://matthewrocklin.com/biased-benchmarks.html\"&gt;https://matthewrocklin.com/biased-benchmarks.html&lt;/a&gt; for more thoughts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This also shows the improvement Dask made in the last six months.    Dask used to suck at benchmarks.  Now it doesn&amp;#39;t win, but reliably places among the top.  This is due to ... &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Arrow strings&lt;/li&gt;\n&lt;li&gt;New shuffling algorithms&lt;/li&gt;\n&lt;li&gt;Query optimization&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of work for projects like Dask and Polars to fix themselves up in this space.  They&amp;#39;re both moving pretty fast right now.  I&amp;#39;m curious to see how they progress in the next few months.&lt;/p&gt;\n\n&lt;p&gt;For future work I&amp;#39;d like to expand this out a bit beyond TPC-H.  TPC-H is great because they&amp;#39;re fairly serious queries (lots of tables, lots of joins) and not micro-benchmarks.  We could use broader coverage though.  Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17pueb4", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pueb4/spark_dask_duckdb_polars_tpch_benchmarks_at_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pueb4/spark_dask_duckdb_polars_tpch_benchmarks_at_scale/", "subreddit_subscribers": 138355, "created_utc": 1699364053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://docs.rivery.io/docs/architecture \nHallo data engineers, could some one look in tjo this and try to explain it with the actual components. There is only a few things written on the official doc of Rivery.io ( link above). I know the Data comes from the left with TLS 1.2 and gets stored in the target. Could some one explain what happens precisely in the whole process? Im a newbie to data engineering. Thank you.", "author_fullname": "t2_h1gtwoh1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Explain this Riverty ELT Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 101, "top_awarded_type": null, "hide_score": false, "name": "t3_17pkmkt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mPHtGsDnvY4mPfz1OJiRffsZgwUuihupFDkTXyYEOlg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699325632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://docs.rivery.io/docs/architecture\"&gt;https://docs.rivery.io/docs/architecture&lt;/a&gt; \nHallo data engineers, could some one look in tjo this and try to explain it with the actual components. There is only a few things written on the official doc of Rivery.io ( link above). I know the Data comes from the left with TLS 1.2 and gets stored in the target. Could some one explain what happens precisely in the whole process? Im a newbie to data engineering. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/gmv0aqekbuyb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/gmv0aqekbuyb1.png?auto=webp&amp;s=c403b8e5c011abb1d62b9209bfbf48750aca0f1a", "width": 887, "height": 646}, "resolutions": [{"url": "https://preview.redd.it/gmv0aqekbuyb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ce3da093fcd7c6fba39263ab197a049f5790b79", "width": 108, "height": 78}, {"url": "https://preview.redd.it/gmv0aqekbuyb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ea4da1c50a57cc4b1647c3d16fb1cc4d8568e2f", "width": 216, "height": 157}, {"url": "https://preview.redd.it/gmv0aqekbuyb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d31040c06f79ba9ceca9ada8dee6c190c670782", "width": 320, "height": 233}, {"url": "https://preview.redd.it/gmv0aqekbuyb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a1947a9a7725e4023124c11c76dde1d4d3a0ac13", "width": 640, "height": 466}], "variants": {}, "id": "OTo3WmRT8q3Wd7AtJe8OsT0dD2r9ih9644-jhtvRt8Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pkmkt", "is_robot_indexable": true, "report_reasons": null, "author": "MammothEntry901", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pkmkt/explain_this_riverty_elt_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/gmv0aqekbuyb1.png", "subreddit_subscribers": 138355, "created_utc": 1699325632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Nowadays, there are a lot of DA achievements in universities, which are very common, is there a bit of oversupply?", "author_fullname": "t2_l48rbkuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are DA jobs becoming more and more popular?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17piy88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699320546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Nowadays, there are a lot of DA achievements in universities, which are very common, is there a bit of oversupply?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17piy88", "is_robot_indexable": true, "report_reasons": null, "author": "digmouse_DS", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17piy88/why_are_da_jobs_becoming_more_and_more_popular/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17piy88/why_are_da_jobs_becoming_more_and_more_popular/", "subreddit_subscribers": 138355, "created_utc": 1699320546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Howdy y'all \ud83e\udd20\ud83d\udc4b  \n\n\nI'm an aspiring data engineer and have been self-teaching a number of skills such as SQL, python, and dbt. One of my main goals now is to build a custom ETL pipeline using python.   \n\n\nWhat are your recommendations for an orchestration tool? Looking for a package that can be used directly from python - trying to avoid a pre-package tool like Fivetran as my goal is to develop useful core DE skills.  \n\n\nI see Airflow mentioned frequently. I know there is a web-based GUI, but can it also be used entirely via python?", "author_fullname": "t2_y2dhb7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for Recommendation for Orchestration Tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pbgyn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699300662.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Howdy y&amp;#39;all \ud83e\udd20\ud83d\udc4b  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m an aspiring data engineer and have been self-teaching a number of skills such as SQL, python, and dbt. One of my main goals now is to build a custom ETL pipeline using python.   &lt;/p&gt;\n\n&lt;p&gt;What are your recommendations for an orchestration tool? Looking for a package that can be used directly from python - trying to avoid a pre-package tool like Fivetran as my goal is to develop useful core DE skills.  &lt;/p&gt;\n\n&lt;p&gt;I see Airflow mentioned frequently. I know there is a web-based GUI, but can it also be used entirely via python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pbgyn", "is_robot_indexable": true, "report_reasons": null, "author": "creamycolslaw", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pbgyn/looking_for_recommendation_for_orchestration_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pbgyn/looking_for_recommendation_for_orchestration_tool/", "subreddit_subscribers": 138355, "created_utc": 1699300662.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\n**TL;DR**\n\n*Looking for an ETL tool that bridges the gap between no-code prototyping and established engineering practices. Want it to auto-generate code from visual steps and run in a serverless context. Seeking input and opinions from the community* \n\nI've searched all over the internet but couldn't find a tool of the following kind (dagster + dbt is close though).\n\n**Problem**:\n\nI work with various clients that start in a no-code environment (Alteryx, Informatica, Knime, etc.), but then they want to move to something else, either because the data has grown in size or they want a more stable and customizable workflow. This could involve building a Python library with testable components hosted on their side or at least sql scripts.\n\nI was wondering if there is an ETL tool that could bridge the gap between quick prototyping and clicking around, and established engineering practices. For example, an analyst starts a project as a no-code one, but behind every click on a step (filter, group by, etc.), the platform generates actual .py files or .sql files and prepares/structures the project so that when a more technical person takes over, they can switch to seeing the generated files, folders, connections instead of the visual aspect of the project. The technical team can then work alongside the analyst and adding new files/folders will automatically render the steps, tables, and so on.\n\nIdeally, this service would be sold as a managed service on the customer's cloud (similar to Databricks) rather than on a separate platform. Each table would be one file executed in a serverless context with the following flow: send the code to the cloud container service, execute it, save the table on the blob, and proceed to the next step.\n\n**Issues with current tools**:\n\n* Dagster + dbt seem to be close to what I envision because of the simplicity of syntax and autogeneration of DAGs. The only issues are that it lacks the visual no-code part, and to set it up with serverless execution, you would need some configuring and self-host it on a K8s cluster or create your own step launcher.\n* Databricks covers the ease of getting started: add your service, create your workflow, choose your cluster, and run it. I understand that it's intended for big data, but I'm adding it as a perspective on the end-user experience. I would like, in addition to VM provisioning, to also be able to spin up bare containers without the Spark overhead.\n\nDoes this concept resonate with anyone and can recommend something on the market? Also, what are your personal thoughts on this idea? Am I overthinking it or do you think it may be useful? ", "author_fullname": "t2_4l3aonbn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Serverless ETL tool as managed service on Azure/AWS/GCP - combination of no code solution, Dagster and Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17p9nba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699295942.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Looking for an ETL tool that bridges the gap between no-code prototyping and established engineering practices. Want it to auto-generate code from visual steps and run in a serverless context. Seeking input and opinions from the community&lt;/em&gt; &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve searched all over the internet but couldn&amp;#39;t find a tool of the following kind (dagster + dbt is close though).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;I work with various clients that start in a no-code environment (Alteryx, Informatica, Knime, etc.), but then they want to move to something else, either because the data has grown in size or they want a more stable and customizable workflow. This could involve building a Python library with testable components hosted on their side or at least sql scripts.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there is an ETL tool that could bridge the gap between quick prototyping and clicking around, and established engineering practices. For example, an analyst starts a project as a no-code one, but behind every click on a step (filter, group by, etc.), the platform generates actual .py files or .sql files and prepares/structures the project so that when a more technical person takes over, they can switch to seeing the generated files, folders, connections instead of the visual aspect of the project. The technical team can then work alongside the analyst and adding new files/folders will automatically render the steps, tables, and so on.&lt;/p&gt;\n\n&lt;p&gt;Ideally, this service would be sold as a managed service on the customer&amp;#39;s cloud (similar to Databricks) rather than on a separate platform. Each table would be one file executed in a serverless context with the following flow: send the code to the cloud container service, execute it, save the table on the blob, and proceed to the next step.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Issues with current tools&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Dagster + dbt seem to be close to what I envision because of the simplicity of syntax and autogeneration of DAGs. The only issues are that it lacks the visual no-code part, and to set it up with serverless execution, you would need some configuring and self-host it on a K8s cluster or create your own step launcher.&lt;/li&gt;\n&lt;li&gt;Databricks covers the ease of getting started: add your service, create your workflow, choose your cluster, and run it. I understand that it&amp;#39;s intended for big data, but I&amp;#39;m adding it as a perspective on the end-user experience. I would like, in addition to VM provisioning, to also be able to spin up bare containers without the Spark overhead.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Does this concept resonate with anyone and can recommend something on the market? Also, what are your personal thoughts on this idea? Am I overthinking it or do you think it may be useful? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17p9nba", "is_robot_indexable": true, "report_reasons": null, "author": "_randomymous_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17p9nba/serverless_etl_tool_as_managed_service_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17p9nba/serverless_etl_tool_as_managed_service_on/", "subreddit_subscribers": 138355, "created_utc": 1699295942.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm fairly new in data engineering but I haven't had the chance to build data warehouse or data infrastructure from scratch. My current company is great and I'm doing and learning variety of things - setting up data pipelines and working on different integrations. We're using Snowflake, dbt, AWS and many more. I mostly handle backend/ infra-related and I barely build any models even in dbt aside from trying to introduce some good practices or maximizing its features; analysts create models more frequently in our company because of dashboarding and visualization. Our data infrastructure setup is in a good state, except there's some areas where we lack monitoring when some parts would fail or some data issues would arise which I've been trying to build recently.\n\nOverall, my work setup is good, but I wonder how will I get to the point that I will be confident in designing and building data infrastructure from scratch? I've studied a bit on fundamentals of data modelling and data warehousing previously, but I can't find a way to apply it in my current job - or I may be lost on how that can be done in action and how I would also be able to gain more experience and knowledge for building data infrastructure from scratch.\n\nI would really appreciate advice or stories that had the same experience. Thanks!", "author_fullname": "t2_uep0sc0m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to be qualified to design and build data infrastructure from scratch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17pvf3t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699367071.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m fairly new in data engineering but I haven&amp;#39;t had the chance to build data warehouse or data infrastructure from scratch. My current company is great and I&amp;#39;m doing and learning variety of things - setting up data pipelines and working on different integrations. We&amp;#39;re using Snowflake, dbt, AWS and many more. I mostly handle backend/ infra-related and I barely build any models even in dbt aside from trying to introduce some good practices or maximizing its features; analysts create models more frequently in our company because of dashboarding and visualization. Our data infrastructure setup is in a good state, except there&amp;#39;s some areas where we lack monitoring when some parts would fail or some data issues would arise which I&amp;#39;ve been trying to build recently.&lt;/p&gt;\n\n&lt;p&gt;Overall, my work setup is good, but I wonder how will I get to the point that I will be confident in designing and building data infrastructure from scratch? I&amp;#39;ve studied a bit on fundamentals of data modelling and data warehousing previously, but I can&amp;#39;t find a way to apply it in my current job - or I may be lost on how that can be done in action and how I would also be able to gain more experience and knowledge for building data infrastructure from scratch.&lt;/p&gt;\n\n&lt;p&gt;I would really appreciate advice or stories that had the same experience. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pvf3t", "is_robot_indexable": true, "report_reasons": null, "author": "pmchaeli", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pvf3t/how_to_be_qualified_to_design_and_build_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pvf3t/how_to_be_qualified_to_design_and_build_data/", "subreddit_subscribers": 138355, "created_utc": 1699367071.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! I\u2019ve come across a certain requirement which I need some help with. My organisation intends to develop a web scrapping ML model for its internal use case. However this model needs to be deployed on GCP. \n\nWhat are the best practices for deploying a web scraping project that utilizes BeautifulSoup and ChromeDriver ( heavily ) to extract data from websites? I'm particularly interested in suggestions for deployment tools, monitoring and error handling strategies, scalability considerations.\n\nInitially we had an approach to go with Cloud functions but due to its time out issue, we\u2019re looking at different alternatives. \n\nPlease help me out here. Open to suggestions.\n\nThanks!", "author_fullname": "t2_3qc9b4bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ML Model Deployment in GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pok69", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699340222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I\u2019ve come across a certain requirement which I need some help with. My organisation intends to develop a web scrapping ML model for its internal use case. However this model needs to be deployed on GCP. &lt;/p&gt;\n\n&lt;p&gt;What are the best practices for deploying a web scraping project that utilizes BeautifulSoup and ChromeDriver ( heavily ) to extract data from websites? I&amp;#39;m particularly interested in suggestions for deployment tools, monitoring and error handling strategies, scalability considerations.&lt;/p&gt;\n\n&lt;p&gt;Initially we had an approach to go with Cloud functions but due to its time out issue, we\u2019re looking at different alternatives. &lt;/p&gt;\n\n&lt;p&gt;Please help me out here. Open to suggestions.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pok69", "is_robot_indexable": true, "report_reasons": null, "author": "apache444", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pok69/ml_model_deployment_in_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pok69/ml_model_deployment_in_gcp/", "subreddit_subscribers": 138355, "created_utc": 1699340222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says, I'm curious how you guys manage your redshift user access. I know one of the typical ways is just manually create users, and grant them to groups with command.\n\nContext: I'm using AWS and terraform\n\nMy current way (tested) is to list down the users in terraform (.tfvars) and \n\n1. Random generate password and store it in secret manager\n2. Import from secret manager and pass the password during redshift user creation\n3. Lambda function to invoke and grab the details from secret manager and send to their respective email\n\nI'm pretty sure there are better ways (google id?) because the list will grow larger and will be harder to maintain. Do share me how u guys do it", "author_fullname": "t2_88m111zn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift user management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pnw59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699337329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, I&amp;#39;m curious how you guys manage your redshift user access. I know one of the typical ways is just manually create users, and grant them to groups with command.&lt;/p&gt;\n\n&lt;p&gt;Context: I&amp;#39;m using AWS and terraform&lt;/p&gt;\n\n&lt;p&gt;My current way (tested) is to list down the users in terraform (.tfvars) and &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Random generate password and store it in secret manager&lt;/li&gt;\n&lt;li&gt;Import from secret manager and pass the password during redshift user creation&lt;/li&gt;\n&lt;li&gt;Lambda function to invoke and grab the details from secret manager and send to their respective email&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m pretty sure there are better ways (google id?) because the list will grow larger and will be harder to maintain. Do share me how u guys do it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pnw59", "is_robot_indexable": true, "report_reasons": null, "author": "shraderson97", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pnw59/redshift_user_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pnw59/redshift_user_management/", "subreddit_subscribers": 138355, "created_utc": 1699337329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some of your favorite tools for making dbt Core development easier?", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are good tools for working with dbt Core?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pfavc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699310276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some of your favorite tools for making dbt Core development easier?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pfavc", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pfavc/what_are_good_tools_for_working_with_dbt_core/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pfavc/what_are_good_tools_for_working_with_dbt_core/", "subreddit_subscribers": 138355, "created_utc": 1699310276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Learning pyspark and would be great to know if I can optimise this even more further.\n\nI have followed this [S/O post](https://stackoverflow.com/questions/69002085/populate-a-column-based-on-previous-value-and-row-pyspark) and was able to optimize one of the process but still Im struggling to figure out is there a better way to solve the below scenario.  \n\n\nThe dataframe will be like this \n\n&amp;#x200B;\n\n|Date|Date 2|Partition|\n|:-|:-|:-|\n| 6 Nov   |6 Nov  |1|\n|6 Nov|7 Nov|2|\n|7 Nov|7 Nov|2|\n|6 Nov|8 Nov|3|\n|7 Nov|8 Nov|3|\n|8 Nov|8 Nov|3|\n\n&amp;#x200B;\n\nPartition here is just a group of data and whatever the calculation that is required to generate the columns will be based on from those current partition.\n\n&gt;while (!threshold) {  \n&gt;  \n&gt;price =+ 25;  \n&gt;  \n&gt;result = // consider an operation like the one mentioned in the previous S/O link is happening here  \n&gt;  \n&gt;// few more calculations which will generate few columns  \n&gt;  \n&gt;if (result &lt; someValue) {   \n&gt;  \n&gt;populate one column here          \n&gt;  \n&gt;} else {  \n&gt;  \n&gt;flips the threshold  \n}\n\nFrom the above while loop we are interested in the final price and the aggregated values and the generated values in the final iteration of the while loop all intermediate results are overwritten or not used.\n\nRight now I have implemented this with while loop in spark and used \\`\\`\\`\\`when / otherwise\\`\\`\\`\\` to do all those if conditions\n\n&amp;#x200B;\n\nI want to know is there any other better spark way to do this, since before knowing the attached S/O implementation my other operation was taking few minutes to complete but now its a breeze.\n\n&amp;#x200B;\n\nAn idea or some reference links to look or concepts to learn would be even more appreciated.\n\nThanks in advance.", "author_fullname": "t2_6oj5b6d9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a better way to write / optimize this loop in pyspark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pa515", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699297203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Learning pyspark and would be great to know if I can optimise this even more further.&lt;/p&gt;\n\n&lt;p&gt;I have followed this &lt;a href=\"https://stackoverflow.com/questions/69002085/populate-a-column-based-on-previous-value-and-row-pyspark\"&gt;S/O post&lt;/a&gt; and was able to optimize one of the process but still Im struggling to figure out is there a better way to solve the below scenario.  &lt;/p&gt;\n\n&lt;p&gt;The dataframe will be like this &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Date&lt;/th&gt;\n&lt;th align=\"left\"&gt;Date 2&lt;/th&gt;\n&lt;th align=\"left\"&gt;Partition&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;6 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;7 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;7 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;6 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;8 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;7 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;8 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;8 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;8 Nov&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Partition here is just a group of data and whatever the calculation that is required to generate the columns will be based on from those current partition.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;while (!threshold) {  &lt;/p&gt;\n\n&lt;p&gt;price =+ 25;  &lt;/p&gt;\n\n&lt;p&gt;result = // consider an operation like the one mentioned in the previous S/O link is happening here  &lt;/p&gt;\n\n&lt;p&gt;// few more calculations which will generate few columns  &lt;/p&gt;\n\n&lt;p&gt;if (result &amp;lt; someValue) {   &lt;/p&gt;\n\n&lt;p&gt;populate one column here          &lt;/p&gt;\n\n&lt;p&gt;} else {  &lt;/p&gt;\n\n&lt;p&gt;flips the threshold&lt;br/&gt;\n}&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;From the above while loop we are interested in the final price and the aggregated values and the generated values in the final iteration of the while loop all intermediate results are overwritten or not used.&lt;/p&gt;\n\n&lt;p&gt;Right now I have implemented this with while loop in spark and used ````when / otherwise```` to do all those if conditions&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I want to know is there any other better spark way to do this, since before knowing the attached S/O implementation my other operation was taking few minutes to complete but now its a breeze.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;An idea or some reference links to look or concepts to learn would be even more appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?auto=webp&amp;s=a70d21ce9f01f64670d2200ca9fc3f39b94a7e48", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aad06750c23b98c9b7595343a8b54a42dc18851", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b66126834977e269be586d07464046049ed09138", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pa515", "is_robot_indexable": true, "report_reasons": null, "author": "zee_wild_runner", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pa515/is_there_a_better_way_to_write_optimize_this_loop/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pa515/is_there_a_better_way_to_write_optimize_this_loop/", "subreddit_subscribers": 138355, "created_utc": 1699297203.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,  \n\n\nI am working on a Pipeline in Azure Data Factory which gets the last modified date data from ADLG2 &amp; On-prem storage.  I have a situation where multiple files are uploaded to the container with a difference of seconds and minutes.  \n\n\nThe issue is all these files are meant to arrive at the same time so with the issue of different time upload and want to create a function that ignores the Timestamp usually generate from \"Last Modified\" date and just consider only the Date (yyyy-MM-dd).\n\nI would love if anyone can help me because this is really a big issue for me. Also links to materials will help.  \n\n\nAlso in a scenario where let say 5-10 files enters the same time using the Last Modified Date activity in ADF will it pick all files.  \n\n\nThanks SOS.", "author_fullname": "t2_6fwa4j9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get Date only using the Azure Data Factory Get Meta Data (Last Modified)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17p52e3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699283946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,  &lt;/p&gt;\n\n&lt;p&gt;I am working on a Pipeline in Azure Data Factory which gets the last modified date data from ADLG2 &amp;amp; On-prem storage.  I have a situation where multiple files are uploaded to the container with a difference of seconds and minutes.  &lt;/p&gt;\n\n&lt;p&gt;The issue is all these files are meant to arrive at the same time so with the issue of different time upload and want to create a function that ignores the Timestamp usually generate from &amp;quot;Last Modified&amp;quot; date and just consider only the Date (yyyy-MM-dd).&lt;/p&gt;\n\n&lt;p&gt;I would love if anyone can help me because this is really a big issue for me. Also links to materials will help.  &lt;/p&gt;\n\n&lt;p&gt;Also in a scenario where let say 5-10 files enters the same time using the Last Modified Date activity in ADF will it pick all files.  &lt;/p&gt;\n\n&lt;p&gt;Thanks SOS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17p52e3", "is_robot_indexable": true, "report_reasons": null, "author": "kiddojazz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17p52e3/how_to_get_date_only_using_the_azure_data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17p52e3/how_to_get_date_only_using_the_azure_data_factory/", "subreddit_subscribers": 138355, "created_utc": 1699283946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The three level namespace in Unity Catalog is a well know limitation, that is especially painful to deal with if you want to replicate data between environments like dev, test, prod.\n\nWhen it comes to naming conventions, I am deciding between these two (Examples from the dbt best practices guide [https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview](https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview)):\n\n1.\\`&lt;env&gt;\\_staging.jaffle\\_shop.orders\\`\n\n2. \\`&lt;env&gt;.staging\\_\\_jaffle\\_shop.orders\\`\n\nThere is also \\`&lt;env&gt;.staging.jaffle\\_shop\\_\\_orders\\`, which i am not considering.\n\n1. Pro: Keeps the list of schemas shorter. Easier to see different source systems/business areas/purposes in one go. Neg: We have to manage more catalogs and potentially expand the namespace to &lt;env&gt;\\_&lt;team&gt;\\_staging, if we don't want to manage the entire organization data in one set of catalogs.\n\n2. Basically the inverse of 1.\n\nWhat is in you opinion the best trade-off here that is most generally applicable?", "author_fullname": "t2_amnwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unity Catalog (Databricks) naming convention best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17pvnt2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699367750.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The three level namespace in Unity Catalog is a well know limitation, that is especially painful to deal with if you want to replicate data between environments like dev, test, prod.&lt;/p&gt;\n\n&lt;p&gt;When it comes to naming conventions, I am deciding between these two (Examples from the dbt best practices guide &lt;a href=\"https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview\"&gt;https://docs.getdbt.com/guides/best-practices/how-we-structure/1-guide-overview&lt;/a&gt;):&lt;/p&gt;\n\n&lt;p&gt;1.`&amp;lt;env&amp;gt;_staging.jaffle_shop.orders`&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;`&amp;lt;env&amp;gt;.staging__jaffle_shop.orders`&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There is also `&amp;lt;env&amp;gt;.staging.jaffle_shop__orders`, which i am not considering.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Pro: Keeps the list of schemas shorter. Easier to see different source systems/business areas/purposes in one go. Neg: We have to manage more catalogs and potentially expand the namespace to &amp;lt;env&amp;gt;_&amp;lt;team&amp;gt;_staging, if we don&amp;#39;t want to manage the entire organization data in one set of catalogs.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Basically the inverse of 1.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What is in you opinion the best trade-off here that is most generally applicable?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?auto=webp&amp;s=2a89f01968bbb7160773570a5739ba364e017ebf", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e39c972215449e24ba187a3b3e6d0289aad02d1b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8e48b5b0440098be5b7b54dcdd6d78e80f77e948", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c583ec988ffb5d6d8292b88b38a2a7ac9fc2b799", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a97be3626c69aab79c2204db47f040a6a8bb9820", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0ba90b674ccf1906f5a13abd09b27db16d203bd0", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/fIOL2kGYaMHNDhtmKe3L_aTjYOpHYh-54HVZsGgnZtE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=197f95d4689989cecbdb537c3aa18035536b0c50", "width": 1080, "height": 567}], "variants": {}, "id": "KBohsdqrfvkRxfqADmI_uqtotFtqgZjYu8NQbRpJlaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pvnt2", "is_robot_indexable": true, "report_reasons": null, "author": "Nobot16k", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pvnt2/unity_catalog_databricks_naming_convention_best/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pvnt2/unity_catalog_databricks_naming_convention_best/", "subreddit_subscribers": 138355, "created_utc": 1699367750.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The question does sound silly. I was a programmer and loved web development and making something out of nothing but the heavy coding around the business functionality, CI/CD, Elaborate testing, is not appealing.\n\n&amp;#x200B;\n\nWhat I love: \n\ndata visualization, cleaning, statistics (the numbers not the math) and generally love information and DB design and optimization.\n\nWhat I hate: \n\nmy mind would rarely be able to wrap itself around sql queries that have more than a couple of joins, specially if its a query inside another. I hated reading those. I also despised functional programming and recursion because I couldn't visualize it\n\nWhy am I considering Data engineering? \n\nI mentioned my love for data and data cleaning, not to mention salary and I imagine with new tools the querying would not need to be SQL style. Is it realistic to do this job well without that skill?", "author_fullname": "t2_h7rgd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a must to be very good at SQL for a data engineer position?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17pvhbv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699367253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The question does sound silly. I was a programmer and loved web development and making something out of nothing but the heavy coding around the business functionality, CI/CD, Elaborate testing, is not appealing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What I love: &lt;/p&gt;\n\n&lt;p&gt;data visualization, cleaning, statistics (the numbers not the math) and generally love information and DB design and optimization.&lt;/p&gt;\n\n&lt;p&gt;What I hate: &lt;/p&gt;\n\n&lt;p&gt;my mind would rarely be able to wrap itself around sql queries that have more than a couple of joins, specially if its a query inside another. I hated reading those. I also despised functional programming and recursion because I couldn&amp;#39;t visualize it&lt;/p&gt;\n\n&lt;p&gt;Why am I considering Data engineering? &lt;/p&gt;\n\n&lt;p&gt;I mentioned my love for data and data cleaning, not to mention salary and I imagine with new tools the querying would not need to be SQL style. Is it realistic to do this job well without that skill?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17pvhbv", "is_robot_indexable": true, "report_reasons": null, "author": "knockedownupagain", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pvhbv/is_it_a_must_to_be_very_good_at_sql_for_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pvhbv/is_it_a_must_to_be_very_good_at_sql_for_a_data/", "subreddit_subscribers": 138355, "created_utc": 1699367253.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}