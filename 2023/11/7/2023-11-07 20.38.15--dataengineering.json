{"kind": "Listing", "data": {"after": "t3_17pyjme", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a big fan and have been exploring adding it as our data warehouse given we're at a small scale. Does anyone use it at work/production environment? If so, how's your experience been with it?", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone using duckDB at work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pfhzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699310785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a big fan and have been exploring adding it as our data warehouse given we&amp;#39;re at a small scale. Does anyone use it at work/production environment? If so, how&amp;#39;s your experience been with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pfhzy", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pfhzy/is_anyone_using_duckdb_at_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pfhzy/is_anyone_using_duckdb_at_work/", "subreddit_subscribers": 138407, "created_utc": 1699310785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I gave this talk at PyData NYC last week.  It was fun working with devs from various projects (Dask, Arrow, Polars, Spark) in the week leading up to the event.  Thought I'd share a re-recording of it here\n\n[https://youtu.be/wKH0-zs2g\\_U](https://youtu.be/wKH0-zs2g_U)\n\nThis is the result of a couple weeks of work comparing large data frameworks on benchmarks ranging in size 10GB to 10TB.  No project wins.  It's really interesting analyzing results though.\n\nDuckDB and Dask are the only projects that reliably finish things (although possibly Dask's success here has to do with me knowing Dask better than the others).  DuckDB is way faster at small scale (along with Polars).  Dask and Spark are generally more robust and performant at large scale, mostly because they're able to parallelize S3 access.  Really-good-S3 access seems to be the way you win at real-world cloud performance.\n\nLooking more deeply at Dask results, we're wildly inefficient.  There's at least a 2x-5x performance increase to be had here.    Given that Dask does about as well as any other project on cloud this really means that \\*no one\\* has optimized cloud well yet.\n\nThis talk also goes into how we attempted to address bias (super hard to do in benchmarks).  We had active collaborations with Polars and Spark people (made Polars quite a bit faster during this process actually).  See [https://matthewrocklin.com/biased-benchmarks.html](https://matthewrocklin.com/biased-benchmarks.html) for more thoughts.\n\n&amp;#x200B;\n\nThis also shows the improvement Dask made in the last six months.    Dask used to suck at benchmarks.  Now it doesn't win, but reliably places among the top.  This is due to ... \n\n1. Arrow strings\n2. New shuffling algorithms\n3. Query optimization  \n\nThere's a lot of work for projects like Dask and Polars to fix themselves up in this space.  They're both moving pretty fast right now.  I'm curious to see how they progress in the next few months.\n\nFor future work I'd like to expand this out a bit beyond TPC-H.  TPC-H is great because they're fairly serious queries (lots of tables, lots of joins) and not micro-benchmarks.  We could use broader coverage though.  Any ideas?", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark, Dask, DuckDB, Polars: TPC-H Benchmarks at Scale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17puedp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699364059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I gave this talk at PyData NYC last week.  It was fun working with devs from various projects (Dask, Arrow, Polars, Spark) in the week leading up to the event.  Thought I&amp;#39;d share a re-recording of it here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/wKH0-zs2g_U\"&gt;https://youtu.be/wKH0-zs2g_U&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is the result of a couple weeks of work comparing large data frameworks on benchmarks ranging in size 10GB to 10TB.  No project wins.  It&amp;#39;s really interesting analyzing results though.&lt;/p&gt;\n\n&lt;p&gt;DuckDB and Dask are the only projects that reliably finish things (although possibly Dask&amp;#39;s success here has to do with me knowing Dask better than the others).  DuckDB is way faster at small scale (along with Polars).  Dask and Spark are generally more robust and performant at large scale, mostly because they&amp;#39;re able to parallelize S3 access.  Really-good-S3 access seems to be the way you win at real-world cloud performance.&lt;/p&gt;\n\n&lt;p&gt;Looking more deeply at Dask results, we&amp;#39;re wildly inefficient.  There&amp;#39;s at least a 2x-5x performance increase to be had here.    Given that Dask does about as well as any other project on cloud this really means that *no one* has optimized cloud well yet.&lt;/p&gt;\n\n&lt;p&gt;This talk also goes into how we attempted to address bias (super hard to do in benchmarks).  We had active collaborations with Polars and Spark people (made Polars quite a bit faster during this process actually).  See &lt;a href=\"https://matthewrocklin.com/biased-benchmarks.html\"&gt;https://matthewrocklin.com/biased-benchmarks.html&lt;/a&gt; for more thoughts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This also shows the improvement Dask made in the last six months.    Dask used to suck at benchmarks.  Now it doesn&amp;#39;t win, but reliably places among the top.  This is due to ... &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Arrow strings&lt;/li&gt;\n&lt;li&gt;New shuffling algorithms&lt;/li&gt;\n&lt;li&gt;Query optimization&lt;br/&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of work for projects like Dask and Polars to fix themselves up in this space.  They&amp;#39;re both moving pretty fast right now.  I&amp;#39;m curious to see how they progress in the next few months.&lt;/p&gt;\n\n&lt;p&gt;For future work I&amp;#39;d like to expand this out a bit beyond TPC-H.  TPC-H is great because they&amp;#39;re fairly serious queries (lots of tables, lots of joins) and not micro-benchmarks.  We could use broader coverage though.  Any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GKIJFAXC5MxDvqoiFAe_JuBjYW_-SN5PG5BvuQzJVFc.jpg?auto=webp&amp;s=da7d0b2e1dddb9a541c5e00e32b0c0febbc324d4", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GKIJFAXC5MxDvqoiFAe_JuBjYW_-SN5PG5BvuQzJVFc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=599f0b55752f80a746571a6d0466f0cdf6c55888", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GKIJFAXC5MxDvqoiFAe_JuBjYW_-SN5PG5BvuQzJVFc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c171018f1df6b821ac69934e98ca441dddb8e196", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GKIJFAXC5MxDvqoiFAe_JuBjYW_-SN5PG5BvuQzJVFc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f10e3936368e5aa15f9ccbf80970e40ff6a6857", "width": 320, "height": 240}], "variants": {}, "id": "NXJ6LdhKF_lKsbQSg6ourXQ88ir9X8cDbT37Oe33T6o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17puedp", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17puedp/spark_dask_duckdb_polars_tpch_benchmarks_at_scale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17puedp/spark_dask_duckdb_polars_tpch_benchmarks_at_scale/", "subreddit_subscribers": 138407, "created_utc": 1699364059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The question does sound silly. I was a programmer and loved web development and making something out of nothing but the heavy coding around the business functionality, CI/CD, Elaborate testing, is not appealing.\n\n&amp;#x200B;\n\nWhat I love: \n\ndata visualization, cleaning, statistics (the numbers not the math) and generally love information and DB design and optimization.\n\nWhat I hate: \n\nmy mind would rarely be able to wrap itself around sql queries that have more than a couple of joins, specially if its a query inside another. I hated reading those. I also despised functional programming and recursion because I couldn't visualize it\n\nWhy am I considering Data engineering? \n\nI mentioned my love for data and data cleaning, not to mention salary and I imagine with new tools the querying would not need to be SQL style. Is it realistic to do this job well without that skill?", "author_fullname": "t2_h7rgd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a must to be very good at SQL for a data engineer position?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pvhbv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699367253.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The question does sound silly. I was a programmer and loved web development and making something out of nothing but the heavy coding around the business functionality, CI/CD, Elaborate testing, is not appealing.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What I love: &lt;/p&gt;\n\n&lt;p&gt;data visualization, cleaning, statistics (the numbers not the math) and generally love information and DB design and optimization.&lt;/p&gt;\n\n&lt;p&gt;What I hate: &lt;/p&gt;\n\n&lt;p&gt;my mind would rarely be able to wrap itself around sql queries that have more than a couple of joins, specially if its a query inside another. I hated reading those. I also despised functional programming and recursion because I couldn&amp;#39;t visualize it&lt;/p&gt;\n\n&lt;p&gt;Why am I considering Data engineering? &lt;/p&gt;\n\n&lt;p&gt;I mentioned my love for data and data cleaning, not to mention salary and I imagine with new tools the querying would not need to be SQL style. Is it realistic to do this job well without that skill?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17pvhbv", "is_robot_indexable": true, "report_reasons": null, "author": "knockedownupagain", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pvhbv/is_it_a_must_to_be_very_good_at_sql_for_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pvhbv/is_it_a_must_to_be_very_good_at_sql_for_a_data/", "subreddit_subscribers": 138407, "created_utc": 1699367253.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI recently completed a personal project, and I am eager to receive feedback. Any suggestions for improvement would be greatly appreciated. Additionally, as a recent graduate, I'm thinking whether this project would be a good fit to include on my resume. Your insights on this matter would be very helpful.\n\n&amp;#x200B;\n\nThe architecture is:\n\nhttps://preview.redd.it/h1j2kx65bvyb1.png?width=3582&amp;format=png&amp;auto=webp&amp;s=a7ecb570b0a95dad3f25d8fef196068ab53a6d72\n\nThe dashboard for the project is: [https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD](https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3w4gqbn8bvyb1.png?width=2482&amp;format=png&amp;auto=webp&amp;s=9d3e65c593dcb3c2155af6b46c058911d1117b1a\n\nGithub repo: [https://github.com/Zzdragon66/ucla-reddit-dahsboard-public](https://github.com/Zzdragon66/ucla-reddit-dahsboard-public)", "author_fullname": "t2_5igde9z6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal Project of End-End ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"3w4gqbn8bvyb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 82, "x": 108, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4f6402e5d8012a1e837fa81ef3ff45ab96a8056d"}, {"y": 165, "x": 216, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=01f12fa8d7a1a69ca487c7d3b3d13644ef5f8cee"}, {"y": 245, "x": 320, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cfd705831ac7d643e23d89f33f02f1e7666b5e98"}, {"y": 490, "x": 640, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=167d14f9eef1b9949f53ebd87b02e60e58edea5b"}, {"y": 735, "x": 960, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=12e8b58f57495af10ae16f05528a4bfe9b5a7db8"}, {"y": 827, "x": 1080, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=592d1036e4a9435c2cb10c898f06423ac42e4bc7"}], "s": {"y": 1902, "x": 2482, "u": "https://preview.redd.it/3w4gqbn8bvyb1.png?width=2482&amp;format=png&amp;auto=webp&amp;s=9d3e65c593dcb3c2155af6b46c058911d1117b1a"}, "id": "3w4gqbn8bvyb1"}, "h1j2kx65bvyb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 46, "x": 108, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ab70939f11be44c99dd4e130ea771a081f6bf57"}, {"y": 92, "x": 216, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=29b3e6f32fc6d7259929edb6e056b165f008179d"}, {"y": 137, "x": 320, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d0b206325d48c4f9b0f0b07c8b08e50aee5d22b"}, {"y": 274, "x": 640, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3ff76f20d77691e39f0d465c2f92b970dd6f83a"}, {"y": 412, "x": 960, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2407495da6a1b63ae5d484163db0faaad6ecd38c"}, {"y": 464, "x": 1080, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=12c6f445b85babe8b7632787426c7675fd7cc1e0"}], "s": {"y": 1539, "x": 3582, "u": "https://preview.redd.it/h1j2kx65bvyb1.png?width=3582&amp;format=png&amp;auto=webp&amp;s=a7ecb570b0a95dad3f25d8fef196068ab53a6d72"}, "id": "h1j2kx65bvyb1"}}, "name": "t3_17po009", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 33, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 33, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fn5_tH4Xl1xvVNvAovmyl8tYc2ILptSU-3I3EfOfAZs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1699337786.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently completed a personal project, and I am eager to receive feedback. Any suggestions for improvement would be greatly appreciated. Additionally, as a recent graduate, I&amp;#39;m thinking whether this project would be a good fit to include on my resume. Your insights on this matter would be very helpful.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The architecture is:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/h1j2kx65bvyb1.png?width=3582&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7ecb570b0a95dad3f25d8fef196068ab53a6d72\"&gt;https://preview.redd.it/h1j2kx65bvyb1.png?width=3582&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a7ecb570b0a95dad3f25d8fef196068ab53a6d72&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The dashboard for the project is: &lt;a href=\"https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD\"&gt;https://lookerstudio.google.com/u/0/reporting/89878867-f944-4ab8-b842-9d3690781fba/page/CxAgD&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3w4gqbn8bvyb1.png?width=2482&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d3e65c593dcb3c2155af6b46c058911d1117b1a\"&gt;https://preview.redd.it/3w4gqbn8bvyb1.png?width=2482&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d3e65c593dcb3c2155af6b46c058911d1117b1a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Github repo: &lt;a href=\"https://github.com/Zzdragon66/ucla-reddit-dahsboard-public\"&gt;https://github.com/Zzdragon66/ucla-reddit-dahsboard-public&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Dt_WDsRInDtBEaAMeDYG27iXGkTTGHX32oVedSsPqyo.jpg?auto=webp&amp;s=76dfa394f4787ae8cb10b052e4dc8c1a5dc1a0f8", "width": 320, "height": 240}, "resolutions": [{"url": "https://external-preview.redd.it/Dt_WDsRInDtBEaAMeDYG27iXGkTTGHX32oVedSsPqyo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=12da9bc423fa95bffe58b4b9fd0bd99e715055c2", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Dt_WDsRInDtBEaAMeDYG27iXGkTTGHX32oVedSsPqyo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3a871ed67d9cef591a15b3877b42a1c7783a546", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Dt_WDsRInDtBEaAMeDYG27iXGkTTGHX32oVedSsPqyo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=45bc45dbfadc1ed4e05ce2f4e3fb34e5224a29c7", "width": 320, "height": 240}], "variants": {}, "id": "q2RV6VdBViSPw6O7YLf2kDPldgE-_Rn0cHvVhCZZlp0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "17po009", "is_robot_indexable": true, "report_reasons": null, "author": "AffectionateEmu8146", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17po009/personal_project_of_endend_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17po009/personal_project_of_endend_etl/", "subreddit_subscribers": 138407, "created_utc": 1699337786.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In this new [article](https://airbyte.com/blog/eltp-extending-elt-for-modern-ai-and-analytics), I introduce the concept of \u201cELTP\u201d.\n\nELTP looks a lot like ELT + Reverse-ETL, but goes beyond SaaS APIs, like building a publish flow to a partner\u2019s SFTP site or to an external database.\n\nThis model seems increasingly important for AI and vector store use cases, and with so many AI folks joining the DE community for the first time, what I try to show in the article is that it's been in the field for quite a while without a proper name.\n\nWhat is your take on this idea? I\u2019m interested to hear if people are already doing this in the wild.\n\n*(Disclaimer: I work at Airbyte and some of the above blog article references related features of the Airbyte product.)*", "author_fullname": "t2_2ilbb4r9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extract, Load, Transform\u2026 Publish?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pxpkq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699373727.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699373366.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this new &lt;a href=\"https://airbyte.com/blog/eltp-extending-elt-for-modern-ai-and-analytics\"&gt;article&lt;/a&gt;, I introduce the concept of \u201cELTP\u201d.&lt;/p&gt;\n\n&lt;p&gt;ELTP looks a lot like ELT + Reverse-ETL, but goes beyond SaaS APIs, like building a publish flow to a partner\u2019s SFTP site or to an external database.&lt;/p&gt;\n\n&lt;p&gt;This model seems increasingly important for AI and vector store use cases, and with so many AI folks joining the DE community for the first time, what I try to show in the article is that it&amp;#39;s been in the field for quite a while without a proper name.&lt;/p&gt;\n\n&lt;p&gt;What is your take on this idea? I\u2019m interested to hear if people are already doing this in the wild.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;(Disclaimer: I work at Airbyte and some of the above blog article references related features of the Airbyte product.)&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3m1OeAEKbSO5GZ0ocmTdtWnDAvuSIXnt6Se7niA5JWc.jpg?auto=webp&amp;s=c36f3c39f7d519d7712a0166ff902b3e7542d247", "width": 1800, "height": 1071}, "resolutions": [{"url": "https://external-preview.redd.it/3m1OeAEKbSO5GZ0ocmTdtWnDAvuSIXnt6Se7niA5JWc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bc20a881d60517d4519baa3033f26ad64b74a2a", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/3m1OeAEKbSO5GZ0ocmTdtWnDAvuSIXnt6Se7niA5JWc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8637df5a263849ada9ea696159c5899bb249fef9", "width": 216, "height": 128}, {"url": "https://external-preview.redd.it/3m1OeAEKbSO5GZ0ocmTdtWnDAvuSIXnt6Se7niA5JWc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=00aec1f95368cd2e1beeb8b47edbdde4bf35a9a5", "width": 320, "height": 190}, {"url": "https://external-preview.redd.it/3m1OeAEKbSO5GZ0ocmTdtWnDAvuSIXnt6Se7niA5JWc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e4405612b9a05f6d921c59050961cae5ea07c834", "width": 640, "height": 380}, {"url": "https://external-preview.redd.it/3m1OeAEKbSO5GZ0ocmTdtWnDAvuSIXnt6Se7niA5JWc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b2845208776ede7d7c3a5a9d91505341377b49f", "width": 960, "height": 571}, {"url": "https://external-preview.redd.it/3m1OeAEKbSO5GZ0ocmTdtWnDAvuSIXnt6Se7niA5JWc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d0e2f9847193a3fd69fe04b8def69765b6636537", "width": 1080, "height": 642}], "variants": {}, "id": "v0_fKpAyFBOxT0gtoFRBWOTXb1-9ps-gg1C6s9O0mpY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17pxpkq", "is_robot_indexable": true, "report_reasons": null, "author": "burnfearless", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pxpkq/extract_load_transform_publish/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pxpkq/extract_load_transform_publish/", "subreddit_subscribers": 138407, "created_utc": 1699373366.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have been working in the data engineering/science space for 3 years at a large government organisation where I\u2019m working in Azure and Databricks on the daily. I also have a computer science degree and a couple Microsoft certs but otherwise I am only 2 years out of uni.\n\nI recently started looking for my next role and have several offers including a contracting role that is paying a substantial daily rate. \n\nBut I can\u2019t help but think I am not experienced enough for this but they seem very keen on me. \n\nIs my imposter syndrome on overdrive or could it be too early in my career to go for a role like this?", "author_fullname": "t2_qc6h84ym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it ever too early to start contracting?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ppziq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699346834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been working in the data engineering/science space for 3 years at a large government organisation where I\u2019m working in Azure and Databricks on the daily. I also have a computer science degree and a couple Microsoft certs but otherwise I am only 2 years out of uni.&lt;/p&gt;\n\n&lt;p&gt;I recently started looking for my next role and have several offers including a contracting role that is paying a substantial daily rate. &lt;/p&gt;\n\n&lt;p&gt;But I can\u2019t help but think I am not experienced enough for this but they seem very keen on me. &lt;/p&gt;\n\n&lt;p&gt;Is my imposter syndrome on overdrive or could it be too early in my career to go for a role like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ppziq", "is_robot_indexable": true, "report_reasons": null, "author": "Agitated-Western1788", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ppziq/is_it_ever_too_early_to_start_contracting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ppziq/is_it_ever_too_early_to_start_contracting/", "subreddit_subscribers": 138407, "created_utc": 1699346834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which would you say is the most marketable (and you could only choose one track)?\n\nI\u2019m trying to decide where to invest my time. I am familiar with Python, SQL, etc. But should I focus on learning Kubenetes, AWS, Azure, etc?\n\nThanks!", "author_fullname": "t2_cz3wp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Code vs. No Code", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17peewm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699308051.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which would you say is the most marketable (and you could only choose one track)?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m trying to decide where to invest my time. I am familiar with Python, SQL, etc. But should I focus on learning Kubenetes, AWS, Azure, etc?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17peewm", "is_robot_indexable": true, "report_reasons": null, "author": "anewguy03", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17peewm/code_vs_no_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17peewm/code_vs_no_code/", "subreddit_subscribers": 138407, "created_utc": 1699308051.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone here have a system for versioning database environments? I know there are some paid services out there for doing this which I am open to hearing about, but would need very strong conviction to actually propose them to the team. I have also seen some systems where they essentially tear down and remake the database functions/stored procs from scratch with every deployment-- which seems like a valid approach, if not a little overkill.\n\nEssentially the situation is this, we have several redshift severless workgroups corresponding to different environments local, dev, test, prod. These databases are connected to their upstream versioned corresponding ETL processes. Often, I want to implement a stored proc or user-defined function or something in these databases, but the issue is deploying and keeping everything in sync. The current \"deployment\" method is just running the create statement in those in all different environments. This a little tedious, doesn't enforce consistency across the environments, and just feels bad. \n\nIdeally, what I would like is a form of ci/cd + version control where I can make a change to the local database (e.g. define/change a stored proc) and then press a button and have that change replicated through the higher envs with some sort of logging in place. There is no current need to version data or data models.", "author_fullname": "t2_kdj5q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Database Versioning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pe4up", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699307345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone here have a system for versioning database environments? I know there are some paid services out there for doing this which I am open to hearing about, but would need very strong conviction to actually propose them to the team. I have also seen some systems where they essentially tear down and remake the database functions/stored procs from scratch with every deployment-- which seems like a valid approach, if not a little overkill.&lt;/p&gt;\n\n&lt;p&gt;Essentially the situation is this, we have several redshift severless workgroups corresponding to different environments local, dev, test, prod. These databases are connected to their upstream versioned corresponding ETL processes. Often, I want to implement a stored proc or user-defined function or something in these databases, but the issue is deploying and keeping everything in sync. The current &amp;quot;deployment&amp;quot; method is just running the create statement in those in all different environments. This a little tedious, doesn&amp;#39;t enforce consistency across the environments, and just feels bad. &lt;/p&gt;\n\n&lt;p&gt;Ideally, what I would like is a form of ci/cd + version control where I can make a change to the local database (e.g. define/change a stored proc) and then press a button and have that change replicated through the higher envs with some sort of logging in place. There is no current need to version data or data models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pe4up", "is_robot_indexable": true, "report_reasons": null, "author": "Touvejs", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pe4up/database_versioning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pe4up/database_versioning/", "subreddit_subscribers": 138407, "created_utc": 1699307345.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been brought onto a team to help them sort out some challenges they are having orchestrating their data pipeline. They are a group of non-technical BI developers and data analysts and so I've been told multiple times that if at all possible I should aim to have a solution be as close to no-code / low-code as possible. I was also told that they are/want to leverage step functions for this effort.\n\nTheir data pipelines involve a series of steps in sequence (Glue Jobs). These are bundled into step functions. The main challenges are:\n\n* There are other step functions that require that multiple other step functions are completed before executing (e.g. Step Function C needs Step Function A and B to be complete)\n* The input step functions are not idempotent and are the inputs/dependencies to multiple other step functions (a 1 to many relationship)\n\nCurrently I'm considering the following:\n\n* Implement a master orchestration step function that encapsulates all the step functions. In order to prevent issues with rerunning the same dependency multiple times I'm planning to setup a little sentinel in state machine that will just check and verify that the state function dependencies had succeeded in the last N hours (where N is the SLA) and wait if not.\n* Setup a combination of EventBridge + SQS + Lambda to basically setup a small service that records when step functions are completed and then use to this as a hub/trigger for step functions that have those dependencies.\n\nI'm currently leaning towards the first option be it is low code, and easier to work within a single DAG (e.g. don't have to go hunting through events to debug the data pipeline).\n\nAm I missing anything obvious? Has anyone done something like this (e.g. Step Function -&gt; Step Function orchestration).", "author_fullname": "t2_6d49ikq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Step Functions as an Orchestration Tool and how to handle multiple dependencies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pih9x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699319121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been brought onto a team to help them sort out some challenges they are having orchestrating their data pipeline. They are a group of non-technical BI developers and data analysts and so I&amp;#39;ve been told multiple times that if at all possible I should aim to have a solution be as close to no-code / low-code as possible. I was also told that they are/want to leverage step functions for this effort.&lt;/p&gt;\n\n&lt;p&gt;Their data pipelines involve a series of steps in sequence (Glue Jobs). These are bundled into step functions. The main challenges are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;There are other step functions that require that multiple other step functions are completed before executing (e.g. Step Function C needs Step Function A and B to be complete)&lt;/li&gt;\n&lt;li&gt;The input step functions are not idempotent and are the inputs/dependencies to multiple other step functions (a 1 to many relationship)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Currently I&amp;#39;m considering the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implement a master orchestration step function that encapsulates all the step functions. In order to prevent issues with rerunning the same dependency multiple times I&amp;#39;m planning to setup a little sentinel in state machine that will just check and verify that the state function dependencies had succeeded in the last N hours (where N is the SLA) and wait if not.&lt;/li&gt;\n&lt;li&gt;Setup a combination of EventBridge + SQS + Lambda to basically setup a small service that records when step functions are completed and then use to this as a hub/trigger for step functions that have those dependencies.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m currently leaning towards the first option be it is low code, and easier to work within a single DAG (e.g. don&amp;#39;t have to go hunting through events to debug the data pipeline).&lt;/p&gt;\n\n&lt;p&gt;Am I missing anything obvious? Has anyone done something like this (e.g. Step Function -&amp;gt; Step Function orchestration).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pih9x", "is_robot_indexable": true, "report_reasons": null, "author": "poppinstacks", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pih9x/aws_step_functions_as_an_orchestration_tool_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pih9x/aws_step_functions_as_an_orchestration_tool_and/", "subreddit_subscribers": 138407, "created_utc": 1699319121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm fairly new in data engineering but I haven't had the chance to build data warehouse or data infrastructure from scratch. My current company is great and I'm doing and learning variety of things - setting up data pipelines and working on different integrations. We're using Snowflake, dbt, AWS and many more. I mostly handle backend/ infra-related and I barely build any models even in dbt aside from trying to introduce some good practices or maximizing its features; analysts create models more frequently in our company because of dashboarding and visualization. Our data infrastructure setup is in a good state, except there's some areas where we lack monitoring when some parts would fail or some data issues would arise which I've been trying to build recently.\n\nOverall, my work setup is good, but I wonder how will I get to the point that I will be confident in designing and building data infrastructure from scratch? I've studied a bit on fundamentals of data modelling and data warehousing previously, but I can't find a way to apply it in my current job - or I may be lost on how that can be done in action and how I would also be able to gain more experience and knowledge for building data infrastructure from scratch.\n\nI would really appreciate advice or stories that had the same experience. Thanks!", "author_fullname": "t2_uep0sc0m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to be qualified to design and build data infrastructure from scratch?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pvf3t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699367071.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m fairly new in data engineering but I haven&amp;#39;t had the chance to build data warehouse or data infrastructure from scratch. My current company is great and I&amp;#39;m doing and learning variety of things - setting up data pipelines and working on different integrations. We&amp;#39;re using Snowflake, dbt, AWS and many more. I mostly handle backend/ infra-related and I barely build any models even in dbt aside from trying to introduce some good practices or maximizing its features; analysts create models more frequently in our company because of dashboarding and visualization. Our data infrastructure setup is in a good state, except there&amp;#39;s some areas where we lack monitoring when some parts would fail or some data issues would arise which I&amp;#39;ve been trying to build recently.&lt;/p&gt;\n\n&lt;p&gt;Overall, my work setup is good, but I wonder how will I get to the point that I will be confident in designing and building data infrastructure from scratch? I&amp;#39;ve studied a bit on fundamentals of data modelling and data warehousing previously, but I can&amp;#39;t find a way to apply it in my current job - or I may be lost on how that can be done in action and how I would also be able to gain more experience and knowledge for building data infrastructure from scratch.&lt;/p&gt;\n\n&lt;p&gt;I would really appreciate advice or stories that had the same experience. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pvf3t", "is_robot_indexable": true, "report_reasons": null, "author": "pmchaeli", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pvf3t/how_to_be_qualified_to_design_and_build_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pvf3t/how_to_be_qualified_to_design_and_build_data/", "subreddit_subscribers": 138407, "created_utc": 1699367071.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have plenty of experience with SQL and there are many projects that people prefer to use SQL for data transformation, doing ELT using BigQuery, for example, since it's so performatic, cheap, and you don't have to put extra pieces to it other than a scheduler/triggers. However, the queries I've built for the last years can get quite lengthy, and might be difficult to maintain, despite my best efforts to make it as simple as I can. \n\nI always end up with many CTEs and a final query. Many times I have like 8 or 10 CTEs, one depending on the other. I already thought of creating MQTs to do it \"step by step\" but then it's nothing more than splitting the same query into different steps, not sure if it would make it easier or actually more complicated.\n\nHow do you guys manage that? I have a good domain of Python and I don't have any limitations in terms of stack, but I would like to keep it as simple as it can be.", "author_fullname": "t2_l7mlroox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to better design a SQL transformation query", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pzbei", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699377672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have plenty of experience with SQL and there are many projects that people prefer to use SQL for data transformation, doing ELT using BigQuery, for example, since it&amp;#39;s so performatic, cheap, and you don&amp;#39;t have to put extra pieces to it other than a scheduler/triggers. However, the queries I&amp;#39;ve built for the last years can get quite lengthy, and might be difficult to maintain, despite my best efforts to make it as simple as I can. &lt;/p&gt;\n\n&lt;p&gt;I always end up with many CTEs and a final query. Many times I have like 8 or 10 CTEs, one depending on the other. I already thought of creating MQTs to do it &amp;quot;step by step&amp;quot; but then it&amp;#39;s nothing more than splitting the same query into different steps, not sure if it would make it easier or actually more complicated.&lt;/p&gt;\n\n&lt;p&gt;How do you guys manage that? I have a good domain of Python and I don&amp;#39;t have any limitations in terms of stack, but I would like to keep it as simple as it can be.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pzbei", "is_robot_indexable": true, "report_reasons": null, "author": "PaleRepresentative70", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pzbei/how_to_better_design_a_sql_transformation_query/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pzbei/how_to_better_design_a_sql_transformation_query/", "subreddit_subscribers": 138407, "created_utc": 1699377672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://docs.rivery.io/docs/architecture \nHallo data engineers, could some one look in tjo this and try to explain it with the actual components. There is only a few things written on the official doc of Rivery.io ( link above). I know the Data comes from the left with TLS 1.2 and gets stored in the target. Could some one explain what happens precisely in the whole process? Im a newbie to data engineering. Thank you.", "author_fullname": "t2_h1gtwoh1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Explain this Riverty ELT Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 101, "top_awarded_type": null, "hide_score": false, "name": "t3_17pkmkt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mPHtGsDnvY4mPfz1OJiRffsZgwUuihupFDkTXyYEOlg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699325632.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://docs.rivery.io/docs/architecture\"&gt;https://docs.rivery.io/docs/architecture&lt;/a&gt; \nHallo data engineers, could some one look in tjo this and try to explain it with the actual components. There is only a few things written on the official doc of Rivery.io ( link above). I know the Data comes from the left with TLS 1.2 and gets stored in the target. Could some one explain what happens precisely in the whole process? Im a newbie to data engineering. Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/gmv0aqekbuyb1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/gmv0aqekbuyb1.png?auto=webp&amp;s=c403b8e5c011abb1d62b9209bfbf48750aca0f1a", "width": 887, "height": 646}, "resolutions": [{"url": "https://preview.redd.it/gmv0aqekbuyb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ce3da093fcd7c6fba39263ab197a049f5790b79", "width": 108, "height": 78}, {"url": "https://preview.redd.it/gmv0aqekbuyb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ea4da1c50a57cc4b1647c3d16fb1cc4d8568e2f", "width": 216, "height": 157}, {"url": "https://preview.redd.it/gmv0aqekbuyb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5d31040c06f79ba9ceca9ada8dee6c190c670782", "width": 320, "height": 233}, {"url": "https://preview.redd.it/gmv0aqekbuyb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a1947a9a7725e4023124c11c76dde1d4d3a0ac13", "width": 640, "height": 466}], "variants": {}, "id": "OTo3WmRT8q3Wd7AtJe8OsT0dD2r9ih9644-jhtvRt8Y"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pkmkt", "is_robot_indexable": true, "report_reasons": null, "author": "MammothEntry901", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pkmkt/explain_this_riverty_elt_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/gmv0aqekbuyb1.png", "subreddit_subscribers": 138407, "created_utc": 1699325632.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Nowadays, there are a lot of DA achievements in universities, which are very common, is there a bit of oversupply?", "author_fullname": "t2_l48rbkuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why are DA jobs becoming more and more popular?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17piy88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699320546.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Nowadays, there are a lot of DA achievements in universities, which are very common, is there a bit of oversupply?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17piy88", "is_robot_indexable": true, "report_reasons": null, "author": "digmouse_DS", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17piy88/why_are_da_jobs_becoming_more_and_more_popular/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17piy88/why_are_da_jobs_becoming_more_and_more_popular/", "subreddit_subscribers": 138407, "created_utc": 1699320546.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey whats up everyone? So, I've been watching a few videos/follwoing some twitter profiles on the Data Engineering pathway. I understand it isnt exactly an entry role, but it seems to be that everyone said you need multiple years as a data analyst before you can think about even becoming a DE.\n\nI was thinking, do you really need multiple years if you learn some DE skills on the side? Just curious on some opinions on this thought process.", "author_fullname": "t2_1osowqn1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How much does prior experience as a Data Analyst really help with Data Engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pyl1y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699375741.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey whats up everyone? So, I&amp;#39;ve been watching a few videos/follwoing some twitter profiles on the Data Engineering pathway. I understand it isnt exactly an entry role, but it seems to be that everyone said you need multiple years as a data analyst before you can think about even becoming a DE.&lt;/p&gt;\n\n&lt;p&gt;I was thinking, do you really need multiple years if you learn some DE skills on the side? Just curious on some opinions on this thought process.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17pyl1y", "is_robot_indexable": true, "report_reasons": null, "author": "brokebulg99", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pyl1y/how_much_does_prior_experience_as_a_data_analyst/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pyl1y/how_much_does_prior_experience_as_a_data_analyst/", "subreddit_subscribers": 138407, "created_utc": 1699375741.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to add dbt CLI autocomplete in Docker", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": true, "name": "t3_17q1ili", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "#46d160", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rdJSMgSF6FlzDp-ljmdtwOHnYryj22o2UGNrwAI7ajA.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699383435.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dataengineering.wiki", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dataengineering.wiki/Tutorials/dbt+CLI+autocomplete+in+Docker", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/EnzJq0WkebYFlAg9Vn2sKCF-VGaGsFdUACPYcQQ98ng.jpg?auto=webp&amp;s=2ce153cbaf58563a0599eba1ec53615eecca8a4b", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/EnzJq0WkebYFlAg9Vn2sKCF-VGaGsFdUACPYcQQ98ng.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9b9b1eb7feea679a40f2c06d7a2bc2436be71b41", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/EnzJq0WkebYFlAg9Vn2sKCF-VGaGsFdUACPYcQQ98ng.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=781ae9f8e7c0b4342a83c6512b8cc948c5d4e7bd", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/EnzJq0WkebYFlAg9Vn2sKCF-VGaGsFdUACPYcQQ98ng.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=489008b918bf7c269ef8b9933fba4131388a67c2", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/EnzJq0WkebYFlAg9Vn2sKCF-VGaGsFdUACPYcQQ98ng.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ff38307da678643f74717321a717d13e100cd4a7", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/EnzJq0WkebYFlAg9Vn2sKCF-VGaGsFdUACPYcQQ98ng.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=febc115188d63c90b44e074b7ac3344846553950", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/EnzJq0WkebYFlAg9Vn2sKCF-VGaGsFdUACPYcQQ98ng.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7e7bda8cfc023e72faee6927765112883c2e0604", "width": 1080, "height": 567}], "variants": {}, "id": "toAOII9ezRnHNk9hYuZoQm1XtgMDxjuRZ7mblrksd9o"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Lead Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17q1ili", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/17q1ili/how_to_add_dbt_cli_autocomplete_in_docker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dataengineering.wiki/Tutorials/dbt+CLI+autocomplete+in+Docker", "subreddit_subscribers": 138407, "created_utc": 1699383435.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://kestra.io/blogs/2023-10-31-kestra-weaviate](https://kestra.io/blogs/2023-10-31-kestra-weaviate)", "author_fullname": "t2_7lbbuuh58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Work with Vector Databases 101", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pze8s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699377883.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://kestra.io/blogs/2023-10-31-kestra-weaviate\"&gt;https://kestra.io/blogs/2023-10-31-kestra-weaviate&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17pze8s", "is_robot_indexable": true, "report_reasons": null, "author": "Round-Following1532", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pze8s/how_to_work_with_vector_databases_101/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pze8s/how_to_work_with_vector_databases_101/", "subreddit_subscribers": 138407, "created_utc": 1699377883.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello.  \n\n\nI work for a company where we have a ADF instance running in Azure with Azure DevOps as the code repository.\n\nBecause some chum decided to pack together ADF code, some SQL-DB code and a lot of PowerBI files into the same repo, with different pipelines, we now have a situation in which, because some of these PowerBI files are quite large, we now have a git-pack file that is very large and our deployment instance runs out of space.  \n\n\nNow imo the ADF deployment process is needlessly convoluted and black-boxy, and thus I an unsure about how my planned cleanup process will affect the ADF deployment pipeline.  \n\n\nI plan to separate the PBI stuff to its own repo, then *fully clean* the git history in order to get rid of the pack file. However, what I am not sure of, is whether this has any negative impact on the **adf\\_publish** branch. Does anyone know if the **adf\\_publish** branch is unaffected by a change in the git-log from the master branch on which it is based?", "author_fullname": "t2_onmeo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help: AzureDataFactory deployment problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pwig6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699370156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.  &lt;/p&gt;\n\n&lt;p&gt;I work for a company where we have a ADF instance running in Azure with Azure DevOps as the code repository.&lt;/p&gt;\n\n&lt;p&gt;Because some chum decided to pack together ADF code, some SQL-DB code and a lot of PowerBI files into the same repo, with different pipelines, we now have a situation in which, because some of these PowerBI files are quite large, we now have a git-pack file that is very large and our deployment instance runs out of space.  &lt;/p&gt;\n\n&lt;p&gt;Now imo the ADF deployment process is needlessly convoluted and black-boxy, and thus I an unsure about how my planned cleanup process will affect the ADF deployment pipeline.  &lt;/p&gt;\n\n&lt;p&gt;I plan to separate the PBI stuff to its own repo, then &lt;em&gt;fully clean&lt;/em&gt; the git history in order to get rid of the pack file. However, what I am not sure of, is whether this has any negative impact on the &lt;strong&gt;adf_publish&lt;/strong&gt; branch. Does anyone know if the &lt;strong&gt;adf_publish&lt;/strong&gt; branch is unaffected by a change in the git-log from the master branch on which it is based?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pwig6", "is_robot_indexable": true, "report_reasons": null, "author": "Hinkakan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pwig6/help_azuredatafactory_deployment_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pwig6/help_azuredatafactory_deployment_problem/", "subreddit_subscribers": 138407, "created_utc": 1699370156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone! I\u2019ve come across a certain requirement which I need some help with. My organisation intends to develop a web scrapping ML model for its internal use case. However this model needs to be deployed on GCP. \n\nWhat are the best practices for deploying a web scraping project that utilizes BeautifulSoup and ChromeDriver ( heavily ) to extract data from websites? I'm particularly interested in suggestions for deployment tools, monitoring and error handling strategies, scalability considerations.\n\nInitially we had an approach to go with Cloud functions but due to its time out issue, we\u2019re looking at different alternatives. \n\nPlease help me out here. Open to suggestions.\n\nThanks!", "author_fullname": "t2_3qc9b4bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ML Model Deployment in GCP", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pok69", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699340222.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! I\u2019ve come across a certain requirement which I need some help with. My organisation intends to develop a web scrapping ML model for its internal use case. However this model needs to be deployed on GCP. &lt;/p&gt;\n\n&lt;p&gt;What are the best practices for deploying a web scraping project that utilizes BeautifulSoup and ChromeDriver ( heavily ) to extract data from websites? I&amp;#39;m particularly interested in suggestions for deployment tools, monitoring and error handling strategies, scalability considerations.&lt;/p&gt;\n\n&lt;p&gt;Initially we had an approach to go with Cloud functions but due to its time out issue, we\u2019re looking at different alternatives. &lt;/p&gt;\n\n&lt;p&gt;Please help me out here. Open to suggestions.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pok69", "is_robot_indexable": true, "report_reasons": null, "author": "apache444", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pok69/ml_model_deployment_in_gcp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pok69/ml_model_deployment_in_gcp/", "subreddit_subscribers": 138407, "created_utc": 1699340222.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says, I'm curious how you guys manage your redshift user access. I know one of the typical ways is just manually create users, and grant them to groups with command.\n\nContext: I'm using AWS and terraform\n\nMy current way (tested) is to list down the users in terraform (.tfvars) and \n\n1. Random generate password and store it in secret manager\n2. Import from secret manager and pass the password during redshift user creation\n3. Lambda function to invoke and grab the details from secret manager and send to their respective email\n\nI'm pretty sure there are better ways (google id?) because the list will grow larger and will be harder to maintain. Do share me how u guys do it", "author_fullname": "t2_88m111zn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift user management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pnw59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699337329.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says, I&amp;#39;m curious how you guys manage your redshift user access. I know one of the typical ways is just manually create users, and grant them to groups with command.&lt;/p&gt;\n\n&lt;p&gt;Context: I&amp;#39;m using AWS and terraform&lt;/p&gt;\n\n&lt;p&gt;My current way (tested) is to list down the users in terraform (.tfvars) and &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Random generate password and store it in secret manager&lt;/li&gt;\n&lt;li&gt;Import from secret manager and pass the password during redshift user creation&lt;/li&gt;\n&lt;li&gt;Lambda function to invoke and grab the details from secret manager and send to their respective email&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m pretty sure there are better ways (google id?) because the list will grow larger and will be harder to maintain. Do share me how u guys do it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pnw59", "is_robot_indexable": true, "report_reasons": null, "author": "shraderson97", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pnw59/redshift_user_management/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pnw59/redshift_user_management/", "subreddit_subscribers": 138407, "created_utc": 1699337329.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some of your favorite tools for making dbt Core development easier?", "author_fullname": "t2_esppz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are good tools for working with dbt Core?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pfavc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699310276.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some of your favorite tools for making dbt Core development easier?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17pfavc", "is_robot_indexable": true, "report_reasons": null, "author": "StartCompaniesNotWar", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pfavc/what_are_good_tools_for_working_with_dbt_core/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pfavc/what_are_good_tools_for_working_with_dbt_core/", "subreddit_subscribers": 138407, "created_utc": 1699310276.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "When working with platform as a service databases (such as Redshift) how do you keep developers from stepping on each other\u2019s toes in a shared dev space?\n\nIf our DW was in Postgres I\u2019d just give everyone a small, local copy of the db via Docker, but I don\u2019t know what the equivalent is in the PaaS database space. A separate db per developer on a shared cluster?\n\nAny suggestions?", "author_fullname": "t2_17k8yb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shared dev environments", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17q2c3e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699385581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When working with platform as a service databases (such as Redshift) how do you keep developers from stepping on each other\u2019s toes in a shared dev space?&lt;/p&gt;\n\n&lt;p&gt;If our DW was in Postgres I\u2019d just give everyone a small, local copy of the db via Docker, but I don\u2019t know what the equivalent is in the PaaS database space. A separate db per developer on a shared cluster?&lt;/p&gt;\n\n&lt;p&gt;Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17q2c3e", "is_robot_indexable": true, "report_reasons": null, "author": "demost11", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17q2c3e/shared_dev_environments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17q2c3e/shared_dev_environments/", "subreddit_subscribers": 138407, "created_utc": 1699385581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Need advice on an ingestion pattern to follow. Our company currently utilizes kafka for database replication against mssql and postgres. We are wanting to get the data from the mssql and postgres tables into databricks.\n\n**The company's recommended approach:**\n\nKafka topic streams changes from database into S3 -&gt; databricks DLT to ingest the data\n\nThis has proven to be a pain for us when initially trying to load the data, not every table has a UID, some are debezium packages, others are not. Its more complex, harder to troubleshoot, prone to failure, and not cheap.\n\n**Data eng approach:**\n\nSetup federated query against postgres and mssql to query the tables directly.\n\nWhat are some things to consider here, what are some pros and cons of each? With federated CDC coming out soon I see it being even more advantageous.", "author_fullname": "t2_81ywblydd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks: Federated Query vs Kafka cdc ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17q1vml", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699384373.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Need advice on an ingestion pattern to follow. Our company currently utilizes kafka for database replication against mssql and postgres. We are wanting to get the data from the mssql and postgres tables into databricks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The company&amp;#39;s recommended approach:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Kafka topic streams changes from database into S3 -&amp;gt; databricks DLT to ingest the data&lt;/p&gt;\n\n&lt;p&gt;This has proven to be a pain for us when initially trying to load the data, not every table has a UID, some are debezium packages, others are not. Its more complex, harder to troubleshoot, prone to failure, and not cheap.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Data eng approach:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Setup federated query against postgres and mssql to query the tables directly.&lt;/p&gt;\n\n&lt;p&gt;What are some things to consider here, what are some pros and cons of each? With federated CDC coming out soon I see it being even more advantageous.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17q1vml", "is_robot_indexable": true, "report_reasons": null, "author": "DataDoyle", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17q1vml/databricks_federated_query_vs_kafka_cdc_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17q1vml/databricks_federated_query_vs_kafka_cdc_ingestion/", "subreddit_subscribers": 138407, "created_utc": 1699384373.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm not sure this is within the rules but I'll try.\n\nI'm a high school student in a college-level course. We are having a career-choice assignment where we interview people in our future interests. One of mine is data engineering/science. So if anyone could answer like ten questions in dms that would be great. I don't need anything identifiable. I may not be able to get to questions in the next few hours but I'll try my best.", "author_fullname": "t2_a87umie9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Q/A interview for a school project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pzepa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699377918.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure this is within the rules but I&amp;#39;ll try.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a high school student in a college-level course. We are having a career-choice assignment where we interview people in our future interests. One of mine is data engineering/science. So if anyone could answer like ten questions in dms that would be great. I don&amp;#39;t need anything identifiable. I may not be able to get to questions in the next few hours but I&amp;#39;ll try my best.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17pzepa", "is_robot_indexable": true, "report_reasons": null, "author": "AssumptionNo5436", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pzepa/qa_interview_for_a_school_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pzepa/qa_interview_for_a_school_project/", "subreddit_subscribers": 138407, "created_utc": 1699377918.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I swear I've looked around and I just don't know what to do. I would like to host either a vue.js or react frontend app, with a small database (probably up to 10 gig), and a .net backend. I looked at azure, in fact I had an app hosted, but just the hosting itself cost me almost $100 with almost no interaction with the app whatsoever. I'm scared what costs I'd see if I actually had people use the app.\n\nThe database would ideally be relational, although I'll work with what I can find cheapest.\n\nI expect maybe up to 1000 users per month. Mostly just viewing data from the DB, occasionally pushing some. What are good choices for this?\n\nThanks for all your support and answers guys!\n\n ", "author_fullname": "t2_174q56", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hosting a small personal project - recommendations?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pzdzx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699377863.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I swear I&amp;#39;ve looked around and I just don&amp;#39;t know what to do. I would like to host either a vue.js or react frontend app, with a small database (probably up to 10 gig), and a .net backend. I looked at azure, in fact I had an app hosted, but just the hosting itself cost me almost $100 with almost no interaction with the app whatsoever. I&amp;#39;m scared what costs I&amp;#39;d see if I actually had people use the app.&lt;/p&gt;\n\n&lt;p&gt;The database would ideally be relational, although I&amp;#39;ll work with what I can find cheapest.&lt;/p&gt;\n\n&lt;p&gt;I expect maybe up to 1000 users per month. Mostly just viewing data from the DB, occasionally pushing some. What are good choices for this?&lt;/p&gt;\n\n&lt;p&gt;Thanks for all your support and answers guys!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pzdzx", "is_robot_indexable": true, "report_reasons": null, "author": "ziguslav", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pzdzx/hosting_a_small_personal_project_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pzdzx/hosting_a_small_personal_project_recommendations/", "subreddit_subscribers": 138407, "created_utc": 1699377863.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there a name or design pattern that describes what I'm trying to do here or resources I should consider?\n\nI support a team of data / business analysts and data scientists who require a lot of adhoc ELT work. While the DS can mostly manage on their own through PySpark or pandas, the analysts are mostly familiar with working in a OBT table designed for their BI work.\n\nWe have have a couple 'gold' tables structured as OBT for analytical use. Think like an EVENTS table and a CARS table. \n\nMaking an ELT pipeline is pretty onerous because of legacy systems and internal bureacracy, but our clients have a constant demand for random adhoc spreadsheets and datasets they want to be ELT'd into our data lake so they can access it in their analytical tools. \n\nAs a result, our data mostly sits in their original raw source format and if analysts want to include that in a chart or report they obviously have to first know it exists and second add it and clean it up in their viz tool. They then end up with unmaintainable charts or dashboards which SELECT * FROM dozens of different tables across various impala schemas. \n\nIs there some kind of data modelling pattern to describe predefining schemas for your gold tables, and when someone ELTs an ad-hoc datasets they label columns with metadata which tells the ELT job how to map that adhoc dataset to master schema  pattern?\n\nHere's a hypothetical scenario:\n\n- business users gets a giant spreadsheet dump from a client or stakeholder of cars for project WINKLE. User wants this ELT'd so that they get included in the master OBT table called MASTER-CARS. Users wants  this dataset to be included in their Tableau dashboards already connected to MASTER-CARS.\n\n- This master OBT for MASTER-CARS has columns such as MAKE, MODEL, YEAR etc. The WINKLE file has a table with similar containing similar data but with different column names, along with a few extra unique columns ones like HAS_LEATHER_SEATS?.\n\n- This scenario occurs every few weeks across different projects and teams. Our complaints about the adhoc nature of this falls on dead ears. \n\nRather than getting a data engineer to manually write a python script or SQL DML statement to update MASTER-OBT with each new dataset as it comes in, we'd like to streamline this with some sort of script or internal CLI or GUI that lets our users input to \"map\" the columns in the raw dataset to the MASTER-OBT schema. For example, the user can identify that the FD_MAKE column in winkle.csv should be mapped to the MAKE column in MASTER-OBT, and so on and so forth for each column. If so, how to handle special columns that can't be mapped 1:1? Remodel MASTER-OBT to include it, drop it or something else?\n\nWe want to be able to do this with different sets of adhoc datasets and master OBT tables in the same sort of workflow (user user identifies which column maps where)\n\nIs there a name for such a pattern? Ideally our team wouldn't be responsible for any of this, and the users would have some application or master ERP tool of some kind to normalize this data (salesforce or whatever) so that our data team could focus on creating stable pipelines and ELT jobs, but they don't.\n\nAnother team is looking to purchase some no-code / low-cost ETL tool for this but all of them are proprietary and I'm worries about our team getting stuck with supporting this lowcode ClickOps workflow that just patches over the lack of good data modelling. \n\n(If it's a bit of a pipedream to create an internal GUI tool for this kind of adhoc dataset CRUD then even an idea of a CLI pattern or example implementation for this for our engineers would be awesome).", "author_fullname": "t2_8r2mw2qe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark ELT pattern based on columnar metadata?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17pyjme", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699375635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a name or design pattern that describes what I&amp;#39;m trying to do here or resources I should consider?&lt;/p&gt;\n\n&lt;p&gt;I support a team of data / business analysts and data scientists who require a lot of adhoc ELT work. While the DS can mostly manage on their own through PySpark or pandas, the analysts are mostly familiar with working in a OBT table designed for their BI work.&lt;/p&gt;\n\n&lt;p&gt;We have have a couple &amp;#39;gold&amp;#39; tables structured as OBT for analytical use. Think like an EVENTS table and a CARS table. &lt;/p&gt;\n\n&lt;p&gt;Making an ELT pipeline is pretty onerous because of legacy systems and internal bureacracy, but our clients have a constant demand for random adhoc spreadsheets and datasets they want to be ELT&amp;#39;d into our data lake so they can access it in their analytical tools. &lt;/p&gt;\n\n&lt;p&gt;As a result, our data mostly sits in their original raw source format and if analysts want to include that in a chart or report they obviously have to first know it exists and second add it and clean it up in their viz tool. They then end up with unmaintainable charts or dashboards which SELECT * FROM dozens of different tables across various impala schemas. &lt;/p&gt;\n\n&lt;p&gt;Is there some kind of data modelling pattern to describe predefining schemas for your gold tables, and when someone ELTs an ad-hoc datasets they label columns with metadata which tells the ELT job how to map that adhoc dataset to master schema  pattern?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a hypothetical scenario:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;business users gets a giant spreadsheet dump from a client or stakeholder of cars for project WINKLE. User wants this ELT&amp;#39;d so that they get included in the master OBT table called MASTER-CARS. Users wants  this dataset to be included in their Tableau dashboards already connected to MASTER-CARS.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This master OBT for MASTER-CARS has columns such as MAKE, MODEL, YEAR etc. The WINKLE file has a table with similar containing similar data but with different column names, along with a few extra unique columns ones like HAS_LEATHER_SEATS?.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This scenario occurs every few weeks across different projects and teams. Our complaints about the adhoc nature of this falls on dead ears. &lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Rather than getting a data engineer to manually write a python script or SQL DML statement to update MASTER-OBT with each new dataset as it comes in, we&amp;#39;d like to streamline this with some sort of script or internal CLI or GUI that lets our users input to &amp;quot;map&amp;quot; the columns in the raw dataset to the MASTER-OBT schema. For example, the user can identify that the FD_MAKE column in winkle.csv should be mapped to the MAKE column in MASTER-OBT, and so on and so forth for each column. If so, how to handle special columns that can&amp;#39;t be mapped 1:1? Remodel MASTER-OBT to include it, drop it or something else?&lt;/p&gt;\n\n&lt;p&gt;We want to be able to do this with different sets of adhoc datasets and master OBT tables in the same sort of workflow (user user identifies which column maps where)&lt;/p&gt;\n\n&lt;p&gt;Is there a name for such a pattern? Ideally our team wouldn&amp;#39;t be responsible for any of this, and the users would have some application or master ERP tool of some kind to normalize this data (salesforce or whatever) so that our data team could focus on creating stable pipelines and ELT jobs, but they don&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;Another team is looking to purchase some no-code / low-cost ETL tool for this but all of them are proprietary and I&amp;#39;m worries about our team getting stuck with supporting this lowcode ClickOps workflow that just patches over the lack of good data modelling. &lt;/p&gt;\n\n&lt;p&gt;(If it&amp;#39;s a bit of a pipedream to create an internal GUI tool for this kind of adhoc dataset CRUD then even an idea of a CLI pattern or example implementation for this for our engineers would be awesome).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17pyjme", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableRoka", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17pyjme/pyspark_elt_pattern_based_on_columnar_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17pyjme/pyspark_elt_pattern_based_on_columnar_metadata/", "subreddit_subscribers": 138407, "created_utc": 1699375635.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}