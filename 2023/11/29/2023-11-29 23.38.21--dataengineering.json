{"kind": "Listing", "data": {"after": "t3_186guvp", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious about containerization and ETLs", "author_fullname": "t2_h5ydkpa8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are DEs using Docker containers for their ETLs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186fvwk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701229107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious about containerization and ETLs&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186fvwk", "is_robot_indexable": true, "report_reasons": null, "author": "SignificantWords", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186fvwk/how_are_des_using_docker_containers_for_their_etls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186fvwk/how_are_des_using_docker_containers_for_their_etls/", "subreddit_subscribers": 142617, "created_utc": 1701229107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am 27 years old, 3 YOE, stuck in a role at a bad company working 8-5 on site for $110K (20K pay cut from last gig). Toxic company culture as well and shoved in basement with IT support. Living at home and don\u2019t want to be in this town. No wife no kids no friends. \n\nOnly been 3 months and can\u2019t find another job especially since I was only at my last gig for 6 months. \nNot even doing DE work, stuck doing low code BI work with piece of shit software called Domo. \n\nDebating quitting and doing my MSDS full time and finishing next July. Have $40K saved up. \n\nAnyone else in a similar situation? How do you cope? Not sure how to get out of this one anytime soon, given my spotty job history and job market.", "author_fullname": "t2_dbas4m3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone Else Stuck in a Bad Job? How Do You Cope?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186aeh4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701214302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am 27 years old, 3 YOE, stuck in a role at a bad company working 8-5 on site for $110K (20K pay cut from last gig). Toxic company culture as well and shoved in basement with IT support. Living at home and don\u2019t want to be in this town. No wife no kids no friends. &lt;/p&gt;\n\n&lt;p&gt;Only been 3 months and can\u2019t find another job especially since I was only at my last gig for 6 months. \nNot even doing DE work, stuck doing low code BI work with piece of shit software called Domo. &lt;/p&gt;\n\n&lt;p&gt;Debating quitting and doing my MSDS full time and finishing next July. Have $40K saved up. &lt;/p&gt;\n\n&lt;p&gt;Anyone else in a similar situation? How do you cope? Not sure how to get out of this one anytime soon, given my spotty job history and job market.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "186aeh4", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Hyena4223", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186aeh4/anyone_else_stuck_in_a_bad_job_how_do_you_cope/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186aeh4/anyone_else_stuck_in_a_bad_job_how_do_you_cope/", "subreddit_subscribers": 142617, "created_utc": 1701214302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is my first job out of college and I\u2019m more of a data analyst with little experience in any data engineering.\n\nMy manager tasked me with finding ways to improve our team\u2019s data management practices.  We use one Excel workbook with 40+ sheets as our central hub for how we store, manage, and interact with our data critical to day-to-day operations.\n\nMost of our team uses this Excel file, oftentimes simultaneously, which causes mistaken data entries, conflicting filtering, and so on.  It has \u201cworked\u201d so far but continues to grow beyond its useful limit.\n\nMy issue is I need to find a solution that my team will accept and be able to continue to maintain should I leave. They do not have rich technical knowledge, so I have to be careful about developing too crazy of a solution.\n\nI\u2019m just totally stuck.  On one hand, it seems stupid to have all this data laying around in Excel.  On the other hand, I don\u2019t know how to find an acceptable solution that balances sophistication and user-friendliness for the \u201cbusiness users\u201d.\n\nOn top of that, I have to develop this all alone and self-guided.  This is one of my first projects and I don\u2019t want to fail.  Btw, this is a fortune 50 company in a highly regulated industry \ud83d\ude2d.\n\nAny suggestions would help greatly for my own sanity", "author_fullname": "t2_4qsy0xuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My team only uses Excel to manage all of our critical data and I\u2019m struggling to fix it", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186zm6c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701290804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is my first job out of college and I\u2019m more of a data analyst with little experience in any data engineering.&lt;/p&gt;\n\n&lt;p&gt;My manager tasked me with finding ways to improve our team\u2019s data management practices.  We use one Excel workbook with 40+ sheets as our central hub for how we store, manage, and interact with our data critical to day-to-day operations.&lt;/p&gt;\n\n&lt;p&gt;Most of our team uses this Excel file, oftentimes simultaneously, which causes mistaken data entries, conflicting filtering, and so on.  It has \u201cworked\u201d so far but continues to grow beyond its useful limit.&lt;/p&gt;\n\n&lt;p&gt;My issue is I need to find a solution that my team will accept and be able to continue to maintain should I leave. They do not have rich technical knowledge, so I have to be careful about developing too crazy of a solution.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m just totally stuck.  On one hand, it seems stupid to have all this data laying around in Excel.  On the other hand, I don\u2019t know how to find an acceptable solution that balances sophistication and user-friendliness for the \u201cbusiness users\u201d.&lt;/p&gt;\n\n&lt;p&gt;On top of that, I have to develop this all alone and self-guided.  This is one of my first projects and I don\u2019t want to fail.  Btw, this is a fortune 50 company in a highly regulated industry \ud83d\ude2d.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions would help greatly for my own sanity&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186zm6c", "is_robot_indexable": true, "report_reasons": null, "author": "CFCNandos", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186zm6c/my_team_only_uses_excel_to_manage_all_of_our/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186zm6c/my_team_only_uses_excel_to_manage_all_of_our/", "subreddit_subscribers": 142617, "created_utc": 1701290804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as an intern in a data consulting company. Often times, I get request to create PoC's on new technologies that I have never worked with. My usual workflow is to learn the basic concepts about the tech first, which usually takes me about 3 to 5 days and then start to implement the PoC.  \n\nIs this an acceptable practice?  If not, what do you suggest as a better approach. \n\nOne problem I felt is, I won't have anything to show up in the status call during the first few days.   ", "author_fullname": "t2_o78u2p44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "While working with new technologies, how much time do you spend on it without actually starting to implement anything?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186mheh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701254211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as an intern in a data consulting company. Often times, I get request to create PoC&amp;#39;s on new technologies that I have never worked with. My usual workflow is to learn the basic concepts about the tech first, which usually takes me about 3 to 5 days and then start to implement the PoC.  &lt;/p&gt;\n\n&lt;p&gt;Is this an acceptable practice?  If not, what do you suggest as a better approach. &lt;/p&gt;\n\n&lt;p&gt;One problem I felt is, I won&amp;#39;t have anything to show up in the status call during the first few days.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186mheh", "is_robot_indexable": true, "report_reasons": null, "author": "SignificanceNo136", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186mheh/while_working_with_new_technologies_how_much_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186mheh/while_working_with_new_technologies_how_much_time/", "subreddit_subscribers": 142617, "created_utc": 1701254211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have used Databricks and Snowflake in the past and learned about Dremio as well, but have never used it so far.\n\nI have a hard time to grasp the differences of the products when it comes to which is the best choice for which scenario.\n\nSo far I would say I prefer Snowflake when I need an easy to use and easy to setup Data Warehouse. It's especially useful if you want to invest all your time in getting value out of data and not invest so much time into operations and setup. Its great for integration in business intelligence solutions like Tableau.\n\nI like Databricks when the data is a bit more in volume and also more diversity in Data formats. The focus of Databricks is in using Spark to move and transform data in streams or batch, but when the main focus is machine learning, AI, but also BI.\n\nWhat about Dremio? What are you pros/cons or maybe use cases for the specific platforms?", "author_fullname": "t2_6n1qa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks vs. Snowflake vs. Dremio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186j31s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701239755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have used Databricks and Snowflake in the past and learned about Dremio as well, but have never used it so far.&lt;/p&gt;\n\n&lt;p&gt;I have a hard time to grasp the differences of the products when it comes to which is the best choice for which scenario.&lt;/p&gt;\n\n&lt;p&gt;So far I would say I prefer Snowflake when I need an easy to use and easy to setup Data Warehouse. It&amp;#39;s especially useful if you want to invest all your time in getting value out of data and not invest so much time into operations and setup. Its great for integration in business intelligence solutions like Tableau.&lt;/p&gt;\n\n&lt;p&gt;I like Databricks when the data is a bit more in volume and also more diversity in Data formats. The focus of Databricks is in using Spark to move and transform data in streams or batch, but when the main focus is machine learning, AI, but also BI.&lt;/p&gt;\n\n&lt;p&gt;What about Dremio? What are you pros/cons or maybe use cases for the specific platforms?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186j31s", "is_robot_indexable": true, "report_reasons": null, "author": "RonBurgundyIsBest", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186j31s/databricks_vs_snowflake_vs_dremio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186j31s/databricks_vs_snowflake_vs_dremio/", "subreddit_subscribers": 142617, "created_utc": 1701239755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One interesting thing I saw was a lot of zero-ETL features for Redshift (from Aurora, DynamoDB, others). A lot of Q stuff that I don't care about. \n\n[Updates](https://aws.amazon.com/new/?whats-new-content-all.sort-by=item.additionalFields.postDateTime&amp;whats-new-content-all.sort-order=desc&amp;awsf.whats-new-analytics=*all&amp;awsf.whats-new-app-integration=*all&amp;awsf.whats-new-arvr=*all&amp;awsf.whats-new-blockchain=*all&amp;awsf.whats-new-business-applications=*all&amp;awsf.whats-new-cloud-financial-management=*all&amp;awsf.whats-new-compute=*all&amp;awsf.whats-new-containers=*all&amp;awsf.whats-new-customer-enablement=*all&amp;awsf.whats-new-customer%20engagement=*all&amp;awsf.whats-new-database=general-products%23amazon-redshift&amp;awsf.whats-new-developer-tools=*all&amp;awsf.whats-new-end-user-computing=*all&amp;awsf.whats-new-mobile=*all&amp;awsf.whats-new-gametech=*all&amp;awsf.whats-new-iot=*all&amp;awsf.whats-new-machine-learning=*all&amp;awsf.whats-new-management-governance=*all&amp;awsf.whats-new-media-services=*all&amp;awsf.whats-new-migration-transfer=*all&amp;awsf.whats-new-networking-content-delivery=*all&amp;awsf.whats-new-quantum-tech=*all&amp;awsf.whats-new-robotics=*all&amp;awsf.whats-new-satellite=*all&amp;awsf.whats-new-security-id-compliance=*all&amp;awsf.whats-new-serverless=*all&amp;awsf.whats-new-storage=*all&amp;awsf.whats-new-categories=*all&amp;whats-new-content-all.q=redshift&amp;whats-new-content-all.q_operator=AND)\n\nNot sure if I missed anything cool. ", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any new updates from AWS re:Invent for data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186b12f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701215951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One interesting thing I saw was a lot of zero-ETL features for Redshift (from Aurora, DynamoDB, others). A lot of Q stuff that I don&amp;#39;t care about. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aws.amazon.com/new/?whats-new-content-all.sort-by=item.additionalFields.postDateTime&amp;amp;whats-new-content-all.sort-order=desc&amp;amp;awsf.whats-new-analytics=*all&amp;amp;awsf.whats-new-app-integration=*all&amp;amp;awsf.whats-new-arvr=*all&amp;amp;awsf.whats-new-blockchain=*all&amp;amp;awsf.whats-new-business-applications=*all&amp;amp;awsf.whats-new-cloud-financial-management=*all&amp;amp;awsf.whats-new-compute=*all&amp;amp;awsf.whats-new-containers=*all&amp;amp;awsf.whats-new-customer-enablement=*all&amp;amp;awsf.whats-new-customer%20engagement=*all&amp;amp;awsf.whats-new-database=general-products%23amazon-redshift&amp;amp;awsf.whats-new-developer-tools=*all&amp;amp;awsf.whats-new-end-user-computing=*all&amp;amp;awsf.whats-new-mobile=*all&amp;amp;awsf.whats-new-gametech=*all&amp;amp;awsf.whats-new-iot=*all&amp;amp;awsf.whats-new-machine-learning=*all&amp;amp;awsf.whats-new-management-governance=*all&amp;amp;awsf.whats-new-media-services=*all&amp;amp;awsf.whats-new-migration-transfer=*all&amp;amp;awsf.whats-new-networking-content-delivery=*all&amp;amp;awsf.whats-new-quantum-tech=*all&amp;amp;awsf.whats-new-robotics=*all&amp;amp;awsf.whats-new-satellite=*all&amp;amp;awsf.whats-new-security-id-compliance=*all&amp;amp;awsf.whats-new-serverless=*all&amp;amp;awsf.whats-new-storage=*all&amp;amp;awsf.whats-new-categories=*all&amp;amp;whats-new-content-all.q=redshift&amp;amp;whats-new-content-all.q_operator=AND\"&gt;Updates&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Not sure if I missed anything cool. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?auto=webp&amp;s=8afacfc14dfed09cec0415cac7d36db9c3374c61", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=64a1b1322ed94c559cb213e6a08f3eb426a3fb0b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9edddfdb28bb0e92ceb041859aacef81ab9ed42e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de73cdc9da2d0b04938bb7d051ab1a3ceb783323", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60037829d2ce04de0705a2b45123d8ab7c12d41c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5a8f0da08b9281c578a8ab6f49a5b3f577ec9b8", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a9a9c1c38bd543a7ea6b718e139a9c1e6b62d18", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186b12f", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186b12f/any_new_updates_from_aws_reinvent_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186b12f/any_new_updates_from_aws_reinvent_for_data/", "subreddit_subscribers": 142617, "created_utc": 1701215951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2cbhndmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Arryo 0.8 released \u2014 streaming SQL engine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_186x68q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Fjv2XXSBLmbhFzW7yYVR1PZG6vtcZ77epzFctFTrJE4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701284415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/ArroyoSystems/arroyo/releases/tag/v0.8.0", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1gPEuYyIQBUHxs39qiUy7C2eY5FLWHhPqfToK0u38Q8.jpg?auto=webp&amp;s=01ede26e42bd2c07a2f5c70c3dddb598757566b5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/1gPEuYyIQBUHxs39qiUy7C2eY5FLWHhPqfToK0u38Q8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e259ac5f440da9e763c879c2d0e057fa8425be0", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/1gPEuYyIQBUHxs39qiUy7C2eY5FLWHhPqfToK0u38Q8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ae2f24941b7c2024b09313a93f1893be04c62439", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/1gPEuYyIQBUHxs39qiUy7C2eY5FLWHhPqfToK0u38Q8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cd1138436c6e0f263fc7d4b9fa86ca743714be30", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/1gPEuYyIQBUHxs39qiUy7C2eY5FLWHhPqfToK0u38Q8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8357e3403c9f8b92fbe69dda5df7f6bfae7b2853", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/1gPEuYyIQBUHxs39qiUy7C2eY5FLWHhPqfToK0u38Q8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ec1b2c745d0b53730c9b1b5e21e3b6b1d243ce1d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/1gPEuYyIQBUHxs39qiUy7C2eY5FLWHhPqfToK0u38Q8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=70d03628bfb4f52fa630ffd1edc2b07a549c0b06", "width": 1080, "height": 540}], "variants": {}, "id": "2aLRr13c2_FncAnelBzURgZcAwUSR6UMk-WTxhKEg-0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "186x68q", "is_robot_indexable": true, "report_reasons": null, "author": "mwylde_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186x68q/arryo_08_released_streaming_sql_engine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/ArroyoSystems/arroyo/releases/tag/v0.8.0", "subreddit_subscribers": 142617, "created_utc": 1701284415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which data ingestion tool do you use?\n\nLooking to use a managed service like [Airbyte Cloud](https://airbyte.com/product/airbyte-cloud), [Rivery.io](https://Rivery.io), or [Google Cloud Data Fusion](https://cloud.google.com/data-fusion?hl=en) to simply extract data from common third party apps like Salesforce, Google Analytics, Google Ads, etc and drop them to Cloud Storage + BigQuery.\n\nWondering if Data Fusion is the way to go? Has anyone used it? Does it make sense just to use for data ingestion alone? Not going to use it for data transformations (looking to use Dataform/dbt for that).", "author_fullname": "t2_4gzaf8mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which data ingestion tool do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186m9gb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701253296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which data ingestion tool do you use?&lt;/p&gt;\n\n&lt;p&gt;Looking to use a managed service like &lt;a href=\"https://airbyte.com/product/airbyte-cloud\"&gt;Airbyte Cloud&lt;/a&gt;, &lt;a href=\"https://Rivery.io\"&gt;Rivery.io&lt;/a&gt;, or &lt;a href=\"https://cloud.google.com/data-fusion?hl=en\"&gt;Google Cloud Data Fusion&lt;/a&gt; to simply extract data from common third party apps like Salesforce, Google Analytics, Google Ads, etc and drop them to Cloud Storage + BigQuery.&lt;/p&gt;\n\n&lt;p&gt;Wondering if Data Fusion is the way to go? Has anyone used it? Does it make sense just to use for data ingestion alone? Not going to use it for data transformations (looking to use Dataform/dbt for that).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186m9gb", "is_robot_indexable": true, "report_reasons": null, "author": "theoriginalmantooth", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186m9gb/which_data_ingestion_tool_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186m9gb/which_data_ingestion_tool_do_you_use/", "subreddit_subscribers": 142617, "created_utc": 1701253296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow DEs,\n\nI currently work as a SWE with almost 2 YOE, I've always been interested in the field of data and came across the concept of DE when researching about different job opportunities. I wanted to stay heavy coding (which rules out DA) and DE seems to be the right fit.\n\nHowever, as I read more and more posts in the sub, I realise a huge portion of DE's responsibilities involve DevOps as well. Setting up environments and configuring has always been my No.1 pet peeve in the world of software development. I didn't need to do much of it in my role of SWE as it's mostly handled by other people in the same company.\n\nAn example would be, me trying to setup Airflow + DuckDB and etc. in docker for experimentation. I am instantly mind-boggled by the configuration process (e.g., how to write a proper YAML to initialise these components). \n\nAll in all, I just want to get some insights from someone already in the field, to know how well I would do if I have an inherent resentment towards doing DevOps. Also, what would be the percentages be like for Python, SQL and DevOps in general?", "author_fullname": "t2_5tffcx87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for someone looking to transition from SWE to DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186f796", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701227216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow DEs,&lt;/p&gt;\n\n&lt;p&gt;I currently work as a SWE with almost 2 YOE, I&amp;#39;ve always been interested in the field of data and came across the concept of DE when researching about different job opportunities. I wanted to stay heavy coding (which rules out DA) and DE seems to be the right fit.&lt;/p&gt;\n\n&lt;p&gt;However, as I read more and more posts in the sub, I realise a huge portion of DE&amp;#39;s responsibilities involve DevOps as well. Setting up environments and configuring has always been my No.1 pet peeve in the world of software development. I didn&amp;#39;t need to do much of it in my role of SWE as it&amp;#39;s mostly handled by other people in the same company.&lt;/p&gt;\n\n&lt;p&gt;An example would be, me trying to setup Airflow + DuckDB and etc. in docker for experimentation. I am instantly mind-boggled by the configuration process (e.g., how to write a proper YAML to initialise these components). &lt;/p&gt;\n\n&lt;p&gt;All in all, I just want to get some insights from someone already in the field, to know how well I would do if I have an inherent resentment towards doing DevOps. Also, what would be the percentages be like for Python, SQL and DevOps in general?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "186f796", "is_robot_indexable": true, "report_reasons": null, "author": "Psychological_Ad9582", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186f796/advice_for_someone_looking_to_transition_from_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186f796/advice_for_someone_looking_to_transition_from_swe/", "subreddit_subscribers": 142617, "created_utc": 1701227216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working with an external ( PM from the client side) project manager who expects every change to be completed in 24 hours regardless of the complexity or competing commitments. Truly, a difficult individual. I'm wondering has anyone else had a similar experience, and how did you navigate working with these individual(s) ? ", "author_fullname": "t2_vjyooqku", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Project Manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186z4c9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701289526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working with an external ( PM from the client side) project manager who expects every change to be completed in 24 hours regardless of the complexity or competing commitments. Truly, a difficult individual. I&amp;#39;m wondering has anyone else had a similar experience, and how did you navigate working with these individual(s) ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186z4c9", "is_robot_indexable": true, "report_reasons": null, "author": "CharmingZucchini6500", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186z4c9/project_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186z4c9/project_manager/", "subreddit_subscribers": 142617, "created_utc": 1701289526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all! \n\nGoing to be frank, I am not entirely sure why I make this post. I guess I am looking for advice, I regretfully don\u2019t have many DE friends, or colleagues, with whom to voice some of these things, so maybe this is half rant, half advice. \n\nI am a data \u201cprofessional\u201d, but I have become one not by the traditional CS major way, but through humanities - first as a data analyst, then as a researcher / data scientist, and now I have been in Data management in an NGO. I am not senior, I am a junior, but I feel way out of place. I have learnt things not through certifications or classes (although I have done my fair share of courses), but by doing and deploying.  Because of that, I feel I could have a shot for a more senior position\u2026 or at least, not a junior one. But, I often feel not up to the level of job requirements,  and that makes me feel \u201chopeless\u201d where to go. I feel it is the degree that hurts me the most, as DE positions often times require CS degrees. I am trying to fix some of that, and now I am preparing for the CKA and KCNA\u2026 I guess that what I am asking is, **can anyone share their experiences on moving from humanities and public/academic sector, towards data engineering ideally in the private sector? **\n\nTo give an idea, or context on what I have done, or have experience:\n- I have deployed our airflow server for my team, deployed though Docker in a cloud environment for now, and I am now tinkering with K8s to deploy it in the future. \n- Created a bunch of ETL pipelines in R &amp; python \n- Optimized scripts from other colleagues to reduce compute time and resources. \n- Setup our internal GitHub workflows, including setting up actions and protocols on how to do code reviews, organize repos, and tought best practices for Git. \n- Lead point for troubleshooting issues in our cloud environment (mostly with Bash scripting)\n- Created scrappers for data mining\n- Built packages in different languages. \n\u2026 and now I\u2019m just working in setting up more efficient data engineering practices and systems. I am also doing IBM\u2019s Data engineering certification, which means I have exposure to other things like MongoDB, Kafka, and others, but I have not been able to apply them in practice yet.\n\n\ud83e\udd37\u200d\u2642\ufe0f perhaps you feel that these things are not enough, or just okay for a junior DE. If you think that, also feel free to mention it, it will help me get a more accurate understanding of where I am at.", "author_fullname": "t2_o5sdswgng", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for some advice for a non-traditional DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186z3tr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701289487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all! &lt;/p&gt;\n\n&lt;p&gt;Going to be frank, I am not entirely sure why I make this post. I guess I am looking for advice, I regretfully don\u2019t have many DE friends, or colleagues, with whom to voice some of these things, so maybe this is half rant, half advice. &lt;/p&gt;\n\n&lt;p&gt;I am a data \u201cprofessional\u201d, but I have become one not by the traditional CS major way, but through humanities - first as a data analyst, then as a researcher / data scientist, and now I have been in Data management in an NGO. I am not senior, I am a junior, but I feel way out of place. I have learnt things not through certifications or classes (although I have done my fair share of courses), but by doing and deploying.  Because of that, I feel I could have a shot for a more senior position\u2026 or at least, not a junior one. But, I often feel not up to the level of job requirements,  and that makes me feel \u201chopeless\u201d where to go. I feel it is the degree that hurts me the most, as DE positions often times require CS degrees. I am trying to fix some of that, and now I am preparing for the CKA and KCNA\u2026 I guess that what I am asking is, *&lt;em&gt;can anyone share their experiences on moving from humanities and public/academic sector, towards data engineering ideally in the private sector? *&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;To give an idea, or context on what I have done, or have experience:\n- I have deployed our airflow server for my team, deployed though Docker in a cloud environment for now, and I am now tinkering with K8s to deploy it in the future. \n- Created a bunch of ETL pipelines in R &amp;amp; python \n- Optimized scripts from other colleagues to reduce compute time and resources. \n- Setup our internal GitHub workflows, including setting up actions and protocols on how to do code reviews, organize repos, and tought best practices for Git. \n- Lead point for troubleshooting issues in our cloud environment (mostly with Bash scripting)\n- Created scrappers for data mining\n- Built packages in different languages. \n\u2026 and now I\u2019m just working in setting up more efficient data engineering practices and systems. I am also doing IBM\u2019s Data engineering certification, which means I have exposure to other things like MongoDB, Kafka, and others, but I have not been able to apply them in practice yet.&lt;/p&gt;\n\n&lt;p&gt;\ud83e\udd37\u200d\u2642\ufe0f perhaps you feel that these things are not enough, or just okay for a junior DE. If you think that, also feel free to mention it, it will help me get a more accurate understanding of where I am at.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "186z3tr", "is_robot_indexable": true, "report_reasons": null, "author": "TostGushMuts", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186z3tr/looking_for_some_advice_for_a_nontraditional_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186z3tr/looking_for_some_advice_for_a_nontraditional_de/", "subreddit_subscribers": 142617, "created_utc": 1701289487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "HI all, Reaching out to all  great data leaders like yourselves for some suggestions.\n\nI am basically working for an industry where tech is part of the  organisation and we build data product solutions for its functions.\n\nI basically wanted to understand the industry standard for opex and capex costing. In order to differentiate the manpower costs and the cloud infra costs, what would be the best approach to classify them into Capex and opex? Usually are the manpower resources classified under Capex and the cloud infra under Opex or is there any other standard to have this classified?\n\nThank you.", "author_fullname": "t2_p5md1ho8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Require some clarification on Capex and Opex costing for Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186xnt2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701285680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;HI all, Reaching out to all  great data leaders like yourselves for some suggestions.&lt;/p&gt;\n\n&lt;p&gt;I am basically working for an industry where tech is part of the  organisation and we build data product solutions for its functions.&lt;/p&gt;\n\n&lt;p&gt;I basically wanted to understand the industry standard for opex and capex costing. In order to differentiate the manpower costs and the cloud infra costs, what would be the best approach to classify them into Capex and opex? Usually are the manpower resources classified under Capex and the cloud infra under Opex or is there any other standard to have this classified?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "186xnt2", "is_robot_indexable": true, "report_reasons": null, "author": "Repulsive_Zombie_587", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186xnt2/require_some_clarification_on_capex_and_opex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186xnt2/require_some_clarification_on_capex_and_opex/", "subreddit_subscribers": 142617, "created_utc": 1701285680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I've been Data Engineer working for one company since the very beginning of my career and I've never worked with any orchestrator like Airflow or Prefect due to in-house developed tool responsible for execution and orchestration of pipelines. Long story short, it' is a tool that utilizes Airflow as a  orchestrator in the background (but I don't need to touch it to orchestrate anything, I set it up in UI), the tool generates DAGs for us with a custom executor that spins up a docker on AWS Batch compute environment and makes all the magic. It's super easy and simple to use, it's like a framework that generates you some set of files and directories needed to work and gives you a space to write your own code, later you deploy to docker repository it with some simple commend and it's done.\n\nMy question is - How do you do stuff like that with Airflow / Prefect?Let's assume you have a pipeline like this:\n\n1. Start some cluster\n2. Pull the data from cluster, make several transformation to that data + running some algorithms on it and load it to some storage like S3 (this is super often build with several python modules to well organize the code)\n3. Run some dbt job on it\n4. Close cluster\n\nFor me each of these steps is a separate script/repository/docker, which keeps everything clean and simple to use. How do you organize your jobs/scripts? Is it laying somewhere outside of airflow/prefect or you just have mono repo for airflow/prefect and there in some directories you're storing your work projects and next you're creating dags to run them? I'm interested in code organization, cause this approach of adding everything to airflow seems to be for me like a collaboration nightmare.", "author_fullname": "t2_3icz5xxl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you do it in Airflow / Prefect and avoid mess?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186w9jj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701282175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I&amp;#39;ve been Data Engineer working for one company since the very beginning of my career and I&amp;#39;ve never worked with any orchestrator like Airflow or Prefect due to in-house developed tool responsible for execution and orchestration of pipelines. Long story short, it&amp;#39; is a tool that utilizes Airflow as a  orchestrator in the background (but I don&amp;#39;t need to touch it to orchestrate anything, I set it up in UI), the tool generates DAGs for us with a custom executor that spins up a docker on AWS Batch compute environment and makes all the magic. It&amp;#39;s super easy and simple to use, it&amp;#39;s like a framework that generates you some set of files and directories needed to work and gives you a space to write your own code, later you deploy to docker repository it with some simple commend and it&amp;#39;s done.&lt;/p&gt;\n\n&lt;p&gt;My question is - How do you do stuff like that with Airflow / Prefect?Let&amp;#39;s assume you have a pipeline like this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Start some cluster&lt;/li&gt;\n&lt;li&gt;Pull the data from cluster, make several transformation to that data + running some algorithms on it and load it to some storage like S3 (this is super often build with several python modules to well organize the code)&lt;/li&gt;\n&lt;li&gt;Run some dbt job on it&lt;/li&gt;\n&lt;li&gt;Close cluster&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For me each of these steps is a separate script/repository/docker, which keeps everything clean and simple to use. How do you organize your jobs/scripts? Is it laying somewhere outside of airflow/prefect or you just have mono repo for airflow/prefect and there in some directories you&amp;#39;re storing your work projects and next you&amp;#39;re creating dags to run them? I&amp;#39;m interested in code organization, cause this approach of adding everything to airflow seems to be for me like a collaboration nightmare.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186w9jj", "is_robot_indexable": true, "report_reasons": null, "author": "thethewaza", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186w9jj/how_do_you_do_it_in_airflow_prefect_and_avoid_mess/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186w9jj/how_do_you_do_it_in_airflow_prefect_and_avoid_mess/", "subreddit_subscribers": 142617, "created_utc": 1701282175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi All\n\nI am new to AWS and am trying to figure out the best way to deploy a script to the cloud that runs hourly doing the following:\n\nPull data from a database Pull data from s3 bucket Use logic to update records from DB with data from s3 Update redshift table with new data\n\nAfter investigating, I thought that using lambda to complete this would work, but I need static ips to be whitelisted in order to access both databases. Lambda doesn\u2019t have an IP associated and I\u2019m worried about the cost of incorporating a VPC\n\nis there a better way/tool to complete this task?", "author_fullname": "t2_edr3jh4p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying script that connects to DB, S3, and Redshift in AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186twtj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701276390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All&lt;/p&gt;\n\n&lt;p&gt;I am new to AWS and am trying to figure out the best way to deploy a script to the cloud that runs hourly doing the following:&lt;/p&gt;\n\n&lt;p&gt;Pull data from a database Pull data from s3 bucket Use logic to update records from DB with data from s3 Update redshift table with new data&lt;/p&gt;\n\n&lt;p&gt;After investigating, I thought that using lambda to complete this would work, but I need static ips to be whitelisted in order to access both databases. Lambda doesn\u2019t have an IP associated and I\u2019m worried about the cost of incorporating a VPC&lt;/p&gt;\n\n&lt;p&gt;is there a better way/tool to complete this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "186twtj", "is_robot_indexable": true, "report_reasons": null, "author": "babababooskio", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186twtj/deploying_script_that_connects_to_db_s3_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186twtj/deploying_script_that_connects_to_db_s3_and/", "subreddit_subscribers": 142617, "created_utc": 1701276390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working on implementing a SAAS MDM solution for a client to replace their legacy database. They are insisting that I provide a logical data model as part of the deliverables but I have never been asked this before. \n\nFrom what I understand, things like LDM and the core database design and architecture is done by whatever company created the SAAS solution and putting a LDM together as someone who is implementing that solution will just be a waste of time. \n\nAnyone have any experience in this? Feel like I am being asked to go backwards here and wasting my time.", "author_fullname": "t2_kmqkk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating a Logical Data Model for a SAAS MDM Implementation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1872nvu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701298601.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on implementing a SAAS MDM solution for a client to replace their legacy database. They are insisting that I provide a logical data model as part of the deliverables but I have never been asked this before. &lt;/p&gt;\n\n&lt;p&gt;From what I understand, things like LDM and the core database design and architecture is done by whatever company created the SAAS solution and putting a LDM together as someone who is implementing that solution will just be a waste of time. &lt;/p&gt;\n\n&lt;p&gt;Anyone have any experience in this? Feel like I am being asked to go backwards here and wasting my time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1872nvu", "is_robot_indexable": true, "report_reasons": null, "author": "aj_ghimire", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1872nvu/creating_a_logical_data_model_for_a_saas_mdm/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1872nvu/creating_a_logical_data_model_for_a_saas_mdm/", "subreddit_subscribers": 142617, "created_utc": 1701298601.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. I am facing a new project and any feedback would be appreciated.\n\nThe project requires to take all data from standard relational databases (multiple origen) and replicate it into a modern data lake structure (delta lakehouse), that will later be used for data warehousing and analytics. The landing/bronze zone of the data lake will be just the same tables of the multiple relational databases replicated.\n\nAs a first step I outlined the need to Change Data Capture in the relational database. The clients wants data to be injected into the Data Lake frequently. \n\nI noticed the following issues:\n\n1.- Data in the relational database could not have appropiated _updatedAt, _createdAt fields. This would trick the \"manual\" CDC approach that I first thought about.\n\n2.- Not sure yet what techonologies the relational database is build on (there are multiple and different sources). Some of them are good for CDC, some of them could be difficult to track CDC due to missings logs (I read Postgres is difficult).\n\n3.- Small tables such as dimensional tables can be just fully checked with a hash function and fully overwrited if need\n\n4.- Big tables (fact tables mostly) can be very tricky if they admit updates/deletes and if they don't have timestamps nor incremental keys\n\n5.- Business/Management mostly think this is a trivial task as it is only moving data from a database (relational databases such as OracleDB, postgres...) to another database (delta lake). I agree if full overwrite could be possible for each batch update, but this most likely won't be the case, database is huge with hundred of tables (size of tables will be discovered later).\n\nWe are working with AWS+databricks stack. Any suggestion would be very welcomed. I am also exploring Amazon DMS and AWS Glue. Most of my doubts are related to just having the raw data in the delta lakehouse (databricks) as this is the area that I have less expertise.", "author_fullname": "t2_247d5tdk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Relational DB to Data Warehouse: Change Data Capture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18724ti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701297241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I am facing a new project and any feedback would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;The project requires to take all data from standard relational databases (multiple origen) and replicate it into a modern data lake structure (delta lakehouse), that will later be used for data warehousing and analytics. The landing/bronze zone of the data lake will be just the same tables of the multiple relational databases replicated.&lt;/p&gt;\n\n&lt;p&gt;As a first step I outlined the need to Change Data Capture in the relational database. The clients wants data to be injected into the Data Lake frequently. &lt;/p&gt;\n\n&lt;p&gt;I noticed the following issues:&lt;/p&gt;\n\n&lt;p&gt;1.- Data in the relational database could not have appropiated _updatedAt, _createdAt fields. This would trick the &amp;quot;manual&amp;quot; CDC approach that I first thought about.&lt;/p&gt;\n\n&lt;p&gt;2.- Not sure yet what techonologies the relational database is build on (there are multiple and different sources). Some of them are good for CDC, some of them could be difficult to track CDC due to missings logs (I read Postgres is difficult).&lt;/p&gt;\n\n&lt;p&gt;3.- Small tables such as dimensional tables can be just fully checked with a hash function and fully overwrited if need&lt;/p&gt;\n\n&lt;p&gt;4.- Big tables (fact tables mostly) can be very tricky if they admit updates/deletes and if they don&amp;#39;t have timestamps nor incremental keys&lt;/p&gt;\n\n&lt;p&gt;5.- Business/Management mostly think this is a trivial task as it is only moving data from a database (relational databases such as OracleDB, postgres...) to another database (delta lake). I agree if full overwrite could be possible for each batch update, but this most likely won&amp;#39;t be the case, database is huge with hundred of tables (size of tables will be discovered later).&lt;/p&gt;\n\n&lt;p&gt;We are working with AWS+databricks stack. Any suggestion would be very welcomed. I am also exploring Amazon DMS and AWS Glue. Most of my doubts are related to just having the raw data in the delta lakehouse (databricks) as this is the area that I have less expertise.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18724ti", "is_robot_indexable": true, "report_reasons": null, "author": "CardGameFanboy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18724ti/relational_db_to_data_warehouse_change_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18724ti/relational_db_to_data_warehouse_change_data/", "subreddit_subscribers": 142617, "created_utc": 1701297241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone,\n\nI\u2019m currently working on a personal project and was considering this while thinking on how to properly model the data. I have a set of raw data derived from CSV files containing insurance claims that I want to model in my data warehouse. Should the first step be to normalize the data beforehand and then convert into a star schema, or is it acceptable to model it as a star directly? Has anyone ever had this problem come up, and if so could you give your input?", "author_fullname": "t2_3iljgzjo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should you normalize a set of CSV files before denormalizing to a star schema?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186yhcn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701287834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m currently working on a personal project and was considering this while thinking on how to properly model the data. I have a set of raw data derived from CSV files containing insurance claims that I want to model in my data warehouse. Should the first step be to normalize the data beforehand and then convert into a star schema, or is it acceptable to model it as a star directly? Has anyone ever had this problem come up, and if so could you give your input?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186yhcn", "is_robot_indexable": true, "report_reasons": null, "author": "bass581", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186yhcn/should_you_normalize_a_set_of_csv_files_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186yhcn/should_you_normalize_a_set_of_csv_files_before/", "subreddit_subscribers": 142617, "created_utc": 1701287834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What kind of data storage is best in this case?\n\nI started working for a farm business that gets information about their cows everyday to a sharepoint in an excel file \n. Everyday it's new 1500 rows and 25 column that are generated and it needs to be kept somewhere. Now it is kept in one excel file but it's getting too big and we want something a bit faster and also want to automize it using power automate so it could upload the file to  dataverse, sharepointlist or some other place add date column when the data was added and then delete the  file it took the data from. What would be the choice for us? It is just one table with relationships that only append it self everytime and then we read that information with power BI and make some calculations and visualizations there.", "author_fullname": "t2_q9hhtqt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best data solution for my problem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186y8pw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701287191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What kind of data storage is best in this case?&lt;/p&gt;\n\n&lt;p&gt;I started working for a farm business that gets information about their cows everyday to a sharepoint in an excel file \n. Everyday it&amp;#39;s new 1500 rows and 25 column that are generated and it needs to be kept somewhere. Now it is kept in one excel file but it&amp;#39;s getting too big and we want something a bit faster and also want to automize it using power automate so it could upload the file to  dataverse, sharepointlist or some other place add date column when the data was added and then delete the  file it took the data from. What would be the choice for us? It is just one table with relationships that only append it self everytime and then we read that information with power BI and make some calculations and visualizations there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "186y8pw", "is_robot_indexable": true, "report_reasons": null, "author": "Mr-Nyan", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186y8pw/whats_the_best_data_solution_for_my_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186y8pw/whats_the_best_data_solution_for_my_problem/", "subreddit_subscribers": 142617, "created_utc": 1701287191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Company considering migrating the cloud provider for their Snowflake account and wants a list of considerations. I found this [article](https://community.snowflake.com/s/article/Replication-How-to-migrate-an-account-from-One-Cloud-Platform-or-Region-to-another-in-Snowflake) that proposes the use of replication, which seems reasonable. According to Snowflake [documentation](https://docs.snowflake.com/en/user-guide/account-replication-intro) a whole lot more than databases can be replicated, such as roles, users etc. The current account uses Terraform to define users, roles and development schemes. \n\nDoes anyone with Terraform knowledge know if this type of replication strategy could cause issues down the line? Would it be better to just let Terraform setup the resources on the new account and then only replicate the data? It\u2019d be neat to replicate everything but I think it could cause issues since terraform did not directly setup the resources and it\u2019s now on another account.", "author_fullname": "t2_kaoxu3a5p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrating Snowflake cloud provider", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186xmgj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701285582.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Company considering migrating the cloud provider for their Snowflake account and wants a list of considerations. I found this &lt;a href=\"https://community.snowflake.com/s/article/Replication-How-to-migrate-an-account-from-One-Cloud-Platform-or-Region-to-another-in-Snowflake\"&gt;article&lt;/a&gt; that proposes the use of replication, which seems reasonable. According to Snowflake &lt;a href=\"https://docs.snowflake.com/en/user-guide/account-replication-intro\"&gt;documentation&lt;/a&gt; a whole lot more than databases can be replicated, such as roles, users etc. The current account uses Terraform to define users, roles and development schemes. &lt;/p&gt;\n\n&lt;p&gt;Does anyone with Terraform knowledge know if this type of replication strategy could cause issues down the line? Would it be better to just let Terraform setup the resources on the new account and then only replicate the data? It\u2019d be neat to replicate everything but I think it could cause issues since terraform did not directly setup the resources and it\u2019s now on another account.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/M0dRXoXEkohwpnF2qpsa4lsnfF7Lh7G1y47ZxywdXl8.jpg?auto=webp&amp;s=450d1618266b0b1da387799bf8ffdbebe5550ac8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/M0dRXoXEkohwpnF2qpsa4lsnfF7Lh7G1y47ZxywdXl8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7be215603b994e87f54ecd3bec5eb48aa55a6bc5", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/M0dRXoXEkohwpnF2qpsa4lsnfF7Lh7G1y47ZxywdXl8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=073bdd50783dc1a9257535f9502b984df39b4b9b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/M0dRXoXEkohwpnF2qpsa4lsnfF7Lh7G1y47ZxywdXl8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5c34e3068a64d34c69635c9dc0ea14cdb636ed4f", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/M0dRXoXEkohwpnF2qpsa4lsnfF7Lh7G1y47ZxywdXl8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=44d3401d730f5e785d561f608689c0c1d342c168", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/M0dRXoXEkohwpnF2qpsa4lsnfF7Lh7G1y47ZxywdXl8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=16c97a18f3f765629eca3103cc56ea5792ad5d4d", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/M0dRXoXEkohwpnF2qpsa4lsnfF7Lh7G1y47ZxywdXl8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77c5b13d9fe017ad47492370c83f481108db18bd", "width": 1080, "height": 567}], "variants": {}, "id": "LXCpfRrF1ITO-giMLk_A7S06VNUKxytDA7rza-1KrTQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "186xmgj", "is_robot_indexable": true, "report_reasons": null, "author": "Euphoric-Clue-4973", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186xmgj/migrating_snowflake_cloud_provider/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186xmgj/migrating_snowflake_cloud_provider/", "subreddit_subscribers": 142617, "created_utc": 1701285582.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cross-post from Snowflake\n\nWe  are currently working on ingestion patters, the discussion is between  using COPY INTO using an internal named stage and the BULK INSERT using  Snowflake JDBC drive.\n\nMy thought  is COPY INTO is faster (based on some experience) but I do not have any  metric to support this. Do you guys have any thoughts on this? Which  approach should we take and why? ", "author_fullname": "t2_dm547n57", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comparison between JDBC BULK INSERT and COPY INTO", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186w2d0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701281680.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cross-post from Snowflake&lt;/p&gt;\n\n&lt;p&gt;We  are currently working on ingestion patters, the discussion is between  using COPY INTO using an internal named stage and the BULK INSERT using  Snowflake JDBC drive.&lt;/p&gt;\n\n&lt;p&gt;My thought  is COPY INTO is faster (based on some experience) but I do not have any  metric to support this. Do you guys have any thoughts on this? Which  approach should we take and why? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186w2d0", "is_robot_indexable": true, "report_reasons": null, "author": "RamBharose984", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186w2d0/comparison_between_jdbc_bulk_insert_and_copy_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186w2d0/comparison_between_jdbc_bulk_insert_and_copy_into/", "subreddit_subscribers": 142617, "created_utc": 1701281680.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI happen to have an access to the book by Lindstedt and Olschimke. I noticed it goes through the fundamentals of DWs and DBs. \n\nMy question is, is it going to be biased or does it work as a solid material for these other concepts too? \n\nIf not, what would be better? \"Database Internals\" by Petrov  is another one I know of, but it mentioned distributed DBs. \n\nThanks!", "author_fullname": "t2_64n7xdby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Vault book in learning fundamentals of DBs or DWs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186rbru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701269876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I happen to have an access to the book by Lindstedt and Olschimke. I noticed it goes through the fundamentals of DWs and DBs. &lt;/p&gt;\n\n&lt;p&gt;My question is, is it going to be biased or does it work as a solid material for these other concepts too? &lt;/p&gt;\n\n&lt;p&gt;If not, what would be better? &amp;quot;Database Internals&amp;quot; by Petrov  is another one I know of, but it mentioned distributed DBs. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "186rbru", "is_robot_indexable": true, "report_reasons": null, "author": "Hyvahar", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186rbru/data_vault_book_in_learning_fundamentals_of_dbs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186rbru/data_vault_book_in_learning_fundamentals_of_dbs/", "subreddit_subscribers": 142617, "created_utc": 1701269876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI am looking some simple solution for data pipeline from MySQL to PostgrSql. I want to move all data without any processing i.e. all tables data in MySQL to Postgresql.\n\n&amp;#x200B;\n\nIs there any simple way? Like in postgresql to postgresql there is pub-sub solution.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4uylr4su", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline between MySQL to PostgeSQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186oidj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701261726.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I am looking some simple solution for data pipeline from MySQL to PostgrSql. I want to move all data without any processing i.e. all tables data in MySQL to Postgresql.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there any simple way? Like in postgresql to postgresql there is pub-sub solution.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186oidj", "is_robot_indexable": true, "report_reasons": null, "author": "harshmah", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186oidj/data_pipeline_between_mysql_to_postgesql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186oidj/data_pipeline_between_mysql_to_postgesql/", "subreddit_subscribers": 142617, "created_utc": 1701261726.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\n I got my master's in data science and machine learning, coming from a computer engineering background. I thought I would get into data science job since it was a hot topic, but my job proposals were pretty much for data engineering.   \nI decided to accept a data engineering job, which was actually a database administration with cloud infrastructure. I left that job and now am a data engineer working with mainly azure data factory and synapse for a small company.   \nI read the book \"Fundamentals of data engineering\" and got 2 more books, \"The data warehouse toolkit\" and \"Learning Spark\". I also wanted to develop a personal project but have no idea where to start.  \nMy questions are: Is data engineering still a good career option? Are technologies like spark, airflow and kafka a must have in this field, and should I try to learn them, or can I just pick them up during my career? Also, I am curious why do you guys think my profile was targeted for data engineering jobs?", "author_fullname": "t2_scb32bzm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Were my career choices correct and what steps should I take?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186npr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701258968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I got my master&amp;#39;s in data science and machine learning, coming from a computer engineering background. I thought I would get into data science job since it was a hot topic, but my job proposals were pretty much for data engineering.&lt;br/&gt;\nI decided to accept a data engineering job, which was actually a database administration with cloud infrastructure. I left that job and now am a data engineer working with mainly azure data factory and synapse for a small company.&lt;br/&gt;\nI read the book &amp;quot;Fundamentals of data engineering&amp;quot; and got 2 more books, &amp;quot;The data warehouse toolkit&amp;quot; and &amp;quot;Learning Spark&amp;quot;. I also wanted to develop a personal project but have no idea where to start.&lt;br/&gt;\nMy questions are: Is data engineering still a good career option? Are technologies like spark, airflow and kafka a must have in this field, and should I try to learn them, or can I just pick them up during my career? Also, I am curious why do you guys think my profile was targeted for data engineering jobs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "186npr6", "is_robot_indexable": true, "report_reasons": null, "author": "CrazyKey4744", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186npr6/were_my_career_choices_correct_and_what_steps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186npr6/were_my_career_choices_correct_and_what_steps/", "subreddit_subscribers": 142617, "created_utc": 1701258968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nAs the title suggests, I have a certain json files available in my GCS buckets and I have to export the same using curl scripts to our vendors. Considering that the access tokens have an expiry timeline, I\u2019m looking into sending a curl script that the vendor can access for longer than the timeout. \n\nWould appreciate any help.\n\nThanks.", "author_fullname": "t2_3qc9b4bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Export JSON files from GCS buckets using curl scripts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186ksd3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701246833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;As the title suggests, I have a certain json files available in my GCS buckets and I have to export the same using curl scripts to our vendors. Considering that the access tokens have an expiry timeline, I\u2019m looking into sending a curl script that the vendor can access for longer than the timeout. &lt;/p&gt;\n\n&lt;p&gt;Would appreciate any help.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186ksd3", "is_robot_indexable": true, "report_reasons": null, "author": "apache444", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186ksd3/export_json_files_from_gcs_buckets_using_curl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186ksd3/export_json_files_from_gcs_buckets_using_curl/", "subreddit_subscribers": 142617, "created_utc": 1701246833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here is my career progression.\n\n0-2 years - SQL DBA/BI Engineer\n\n2-4 years - BI engineer\n\n4-5 1/2 years - Big Data Analyst\n\nNow trying for DE roles and going through interviews.Got offer for a DE role, but have few more interviews to be completed.\nBefore working as a Big Data analyst I tried for DE roles but I was not able to move forward in the interviews as I didn\u2019t have any professional experience with Cloud Technologies or big data tools/technologies.\n\nIs this a general career progression or y\u2019all got into DE roles much earlier in the career.?", "author_fullname": "t2_slia7yw0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Progression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186guvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701232134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is my career progression.&lt;/p&gt;\n\n&lt;p&gt;0-2 years - SQL DBA/BI Engineer&lt;/p&gt;\n\n&lt;p&gt;2-4 years - BI engineer&lt;/p&gt;\n\n&lt;p&gt;4-5 1/2 years - Big Data Analyst&lt;/p&gt;\n\n&lt;p&gt;Now trying for DE roles and going through interviews.Got offer for a DE role, but have few more interviews to be completed.\nBefore working as a Big Data analyst I tried for DE roles but I was not able to move forward in the interviews as I didn\u2019t have any professional experience with Cloud Technologies or big data tools/technologies.&lt;/p&gt;\n\n&lt;p&gt;Is this a general career progression or y\u2019all got into DE roles much earlier in the career.?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "186guvp", "is_robot_indexable": true, "report_reasons": null, "author": "cruze_8907", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186guvp/career_progression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186guvp/career_progression/", "subreddit_subscribers": 142617, "created_utc": 1701232134.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}