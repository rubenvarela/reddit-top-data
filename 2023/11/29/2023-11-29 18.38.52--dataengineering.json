{"kind": "Listing", "data": {"after": "t3_186agw6", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to break into the world of data engineering and I am seeing that you need to learn all these different tools like ADF, dbt, etc. to perform ETL. \n\nTo me it seems like Python does all this but clearly I am new to this world and there is something I am missing. \n\nWhat are the benefits of using tools like the ones listed above as opposed to Python?", "author_fullname": "t2_sgpy2k4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python versus ETL tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1866ogd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 53, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 53, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701205487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to break into the world of data engineering and I am seeing that you need to learn all these different tools like ADF, dbt, etc. to perform ETL. &lt;/p&gt;\n\n&lt;p&gt;To me it seems like Python does all this but clearly I am new to this world and there is something I am missing. &lt;/p&gt;\n\n&lt;p&gt;What are the benefits of using tools like the ones listed above as opposed to Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1866ogd", "is_robot_indexable": true, "report_reasons": null, "author": "manseekingmemes1", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1866ogd/python_versus_etl_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1866ogd/python_versus_etl_tools/", "subreddit_subscribers": 142566, "created_utc": 1701205487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious about containerization and ETLs", "author_fullname": "t2_h5ydkpa8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are DEs using Docker containers for their ETLs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186fvwk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701229107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious about containerization and ETLs&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186fvwk", "is_robot_indexable": true, "report_reasons": null, "author": "SignificantWords", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186fvwk/how_are_des_using_docker_containers_for_their_etls/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186fvwk/how_are_des_using_docker_containers_for_their_etls/", "subreddit_subscribers": 142566, "created_utc": 1701229107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am 27 years old, 3 YOE, stuck in a role at a bad company working 8-5 on site for $110K (20K pay cut from last gig). Toxic company culture as well and shoved in basement with IT support. Living at home and don\u2019t want to be in this town. No wife no kids no friends. \n\nOnly been 3 months and can\u2019t find another job especially since I was only at my last gig for 6 months. \nNot even doing DE work, stuck doing low code BI work with piece of shit software called Domo. \n\nDebating quitting and doing my MSDS full time and finishing next July. Have $40K saved up. \n\nAnyone else in a similar situation? How do you cope? Not sure how to get out of this one anytime soon, given my spotty job history and job market.", "author_fullname": "t2_dbas4m3c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone Else Stuck in a Bad Job? How Do You Cope?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186aeh4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701214302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am 27 years old, 3 YOE, stuck in a role at a bad company working 8-5 on site for $110K (20K pay cut from last gig). Toxic company culture as well and shoved in basement with IT support. Living at home and don\u2019t want to be in this town. No wife no kids no friends. &lt;/p&gt;\n\n&lt;p&gt;Only been 3 months and can\u2019t find another job especially since I was only at my last gig for 6 months. \nNot even doing DE work, stuck doing low code BI work with piece of shit software called Domo. &lt;/p&gt;\n\n&lt;p&gt;Debating quitting and doing my MSDS full time and finishing next July. Have $40K saved up. &lt;/p&gt;\n\n&lt;p&gt;Anyone else in a similar situation? How do you cope? Not sure how to get out of this one anytime soon, given my spotty job history and job market.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "186aeh4", "is_robot_indexable": true, "report_reasons": null, "author": "Proof_Hyena4223", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186aeh4/anyone_else_stuck_in_a_bad_job_how_do_you_cope/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186aeh4/anyone_else_stuck_in_a_bad_job_how_do_you_cope/", "subreddit_subscribers": 142566, "created_utc": 1701214302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI am not a data engineer. I was a reporting analyst in a job many years ago, then moved into product owner role at a different company for a mobile application. I got hired into this role (product manager/owner) because I had previous job in the industry - very niche, think healthcare, mortgage loans, etc. I've learned there is a lot of magic that goes on behind the scenes. The data doesn't just automatically move from point A to point B - there is a whole process, imagine that!\n\nSome of the products we have are:\n\n1. data lake and data warehouse\n   1. main customers are business intelligence and various reporting/analytic users\n   2. technology stack is:\n      1. airflow\n      2. dbt\n      3. postgres\n      4. custom inhouse python to extract/load data using AWS lambda or fargate \n2. report delivery\n   1. we have 2 main applications to schedule reports at designated time/frequency and drop into given location. Both systems can produce the file in various extensions (.csv, .txt, etc.)\n\nMy job in a nutshell is to intake \n\n* the various adhoc requests from business users (usually wanting to ingest data from a new source or change request to existing data ingestion process or report)\n* our internal road map items which usually consists of techdebt or improving/enhancing our processes or technologies \n* the actual project management team with various regulatory requests or new data ingestion requests\n* the business intelligence team with creating new semantic layer/wh views or supporting the reporting systems\n\nThen I will gather requirements (for example, for a new data ingestion request, I would document the source data, how do we connect or get the data, what is the primary key, is the data an alpha/delta load, what kind of error handling and notifications need to be set up, etc) then create tickets in JIRA for our engineers. I look at all initiatives in our backlog, and prioritize the work. We then commit to the work in 2 week sprints.\n\nAirflow and dbt are new technologies we implemented this year. Our team had built and was using an in house ETL process via AWS lambda, and scheduling data pipelines off of expected data refresh run times (cron job concept). As in - task 1 takes average of 2 hours to run, then schedule task 2 to start 2.5 hours after task1 is scheduled to start. It was all very custom and not the best.\n\nIn essence, I'm acting as a product manager/owner for the data engineering team at my company. I was wanting to ask this DE community what kinds of things they wish a project/product manager would define when gathering requirements for various data solutions or requests. Having what kind of details or documentation would make you most successful? \n\nAlso, in your opinion, would the data engineers create the data model or the product manager?\n\n&amp;#x200B;", "author_fullname": "t2_50son", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I am a Product Manager for data, and I have some questions for Data Engineers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1866sm4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701205772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am not a data engineer. I was a reporting analyst in a job many years ago, then moved into product owner role at a different company for a mobile application. I got hired into this role (product manager/owner) because I had previous job in the industry - very niche, think healthcare, mortgage loans, etc. I&amp;#39;ve learned there is a lot of magic that goes on behind the scenes. The data doesn&amp;#39;t just automatically move from point A to point B - there is a whole process, imagine that!&lt;/p&gt;\n\n&lt;p&gt;Some of the products we have are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;data lake and data warehouse\n\n&lt;ol&gt;\n&lt;li&gt;main customers are business intelligence and various reporting/analytic users&lt;/li&gt;\n&lt;li&gt;technology stack is:\n\n&lt;ol&gt;\n&lt;li&gt;airflow&lt;/li&gt;\n&lt;li&gt;dbt&lt;/li&gt;\n&lt;li&gt;postgres&lt;/li&gt;\n&lt;li&gt;custom inhouse python to extract/load data using AWS lambda or fargate &lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;li&gt;report delivery\n\n&lt;ol&gt;\n&lt;li&gt;we have 2 main applications to schedule reports at designated time/frequency and drop into given location. Both systems can produce the file in various extensions (.csv, .txt, etc.)&lt;/li&gt;\n&lt;/ol&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My job in a nutshell is to intake &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;the various adhoc requests from business users (usually wanting to ingest data from a new source or change request to existing data ingestion process or report)&lt;/li&gt;\n&lt;li&gt;our internal road map items which usually consists of techdebt or improving/enhancing our processes or technologies &lt;/li&gt;\n&lt;li&gt;the actual project management team with various regulatory requests or new data ingestion requests&lt;/li&gt;\n&lt;li&gt;the business intelligence team with creating new semantic layer/wh views or supporting the reporting systems&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Then I will gather requirements (for example, for a new data ingestion request, I would document the source data, how do we connect or get the data, what is the primary key, is the data an alpha/delta load, what kind of error handling and notifications need to be set up, etc) then create tickets in JIRA for our engineers. I look at all initiatives in our backlog, and prioritize the work. We then commit to the work in 2 week sprints.&lt;/p&gt;\n\n&lt;p&gt;Airflow and dbt are new technologies we implemented this year. Our team had built and was using an in house ETL process via AWS lambda, and scheduling data pipelines off of expected data refresh run times (cron job concept). As in - task 1 takes average of 2 hours to run, then schedule task 2 to start 2.5 hours after task1 is scheduled to start. It was all very custom and not the best.&lt;/p&gt;\n\n&lt;p&gt;In essence, I&amp;#39;m acting as a product manager/owner for the data engineering team at my company. I was wanting to ask this DE community what kinds of things they wish a project/product manager would define when gathering requirements for various data solutions or requests. Having what kind of details or documentation would make you most successful? &lt;/p&gt;\n\n&lt;p&gt;Also, in your opinion, would the data engineers create the data model or the product manager?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1866sm4", "is_robot_indexable": true, "report_reasons": null, "author": "Honeychild06", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1866sm4/i_am_a_product_manager_for_data_and_i_have_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1866sm4/i_am_a_product_manager_for_data_and_i_have_some/", "subreddit_subscribers": 142566, "created_utc": 1701205772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have used Databricks and Snowflake in the past and learned about Dremio as well, but have never used it so far.\n\nI have a hard time to grasp the differences of the products when it comes to which is the best choice for which scenario.\n\nSo far I would say I prefer Snowflake when I need an easy to use and easy to setup Data Warehouse. It's especially useful if you want to invest all your time in getting value out of data and not invest so much time into operations and setup. Its great for integration in business intelligence solutions like Tableau.\n\nI like Databricks when the data is a bit more in volume and also more diversity in Data formats. The focus of Databricks is in using Spark to move and transform data in streams or batch, but when the main focus is machine learning, AI, but also BI.\n\nWhat about Dremio? What are you pros/cons or maybe use cases for the specific platforms?", "author_fullname": "t2_6n1qa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks vs. Snowflake vs. Dremio", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186j31s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701239755.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have used Databricks and Snowflake in the past and learned about Dremio as well, but have never used it so far.&lt;/p&gt;\n\n&lt;p&gt;I have a hard time to grasp the differences of the products when it comes to which is the best choice for which scenario.&lt;/p&gt;\n\n&lt;p&gt;So far I would say I prefer Snowflake when I need an easy to use and easy to setup Data Warehouse. It&amp;#39;s especially useful if you want to invest all your time in getting value out of data and not invest so much time into operations and setup. Its great for integration in business intelligence solutions like Tableau.&lt;/p&gt;\n\n&lt;p&gt;I like Databricks when the data is a bit more in volume and also more diversity in Data formats. The focus of Databricks is in using Spark to move and transform data in streams or batch, but when the main focus is machine learning, AI, but also BI.&lt;/p&gt;\n\n&lt;p&gt;What about Dremio? What are you pros/cons or maybe use cases for the specific platforms?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186j31s", "is_robot_indexable": true, "report_reasons": null, "author": "RonBurgundyIsBest", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186j31s/databricks_vs_snowflake_vs_dremio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186j31s/databricks_vs_snowflake_vs_dremio/", "subreddit_subscribers": 142566, "created_utc": 1701239755.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One interesting thing I saw was a lot of zero-ETL features for Redshift (from Aurora, DynamoDB, others). A lot of Q stuff that I don't care about. \n\n[Updates](https://aws.amazon.com/new/?whats-new-content-all.sort-by=item.additionalFields.postDateTime&amp;whats-new-content-all.sort-order=desc&amp;awsf.whats-new-analytics=*all&amp;awsf.whats-new-app-integration=*all&amp;awsf.whats-new-arvr=*all&amp;awsf.whats-new-blockchain=*all&amp;awsf.whats-new-business-applications=*all&amp;awsf.whats-new-cloud-financial-management=*all&amp;awsf.whats-new-compute=*all&amp;awsf.whats-new-containers=*all&amp;awsf.whats-new-customer-enablement=*all&amp;awsf.whats-new-customer%20engagement=*all&amp;awsf.whats-new-database=general-products%23amazon-redshift&amp;awsf.whats-new-developer-tools=*all&amp;awsf.whats-new-end-user-computing=*all&amp;awsf.whats-new-mobile=*all&amp;awsf.whats-new-gametech=*all&amp;awsf.whats-new-iot=*all&amp;awsf.whats-new-machine-learning=*all&amp;awsf.whats-new-management-governance=*all&amp;awsf.whats-new-media-services=*all&amp;awsf.whats-new-migration-transfer=*all&amp;awsf.whats-new-networking-content-delivery=*all&amp;awsf.whats-new-quantum-tech=*all&amp;awsf.whats-new-robotics=*all&amp;awsf.whats-new-satellite=*all&amp;awsf.whats-new-security-id-compliance=*all&amp;awsf.whats-new-serverless=*all&amp;awsf.whats-new-storage=*all&amp;awsf.whats-new-categories=*all&amp;whats-new-content-all.q=redshift&amp;whats-new-content-all.q_operator=AND)\n\nNot sure if I missed anything cool. ", "author_fullname": "t2_3yozg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any new updates from AWS re:Invent for data engineers?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186b12f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701215951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One interesting thing I saw was a lot of zero-ETL features for Redshift (from Aurora, DynamoDB, others). A lot of Q stuff that I don&amp;#39;t care about. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aws.amazon.com/new/?whats-new-content-all.sort-by=item.additionalFields.postDateTime&amp;amp;whats-new-content-all.sort-order=desc&amp;amp;awsf.whats-new-analytics=*all&amp;amp;awsf.whats-new-app-integration=*all&amp;amp;awsf.whats-new-arvr=*all&amp;amp;awsf.whats-new-blockchain=*all&amp;amp;awsf.whats-new-business-applications=*all&amp;amp;awsf.whats-new-cloud-financial-management=*all&amp;amp;awsf.whats-new-compute=*all&amp;amp;awsf.whats-new-containers=*all&amp;amp;awsf.whats-new-customer-enablement=*all&amp;amp;awsf.whats-new-customer%20engagement=*all&amp;amp;awsf.whats-new-database=general-products%23amazon-redshift&amp;amp;awsf.whats-new-developer-tools=*all&amp;amp;awsf.whats-new-end-user-computing=*all&amp;amp;awsf.whats-new-mobile=*all&amp;amp;awsf.whats-new-gametech=*all&amp;amp;awsf.whats-new-iot=*all&amp;amp;awsf.whats-new-machine-learning=*all&amp;amp;awsf.whats-new-management-governance=*all&amp;amp;awsf.whats-new-media-services=*all&amp;amp;awsf.whats-new-migration-transfer=*all&amp;amp;awsf.whats-new-networking-content-delivery=*all&amp;amp;awsf.whats-new-quantum-tech=*all&amp;amp;awsf.whats-new-robotics=*all&amp;amp;awsf.whats-new-satellite=*all&amp;amp;awsf.whats-new-security-id-compliance=*all&amp;amp;awsf.whats-new-serverless=*all&amp;amp;awsf.whats-new-storage=*all&amp;amp;awsf.whats-new-categories=*all&amp;amp;whats-new-content-all.q=redshift&amp;amp;whats-new-content-all.q_operator=AND\"&gt;Updates&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Not sure if I missed anything cool. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?auto=webp&amp;s=8afacfc14dfed09cec0415cac7d36db9c3374c61", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=64a1b1322ed94c559cb213e6a08f3eb426a3fb0b", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9edddfdb28bb0e92ceb041859aacef81ab9ed42e", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de73cdc9da2d0b04938bb7d051ab1a3ceb783323", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=60037829d2ce04de0705a2b45123d8ab7c12d41c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e5a8f0da08b9281c578a8ab6f49a5b3f577ec9b8", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Q6q5DgwzcfU6chKw4uevixQcBq1Ipi2NYLheQ4lv0Vk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a9a9c1c38bd543a7ea6b718e139a9c1e6b62d18", "width": 1080, "height": 567}], "variants": {}, "id": "RUqh18uQTwuGJocqdUcC-6UfvfWS63SRDdr8AQqU3uM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186b12f", "is_robot_indexable": true, "report_reasons": null, "author": "gman1023", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186b12f/any_new_updates_from_aws_reinvent_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186b12f/any_new_updates_from_aws_reinvent_for_data/", "subreddit_subscribers": 142566, "created_utc": 1701215951.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi guys,\n\nThere are loads of full courses for learning how to work with apis as a beginner and I was wondering which one is really recommender for a data engineer?\n\nI\u2019m new to learning api and I think it would be important for me to learn how to authenticate apis and pull data from api (post and get?) \n\nHope to hear from you!", "author_fullname": "t2_2cd0q7u5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best youtube course on learning api?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1868qu3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701210284.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys,&lt;/p&gt;\n\n&lt;p&gt;There are loads of full courses for learning how to work with apis as a beginner and I was wondering which one is really recommender for a data engineer?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m new to learning api and I think it would be important for me to learn how to authenticate apis and pull data from api (post and get?) &lt;/p&gt;\n\n&lt;p&gt;Hope to hear from you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1868qu3", "is_robot_indexable": true, "report_reasons": null, "author": "kbic93", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1868qu3/best_youtube_course_on_learning_api/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1868qu3/best_youtube_course_on_learning_api/", "subreddit_subscribers": 142566, "created_utc": 1701210284.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_e5fjdth0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dashboard: The state of Data Stack 2023 (Metabase community)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "name": "t3_1862o6u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kPzBYrXb59x4jO-68CuAN4T6iqF7yj8Tw9N4NQR2uSw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701195466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/p6ivlpjgr43c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/p6ivlpjgr43c1.png?auto=webp&amp;s=61aa62378b4cc3ef92a8d08d8b2340c85bc0f8c5", "width": 2852, "height": 1582}, "resolutions": [{"url": "https://preview.redd.it/p6ivlpjgr43c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8d7cb1ef842e75775ec0b62f3453d254a315fca1", "width": 108, "height": 59}, {"url": "https://preview.redd.it/p6ivlpjgr43c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=20a2c3bfe27477dc6daf834fb5b5d2b163fc6fb7", "width": 216, "height": 119}, {"url": "https://preview.redd.it/p6ivlpjgr43c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8c3415f8478bfd6c2055fa93de0e041275db50cb", "width": 320, "height": 177}, {"url": "https://preview.redd.it/p6ivlpjgr43c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=01819bbc2ad43db644f3bf3ac582f690537be6a1", "width": 640, "height": 355}, {"url": "https://preview.redd.it/p6ivlpjgr43c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e801796352fe033aa5ef4d89a710af6d68a8510", "width": 960, "height": 532}, {"url": "https://preview.redd.it/p6ivlpjgr43c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57858bb45a5ffd4798a30a0bd2383906f7bd707e", "width": 1080, "height": 599}], "variants": {}, "id": "1TaasZfLhsEDECE6-HaJk1rr8UgmjMWpjhLEMMj8Usc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1862o6u", "is_robot_indexable": true, "report_reasons": null, "author": "Miserable_Fold4086", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1862o6u/dashboard_the_state_of_data_stack_2023_metabase/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/p6ivlpjgr43c1.png", "subreddit_subscribers": 142566, "created_utc": 1701195466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as an intern in a data consulting company. Often times, I get request to create PoC's on new technologies that I have never worked with. My usual workflow is to learn the basic concepts about the tech first, which usually takes me about 3 to 5 days and then start to implement the PoC.  \n\nIs this an acceptable practice?  If not, what do you suggest as a better approach. \n\nOne problem I felt is, I won't have anything to show up in the status call during the first few days.   ", "author_fullname": "t2_o78u2p44", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "While working with new technologies, how much time do you spend on it without actually starting to implement anything?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186mheh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701254211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as an intern in a data consulting company. Often times, I get request to create PoC&amp;#39;s on new technologies that I have never worked with. My usual workflow is to learn the basic concepts about the tech first, which usually takes me about 3 to 5 days and then start to implement the PoC.  &lt;/p&gt;\n\n&lt;p&gt;Is this an acceptable practice?  If not, what do you suggest as a better approach. &lt;/p&gt;\n\n&lt;p&gt;One problem I felt is, I won&amp;#39;t have anything to show up in the status call during the first few days.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186mheh", "is_robot_indexable": true, "report_reasons": null, "author": "SignificanceNo136", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186mheh/while_working_with_new_technologies_how_much_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186mheh/while_working_with_new_technologies_how_much_time/", "subreddit_subscribers": 142566, "created_utc": 1701254211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi fellow DEs,\n\nI currently work as a SWE with almost 2 YOE, I've always been interested in the field of data and came across the concept of DE when researching about different job opportunities. I wanted to stay heavy coding (which rules out DA) and DE seems to be the right fit.\n\nHowever, as I read more and more posts in the sub, I realise a huge portion of DE's responsibilities involve DevOps as well. Setting up environments and configuring has always been my No.1 pet peeve in the world of software development. I didn't need to do much of it in my role of SWE as it's mostly handled by other people in the same company.\n\nAn example would be, me trying to setup Airflow + DuckDB and etc. in docker for experimentation. I am instantly mind-boggled by the configuration process (e.g., how to write a proper YAML to initialise these components). \n\nAll in all, I just want to get some insights from someone already in the field, to know how well I would do if I have an inherent resentment towards doing DevOps. Also, what would be the percentages be like for Python, SQL and DevOps in general?", "author_fullname": "t2_5tffcx87", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for someone looking to transition from SWE to DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186f796", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701227216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi fellow DEs,&lt;/p&gt;\n\n&lt;p&gt;I currently work as a SWE with almost 2 YOE, I&amp;#39;ve always been interested in the field of data and came across the concept of DE when researching about different job opportunities. I wanted to stay heavy coding (which rules out DA) and DE seems to be the right fit.&lt;/p&gt;\n\n&lt;p&gt;However, as I read more and more posts in the sub, I realise a huge portion of DE&amp;#39;s responsibilities involve DevOps as well. Setting up environments and configuring has always been my No.1 pet peeve in the world of software development. I didn&amp;#39;t need to do much of it in my role of SWE as it&amp;#39;s mostly handled by other people in the same company.&lt;/p&gt;\n\n&lt;p&gt;An example would be, me trying to setup Airflow + DuckDB and etc. in docker for experimentation. I am instantly mind-boggled by the configuration process (e.g., how to write a proper YAML to initialise these components). &lt;/p&gt;\n\n&lt;p&gt;All in all, I just want to get some insights from someone already in the field, to know how well I would do if I have an inherent resentment towards doing DevOps. Also, what would be the percentages be like for Python, SQL and DevOps in general?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "186f796", "is_robot_indexable": true, "report_reasons": null, "author": "Psychological_Ad9582", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186f796/advice_for_someone_looking_to_transition_from_swe/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186f796/advice_for_someone_looking_to_transition_from_swe/", "subreddit_subscribers": 142566, "created_utc": 1701227216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to ask anyone finished MIT MicroMaster program of Data Science (via edx), how is it useful or not at Job Market? may have your experience shared? Does hiring company recognize it and give it a credit? Sorry I intend to ask this question but can't change post subject after submitted.\n\n'm afraid a little bit for it's 'not a formal Master Degree, so employer won't take it seriously as much as MIT regular on-campus students.   FYI, I finished one of its Course 'MIT x6.431 - Probability and Data Uncertainty', get good score 92/100,  have 3 remains, but I'm working full-time, progress Very slowly, by time I graduate I'll be over 50...   see my certificate in attached Image", "author_fullname": "t2_bt52hv18", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My MIT course certificate in Data Science MicroMaster program", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186456j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701199107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to ask anyone finished MIT MicroMaster program of Data Science (via edx), how is it useful or not at Job Market? may have your experience shared? Does hiring company recognize it and give it a credit? Sorry I intend to ask this question but can&amp;#39;t change post subject after submitted.&lt;/p&gt;\n\n&lt;p&gt;&amp;#39;m afraid a little bit for it&amp;#39;s &amp;#39;not a formal Master Degree, so employer won&amp;#39;t take it seriously as much as MIT regular on-campus students.   FYI, I finished one of its Course &amp;#39;MIT x6.431 - Probability and Data Uncertainty&amp;#39;, get good score 92/100,  have 3 remains, but I&amp;#39;m working full-time, progress Very slowly, by time I graduate I&amp;#39;ll be over 50...   see my certificate in attached Image&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186456j", "is_robot_indexable": true, "report_reasons": null, "author": "SG-Dani20", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186456j/my_mit_course_certificate_in_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186456j/my_mit_course_certificate_in_data_science/", "subreddit_subscribers": 142566, "created_utc": 1701199107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nHi All\n\nI am new to AWS and am trying to figure out the best way to deploy a script to the cloud that runs hourly doing the following:\n\nPull data from a database Pull data from s3 bucket Use logic to update records from DB with data from s3 Update redshift table with new data\n\nAfter investigating, I thought that using lambda to complete this would work, but I need static ips to be whitelisted in order to access both databases. Lambda doesn\u2019t have an IP associated and I\u2019m worried about the cost of incorporating a VPC\n\nis there a better way/tool to complete this task?", "author_fullname": "t2_edr3jh4p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Deploying script that connects to DB, S3, and Redshift in AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_186twtj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701276390.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All&lt;/p&gt;\n\n&lt;p&gt;I am new to AWS and am trying to figure out the best way to deploy a script to the cloud that runs hourly doing the following:&lt;/p&gt;\n\n&lt;p&gt;Pull data from a database Pull data from s3 bucket Use logic to update records from DB with data from s3 Update redshift table with new data&lt;/p&gt;\n\n&lt;p&gt;After investigating, I thought that using lambda to complete this would work, but I need static ips to be whitelisted in order to access both databases. Lambda doesn\u2019t have an IP associated and I\u2019m worried about the cost of incorporating a VPC&lt;/p&gt;\n\n&lt;p&gt;is there a better way/tool to complete this task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "186twtj", "is_robot_indexable": true, "report_reasons": null, "author": "babababooskio", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186twtj/deploying_script_that_connects_to_db_s3_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186twtj/deploying_script_that_connects_to_db_s3_and/", "subreddit_subscribers": 142566, "created_utc": 1701276390.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Which data ingestion tool do you use?\n\nLooking to use a managed service like [Airbyte Cloud](https://airbyte.com/product/airbyte-cloud), [Rivery.io](https://Rivery.io), or [Google Cloud Data Fusion](https://cloud.google.com/data-fusion?hl=en) to simply extract data from common third party apps like Salesforce, Google Analytics, Google Ads, etc and drop them to Cloud Storage + BigQuery.\n\nWondering if Data Fusion is the way to go? Has anyone used it? Does it make sense just to use for data ingestion alone? Not going to use it for data transformations (looking to use Dataform/dbt for that).", "author_fullname": "t2_4gzaf8mv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which data ingestion tool do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186m9gb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701253296.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which data ingestion tool do you use?&lt;/p&gt;\n\n&lt;p&gt;Looking to use a managed service like &lt;a href=\"https://airbyte.com/product/airbyte-cloud\"&gt;Airbyte Cloud&lt;/a&gt;, &lt;a href=\"https://Rivery.io\"&gt;Rivery.io&lt;/a&gt;, or &lt;a href=\"https://cloud.google.com/data-fusion?hl=en\"&gt;Google Cloud Data Fusion&lt;/a&gt; to simply extract data from common third party apps like Salesforce, Google Analytics, Google Ads, etc and drop them to Cloud Storage + BigQuery.&lt;/p&gt;\n\n&lt;p&gt;Wondering if Data Fusion is the way to go? Has anyone used it? Does it make sense just to use for data ingestion alone? Not going to use it for data transformations (looking to use Dataform/dbt for that).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186m9gb", "is_robot_indexable": true, "report_reasons": null, "author": "theoriginalmantooth", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186m9gb/which_data_ingestion_tool_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186m9gb/which_data_ingestion_tool_do_you_use/", "subreddit_subscribers": 142566, "created_utc": 1701253296.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I put together an ETL recently for my work on a chron job API pull that has amassed something like 70,000 rows of data, 100mb of entries a month into a BigQuery dataset for downstream reporting and BI analysis. I expressed concern about aggregation of duplicate entries and entries over time to my manager, suggesting that to save some money I could switch it to a cloud-hosted traditional GCP database (Postgres or similar) with cold storage options for old data that could easily be pumped into Bigquery. They did not seem concerned enough to let me do this. \n\nI have just found out that we have a 13GB dataset for similar reasons, millions of rows of columnar data that get hit with queries frequently and those queries use at least 1GB of data each to process. That puts my smaller dataset costs into context, I suppose. \n\nI am reasonably certain I can at minimum use Dataform to create a secondary dataset adding 100 entries a month instead of 70k, since many pseudo-duplicates are created with no difference other than dates (this is a dataset to catch changes to certain things). I did this actually, because I had to know. It's just not implemented and worried proposing it will seem foolish. \n\nAt what point is it actually smart to worry about these things? Manager seems unworried. I am baby. Please help me look smart.", "author_fullname": "t2_dbmvkyyq2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How big is too big for a BigQuery Dataset getting frequent queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1865u8u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701203342.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I put together an ETL recently for my work on a chron job API pull that has amassed something like 70,000 rows of data, 100mb of entries a month into a BigQuery dataset for downstream reporting and BI analysis. I expressed concern about aggregation of duplicate entries and entries over time to my manager, suggesting that to save some money I could switch it to a cloud-hosted traditional GCP database (Postgres or similar) with cold storage options for old data that could easily be pumped into Bigquery. They did not seem concerned enough to let me do this. &lt;/p&gt;\n\n&lt;p&gt;I have just found out that we have a 13GB dataset for similar reasons, millions of rows of columnar data that get hit with queries frequently and those queries use at least 1GB of data each to process. That puts my smaller dataset costs into context, I suppose. &lt;/p&gt;\n\n&lt;p&gt;I am reasonably certain I can at minimum use Dataform to create a secondary dataset adding 100 entries a month instead of 70k, since many pseudo-duplicates are created with no difference other than dates (this is a dataset to catch changes to certain things). I did this actually, because I had to know. It&amp;#39;s just not implemented and worried proposing it will seem foolish. &lt;/p&gt;\n\n&lt;p&gt;At what point is it actually smart to worry about these things? Manager seems unworried. I am baby. Please help me look smart.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1865u8u", "is_robot_indexable": true, "report_reasons": null, "author": "artfully_rearranged", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/1865u8u/how_big_is_too_big_for_a_bigquery_dataset_getting/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1865u8u/how_big_is_too_big_for_a_bigquery_dataset_getting/", "subreddit_subscribers": 142566, "created_utc": 1701203342.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nI happen to have an access to the book by Lindstedt and Olschimke. I noticed it goes through the fundamentals of DWs and DBs. \n\nMy question is, is it going to be biased or does it work as a solid material for these other concepts too? \n\nIf not, what would be better? \"Database Internals\" by Petrov  is another one I know of, but it mentioned distributed DBs. \n\nThanks!", "author_fullname": "t2_64n7xdby", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Vault book in learning fundamentals of DBs or DWs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186rbru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701269876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I happen to have an access to the book by Lindstedt and Olschimke. I noticed it goes through the fundamentals of DWs and DBs. &lt;/p&gt;\n\n&lt;p&gt;My question is, is it going to be biased or does it work as a solid material for these other concepts too? &lt;/p&gt;\n\n&lt;p&gt;If not, what would be better? &amp;quot;Database Internals&amp;quot; by Petrov  is another one I know of, but it mentioned distributed DBs. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "186rbru", "is_robot_indexable": true, "report_reasons": null, "author": "Hyvahar", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186rbru/data_vault_book_in_learning_fundamentals_of_dbs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186rbru/data_vault_book_in_learning_fundamentals_of_dbs/", "subreddit_subscribers": 142566, "created_utc": 1701269876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI am looking some simple solution for data pipeline from MySQL to PostgrSql. I want to move all data without any processing i.e. all tables data in MySQL to Postgresql.\n\n&amp;#x200B;\n\nIs there any simple way? Like in postgresql to postgresql there is pub-sub solution.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_4uylr4su", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipeline between MySQL to PostgeSQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186oidj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701261726.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I am looking some simple solution for data pipeline from MySQL to PostgrSql. I want to move all data without any processing i.e. all tables data in MySQL to Postgresql.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there any simple way? Like in postgresql to postgresql there is pub-sub solution.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186oidj", "is_robot_indexable": true, "report_reasons": null, "author": "harshmah", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186oidj/data_pipeline_between_mysql_to_postgesql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186oidj/data_pipeline_between_mysql_to_postgesql/", "subreddit_subscribers": 142566, "created_utc": 1701261726.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone!\n\n I got my master's in data science and machine learning, coming from a computer engineering background. I thought I would get into data science job since it was a hot topic, but my job proposals were pretty much for data engineering.   \nI decided to accept a data engineering job, which was actually a database administration with cloud infrastructure. I left that job and now am a data engineer working with mainly azure data factory and synapse for a small company.   \nI read the book \"Fundamentals of data engineering\" and got 2 more books, \"The data warehouse toolkit\" and \"Learning Spark\". I also wanted to develop a personal project but have no idea where to start.  \nMy questions are: Is data engineering still a good career option? Are technologies like spark, airflow and kafka a must have in this field, and should I try to learn them, or can I just pick them up during my career? Also, I am curious why do you guys think my profile was targeted for data engineering jobs?", "author_fullname": "t2_scb32bzm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Were my career choices correct and what steps should I take?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186npr6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701258968.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt;\n\n&lt;p&gt;I got my master&amp;#39;s in data science and machine learning, coming from a computer engineering background. I thought I would get into data science job since it was a hot topic, but my job proposals were pretty much for data engineering.&lt;br/&gt;\nI decided to accept a data engineering job, which was actually a database administration with cloud infrastructure. I left that job and now am a data engineer working with mainly azure data factory and synapse for a small company.&lt;br/&gt;\nI read the book &amp;quot;Fundamentals of data engineering&amp;quot; and got 2 more books, &amp;quot;The data warehouse toolkit&amp;quot; and &amp;quot;Learning Spark&amp;quot;. I also wanted to develop a personal project but have no idea where to start.&lt;br/&gt;\nMy questions are: Is data engineering still a good career option? Are technologies like spark, airflow and kafka a must have in this field, and should I try to learn them, or can I just pick them up during my career? Also, I am curious why do you guys think my profile was targeted for data engineering jobs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "186npr6", "is_robot_indexable": true, "report_reasons": null, "author": "CrazyKey4744", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186npr6/were_my_career_choices_correct_and_what_steps/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186npr6/were_my_career_choices_correct_and_what_steps/", "subreddit_subscribers": 142566, "created_utc": 1701258968.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nAs the title suggests, I have a certain json files available in my GCS buckets and I have to export the same using curl scripts to our vendors. Considering that the access tokens have an expiry timeline, I\u2019m looking into sending a curl script that the vendor can access for longer than the timeout. \n\nWould appreciate any help.\n\nThanks.", "author_fullname": "t2_3qc9b4bu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Export JSON files from GCS buckets using curl scripts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186ksd3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701246833.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;As the title suggests, I have a certain json files available in my GCS buckets and I have to export the same using curl scripts to our vendors. Considering that the access tokens have an expiry timeline, I\u2019m looking into sending a curl script that the vendor can access for longer than the timeout. &lt;/p&gt;\n\n&lt;p&gt;Would appreciate any help.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "186ksd3", "is_robot_indexable": true, "report_reasons": null, "author": "apache444", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186ksd3/export_json_files_from_gcs_buckets_using_curl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186ksd3/export_json_files_from_gcs_buckets_using_curl/", "subreddit_subscribers": 142566, "created_utc": 1701246833.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "we\u2019re using Fivetran as a ETL tool and Odoo for accounting &amp; finance. However, Odoo is not one of Fivetrans\u2019 applications. \nThere\u2019s no connection between them.\n\nIs there any workaround to make this happen? and how? \n\n+ BigQuery is the DW", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fivetran Connectors \u2014Odoo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1868hvd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701209711.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;we\u2019re using Fivetran as a ETL tool and Odoo for accounting &amp;amp; finance. However, Odoo is not one of Fivetrans\u2019 applications. \nThere\u2019s no connection between them.&lt;/p&gt;\n\n&lt;p&gt;Is there any workaround to make this happen? and how? &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;BigQuery is the DW&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1868hvd", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1868hvd/fivetran_connectors_odoo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1868hvd/fivetran_connectors_odoo/", "subreddit_subscribers": 142566, "created_utc": 1701209711.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking to create an on premise to cloud (Azure) ETL pipeline for an old ERP system.\n\nAny experience / recommendations for working with older versions of SQL Server?\n\nAlso, is there a common method of data migration for cases like this?", "author_fullname": "t2_ajotdw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Server 2012 ERP system", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1867nbe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701209051.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701207753.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking to create an on premise to cloud (Azure) ETL pipeline for an old ERP system.&lt;/p&gt;\n\n&lt;p&gt;Any experience / recommendations for working with older versions of SQL Server?&lt;/p&gt;\n\n&lt;p&gt;Also, is there a common method of data migration for cases like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1867nbe", "is_robot_indexable": true, "report_reasons": null, "author": "circuitpatrol", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1867nbe/sql_server_2012_erp_system/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1867nbe/sql_server_2012_erp_system/", "subreddit_subscribers": 142566, "created_utc": 1701207753.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm tasked with designing a system for a database-per-client SaaS WMS. This WMS is based on MySQL and currently retains data indefinitely (with PII redacted). However, this indefinite data retention is bloating our databases for data that is accessed very infrequently, but our customers don't want us to purge the data completely.\n\nMy thoughts are to offload it to an \"archive\" data store of some sort. Cheap storage, low maintenance, durable. It needs to support occasional ad-hoc queries, grids, dashboards, exports, single-record lookups, etc. Simultaneously, we want to improve our user-friendliness for data engineers and make all of their data easily accessible without providing access to the main OLTP database, just a sort of sanitized view of it (e.g. 40 tables instead of 200) in an easy-to-consume format (a read-only db connection, or bucket containing Delta Lake files, an Airbyte connector, etc).\n\nThere are so many options I am struggling to make some decisions... Batch vs stream? DBMS vs query engine? The system doesn't need to be highly-available but the data needs to be extremely durable. So far my research has led me to strongly consider a data lakehouse backed by S3 storage and either a query engine or a materialized view of it in a non-HA database that can easily be rebuilt. I think our upper limits for the foreseeable future are well within &lt;100 million new rows per year so single-node solutions like DuckDb or just Postgres with partitioning look reasonable. However, solutions like Clickhouse also look attractive and could be a strong selling point to enterprise customers who just want to connect to the data easily with their tool of choice.\n\nSo, three questions:\n\n1. Having the data split into two separate systems (live and archived) seems like it will be cumbersome so we'd like the \"archive\" to also contain near-real-time data so that BAs can just go to one source for all. Is this a bad idea? If so, what is a better way around this?\n2. There is a challenge with some of the ETL tools since we want to \"archive\" our data: We will be deleting rows from our MySQL database to archive them, not because they should be destroyed in the data lake. How do we work around this challenge with CDC or batch tools so that the archived rows continue to exist in the archive database even though they are purged from the source database? Surely this is not a new problem but I haven't seen it discussed. Most of our data is append-only but there is some that if we wanted to use the same system for the live data we would need to be able to support deletes.\n3. What are your recommendations for this data pipeline and resting place? We want to remain on FOSS at this time. Some of the specific projects that have caught my eye so far for inclusion in our stack are:\n\n* Cube ([cube.dev](https://cube.dev)) for our out-of-the-box dashboards and reports, possibly merging data sources\n* Delta Lake or Hudi formats for long-term storage and data sharing\n* DuckDb/MotherDuck, Clickhouse or Trino for querying\n* dbt, Debezium, home-grown Python for data ingestion - is it common to use batch tool to load the initial batch and then a CDC streaming tool to keep it up-to-date?\n* Airbyte and other \"easy\" tools to either mirror a real-time database or generally provide more options for data destinations as long as they can be automated for many clients\n* Python's MySQL binlog streamer for reading updates from MySQL if we need to write a custom tool\n\nOur main core competency is still the WMS features so this is a \"side\" project (but necessary to keep our main database nimble), but we also want data availability and analytics to become a differentiating strength - keeping in mind it isn't our #1 core competency so overall investment cost needs to be relatively low at this time.\n\nWe are strongest with PHP and Node but are willing to delve into light Python or Java development if needed since we recognize PHP is not a big player in this space. We are comfortable working with SQL so that is generally more attractive than things like DataFrames.\n\nIf you got this far, thanks for that! Your thoughts would be much appreciated!", "author_fullname": "t2_ct83gg0tt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archive to Data Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1865ulw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701203368.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m tasked with designing a system for a database-per-client SaaS WMS. This WMS is based on MySQL and currently retains data indefinitely (with PII redacted). However, this indefinite data retention is bloating our databases for data that is accessed very infrequently, but our customers don&amp;#39;t want us to purge the data completely.&lt;/p&gt;\n\n&lt;p&gt;My thoughts are to offload it to an &amp;quot;archive&amp;quot; data store of some sort. Cheap storage, low maintenance, durable. It needs to support occasional ad-hoc queries, grids, dashboards, exports, single-record lookups, etc. Simultaneously, we want to improve our user-friendliness for data engineers and make all of their data easily accessible without providing access to the main OLTP database, just a sort of sanitized view of it (e.g. 40 tables instead of 200) in an easy-to-consume format (a read-only db connection, or bucket containing Delta Lake files, an Airbyte connector, etc).&lt;/p&gt;\n\n&lt;p&gt;There are so many options I am struggling to make some decisions... Batch vs stream? DBMS vs query engine? The system doesn&amp;#39;t need to be highly-available but the data needs to be extremely durable. So far my research has led me to strongly consider a data lakehouse backed by S3 storage and either a query engine or a materialized view of it in a non-HA database that can easily be rebuilt. I think our upper limits for the foreseeable future are well within &amp;lt;100 million new rows per year so single-node solutions like DuckDb or just Postgres with partitioning look reasonable. However, solutions like Clickhouse also look attractive and could be a strong selling point to enterprise customers who just want to connect to the data easily with their tool of choice.&lt;/p&gt;\n\n&lt;p&gt;So, three questions:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Having the data split into two separate systems (live and archived) seems like it will be cumbersome so we&amp;#39;d like the &amp;quot;archive&amp;quot; to also contain near-real-time data so that BAs can just go to one source for all. Is this a bad idea? If so, what is a better way around this?&lt;/li&gt;\n&lt;li&gt;There is a challenge with some of the ETL tools since we want to &amp;quot;archive&amp;quot; our data: We will be deleting rows from our MySQL database to archive them, not because they should be destroyed in the data lake. How do we work around this challenge with CDC or batch tools so that the archived rows continue to exist in the archive database even though they are purged from the source database? Surely this is not a new problem but I haven&amp;#39;t seen it discussed. Most of our data is append-only but there is some that if we wanted to use the same system for the live data we would need to be able to support deletes.&lt;/li&gt;\n&lt;li&gt;What are your recommendations for this data pipeline and resting place? We want to remain on FOSS at this time. Some of the specific projects that have caught my eye so far for inclusion in our stack are:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Cube (&lt;a href=\"https://cube.dev\"&gt;cube.dev&lt;/a&gt;) for our out-of-the-box dashboards and reports, possibly merging data sources&lt;/li&gt;\n&lt;li&gt;Delta Lake or Hudi formats for long-term storage and data sharing&lt;/li&gt;\n&lt;li&gt;DuckDb/MotherDuck, Clickhouse or Trino for querying&lt;/li&gt;\n&lt;li&gt;dbt, Debezium, home-grown Python for data ingestion - is it common to use batch tool to load the initial batch and then a CDC streaming tool to keep it up-to-date?&lt;/li&gt;\n&lt;li&gt;Airbyte and other &amp;quot;easy&amp;quot; tools to either mirror a real-time database or generally provide more options for data destinations as long as they can be automated for many clients&lt;/li&gt;\n&lt;li&gt;Python&amp;#39;s MySQL binlog streamer for reading updates from MySQL if we need to write a custom tool&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Our main core competency is still the WMS features so this is a &amp;quot;side&amp;quot; project (but necessary to keep our main database nimble), but we also want data availability and analytics to become a differentiating strength - keeping in mind it isn&amp;#39;t our #1 core competency so overall investment cost needs to be relatively low at this time.&lt;/p&gt;\n\n&lt;p&gt;We are strongest with PHP and Node but are willing to delve into light Python or Java development if needed since we recognize PHP is not a big player in this space. We are comfortable working with SQL so that is generally more attractive than things like DataFrames.&lt;/p&gt;\n\n&lt;p&gt;If you got this far, thanks for that! Your thoughts would be much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5GV9Mg6KCJjh6B6cKFsrECc1hNwneInEJmgAKhpt8bg.jpg?auto=webp&amp;s=54ecda4166c9e908f44e7198a512fd8217bde76f", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/5GV9Mg6KCJjh6B6cKFsrECc1hNwneInEJmgAKhpt8bg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c336be41f508d0558022cfdf2329a3aa03d0837", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/5GV9Mg6KCJjh6B6cKFsrECc1hNwneInEJmgAKhpt8bg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8cf54898c464185d690b5dc924442212254ac816", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/5GV9Mg6KCJjh6B6cKFsrECc1hNwneInEJmgAKhpt8bg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff72f2eff48667de83eb6f0e56b166d00a6157e2", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/5GV9Mg6KCJjh6B6cKFsrECc1hNwneInEJmgAKhpt8bg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb3d40df634bb73fb7fe0ace9cce99fb91709885", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/5GV9Mg6KCJjh6B6cKFsrECc1hNwneInEJmgAKhpt8bg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e91004d95d8289e23224a7f755fe0ffbe4b7f3c9", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/5GV9Mg6KCJjh6B6cKFsrECc1hNwneInEJmgAKhpt8bg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cd9da78c13e4e678d32ffb59d64e6932b9c9c71b", "width": 1080, "height": 567}], "variants": {}, "id": "CYFlWqFefFx0WAlgFZvtSzIVYhX58H2hKywSvmvXXxw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1865ulw", "is_robot_indexable": true, "report_reasons": null, "author": "Prudent-Eye-2653", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1865ulw/archive_to_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1865ulw/archive_to_data_lakehouse/", "subreddit_subscribers": 142566, "created_utc": 1701203368.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an SQL and Database &amp; Data table design coding round coming up with a gene-omics startup scale firm.\nWhat level of difficulty should I be expecting in the interview?\nI have some SQL foundation and experience but will have to invest some time into database design.\nThanks", "author_fullname": "t2_kgmdgive", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL and Database &amp; Data table design interview coming up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186i9g5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701236827.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an SQL and Database &amp;amp; Data table design coding round coming up with a gene-omics startup scale firm.\nWhat level of difficulty should I be expecting in the interview?\nI have some SQL foundation and experience but will have to invest some time into database design.\nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "186i9g5", "is_robot_indexable": true, "report_reasons": null, "author": "Grouchy-Method6979", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186i9g5/sql_and_database_data_table_design_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186i9g5/sql_and_database_data_table_design_interview/", "subreddit_subscribers": 142566, "created_utc": 1701236827.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here is my career progression.\n\n0-2 years - SQL DBA/BI Engineer\n\n2-4 years - BI engineer\n\n4-5 1/2 years - Big Data Analyst\n\nNow trying for DE roles and going through interviews.Got offer for a DE role, but have few more interviews to be completed.\nBefore working as a Big Data analyst I tried for DE roles but I was not able to move forward in the interviews as I didn\u2019t have any professional experience with Cloud Technologies or big data tools/technologies.\n\nIs this a general career progression or y\u2019all got into DE roles much earlier in the career.?", "author_fullname": "t2_slia7yw0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Progression", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186guvp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701232134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is my career progression.&lt;/p&gt;\n\n&lt;p&gt;0-2 years - SQL DBA/BI Engineer&lt;/p&gt;\n\n&lt;p&gt;2-4 years - BI engineer&lt;/p&gt;\n\n&lt;p&gt;4-5 1/2 years - Big Data Analyst&lt;/p&gt;\n\n&lt;p&gt;Now trying for DE roles and going through interviews.Got offer for a DE role, but have few more interviews to be completed.\nBefore working as a Big Data analyst I tried for DE roles but I was not able to move forward in the interviews as I didn\u2019t have any professional experience with Cloud Technologies or big data tools/technologies.&lt;/p&gt;\n\n&lt;p&gt;Is this a general career progression or y\u2019all got into DE roles much earlier in the career.?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "186guvp", "is_robot_indexable": true, "report_reasons": null, "author": "cruze_8907", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186guvp/career_progression/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186guvp/career_progression/", "subreddit_subscribers": 142566, "created_utc": 1701232134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_12wi0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Selling ELT Connectors on CloudQuery Marketplace is now open for early access.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1867u8l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1701208204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "docs.cloudquery.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://docs.cloudquery.io/blog/announcing-cloudquery-marketplace", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1867u8l", "is_robot_indexable": true, "report_reasons": null, "author": "jekapats", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1867u8l/selling_elt_connectors_on_cloudquery_marketplace/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://docs.cloudquery.io/blog/announcing-cloudquery-marketplace", "subreddit_subscribers": 142566, "created_utc": 1701208204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was trying to revise my cv, and while looking for some advice on reddit, the tip I come across a lot is to describe your professional experience in term of results instead of tasks.\n\nAnd I started to see a lot of examples where CVs look too much alike, and they all include something like: \"improved performance of the system by 800% and saved the company $2M+\"\n\nWhile the premise behind this approach has some merit, in the sense that you should highlight what you achieved in this role and what matters at the end are the results, for a recruiter reviewing your CV it doesn't give him any idea what are really your skills, the tasks you have been working on daily, and the kind of situations you were exposed too. Those numbers can mean anything, and definitely not the best indicator at how good you are at your job.\n\nI know that at the end of the day businesses care about how much money you can generate for them, but this approach seems a little forced.", "author_fullname": "t2_auw7ykll6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Focusing on results in a CV", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186agw6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701214465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to revise my cv, and while looking for some advice on reddit, the tip I come across a lot is to describe your professional experience in term of results instead of tasks.&lt;/p&gt;\n\n&lt;p&gt;And I started to see a lot of examples where CVs look too much alike, and they all include something like: &amp;quot;improved performance of the system by 800% and saved the company $2M+&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;While the premise behind this approach has some merit, in the sense that you should highlight what you achieved in this role and what matters at the end are the results, for a recruiter reviewing your CV it doesn&amp;#39;t give him any idea what are really your skills, the tasks you have been working on daily, and the kind of situations you were exposed too. Those numbers can mean anything, and definitely not the best indicator at how good you are at your job.&lt;/p&gt;\n\n&lt;p&gt;I know that at the end of the day businesses care about how much money you can generate for them, but this approach seems a little forced.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "186agw6", "is_robot_indexable": true, "report_reasons": null, "author": "Business-Corgi9653", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/186agw6/focusing_on_results_in_a_cv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/186agw6/focusing_on_results_in_a_cv/", "subreddit_subscribers": 142566, "created_utc": 1701214465.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}