{"kind": "Listing", "data": {"after": null, "dist": 10, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I guess sex just drives a lot of things\u2026", "author_fullname": "t2_y9a4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The cover of my linear regression textbook would seem to indicate that sex is the primary driver of salary.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1868lzn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 139, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 139, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xY67aT_BeS4myOPknNx-Xb5nCX5oyMncdBogFYBnYWs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701209981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I guess sex just drives a lot of things\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/f2y15d6py53c1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?auto=webp&amp;s=f9d84e26e1aeda914329fefcd117d7bbfce34f29", "width": 3024, "height": 4032}, "resolutions": [{"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f16e1f8e1d5c6604a39e5c6703c324a3cd6a0db7", "width": 108, "height": 144}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a514e62e236c6bb084acac916742eb0e621fc55a", "width": 216, "height": 288}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b33013c4eec5f939c74e183490c33e03db7dc41b", "width": 320, "height": 426}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=259e8b85bfcfc1e9c8f92bd128139c9963cef371", "width": 640, "height": 853}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b666ab4efc2f022b54885e60e75e85de116f6c64", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28b00aa2195cc5dae8733ed3f1d687b4a3d2acba", "width": 1080, "height": 1440}], "variants": {}, "id": "wCOziiXNYUc4zLReujU89Cs6H64imHl3b9SOiqTMAAA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "1868lzn", "is_robot_indexable": true, "report_reasons": null, "author": "WartimeHotTot", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1868lzn/the_cover_of_my_linear_regression_textbook_would/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/f2y15d6py53c1.jpg", "subreddit_subscribers": 1154540, "created_utc": 1701209981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There are too many case studies on teams and leadership that don't relate to analytics or data science. What are the companies which have really innovated or advanced how to do data (science, engineering, analytics, etc) in teams. I'm thinking about Hillary Parker's work at Stitch Fix for example. What are some examples from modern business history? Know of any specific examples about LLM data? How about smaller companies than the usual Silicon Valley names? I'm thinking about writing a blog or book on the subject but still in the exploratory phase.", "author_fullname": "t2_dtic2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best data teams in business history?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1863f7q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 86, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 86, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701197335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are too many case studies on teams and leadership that don&amp;#39;t relate to analytics or data science. What are the companies which have really innovated or advanced how to do data (science, engineering, analytics, etc) in teams. I&amp;#39;m thinking about Hillary Parker&amp;#39;s work at Stitch Fix for example. What are some examples from modern business history? Know of any specific examples about LLM data? How about smaller companies than the usual Silicon Valley names? I&amp;#39;m thinking about writing a blog or book on the subject but still in the exploratory phase.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "1863f7q", "is_robot_indexable": true, "report_reasons": null, "author": "eastofwestla", "discussion_type": null, "num_comments": 75, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1863f7q/what_are_the_best_data_teams_in_business_history/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1863f7q/what_are_the_best_data_teams_in_business_history/", "subreddit_subscribers": 1154540, "created_utc": 1701197335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey so I recently got a new grad job offer as a data scientist with TC about 125k in Dallas, Texas. But I have never really done data science before in my life and I'm a little worried about going in there and just complete flopping. My statistics teacher made the class wayyyy too easy so I'm really going in with only a little knowledge. I barely know what a standard deviation is.\n\nI have worked on projects as an intern software developer where I built a tool which helps people who do data analysis but I don't actually know how to do any of it myself. I think the hiring manager was more impressed with what I can do in software development, but the job description was tons of what looks like traditional DS stuff.\n\nJust wondering if anybody had any ideas on what I should be focusing on to improve upon my weak points? I have a BS in CS.\n\nSkills: python,  using LLMs, full stack swe, a bit of pandas, beautifulsoup, databases, sql\n\nLacking: actual data science skills\n\nSide note: how are the opportunities for remote work in DS as compared to software development?", "author_fullname": "t2_pt7x3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "125k offer as a data scientist but I have no idea what a data scientist does", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186id1x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 91, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 91, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701237165.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey so I recently got a new grad job offer as a data scientist with TC about 125k in Dallas, Texas. But I have never really done data science before in my life and I&amp;#39;m a little worried about going in there and just complete flopping. My statistics teacher made the class wayyyy too easy so I&amp;#39;m really going in with only a little knowledge. I barely know what a standard deviation is.&lt;/p&gt;\n\n&lt;p&gt;I have worked on projects as an intern software developer where I built a tool which helps people who do data analysis but I don&amp;#39;t actually know how to do any of it myself. I think the hiring manager was more impressed with what I can do in software development, but the job description was tons of what looks like traditional DS stuff.&lt;/p&gt;\n\n&lt;p&gt;Just wondering if anybody had any ideas on what I should be focusing on to improve upon my weak points? I have a BS in CS.&lt;/p&gt;\n\n&lt;p&gt;Skills: python,  using LLMs, full stack swe, a bit of pandas, beautifulsoup, databases, sql&lt;/p&gt;\n\n&lt;p&gt;Lacking: actual data science skills&lt;/p&gt;\n\n&lt;p&gt;Side note: how are the opportunities for remote work in DS as compared to software development?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "186id1x", "is_robot_indexable": true, "report_reasons": null, "author": "Yung-Split", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/186id1x/125k_offer_as_a_data_scientist_but_i_have_no_idea/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/186id1x/125k_offer_as_a_data_scientist_but_i_have_no_idea/", "subreddit_subscribers": 1154540, "created_utc": 1701237165.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been doing some form of data analysis for 15 years and I\u2019m so exhausted. My job has been mostly in retail and selling office supplies isn\u2019t very exciting, but at this point I think I\u2019d feel the same at a non-profit for a good cause. I\u2019m not really interested in keeping up with industry trends and tech. I just want to enjoy the time I have left on this world.\n\nBut unless I go full hermit, what the hell do I do to earn a living until I can finally retire (assuming that happens at 65)?", "author_fullname": "t2_nya9l4wu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nearly 40 and burnt out. How am I supposed to last to retirement?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186chg2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 75, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 75, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701219765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been doing some form of data analysis for 15 years and I\u2019m so exhausted. My job has been mostly in retail and selling office supplies isn\u2019t very exciting, but at this point I think I\u2019d feel the same at a non-profit for a good cause. I\u2019m not really interested in keeping up with industry trends and tech. I just want to enjoy the time I have left on this world.&lt;/p&gt;\n\n&lt;p&gt;But unless I go full hermit, what the hell do I do to earn a living until I can finally retire (assuming that happens at 65)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "186chg2", "is_robot_indexable": true, "report_reasons": null, "author": "TheUserAboveFarted", "discussion_type": null, "num_comments": 65, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/186chg2/nearly_40_and_burnt_out_how_am_i_supposed_to_last/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/186chg2/nearly_40_and_burnt_out_how_am_i_supposed_to_last/", "subreddit_subscribers": 1154540, "created_utc": 1701219765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_anwr3x7ou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A new, reactive Python+SQL notebook to help you turn your data exploration into a live app", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18668pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/y0-xdzYOfuuA5fBxv09lHdwkARWj9azT1L6Tforf3Lg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701204384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/Zero-True/zero-true", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?auto=webp&amp;s=7e0ce8d5269dac3c70fba1667406432a64a7acb5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe03888f8be2ca002b0d2c49638a998b65a470e8", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1464921e37cb796989b1c7020dbbe7f1a0d196ef", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=911046a79995054c276fe95227ca1280bcbf0c13", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=58449164475d974f84601b4ea206b9aed8077094", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=50e02cd72f26ecff41ff6253165e642408503a25", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d469fc82ce1aae16779605523373f994f4ce0d6b", "width": 1080, "height": 540}], "variants": {}, "id": "6WSnC8vemhiYjdqr2X32oPJ6YfOIzAoo7SYByY-tJw8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "18668pe", "is_robot_indexable": true, "report_reasons": null, "author": "zero-true", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18668pe/a_new_reactive_pythonsql_notebook_to_help_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/Zero-True/zero-true", "subreddit_subscribers": 1154540, "created_utc": 1701204384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, haha. How do you people deal with time data weighing holidays? Do you all just hard-code your own list of holidays? Do you have to make the dates again manually each new year for the holidays that change dates every different year / get pushed to mondays or fridays? How do you weigh the holidays? Is it like 7-day window that grows and drops by weight with the peak at the holiday itself? (like x2, x3, x4, x5, x4, x3, x2).\n\nHow do you people weigh accordingly by holiday importance? Also just eyeballing? E.g. Labor day, mother's day, and valentines day are mid-tier holidays, but thanksgiving and christimas are high-tier days so they will instead have say, a 15 day window with x9 at the peak instead of x5. etc.\n\nI'm assuming each company's business data science/analysis dep does this thing their own way according to their business industry's context (or country the company belongs to). Or is there any go-to industry standard that I'm out of the loop for?", "author_fullname": "t2_abnu12cen", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time data and holidays", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186jolv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701242627.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701242159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, haha. How do you people deal with time data weighing holidays? Do you all just hard-code your own list of holidays? Do you have to make the dates again manually each new year for the holidays that change dates every different year / get pushed to mondays or fridays? How do you weigh the holidays? Is it like 7-day window that grows and drops by weight with the peak at the holiday itself? (like x2, x3, x4, x5, x4, x3, x2).&lt;/p&gt;\n\n&lt;p&gt;How do you people weigh accordingly by holiday importance? Also just eyeballing? E.g. Labor day, mother&amp;#39;s day, and valentines day are mid-tier holidays, but thanksgiving and christimas are high-tier days so they will instead have say, a 15 day window with x9 at the peak instead of x5. etc.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m assuming each company&amp;#39;s business data science/analysis dep does this thing their own way according to their business industry&amp;#39;s context (or country the company belongs to). Or is there any go-to industry standard that I&amp;#39;m out of the loop for?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "186jolv", "is_robot_indexable": true, "report_reasons": null, "author": "newauthry", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/186jolv/time_data_and_holidays/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/186jolv/time_data_and_holidays/", "subreddit_subscribers": 1154540, "created_utc": 1701242159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a binary classification problem. Imbalanced dataset of 30/70. \n\nIn this example, I know that the actual percentage of the target variable is closer 45% in the training data, the 15% is just labeled incorrectly/missed. \n\nSo 15% of the training data is false negatives.\n\nWould unsupervised ML be an acceptable approach here given that the 15% is pretty similar to the original 30%?\n\nWould regular supervised learning not work here or am I completely overthinking this?", "author_fullname": "t2_495cn7pm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is this the correct usage of unsupervised ML?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186la5y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701249095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a binary classification problem. Imbalanced dataset of 30/70. &lt;/p&gt;\n\n&lt;p&gt;In this example, I know that the actual percentage of the target variable is closer 45% in the training data, the 15% is just labeled incorrectly/missed. &lt;/p&gt;\n\n&lt;p&gt;So 15% of the training data is false negatives.&lt;/p&gt;\n\n&lt;p&gt;Would unsupervised ML be an acceptable approach here given that the 15% is pretty similar to the original 30%?&lt;/p&gt;\n\n&lt;p&gt;Would regular supervised learning not work here or am I completely overthinking this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "186la5y", "is_robot_indexable": true, "report_reasons": null, "author": "Throwawayforgainz99", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/186la5y/is_this_the_correct_usage_of_unsupervised_ml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/186la5y/is_this_the_correct_usage_of_unsupervised_ml/", "subreddit_subscribers": 1154540, "created_utc": 1701249095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Basically we are trying to classify a bunch of products into a handful of categories, and we know certain things about how they should be classified. For example, if the product number starts with T, it's probably classification 5, or if the description contains the word \"cereal\" then it's probably classification 7. We already know most of the rules, and also, we need to have something like 99% precision in this model. If we cannot be sure of a classification, it's better to leave it unclassified. I was on my way to code this from scratch with a bunch of string manipulation, if-else blocks. Just hardcoding the rules that we receive from our SMEs. The data distribution is probably imbalanced and I am not sure the right proportions exist for a machine learning model to infer the rules correctly, hence I am hardcoding them. Also, like I said, we must have 99% or better precision, so I think it's safer to use these explicit rules. But would there be any advantages to trying scikit-learn's decision tree? One thing I like is how you can build a nice flowchart graphic easily. Or maybe another abstraction would make sense, like a finite state machine?", "author_fullname": "t2_15xuhx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I bother with scikit-learn's decision tree model for a business rules \"model\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_186kep0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701245182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically we are trying to classify a bunch of products into a handful of categories, and we know certain things about how they should be classified. For example, if the product number starts with T, it&amp;#39;s probably classification 5, or if the description contains the word &amp;quot;cereal&amp;quot; then it&amp;#39;s probably classification 7. We already know most of the rules, and also, we need to have something like 99% precision in this model. If we cannot be sure of a classification, it&amp;#39;s better to leave it unclassified. I was on my way to code this from scratch with a bunch of string manipulation, if-else blocks. Just hardcoding the rules that we receive from our SMEs. The data distribution is probably imbalanced and I am not sure the right proportions exist for a machine learning model to infer the rules correctly, hence I am hardcoding them. Also, like I said, we must have 99% or better precision, so I think it&amp;#39;s safer to use these explicit rules. But would there be any advantages to trying scikit-learn&amp;#39;s decision tree? One thing I like is how you can build a nice flowchart graphic easily. Or maybe another abstraction would make sense, like a finite state machine?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "186kep0", "is_robot_indexable": true, "report_reasons": null, "author": "question_23", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/186kep0/should_i_bother_with_scikitlearns_decision_tree/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/186kep0/should_i_bother_with_scikitlearns_decision_tree/", "subreddit_subscribers": 1154540, "created_utc": 1701245182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all!\n\nIn this 1st tutorial published on Medium, I provide some general guidelines around ARIMA models/parameters for Time Series Forecasting. A series of tutorials will follow, as well.\n\nPlease note that this post is not promotional, as this article is not behind Medium's paywall at the moment.\n\nI would like to know your thoughts and/or experience on such topics and please comment in case you have anything to add!\n\n[https://medium.com/python-in-plain-english/time-series-episode-0-brief-theory-of-forecasting-with-arima-eaf973e4dc30](https://medium.com/python-in-plain-english/time-series-episode-0-brief-theory-of-forecasting-with-arima-eaf973e4dc30)", "author_fullname": "t2_leswydvt8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ARIMA models and parameters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1866p5x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Analysis", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701205536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;In this 1st tutorial published on Medium, I provide some general guidelines around ARIMA models/parameters for Time Series Forecasting. A series of tutorials will follow, as well.&lt;/p&gt;\n\n&lt;p&gt;Please note that this post is not promotional, as this article is not behind Medium&amp;#39;s paywall at the moment.&lt;/p&gt;\n\n&lt;p&gt;I would like to know your thoughts and/or experience on such topics and please comment in case you have anything to add!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/python-in-plain-english/time-series-episode-0-brief-theory-of-forecasting-with-arima-eaf973e4dc30\"&gt;https://medium.com/python-in-plain-english/time-series-episode-0-brief-theory-of-forecasting-with-arima-eaf973e4dc30&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?auto=webp&amp;s=ffd91b0077157120cb0a3758ec0f478c45302266", "width": 1200, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=979f34d475a7a05d09f6f38ddd45f35b72c37e41", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=706887baff530fa0bb1e1b8ec75b36888ec789c5", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c951b7dda962ec8e8c7c095ec5e639196d7ac1f9", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ecd595b0dd14dd3e098a6ca607bd06dc1fbe21ef", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=be8d215af4259d2227b38ac80e665b16da7e3dbf", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a53d4231c45c8cb95e5243bdc4068a6842a0c15b", "width": 1080, "height": 810}], "variants": {}, "id": "UURDOTgc21yplVXOwVaUOnjeoK3rIot5VOM9OO7cDG4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1866p5x", "is_robot_indexable": true, "report_reasons": null, "author": "vasikal", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1866p5x/arima_models_and_parameters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1866p5x/arima_models_and_parameters/", "subreddit_subscribers": 1154540, "created_utc": 1701205536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let's say there's an binary classification problem for imbalanced response variable. Assume Xgboost &amp; up/down sampling is used &amp; train/validation/test partitions. The goal is to make a final model that maximizes recall while keeping false positive rate(fpr) below a certain value(threshold).\n\nProcess is to \n1. Find best set of hyperparameters where each tuning trial fits model on train dataset and evaluated against validation dataset. The score from the evaluation against the validation dataset is returned to the tuner at the end of every trial to provide feedback to the tuner.\n2. With best parameters found, train final model on train dataset and look at performance using hold out test dataset partition.\n\n Which option below is the right way to do this with bayesian hypertuning?\n\nA) make a custom evaluation metric like this:\nMetric = recall if fpr &lt; threshold else 0. Logic is to inform the tuner that this outcome is unacceptable if the fpr is past threshold.\n\nB) make a custom evaluation metric like this:\nMetric = recall if fpr &lt; threshold else recall*(some penalty between 0 and 1). Logic is that we inform the tuner that it has exceeded the threshold without making metric become 0. \n\nC) use logloss for the evaluation metric to find best parameters. Fit the final model on the train partition, use the validation dataset to find the threshold that maximizes the recall while remaining under the fpr threshold. \n\nD) Something else. I would love to hear any well thought ideas.", "author_fullname": "t2_3m0iqrm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Binary classification with multiple goals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1864i6o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701199990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say there&amp;#39;s an binary classification problem for imbalanced response variable. Assume Xgboost &amp;amp; up/down sampling is used &amp;amp; train/validation/test partitions. The goal is to make a final model that maximizes recall while keeping false positive rate(fpr) below a certain value(threshold).&lt;/p&gt;\n\n&lt;p&gt;Process is to \n1. Find best set of hyperparameters where each tuning trial fits model on train dataset and evaluated against validation dataset. The score from the evaluation against the validation dataset is returned to the tuner at the end of every trial to provide feedback to the tuner.\n2. With best parameters found, train final model on train dataset and look at performance using hold out test dataset partition.&lt;/p&gt;\n\n&lt;p&gt;Which option below is the right way to do this with bayesian hypertuning?&lt;/p&gt;\n\n&lt;p&gt;A) make a custom evaluation metric like this:\nMetric = recall if fpr &amp;lt; threshold else 0. Logic is to inform the tuner that this outcome is unacceptable if the fpr is past threshold.&lt;/p&gt;\n\n&lt;p&gt;B) make a custom evaluation metric like this:\nMetric = recall if fpr &amp;lt; threshold else recall*(some penalty between 0 and 1). Logic is that we inform the tuner that it has exceeded the threshold without making metric become 0. &lt;/p&gt;\n\n&lt;p&gt;C) use logloss for the evaluation metric to find best parameters. Fit the final model on the train partition, use the validation dataset to find the threshold that maximizes the recall while remaining under the fpr threshold. &lt;/p&gt;\n\n&lt;p&gt;D) Something else. I would love to hear any well thought ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "1864i6o", "is_robot_indexable": true, "report_reasons": null, "author": "DSby2021", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1864i6o/binary_classification_with_multiple_goals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1864i6o/binary_classification_with_multiple_goals/", "subreddit_subscribers": 1154540, "created_utc": 1701199990.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}