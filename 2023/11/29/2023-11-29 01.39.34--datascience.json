{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "There are too many case studies on teams and leadership that don't relate to analytics or data science. What are the companies which have really innovated or advanced how to do data (science, engineering, analytics, etc) in teams. I'm thinking about Hillary Parker's work at Stitch Fix for example. What are some examples from modern business history? Know of any specific examples about LLM data? How about smaller companies than the usual Silicon Valley names? I'm thinking about writing a blog or book on the subject but still in the exploratory phase.", "author_fullname": "t2_dtic2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best data teams in business history?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1863f7q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701197335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There are too many case studies on teams and leadership that don&amp;#39;t relate to analytics or data science. What are the companies which have really innovated or advanced how to do data (science, engineering, analytics, etc) in teams. I&amp;#39;m thinking about Hillary Parker&amp;#39;s work at Stitch Fix for example. What are some examples from modern business history? Know of any specific examples about LLM data? How about smaller companies than the usual Silicon Valley names? I&amp;#39;m thinking about writing a blog or book on the subject but still in the exploratory phase.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "1863f7q", "is_robot_indexable": true, "report_reasons": null, "author": "eastofwestla", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1863f7q/what_are_the_best_data_teams_in_business_history/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1863f7q/what_are_the_best_data_teams_in_business_history/", "subreddit_subscribers": 1153739, "created_utc": 1701197335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I guess sex just drives a lot of things\u2026", "author_fullname": "t2_y9a4a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The cover of my linear regression textbook would seem to indicate that sex is the primary driver of salary.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_1868lzn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xY67aT_BeS4myOPknNx-Xb5nCX5oyMncdBogFYBnYWs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701209981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I guess sex just drives a lot of things\u2026&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/f2y15d6py53c1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?auto=webp&amp;s=f9d84e26e1aeda914329fefcd117d7bbfce34f29", "width": 3024, "height": 4032}, "resolutions": [{"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f16e1f8e1d5c6604a39e5c6703c324a3cd6a0db7", "width": 108, "height": 144}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a514e62e236c6bb084acac916742eb0e621fc55a", "width": 216, "height": 288}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b33013c4eec5f939c74e183490c33e03db7dc41b", "width": 320, "height": 426}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=259e8b85bfcfc1e9c8f92bd128139c9963cef371", "width": 640, "height": 853}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b666ab4efc2f022b54885e60e75e85de116f6c64", "width": 960, "height": 1280}, {"url": "https://preview.redd.it/f2y15d6py53c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=28b00aa2195cc5dae8733ed3f1d687b4a3d2acba", "width": 1080, "height": 1440}], "variants": {}, "id": "wCOziiXNYUc4zLReujU89Cs6H64imHl3b9SOiqTMAAA"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "1868lzn", "is_robot_indexable": true, "report_reasons": null, "author": "WartimeHotTot", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1868lzn/the_cover_of_my_linear_regression_textbook_would/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/f2y15d6py53c1.jpg", "subreddit_subscribers": 1153739, "created_utc": 1701209981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I ran into an intersting case today, which I would like to discuss. I have changed some details to keep the companies involved anonymous.\n\nThe shopping portal GoGreen\ud83c\udf4f allows customers to view the carbon footprint of each item they shop, enabling them to make informed decisions that could minimize their environmental impact. While shopping, the customer sees a bar chart that shows both the stacked CO2 emisions from all the items they have selected so far, and a second bar that that shows the stacked CO2 emisions for alternative items with a smaller footprint.\n\n[An illustration of the described bar chart](https://preview.redd.it/fzl2lt2ud23c1.png?width=400&amp;format=png&amp;auto=webp&amp;s=85cae8280eb2fc32cf5c385af3760efd485f02c9)\n\nUnfortunatelt, not all sub-vendors are able to provide an CO2 estimate for all their items. When this happens, GoGreen\ud83c\udf4f instead relys on an official rapport that have calculated the average carbon footprint for a huge amount of items. This will be an average value based on similar products.\n\nThe problem is this: Vendors who are spending their ressources on providing good CO2 estiamates naturally wants to be rewarded for this effort. However, it is not clear how this should be done in a fair way. I see several solutions, but neither are good:\n\n1. If we fill the nan-values based on the averages from the rapport, the averages will sometimes comes out better. This would create an incentive to not rapport the carbon footprint if the vendors suspects they will come out worse than the average\n2. If we exclude the nan-values, this will essentailly be the same as saying the footprint is zero, which would insentivice the previous behaviour even more\n3. If we penalise the vendors who don't report a CO2 estimate with a \"bad data quality tax\", this will be seen unfair by the user's who don't rapport such estimates (who in many cases have legitimate reasons why they are not able to). And we simply do not have enough data to create a fair penalty.\n\nThis whole thing would not be a problem if all vendors either did or did not offer CO2 estimates. However, it becomes a huge fairness-issue when there is a mix. We want to reward good data quality, but is has to be done fairly. We also need to present this in a way that enables customers to make well-informed decisions.\n\nAnyone with relevant tips or experiences? \ud83e\udd13", "author_fullname": "t2_2boatuxv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to reward good data quality in decision making?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 134, "top_awarded_type": null, "hide_score": false, "media_metadata": {"fzl2lt2ud23c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 103, "x": 108, "u": "https://preview.redd.it/fzl2lt2ud23c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ddab52c1c22e111c6f08137feaceebc1492301c"}, {"y": 207, "x": 216, "u": "https://preview.redd.it/fzl2lt2ud23c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7512f8d09eb05df991b979e804e8b1cf6a0757c2"}, {"y": 307, "x": 320, "u": "https://preview.redd.it/fzl2lt2ud23c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=977eac0ed475303e85da1cc874f4bda4661bd64a"}], "s": {"y": 384, "x": 400, "u": "https://preview.redd.it/fzl2lt2ud23c1.png?width=400&amp;format=png&amp;auto=webp&amp;s=85cae8280eb2fc32cf5c385af3760efd485f02c9"}, "id": "fzl2lt2ud23c1"}}, "name": "t3_185su4m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Am6pP6EHxv93XV6vCaENJAVGFjzC-xt-8cB2jGJEK-Q.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701166672.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I ran into an intersting case today, which I would like to discuss. I have changed some details to keep the companies involved anonymous.&lt;/p&gt;\n\n&lt;p&gt;The shopping portal GoGreen\ud83c\udf4f allows customers to view the carbon footprint of each item they shop, enabling them to make informed decisions that could minimize their environmental impact. While shopping, the customer sees a bar chart that shows both the stacked CO2 emisions from all the items they have selected so far, and a second bar that that shows the stacked CO2 emisions for alternative items with a smaller footprint.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fzl2lt2ud23c1.png?width=400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85cae8280eb2fc32cf5c385af3760efd485f02c9\"&gt;An illustration of the described bar chart&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Unfortunatelt, not all sub-vendors are able to provide an CO2 estimate for all their items. When this happens, GoGreen\ud83c\udf4f instead relys on an official rapport that have calculated the average carbon footprint for a huge amount of items. This will be an average value based on similar products.&lt;/p&gt;\n\n&lt;p&gt;The problem is this: Vendors who are spending their ressources on providing good CO2 estiamates naturally wants to be rewarded for this effort. However, it is not clear how this should be done in a fair way. I see several solutions, but neither are good:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;If we fill the nan-values based on the averages from the rapport, the averages will sometimes comes out better. This would create an incentive to not rapport the carbon footprint if the vendors suspects they will come out worse than the average&lt;/li&gt;\n&lt;li&gt;If we exclude the nan-values, this will essentailly be the same as saying the footprint is zero, which would insentivice the previous behaviour even more&lt;/li&gt;\n&lt;li&gt;If we penalise the vendors who don&amp;#39;t report a CO2 estimate with a &amp;quot;bad data quality tax&amp;quot;, this will be seen unfair by the user&amp;#39;s who don&amp;#39;t rapport such estimates (who in many cases have legitimate reasons why they are not able to). And we simply do not have enough data to create a fair penalty.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;This whole thing would not be a problem if all vendors either did or did not offer CO2 estimates. However, it becomes a huge fairness-issue when there is a mix. We want to reward good data quality, but is has to be done fairly. We also need to present this in a way that enables customers to make well-informed decisions.&lt;/p&gt;\n\n&lt;p&gt;Anyone with relevant tips or experiences? \ud83e\udd13&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "185su4m", "is_robot_indexable": true, "report_reasons": null, "author": "PostponeIdiocracy", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/185su4m/how_to_reward_good_data_quality_in_decision_making/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/185su4m/how_to_reward_good_data_quality_in_decision_making/", "subreddit_subscribers": 1153739, "created_utc": 1701166672.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What are some useful relationships/graphs you guys use with independent variables and the target variable when doing the initial EDA? Assuming most of your variables are categorical.", "author_fullname": "t2_495cn7pm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EDA With Binary Classification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185sk3d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701165553.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some useful relationships/graphs you guys use with independent variables and the target variable when doing the initial EDA? Assuming most of your variables are categorical.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "185sk3d", "is_robot_indexable": true, "report_reasons": null, "author": "Throwawayforgainz99", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/185sk3d/eda_with_binary_classification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/185sk3d/eda_with_binary_classification/", "subreddit_subscribers": 1153739, "created_utc": 1701165553.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_anwr3x7ou", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A new, reactive Python+SQL notebook to help you turn your data exploration into a live app", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_18668pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/y0-xdzYOfuuA5fBxv09lHdwkARWj9azT1L6Tforf3Lg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701204384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/Zero-True/zero-true", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?auto=webp&amp;s=7e0ce8d5269dac3c70fba1667406432a64a7acb5", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fe03888f8be2ca002b0d2c49638a998b65a470e8", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1464921e37cb796989b1c7020dbbe7f1a0d196ef", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=911046a79995054c276fe95227ca1280bcbf0c13", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=58449164475d974f84601b4ea206b9aed8077094", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=50e02cd72f26ecff41ff6253165e642408503a25", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/66uteh7CXtg9GpdiYejBxmZ1FnbEYMY3LO5ywgCgZYU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d469fc82ce1aae16779605523373f994f4ce0d6b", "width": 1080, "height": 540}], "variants": {}, "id": "6WSnC8vemhiYjdqr2X32oPJ6YfOIzAoo7SYByY-tJw8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "18668pe", "is_robot_indexable": true, "report_reasons": null, "author": "zero-true", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18668pe/a_new_reactive_pythonsql_notebook_to_help_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/Zero-True/zero-true", "subreddit_subscribers": 1153739, "created_utc": 1701204384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm curious if anyone has dared putting a solution that uses an LLM (e.g. ChatGPT) to parse unstructured text into e.g. JSON?\n\nI've only tested this with ChatGPT and it works surprisingly well. But I'm still not convinced I could trust it to go through large scales of text in production.", "author_fullname": "t2_2boatuxv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experiences using ChatGPT to structure unstructured data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185sq4v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701166220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious if anyone has dared putting a solution that uses an LLM (e.g. ChatGPT) to parse unstructured text into e.g. JSON?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve only tested this with ChatGPT and it works surprisingly well. But I&amp;#39;m still not convinced I could trust it to go through large scales of text in production.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "185sq4v", "is_robot_indexable": true, "report_reasons": null, "author": "PostponeIdiocracy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/185sq4v/experiences_using_chatgpt_to_structure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/185sq4v/experiences_using_chatgpt_to_structure/", "subreddit_subscribers": 1153739, "created_utc": 1701166220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ve been doing some form of data analysis for 15 years and I\u2019m so exhausted. My job has been mostly in retail and selling office supplies isn\u2019t very exciting, but at this point I think I\u2019d feel the same at a non-profit for a good cause. I\u2019m not really interested in keeping up with industry trends and tech. I just want to enjoy the time I have left on this world.\n\nBut unless I go full hermit, what the hell do I do to earn a living until I can finally retire (assuming that happens at 65)?", "author_fullname": "t2_nya9l4wu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nearly 40 and burnt out. How am I supposed to last to retirement?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_186chg2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701219765.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been doing some form of data analysis for 15 years and I\u2019m so exhausted. My job has been mostly in retail and selling office supplies isn\u2019t very exciting, but at this point I think I\u2019d feel the same at a non-profit for a good cause. I\u2019m not really interested in keeping up with industry trends and tech. I just want to enjoy the time I have left on this world.&lt;/p&gt;\n\n&lt;p&gt;But unless I go full hermit, what the hell do I do to earn a living until I can finally retire (assuming that happens at 65)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "186chg2", "is_robot_indexable": true, "report_reasons": null, "author": "TheUserAboveFarted", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/186chg2/nearly_40_and_burnt_out_how_am_i_supposed_to_last/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/186chg2/nearly_40_and_burnt_out_how_am_i_supposed_to_last/", "subreddit_subscribers": 1153739, "created_utc": 1701219765.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Do you ever embed sanity checks in your reporting?\n\nHow thorough are you, what kind do you use, etc?\n\nDo you make it part of the reporting query, or do you trust a separate query with the task?", "author_fullname": "t2_a1m88yyw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Losing my sanity (checks)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185r2od", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701159215.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you ever embed sanity checks in your reporting?&lt;/p&gt;\n\n&lt;p&gt;How thorough are you, what kind do you use, etc?&lt;/p&gt;\n\n&lt;p&gt;Do you make it part of the reporting query, or do you trust a separate query with the task?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "185r2od", "is_robot_indexable": true, "report_reasons": null, "author": "Status-Efficiency851", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/185r2od/losing_my_sanity_checks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/185r2od/losing_my_sanity_checks/", "subreddit_subscribers": 1153739, "created_utc": 1701159215.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all!\n\nIn this 1st tutorial published on Medium, I provide some general guidelines around ARIMA models/parameters for Time Series Forecasting. A series of tutorials will follow, as well.\n\nPlease note that this post is not promotional, as this article is not behind Medium's paywall at the moment.\n\nI would like to know your thoughts and/or experience on such topics and please comment in case you have anything to add!\n\n[https://medium.com/python-in-plain-english/time-series-episode-0-brief-theory-of-forecasting-with-arima-eaf973e4dc30](https://medium.com/python-in-plain-english/time-series-episode-0-brief-theory-of-forecasting-with-arima-eaf973e4dc30)", "author_fullname": "t2_leswydvt8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ARIMA models and parameters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1866p5x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Analysis", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701205536.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all!&lt;/p&gt;\n\n&lt;p&gt;In this 1st tutorial published on Medium, I provide some general guidelines around ARIMA models/parameters for Time Series Forecasting. A series of tutorials will follow, as well.&lt;/p&gt;\n\n&lt;p&gt;Please note that this post is not promotional, as this article is not behind Medium&amp;#39;s paywall at the moment.&lt;/p&gt;\n\n&lt;p&gt;I would like to know your thoughts and/or experience on such topics and please comment in case you have anything to add!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/python-in-plain-english/time-series-episode-0-brief-theory-of-forecasting-with-arima-eaf973e4dc30\"&gt;https://medium.com/python-in-plain-english/time-series-episode-0-brief-theory-of-forecasting-with-arima-eaf973e4dc30&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?auto=webp&amp;s=ffd91b0077157120cb0a3758ec0f478c45302266", "width": 1200, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=979f34d475a7a05d09f6f38ddd45f35b72c37e41", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=706887baff530fa0bb1e1b8ec75b36888ec789c5", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c951b7dda962ec8e8c7c095ec5e639196d7ac1f9", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ecd595b0dd14dd3e098a6ca607bd06dc1fbe21ef", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=be8d215af4259d2227b38ac80e665b16da7e3dbf", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/F7Ne61FVgmyQJERjo7ktedrCzVblvifNzwgMAblnx6c.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a53d4231c45c8cb95e5243bdc4068a6842a0c15b", "width": 1080, "height": 810}], "variants": {}, "id": "UURDOTgc21yplVXOwVaUOnjeoK3rIot5VOM9OO7cDG4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1866p5x", "is_robot_indexable": true, "report_reasons": null, "author": "vasikal", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1866p5x/arima_models_and_parameters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1866p5x/arima_models_and_parameters/", "subreddit_subscribers": 1153739, "created_utc": 1701205536.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Let's say there's an binary classification problem for imbalanced response variable. Assume Xgboost &amp; up/down sampling is used &amp; train/validation/test partitions. The goal is to make a final model that maximizes recall while keeping false positive rate(fpr) below a certain value(threshold).\n\nProcess is to \n1. Find best set of hyperparameters where each tuning trial fits model on train dataset and evaluated against validation dataset. The score from the evaluation against the validation dataset is returned to the tuner at the end of every trial to provide feedback to the tuner.\n2. With best parameters found, train final model on train dataset and look at performance using hold out test dataset partition.\n\n Which option below is the right way to do this with bayesian hypertuning?\n\nA) make a custom evaluation metric like this:\nMetric = recall if fpr &lt; threshold else 0. Logic is to inform the tuner that this outcome is unacceptable if the fpr is past threshold.\n\nB) make a custom evaluation metric like this:\nMetric = recall if fpr &lt; threshold else recall*(some penalty between 0 and 1). Logic is that we inform the tuner that it has exceeded the threshold without making metric become 0. \n\nC) use logloss for the evaluation metric to find best parameters. Fit the final model on the train partition, use the validation dataset to find the threshold that maximizes the recall while remaining under the fpr threshold. \n\nD) Something else. I would love to hear any well thought ideas.", "author_fullname": "t2_3m0iqrm1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Binary classification with multiple goals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1864i6o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701199990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s say there&amp;#39;s an binary classification problem for imbalanced response variable. Assume Xgboost &amp;amp; up/down sampling is used &amp;amp; train/validation/test partitions. The goal is to make a final model that maximizes recall while keeping false positive rate(fpr) below a certain value(threshold).&lt;/p&gt;\n\n&lt;p&gt;Process is to \n1. Find best set of hyperparameters where each tuning trial fits model on train dataset and evaluated against validation dataset. The score from the evaluation against the validation dataset is returned to the tuner at the end of every trial to provide feedback to the tuner.\n2. With best parameters found, train final model on train dataset and look at performance using hold out test dataset partition.&lt;/p&gt;\n\n&lt;p&gt;Which option below is the right way to do this with bayesian hypertuning?&lt;/p&gt;\n\n&lt;p&gt;A) make a custom evaluation metric like this:\nMetric = recall if fpr &amp;lt; threshold else 0. Logic is to inform the tuner that this outcome is unacceptable if the fpr is past threshold.&lt;/p&gt;\n\n&lt;p&gt;B) make a custom evaluation metric like this:\nMetric = recall if fpr &amp;lt; threshold else recall*(some penalty between 0 and 1). Logic is that we inform the tuner that it has exceeded the threshold without making metric become 0. &lt;/p&gt;\n\n&lt;p&gt;C) use logloss for the evaluation metric to find best parameters. Fit the final model on the train partition, use the validation dataset to find the threshold that maximizes the recall while remaining under the fpr threshold. &lt;/p&gt;\n\n&lt;p&gt;D) Something else. I would love to hear any well thought ideas.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "1864i6o", "is_robot_indexable": true, "report_reasons": null, "author": "DSby2021", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1864i6o/binary_classification_with_multiple_goals/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1864i6o/binary_classification_with_multiple_goals/", "subreddit_subscribers": 1153739, "created_utc": 1701199990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello!\n\nAll my data wrangling is currently done. I have 4 (2 in each pair) sets of data each with 500 \"baskets\" of groceries . The data contains all the item descriptions, together with the target variable of interest, the subdepartment the item is in. (think carrots--&gt;produce). Each data pair contains the first bought and then second bought basket in different files. I am trying to predict, given a bought basket of groceries, what is the mostly likely sub department a customer would buy from on their next visit.\n\nI am still just a Computer Science student so I lack the experience here to pick a good predictive model for my situation. I was thinking of doing a Hollistic Linear Regression, or a simple KNN, but I thought about asking here first to see if anyone had any recommendations for me. Unfortunately I cannot use any models that involve hypothesis testing.\n\nI apologize if I missed a rule before posting this. Thanks in advance!", "author_fullname": "t2_1713jl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Recommendations for a Prediction Model", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "meta", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185qkmm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701157143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;All my data wrangling is currently done. I have 4 (2 in each pair) sets of data each with 500 &amp;quot;baskets&amp;quot; of groceries . The data contains all the item descriptions, together with the target variable of interest, the subdepartment the item is in. (think carrots--&amp;gt;produce). Each data pair contains the first bought and then second bought basket in different files. I am trying to predict, given a bought basket of groceries, what is the mostly likely sub department a customer would buy from on their next visit.&lt;/p&gt;\n\n&lt;p&gt;I am still just a Computer Science student so I lack the experience here to pick a good predictive model for my situation. I was thinking of doing a Hollistic Linear Regression, or a simple KNN, but I thought about asking here first to see if anyone had any recommendations for me. Unfortunately I cannot use any models that involve hypothesis testing.&lt;/p&gt;\n\n&lt;p&gt;I apologize if I missed a rule before posting this. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "481ee318-d77d-11e7-a4a3-0e8624d7129a", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#7193ff", "id": "185qkmm", "is_robot_indexable": true, "report_reasons": null, "author": "Akavire", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/185qkmm/recommendations_for_a_prediction_model/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/185qkmm/recommendations_for_a_prediction_model/", "subreddit_subscribers": 1153739, "created_utc": 1701157143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How would you identify them? How do you think they get in? \n\nDo you think they enjoy an unfair advantage of calling the shots, higher salary, etc. in your org?\n\nEDIT: [Same question. Different Sub. Interesting responses. Does region influence how data science is perceived as a profession?\n](https://www.reddit.com/r/developersIndia/s/xfCukCZg0O)", "author_fullname": "t2_al1087x2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you think there are too many frauds masquerading as data scientists?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185kril", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.35, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701152428.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701137268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How would you identify them? How do you think they get in? &lt;/p&gt;\n\n&lt;p&gt;Do you think they enjoy an unfair advantage of calling the shots, higher salary, etc. in your org?&lt;/p&gt;\n\n&lt;p&gt;EDIT: &lt;a href=\"https://www.reddit.com/r/developersIndia/s/xfCukCZg0O\"&gt;Same question. Different Sub. Interesting responses. Does region influence how data science is perceived as a profession?\n&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "185kril", "is_robot_indexable": true, "report_reasons": null, "author": "OverratedDataScience", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/185kril/do_you_think_there_are_too_many_frauds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/185kril/do_you_think_there_are_too_many_frauds/", "subreddit_subscribers": 1153739, "created_utc": 1701137268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I don't really care to argue the semantics of if it's going to happen, because I've been seeing it happen already. 10 years ago, at a Fortune 500 company, there were hundreds upon hundreds of analysts and technical people in office or working for a company. Now, any team that you get placed on, there is 5 to 15 people maximum, and The number of people who are losing their jobs is in the tens of thousands... People are just automating out everything, and now with AI and Microsoft partnering together, Microsoft Azure has the capability to automate even data science capabilities....\n\n\nSo the writing is on the wall. It's only a matter of time, I would estimate like 10 years or so, before most of our jobs are eliminated. What are you guys see yourselves doing after that time? What's your 10-year plan since they want to eliminate all human beings entirely from big companies?", "author_fullname": "t2_dmawn6hx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your 10 year plan after AI and automation eliminates most jobs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18610ba", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701191273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t really care to argue the semantics of if it&amp;#39;s going to happen, because I&amp;#39;ve been seeing it happen already. 10 years ago, at a Fortune 500 company, there were hundreds upon hundreds of analysts and technical people in office or working for a company. Now, any team that you get placed on, there is 5 to 15 people maximum, and The number of people who are losing their jobs is in the tens of thousands... People are just automating out everything, and now with AI and Microsoft partnering together, Microsoft Azure has the capability to automate even data science capabilities....&lt;/p&gt;\n\n&lt;p&gt;So the writing is on the wall. It&amp;#39;s only a matter of time, I would estimate like 10 years or so, before most of our jobs are eliminated. What are you guys see yourselves doing after that time? What&amp;#39;s your 10-year plan since they want to eliminate all human beings entirely from big companies?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "18610ba", "is_robot_indexable": true, "report_reasons": null, "author": "InevitableTraining69", "discussion_type": null, "num_comments": 112, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/18610ba/what_is_your_10_year_plan_after_ai_and_automation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/18610ba/what_is_your_10_year_plan_after_ai_and_automation/", "subreddit_subscribers": 1153739, "created_utc": 1701191273.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}