{"kind": "Listing", "data": {"after": "t3_1823qih", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_11093gvf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A takedown of Alteryx, no-code data as a concept and the people who force it on talented data folks.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181mou1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1700694719.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ucovi-data.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ucovi-data.com/BlogLatest.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181mou1", "is_robot_indexable": true, "report_reasons": null, "author": "UCOVINed", "discussion_type": null, "num_comments": 68, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181mou1/a_takedown_of_alteryx_nocode_data_as_a_concept/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ucovi-data.com/BlogLatest.html", "subreddit_subscribers": 141304, "created_utc": 1700694719.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Without going into too much detail, I am a data engineer in an industry with antiquated data collecting methods (so data transformations are complex), and often convoluted business logic (due to nature of the business).\n\nMy team works close to the end of the data stream, so we build data pipelines that are served directly to analytics teams. Because of this, we are expected to develop strong business knowledge, while also working through the complexity of the data itself.\n\nI find myself often struggling to balance these two sides - planning and building infrastructure (understanding \u201cwhat\u201d the data is), but also fielding questions about the intricacies of the data itself from analysts (\u201cwhy\u201d the data is what it is). It feels like my work goes between being a developer and something like an \u201canalytics engineer\u201d in close proximity to the analysis.\n\nMy documentation and note taking feels pretty strong, but at times it is challenging and the context switching can be tough. My manager and colleagues are happy with my work, so that part is good. \n\nCurious to hear from others on this balancing act and how they handled it. I know there is a lot of variance in this type of thing across teams/companies/industries.", "author_fullname": "t2_3jl2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Balance of \u201cWhat\u201d vs \u201cWhy\u201d in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181gm5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700679137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Without going into too much detail, I am a data engineer in an industry with antiquated data collecting methods (so data transformations are complex), and often convoluted business logic (due to nature of the business).&lt;/p&gt;\n\n&lt;p&gt;My team works close to the end of the data stream, so we build data pipelines that are served directly to analytics teams. Because of this, we are expected to develop strong business knowledge, while also working through the complexity of the data itself.&lt;/p&gt;\n\n&lt;p&gt;I find myself often struggling to balance these two sides - planning and building infrastructure (understanding \u201cwhat\u201d the data is), but also fielding questions about the intricacies of the data itself from analysts (\u201cwhy\u201d the data is what it is). It feels like my work goes between being a developer and something like an \u201canalytics engineer\u201d in close proximity to the analysis.&lt;/p&gt;\n\n&lt;p&gt;My documentation and note taking feels pretty strong, but at times it is challenging and the context switching can be tough. My manager and colleagues are happy with my work, so that part is good. &lt;/p&gt;\n\n&lt;p&gt;Curious to hear from others on this balancing act and how they handled it. I know there is a lot of variance in this type of thing across teams/companies/industries.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181gm5g", "is_robot_indexable": true, "report_reasons": null, "author": "Stupideye", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/181gm5g/balance_of_what_vs_why_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181gm5g/balance_of_what_vs_why_in_data_engineering/", "subreddit_subscribers": 141304, "created_utc": 1700679137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TL;DR: Should we add dbt to our tech stack when our existing stack doesn't really require it?\n\nWe're using Google Cloud Storage (parquet) and Snowflake as storage layer and Google Dataproc (managed Spark, via PySpark) for data processing. Dataproc jobs are scheduled via Airflow. \n\nWe recently started using dbt for some transformations and data integration steps within Snowflake (filtering, joining, aggregating).\n\nThe data scientists in our team like it because they can basically work in SQL. A few software developers and data engineers are sceptical, arguing that maintaining an additional data processing framework increases the complexity of the tool chain - especially given that Python + PySpark already do the job. \n\nI'm torn between seeing the case for a simple set of tools and enjoying the simplicity of implementing data integration steps in dbt. How would you decide whether or not to add the complexity of another data processing framework? Have you been in similar situations? ", "author_fullname": "t2_l895f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "More tools, more complexity?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181f78m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700675579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: Should we add dbt to our tech stack when our existing stack doesn&amp;#39;t really require it?&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re using Google Cloud Storage (parquet) and Snowflake as storage layer and Google Dataproc (managed Spark, via PySpark) for data processing. Dataproc jobs are scheduled via Airflow. &lt;/p&gt;\n\n&lt;p&gt;We recently started using dbt for some transformations and data integration steps within Snowflake (filtering, joining, aggregating).&lt;/p&gt;\n\n&lt;p&gt;The data scientists in our team like it because they can basically work in SQL. A few software developers and data engineers are sceptical, arguing that maintaining an additional data processing framework increases the complexity of the tool chain - especially given that Python + PySpark already do the job. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m torn between seeing the case for a simple set of tools and enjoying the simplicity of implementing data integration steps in dbt. How would you decide whether or not to add the complexity of another data processing framework? Have you been in similar situations? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181f78m", "is_robot_indexable": true, "report_reasons": null, "author": "FlatRobots", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181f78m/more_tools_more_complexity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181f78m/more_tools_more_complexity/", "subreddit_subscribers": 141304, "created_utc": 1700675579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have a cross platform (android + iOS + web) marketplace with many users and several user journeys and use many 3rd party tools. We need to totally refine our data layer and tagging. \n\nIs it really possible to exchange many SDK-s (e.g: CRM, Facebook ads, etc..) with the segment's and re-use segment events in our 3rd party tools? \n\nAny experience? What is possible what is not? ", "author_fullname": "t2_599d1gmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with Segment or Rudderstack? Does it really worth to use it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181wome", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700728395.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have a cross platform (android + iOS + web) marketplace with many users and several user journeys and use many 3rd party tools. We need to totally refine our data layer and tagging. &lt;/p&gt;\n\n&lt;p&gt;Is it really possible to exchange many SDK-s (e.g: CRM, Facebook ads, etc..) with the segment&amp;#39;s and re-use segment events in our 3rd party tools? &lt;/p&gt;\n\n&lt;p&gt;Any experience? What is possible what is not? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181wome", "is_robot_indexable": true, "report_reasons": null, "author": "kozakdavid07", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181wome/experience_with_segment_or_rudderstack_does_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181wome/experience_with_segment_or_rudderstack_does_it/", "subreddit_subscribers": 141304, "created_utc": 1700728395.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Who should do the dimensional modelling? By that I mean who is responsible for creating a logic layer in the warehouse  from a transactional system. The data engineer says its not their job and the data scientist seems not capable. \n\nCurrently we just have a massive dump of data , not really a warehouse IMO and there is no single schema whatsoever . I am new in this company but not new to data and never saw something like this before.\n\nAny pointers appreciated.", "author_fullname": "t2_ndvkq3ghs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling ownership", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181vzlj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700725475.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Who should do the dimensional modelling? By that I mean who is responsible for creating a logic layer in the warehouse  from a transactional system. The data engineer says its not their job and the data scientist seems not capable. &lt;/p&gt;\n\n&lt;p&gt;Currently we just have a massive dump of data , not really a warehouse IMO and there is no single schema whatsoever . I am new in this company but not new to data and never saw something like this before.&lt;/p&gt;\n\n&lt;p&gt;Any pointers appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181vzlj", "is_robot_indexable": true, "report_reasons": null, "author": "Special_Bid_3176", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181vzlj/data_modelling_ownership/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181vzlj/data_modelling_ownership/", "subreddit_subscribers": 141304, "created_utc": 1700725475.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a rant.\nI've talked with manager and He told me that at end of january I've to do PL-300.\n\nI work since two years as Data Consultant(mainly DE/ DA / PBI /PMO/ dwh)for one of big 4 consultancy, 6 role in 1.\n\nMy feedback are extremely good, deadlines are meet, numbers do match and since I started pipelines only crash few times during year thanks to my fixes, working on 5-6 project as same time,\nProposed new innovations, projects that has been accepted and made the company earn.\n\nI've done PL-300 twice and did not pass it both for lack of time in studying and because I was forced to do it because my manager was like \"try it, if you pass, it's less pain\".\nI'm more strong in DE and we agreed to go for DE certification but they want PL-300.\n\nIn my team I'm the only one doing PowerBi and DE / SA / PM while I build like 5-6 dashboard for the same client and sometimes I do finish late but is hard for me to find time to study as\n\nI'm always fighting for good data quality, messy data model and all dirty hell of data made by multiple people working on customer.\n\nClient doesn't require it but my company does as it has partnerships.\n\nI told my manager that I hate certifications due to memory and the thought of passing hours after work and during holidays studying it's a no for me\nAs they're important for me as I De-stress.\nAlso I've added \"if I take the certification, you take it too. It is not fair and you'll understand what it will mean to study for it, why since you're tech manager and you work with me, I'm the only one forced to take certs but you are not?\"\n\nI've met a lot of people with senior experience working in big companies that hates certifications.\nI do like studying but not to sacrifice my holidays.\n\n1. Should I start a plan B?\n2. Have you been in same shoes?", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rant about certifications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181zzgt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700741938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a rant.\nI&amp;#39;ve talked with manager and He told me that at end of january I&amp;#39;ve to do PL-300.&lt;/p&gt;\n\n&lt;p&gt;I work since two years as Data Consultant(mainly DE/ DA / PBI /PMO/ dwh)for one of big 4 consultancy, 6 role in 1.&lt;/p&gt;\n\n&lt;p&gt;My feedback are extremely good, deadlines are meet, numbers do match and since I started pipelines only crash few times during year thanks to my fixes, working on 5-6 project as same time,\nProposed new innovations, projects that has been accepted and made the company earn.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done PL-300 twice and did not pass it both for lack of time in studying and because I was forced to do it because my manager was like &amp;quot;try it, if you pass, it&amp;#39;s less pain&amp;quot;.\nI&amp;#39;m more strong in DE and we agreed to go for DE certification but they want PL-300.&lt;/p&gt;\n\n&lt;p&gt;In my team I&amp;#39;m the only one doing PowerBi and DE / SA / PM while I build like 5-6 dashboard for the same client and sometimes I do finish late but is hard for me to find time to study as&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m always fighting for good data quality, messy data model and all dirty hell of data made by multiple people working on customer.&lt;/p&gt;\n\n&lt;p&gt;Client doesn&amp;#39;t require it but my company does as it has partnerships.&lt;/p&gt;\n\n&lt;p&gt;I told my manager that I hate certifications due to memory and the thought of passing hours after work and during holidays studying it&amp;#39;s a no for me\nAs they&amp;#39;re important for me as I De-stress.\nAlso I&amp;#39;ve added &amp;quot;if I take the certification, you take it too. It is not fair and you&amp;#39;ll understand what it will mean to study for it, why since you&amp;#39;re tech manager and you work with me, I&amp;#39;m the only one forced to take certs but you are not?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve met a lot of people with senior experience working in big companies that hates certifications.\nI do like studying but not to sacrifice my holidays.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should I start a plan B?&lt;/li&gt;\n&lt;li&gt;Have you been in same shoes?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181zzgt", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181zzgt/rant_about_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181zzgt/rant_about_certifications/", "subreddit_subscribers": 141304, "created_utc": 1700741938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,   \nHas anyone had experience using either of these two operators to set up Spark in K8s? Any preferences or weird things you ran into?\n\n  \n\\- [https://github.com/GoogleCloudPlatform/spark-on-k8s-operator](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator)\n\n\\- [https://github.com/stackabletech/spark-k8s-operator](https://github.com/stackabletech/spark-k8s-operator)  \n\n\n&amp;#x200B;", "author_fullname": "t2_t9ep875j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache Spark Operators for K8s", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181uv84", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700721026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nHas anyone had experience using either of these two operators to set up Spark in K8s? Any preferences or weird things you ran into?&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://github.com/GoogleCloudPlatform/spark-on-k8s-operator\"&gt;https://github.com/GoogleCloudPlatform/spark-on-k8s-operator&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://github.com/stackabletech/spark-k8s-operator\"&gt;https://github.com/stackabletech/spark-k8s-operator&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/v1jYKbJQG64j9YIa8o09Nnj386fjD8WmiwjaFOk9J5E.jpg?auto=webp&amp;s=d76e7fe9870e04a56ecb8f1e86b453d8c4c8e715", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/v1jYKbJQG64j9YIa8o09Nnj386fjD8WmiwjaFOk9J5E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b20b492420ba8e3c83dc5315b56d19258ac1c44b", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/v1jYKbJQG64j9YIa8o09Nnj386fjD8WmiwjaFOk9J5E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d6d47be570ef97c2f9d059ee648965d3c8fa8a2f", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/v1jYKbJQG64j9YIa8o09Nnj386fjD8WmiwjaFOk9J5E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ecd9635a589e9f11bbf06eed4f8c3e4eb3bebd8c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/v1jYKbJQG64j9YIa8o09Nnj386fjD8WmiwjaFOk9J5E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ea20c577cd85ad373d8d8aa71b60254e4ad388f", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/v1jYKbJQG64j9YIa8o09Nnj386fjD8WmiwjaFOk9J5E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee02da9d2f6d28dc6e5c39f908f71820e925ce1b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/v1jYKbJQG64j9YIa8o09Nnj386fjD8WmiwjaFOk9J5E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ba7a4f2ef018c543e69ad32c884494bdcadc2543", "width": 1080, "height": 540}], "variants": {}, "id": "a_n2GxAwj_f17W6xqN2FO3Y2b2YmsmfGk05Lw1oNnEc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181uv84", "is_robot_indexable": true, "report_reasons": null, "author": "LoquatAlternative680", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181uv84/apache_spark_operators_for_k8s/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181uv84/apache_spark_operators_for_k8s/", "subreddit_subscribers": 141304, "created_utc": 1700721026.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I went from fast pase live updates to whatever B's we have now at the moment profit I work for. I got burnt out on the fast and urgency in the finance world but this is crazy boring. My co workers seem to be busy but there's an hours worth of work per day, max. I'm so bored, give me a challenge", "author_fullname": "t2_h3vtt1xe3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm bored", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1821mrt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700747508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I went from fast pase live updates to whatever B&amp;#39;s we have now at the moment profit I work for. I got burnt out on the fast and urgency in the finance world but this is crazy boring. My co workers seem to be busy but there&amp;#39;s an hours worth of work per day, max. I&amp;#39;m so bored, give me a challenge&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1821mrt", "is_robot_indexable": true, "report_reasons": null, "author": "SoggyChilli", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1821mrt/im_bored/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1821mrt/im_bored/", "subreddit_subscribers": 141304, "created_utc": 1700747508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi\n\n37F \n\nSeeking advice on useful, practical gifts I can get my brother\n\nHe is a beginner coder and is building experience, networking, etc \n\nHe is not yet at the point of searching for jobs, but will be within some months \n\nI have read other threads and found suggestions for books such as three easy pieces and DDIA\n\nSaw a workbook that accompany DDIA and thought maybe that would be practical?\n\nI really want to give him something that will include new and useful information \n\nPlease help!\n\nThank you so much", "author_fullname": "t2_dk413ars", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please advise: Christmas gifts for brother who is beginner coder and programmer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181sldl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700712731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi&lt;/p&gt;\n\n&lt;p&gt;37F &lt;/p&gt;\n\n&lt;p&gt;Seeking advice on useful, practical gifts I can get my brother&lt;/p&gt;\n\n&lt;p&gt;He is a beginner coder and is building experience, networking, etc &lt;/p&gt;\n\n&lt;p&gt;He is not yet at the point of searching for jobs, but will be within some months &lt;/p&gt;\n\n&lt;p&gt;I have read other threads and found suggestions for books such as three easy pieces and DDIA&lt;/p&gt;\n\n&lt;p&gt;Saw a workbook that accompany DDIA and thought maybe that would be practical?&lt;/p&gt;\n\n&lt;p&gt;I really want to give him something that will include new and useful information &lt;/p&gt;\n\n&lt;p&gt;Please help!&lt;/p&gt;\n\n&lt;p&gt;Thank you so much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181sldl", "is_robot_indexable": true, "report_reasons": null, "author": "Cripkate", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181sldl/please_advise_christmas_gifts_for_brother_who_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181sldl/please_advise_christmas_gifts_for_brother_who_is/", "subreddit_subscribers": 141304, "created_utc": 1700712731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\ud83d\udc4b Hey, check out my latest dagster tutorial video of how to build you first asset graph \ud83d\ude1c.", "author_fullname": "t2_h4j43yry", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster Tutorial: Building an Asset Graph", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_181wqix", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gi3tOjDeMVY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Dagster Tutorial: Building an Asset Graph\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Dagster Tutorial: Building an Asset Graph", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gi3tOjDeMVY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Dagster Tutorial: Building an Asset Graph\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/gi3tOjDeMVY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gi3tOjDeMVY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Dagster Tutorial: Building an Asset Graph\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/181wqix", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/JdVuKSIMCHm5yN8UxQ4VD97rzhpQSh4ayGHRAdZL5K0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700728616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\ud83d\udc4b Hey, check out my latest dagster tutorial video of how to build you first asset graph \ud83d\ude1c.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/gi3tOjDeMVY", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lKk_aYdpYV1ji2yTG6DXHUHuVHU9TCt5-S98mjsS_w8.jpg?auto=webp&amp;s=4067ae987c49a2f460061d25e2467f07bf97a515", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/lKk_aYdpYV1ji2yTG6DXHUHuVHU9TCt5-S98mjsS_w8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=25d686673dacee5bcd563bf6aaed33da7c67df5d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/lKk_aYdpYV1ji2yTG6DXHUHuVHU9TCt5-S98mjsS_w8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c008ab0fbc8b45967deafb43286b1c7099e3c951", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/lKk_aYdpYV1ji2yTG6DXHUHuVHU9TCt5-S98mjsS_w8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=de322f5e14cce9981f95f044f8cf1fd25fcf307e", "width": 320, "height": 240}], "variants": {}, "id": "0agbsI1_biZWYJATsFgLDkhG0PU3wLlUKsi_wcuH0zA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "181wqix", "is_robot_indexable": true, "report_reasons": null, "author": "Coder2j", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181wqix/dagster_tutorial_building_an_asset_graph/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/gi3tOjDeMVY", "subreddit_subscribers": 141304, "created_utc": 1700728616.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Dagster Tutorial: Building an Asset Graph", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gi3tOjDeMVY?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Dagster Tutorial: Building an Asset Graph\"&gt;&lt;/iframe&gt;", "author_name": "coder2j", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/gi3tOjDeMVY/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@coder2j"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_21nis1o2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bytebase 2.11.1: Customizable Masking Algorithms and Semantic Types for Data Masking.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_181wcsb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://a.thumbs.redditmedia.com/xnwNlxiy3NEewXf9I3GeJglGBjK-ouKeZwNC4x8PaP0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700726992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bytebase.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.bytebase.com/changelog/bytebase-2-11-1/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/imRbdfXgVGZpJ8LZ74Lu-CUoFgnchJ3kZ_BRGC13ZEE.jpg?auto=webp&amp;s=5a0d045c73f3056936040c38b958372d7a8b3b90", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/imRbdfXgVGZpJ8LZ74Lu-CUoFgnchJ3kZ_BRGC13ZEE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a025e4dc2b64f4e27c3095e2dd5bb691a617f886", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/imRbdfXgVGZpJ8LZ74Lu-CUoFgnchJ3kZ_BRGC13ZEE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5855fcee9ca13834409984a1c10f7b26d47c6b58", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/imRbdfXgVGZpJ8LZ74Lu-CUoFgnchJ3kZ_BRGC13ZEE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7476df5b07a773ddbeffb85005820d0106fe224d", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/imRbdfXgVGZpJ8LZ74Lu-CUoFgnchJ3kZ_BRGC13ZEE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=e9479518ee288617b4eb8069328ae130a25ad37f", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/imRbdfXgVGZpJ8LZ74Lu-CUoFgnchJ3kZ_BRGC13ZEE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e45aa6537e51539ae95e464d6d5b0c3c700afc2", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/imRbdfXgVGZpJ8LZ74Lu-CUoFgnchJ3kZ_BRGC13ZEE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91898d2a32ec5b83c3275f62b25a3759b99e1ca9", "width": 1080, "height": 607}], "variants": {}, "id": "Mh3ROjg5Y--EPzTtldGfm0Gm4BDULuaJj3U5jVSs3t8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "181wcsb", "is_robot_indexable": true, "report_reasons": null, "author": "placestamphere_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181wcsb/bytebase_2111_customizable_masking_algorithms_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.bytebase.com/changelog/bytebase-2-11-1/", "subreddit_subscribers": 141304, "created_utc": 1700726992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_uwe2fsd1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Scrape Walmart Data with Ease: A Step-by-Step Guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_181vwrj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3plBK_xTsipBzZ3dQp_mQ_KCyV2EMnOyF4wqS7k_EUw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700725154.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "plainenglish.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://plainenglish.io/blog/how-to-scrape-walmart-data-with-ease-a-step-by-step-guide", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7K6aB6miR_LFO4dTIiQo6en3LJo9zuOn4s2lenKsWno.jpg?auto=webp&amp;s=78ee7a2f14f06e28fa21f1e29273dda95ec16c96", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/7K6aB6miR_LFO4dTIiQo6en3LJo9zuOn4s2lenKsWno.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6831353729e3e9e5a1f112b9c293ee70cf14a525", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/7K6aB6miR_LFO4dTIiQo6en3LJo9zuOn4s2lenKsWno.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=afe587eeb1ff23d291fac7c37a1d17ca2bab83c1", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/7K6aB6miR_LFO4dTIiQo6en3LJo9zuOn4s2lenKsWno.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=951b06eaf8e3f3a420df8ceadcb83ad01e2aae07", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/7K6aB6miR_LFO4dTIiQo6en3LJo9zuOn4s2lenKsWno.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=448432bf5f0c4b8ce1b1b0b73efb44569437cb56", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/7K6aB6miR_LFO4dTIiQo6en3LJo9zuOn4s2lenKsWno.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=dbf0a3a452f7c87cbe75fc5d057def73fcb7aa61", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/7K6aB6miR_LFO4dTIiQo6en3LJo9zuOn4s2lenKsWno.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0a9cc05477d7949381f297e160a3d3f2b74673d5", "width": 1080, "height": 567}], "variants": {}, "id": "9I1SgUFBzx1BX6uI79jcxcfAPoqd3CQW7hWPvc-4lZc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "181vwrj", "is_robot_indexable": true, "report_reasons": null, "author": "TheLostWanderer47", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181vwrj/how_to_scrape_walmart_data_with_ease_a_stepbystep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://plainenglish.io/blog/how-to-scrape-walmart-data-with-ease-a-step-by-step-guide", "subreddit_subscribers": 141304, "created_utc": 1700725154.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI just completed a small personal project which is a ELT pipeline that ingests data from API daily at a specific time and gets the air-quality data of the previous day and load it as a parquet file in a GCS bucket partitioned by date. Then it takes the same file and loads it into BigQuery. This is orchestrated by prefect and it occurs right after each other, they have a parent flow and are subflows. It works fine so far but I have a few doubts,\n\n1. What if I want to have different time intervals for API to GCS and GCS to BigQuery. How do I partition the data then. Lets say I hit the API every hour, they should there be a hourly partition where there are separate files for each hour or can I have one file for each day where i append the hourly data?(Does that depend on the size of the file)\n2. Let's assume i change it to hourly partition and sperate the two flows. Data is loaded into BigQuery once a day while data is loaded to GCS once an hour. Now should i just go through the partitions that has the matching date(24 partitions) everytime or is there a way to find out which partitions have already been loaded and which one haven't?\n3. Is decoupling these 2 flows the better practice? How does it work in production pipelines? If yes then how do I ensure all of the previous days data is in the bucket before loading it to BigQuery?", "author_fullname": "t2_w16t5qhn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to orchestrate flows with 2 different time intervals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181h5j3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700680515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just completed a small personal project which is a ELT pipeline that ingests data from API daily at a specific time and gets the air-quality data of the previous day and load it as a parquet file in a GCS bucket partitioned by date. Then it takes the same file and loads it into BigQuery. This is orchestrated by prefect and it occurs right after each other, they have a parent flow and are subflows. It works fine so far but I have a few doubts,&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What if I want to have different time intervals for API to GCS and GCS to BigQuery. How do I partition the data then. Lets say I hit the API every hour, they should there be a hourly partition where there are separate files for each hour or can I have one file for each day where i append the hourly data?(Does that depend on the size of the file)&lt;/li&gt;\n&lt;li&gt;Let&amp;#39;s assume i change it to hourly partition and sperate the two flows. Data is loaded into BigQuery once a day while data is loaded to GCS once an hour. Now should i just go through the partitions that has the matching date(24 partitions) everytime or is there a way to find out which partitions have already been loaded and which one haven&amp;#39;t?&lt;/li&gt;\n&lt;li&gt;Is decoupling these 2 flows the better practice? How does it work in production pipelines? If yes then how do I ensure all of the previous days data is in the bucket before loading it to BigQuery?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181h5j3", "is_robot_indexable": true, "report_reasons": null, "author": "booberrypie_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/181h5j3/how_to_orchestrate_flows_with_2_different_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181h5j3/how_to_orchestrate_flows_with_2_different_time/", "subreddit_subscribers": 141304, "created_utc": 1700680515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a question on how you would approach this data model design. \n\nMy data model records Sales Revenue, with the special case that a Sales Transaction can generate Sales Revenue for multiple offices.\n\nAn example of a Sales Transactions worth 10k can generate 3k revenue for Office 1 and 7k revenue for Office 2.\n\nCurrently, my revenue fact table stores the generated revenue like this:\noffice_key, date_key, transaction_key, revenue_amount\n\nI have a reporting requirement where I want to drill through from revenue on a certain day to the transactions that attributed to revenue on that day. \n\nDoes it make sense to link from my revenue fact table to the transaction fact table by transaction_key?", "author_fullname": "t2_75hil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181xfa5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700731517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a question on how you would approach this data model design. &lt;/p&gt;\n\n&lt;p&gt;My data model records Sales Revenue, with the special case that a Sales Transaction can generate Sales Revenue for multiple offices.&lt;/p&gt;\n\n&lt;p&gt;An example of a Sales Transactions worth 10k can generate 3k revenue for Office 1 and 7k revenue for Office 2.&lt;/p&gt;\n\n&lt;p&gt;Currently, my revenue fact table stores the generated revenue like this:\noffice_key, date_key, transaction_key, revenue_amount&lt;/p&gt;\n\n&lt;p&gt;I have a reporting requirement where I want to drill through from revenue on a certain day to the transactions that attributed to revenue on that day. &lt;/p&gt;\n\n&lt;p&gt;Does it make sense to link from my revenue fact table to the transaction fact table by transaction_key?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181xfa5", "is_robot_indexable": true, "report_reasons": null, "author": "dezwarteridder", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181xfa5/data_modelling_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181xfa5/data_modelling_question/", "subreddit_subscribers": 141304, "created_utc": 1700731517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have hands-on work experience in python, SQL and frameoworks like SQL alchemy pandas and Numpy. I know basics of other frameworks such as django, react js and react native. \n\nI need to get a job as fast as possible, orphan will need to leave uncles house in 6 months.  \nI dont have a degree but i have worked with a early stage startup writing some I/O scripts I am confused as on what type of portfolio projects i should work on to land me a job in next 6 months. As i have better hands on backend stuff and i just got to know the work i did is actually part of data engineering(correct me if im wrong)\n\nDo employers in data engineering field even consider guys with no degree? if yes, please suggest some portfolio projects. Thanks.", "author_fullname": "t2_vffigorh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "In which field its easier to get jobs through portfolio projects - data eng or web dev?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181qsng", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700706865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have hands-on work experience in python, SQL and frameoworks like SQL alchemy pandas and Numpy. I know basics of other frameworks such as django, react js and react native. &lt;/p&gt;\n\n&lt;p&gt;I need to get a job as fast as possible, orphan will need to leave uncles house in 6 months.&lt;br/&gt;\nI dont have a degree but i have worked with a early stage startup writing some I/O scripts I am confused as on what type of portfolio projects i should work on to land me a job in next 6 months. As i have better hands on backend stuff and i just got to know the work i did is actually part of data engineering(correct me if im wrong)&lt;/p&gt;\n\n&lt;p&gt;Do employers in data engineering field even consider guys with no degree? if yes, please suggest some portfolio projects. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "181qsng", "is_robot_indexable": true, "report_reasons": null, "author": "guvavava", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181qsng/in_which_field_its_easier_to_get_jobs_through/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181qsng/in_which_field_its_easier_to_get_jobs_through/", "subreddit_subscribers": 141304, "created_utc": 1700706865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So, I've been studying for the DP 203 and the more I learn, the more questions I have about what's going on, so I'll try to get some answers here\n\n1. Azure data lake gen 2 is a NoSQL database? \n2. Database is the same as data store? \n3. What's the point of using something like CosmosDB instead of an azure data lake gen 2?\n4. A delta lake is a NoSQL &amp; SQL database? \n5.the purpose of azure synapse analytics + a data lake + a dedicated sql pool is to have everything you need together?", "author_fullname": "t2_38po62bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure concepts questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181n1mq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700695679.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I&amp;#39;ve been studying for the DP 203 and the more I learn, the more questions I have about what&amp;#39;s going on, so I&amp;#39;ll try to get some answers here&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Azure data lake gen 2 is a NoSQL database? &lt;/li&gt;\n&lt;li&gt;Database is the same as data store? &lt;/li&gt;\n&lt;li&gt;What&amp;#39;s the point of using something like CosmosDB instead of an azure data lake gen 2?&lt;/li&gt;\n&lt;li&gt;A delta lake is a NoSQL &amp;amp; SQL database? \n5.the purpose of azure synapse analytics + a data lake + a dedicated sql pool is to have everything you need together?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181n1mq", "is_robot_indexable": true, "report_reasons": null, "author": "Esteban_Rdz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181n1mq/azure_concepts_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181n1mq/azure_concepts_questions/", "subreddit_subscribers": 141304, "created_utc": 1700695679.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone have experience orchestrating AKS containers via Airflow? At the moment we are using a kubectl cli command to alert the agent pool to run in a bash operator in airflow, but doing so doesn\u2019t actually wait for the task to finish or provide any indication of if the task succeeded or failed.", "author_fullname": "t2_9d5p6jq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure K8s Service via Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18217zc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700746173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have experience orchestrating AKS containers via Airflow? At the moment we are using a kubectl cli command to alert the agent pool to run in a bash operator in airflow, but doing so doesn\u2019t actually wait for the task to finish or provide any indication of if the task succeeded or failed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18217zc", "is_robot_indexable": true, "report_reasons": null, "author": "avclipavclip", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18217zc/azure_k8s_service_via_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18217zc/azure_k8s_service_via_airflow/", "subreddit_subscribers": 141304, "created_utc": 1700746173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I\u2019m very new to working in Azure and with ADFs. I\u2019m working on a project where I need to transform XML files to a CSV output in a specific format. I\u2019ve managed to implement a data flow to get the transformation working as intended, with a blob container for the source and sink respectively. It works as intended when I have one file in the source blob, but when I try with multiple it starts to act a bit weird and tries pulling data from each file into one csv file I think? Ideally I want it so that a specific csv file is generated for each xml file in the source system. Even better if I could get the ADF to trigger with a new blob created and only process the most recently uploaded files. I\u2019ve tried figuring this out but I\u2019m pretty stuck with this processing source items separately thing, is this a for each I should be using?", "author_fullname": "t2_rrfc2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF data flow, scaling up for multiple file inputs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181yrb3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700737144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I\u2019m very new to working in Azure and with ADFs. I\u2019m working on a project where I need to transform XML files to a CSV output in a specific format. I\u2019ve managed to implement a data flow to get the transformation working as intended, with a blob container for the source and sink respectively. It works as intended when I have one file in the source blob, but when I try with multiple it starts to act a bit weird and tries pulling data from each file into one csv file I think? Ideally I want it so that a specific csv file is generated for each xml file in the source system. Even better if I could get the ADF to trigger with a new blob created and only process the most recently uploaded files. I\u2019ve tried figuring this out but I\u2019m pretty stuck with this processing source items separately thing, is this a for each I should be using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181yrb3", "is_robot_indexable": true, "report_reasons": null, "author": "EvilDoctorShadex", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181yrb3/adf_data_flow_scaling_up_for_multiple_file_inputs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181yrb3/adf_data_flow_scaling_up_for_multiple_file_inputs/", "subreddit_subscribers": 141304, "created_utc": 1700737144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Is adopting this approach advisable? I granted the Cloud Function Service Agent role to the Cloud Function Service Agent Service Account in another project, enabling Cloud Functions to trigger upon the arrival of new files. However, due to the absence of official documentation or troubleshooting specifics for this use case, I remain uncertain. Your assistance would be appreciated. ", "author_fullname": "t2_4cullil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud function storage trigger on bucket of other project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181xqek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700732819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is adopting this approach advisable? I granted the Cloud Function Service Agent role to the Cloud Function Service Agent Service Account in another project, enabling Cloud Functions to trigger upon the arrival of new files. However, due to the absence of official documentation or troubleshooting specifics for this use case, I remain uncertain. Your assistance would be appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181xqek", "is_robot_indexable": true, "report_reasons": null, "author": "tmanipra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181xqek/cloud_function_storage_trigger_on_bucket_of_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181xqek/cloud_function_storage_trigger_on_bucket_of_other/", "subreddit_subscribers": 141304, "created_utc": 1700732819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. I'm sorry to ask this but I've been looking everywhere and don't seem to find any source or example. I'm new to a data engineering position in a financial institution, and they asked me how can they establish a risk catalog, meaning which of the data has the highest risk. Since every piece of data is confidential (obviously)  how can they establish higher risk and based on what?? Been reading DAMA book on data security, but it speaks too broadly of this subject. Do you recommend any sources?", "author_fullname": "t2_6c61zfmp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sources for risk catalog??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181s54c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700711192.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I&amp;#39;m sorry to ask this but I&amp;#39;ve been looking everywhere and don&amp;#39;t seem to find any source or example. I&amp;#39;m new to a data engineering position in a financial institution, and they asked me how can they establish a risk catalog, meaning which of the data has the highest risk. Since every piece of data is confidential (obviously)  how can they establish higher risk and based on what?? Been reading DAMA book on data security, but it speaks too broadly of this subject. Do you recommend any sources?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181s54c", "is_robot_indexable": true, "report_reasons": null, "author": "omar_firestorm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181s54c/sources_for_risk_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181s54c/sources_for_risk_catalog/", "subreddit_subscribers": 141304, "created_utc": 1700711192.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys. I'm Brazilian and I've been studying English for 11 months for work remotely from my country. I wanna know the right english level to work as a Data engineer. Many people in my country can get a job as a programmer from B2 english level, but I don't know any data Engineers to ask. ", "author_fullname": "t2_3ms6yvbu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "English level for beginners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181jrol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700687310.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys. I&amp;#39;m Brazilian and I&amp;#39;ve been studying English for 11 months for work remotely from my country. I wanna know the right english level to work as a Data engineer. Many people in my country can get a job as a programmer from B2 english level, but I don&amp;#39;t know any data Engineers to ask. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181jrol", "is_robot_indexable": true, "report_reasons": null, "author": "brunobritohp", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181jrol/english_level_for_beginners/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181jrol/english_level_for_beginners/", "subreddit_subscribers": 141304, "created_utc": 1700687310.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Let's consider simple kappa architecture use-case.\n\n&amp;#x200B;\n\n**Hot path:**\n\n* request(id, x) to process single object characterized by identity id and data x. So request applies some transformation y = f(x) and returns y to client.\n\n&amp;#x200B;\n\n**Cold path:**\n\n* request mentioned before stores all pairs (id, x) to some log file\n* batch\\_request(id1, id2, id3 ...) should take corresponding x1, x2, x3 ... from the log file, apply transformation f on each of them and return y1, y2, y3 ... to the client.\n\n&amp;#x200B;\n\nMy question is: how to support schema evolution of x and versioning of processing logic of f?\n\nSo schema of x can change at some time and logic implemented in f also can change.\n\nSo I want batch\\_request(id1, id2, id3, ...) to use corresponding version of f to each idN.\n\n&amp;#x200B;\n\nWhat are good modern practices / technologies / libraries to implement this?\n\nBoth: single server technologies /  distributed big data stack are interesting.\n\n&amp;#x200B;", "author_fullname": "t2_2o83rdv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Schema evolution and processing logic version in kappa architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181hvrh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700682434.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Let&amp;#39;s consider simple kappa architecture use-case.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hot path:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;request(id, x) to process single object characterized by identity id and data x. So request applies some transformation y = f(x) and returns y to client.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cold path:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;request mentioned before stores all pairs (id, x) to some log file&lt;/li&gt;\n&lt;li&gt;batch_request(id1, id2, id3 ...) should take corresponding x1, x2, x3 ... from the log file, apply transformation f on each of them and return y1, y2, y3 ... to the client.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My question is: how to support schema evolution of x and versioning of processing logic of f?&lt;/p&gt;\n\n&lt;p&gt;So schema of x can change at some time and logic implemented in f also can change.&lt;/p&gt;\n\n&lt;p&gt;So I want batch_request(id1, id2, id3, ...) to use corresponding version of f to each idN.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What are good modern practices / technologies / libraries to implement this?&lt;/p&gt;\n\n&lt;p&gt;Both: single server technologies /  distributed big data stack are interesting.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181hvrh", "is_robot_indexable": true, "report_reasons": null, "author": "Internal-Equal4475", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181hvrh/schema_evolution_and_processing_logic_version_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181hvrh/schema_evolution_and_processing_logic_version_in/", "subreddit_subscribers": 141304, "created_utc": 1700682434.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a long weekend coming up with the holidays and wanted to use it to learn a new technology domain that will help me grow my career.\n\nWhich one do you think has more career potential? I am trying to avoid hyped technologies and focus on what companies sought-out for.  Perhaps you can share which skill is more strategic to your company? \n\n[View Poll](https://www.reddit.com/poll/181hts2)", "author_fullname": "t2_5n7kgpm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long weekend learnings \u2014 stream processing or LLMs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181hts2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700682285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a long weekend coming up with the holidays and wanted to use it to learn a new technology domain that will help me grow my career.&lt;/p&gt;\n\n&lt;p&gt;Which one do you think has more career potential? I am trying to avoid hyped technologies and focus on what companies sought-out for.  Perhaps you can share which skill is more strategic to your company? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/181hts2\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "181hts2", "is_robot_indexable": true, "report_reasons": null, "author": "Striking_Solid_5020", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1700941485258, "options": [{"text": "LLM", "id": "26114307"}, {"text": "Stream processing", "id": "26114308"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 51, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181hts2/long_weekend_learnings_stream_processing_or_llms/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/181hts2/long_weekend_learnings_stream_processing_or_llms/", "subreddit_subscribers": 141304, "created_utc": 1700682285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey yall, I'm setting up a hybrid ML pipeline for my company and have a question about how to implement a triggered function locally. We're using GCP as a platform and ideally, we would have a bigquery table update publish a message into pubsub that triggers a cloud function that does inference on one of their Vertex AI custom-model endpoints. But for some reason, that endpoint does NOT play well with the xgboost datatypes, no matter what we tried. So my next approach was to run the models on a local Ray serve cluster and run the inference from there. Everything went well re:inference. In order to implement that, I would have to have a local subscription to the same topic and have that trigger a script locally that does the inference. What would the equivalent trigger be in a local set up like this (mesage --&gt; function)? Do I have to set it up in like an Airflow situation? Am I thinking about this correctly? Any input would be appreciated!", "author_fullname": "t2_h2af93ho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Assistance on triggering local function", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181ejsr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700673962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey yall, I&amp;#39;m setting up a hybrid ML pipeline for my company and have a question about how to implement a triggered function locally. We&amp;#39;re using GCP as a platform and ideally, we would have a bigquery table update publish a message into pubsub that triggers a cloud function that does inference on one of their Vertex AI custom-model endpoints. But for some reason, that endpoint does NOT play well with the xgboost datatypes, no matter what we tried. So my next approach was to run the models on a local Ray serve cluster and run the inference from there. Everything went well re:inference. In order to implement that, I would have to have a local subscription to the same topic and have that trigger a script locally that does the inference. What would the equivalent trigger be in a local set up like this (mesage --&amp;gt; function)? Do I have to set it up in like an Airflow situation? Am I thinking about this correctly? Any input would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181ejsr", "is_robot_indexable": true, "report_reasons": null, "author": "Plusdebeurre", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181ejsr/assistance_on_triggering_local_function/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181ejsr/assistance_on_triggering_local_function/", "subreddit_subscribers": 141304, "created_utc": 1700673962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_h557nj7lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "44 Best Resources to learn Data Engineering (YouTube, Books, Courses, &amp; Tutorials)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1823qih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CqnD0jXp1zy-FvPpGjVU-Jw9G9sUiC_Xd-9NZ2V_A3c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700753655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mltut.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.mltut.com/best-resources-to-learn-data-engineering/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?auto=webp&amp;s=a500e2d0a55645b45e0cebf7674818a8b1a001ec", "width": 2240, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=846b2e4d36e11dd8b46ba3ae70e61fe3cb2d6be6", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=93e59e083e2cd40196a9c0e4ea2783db8f4a95a3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a47b29eae87c633967e98ac37f5cd8052261793", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d441865f1baacfeef03dee52e49abfad9bb30a7e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3fc69de998a7ca3ca810b9446754b56529c75f73", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efcb52b6d7702e76f91f823e3c4894de7dda24e9", "width": 1080, "height": 607}], "variants": {}, "id": "Dfiy7g_nIo24a7MPAubj8Jy84Wz8r8RZoQE-MJidPWc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1823qih", "is_robot_indexable": true, "report_reasons": null, "author": "Aqsa81", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1823qih/44_best_resources_to_learn_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.mltut.com/best-resources-to-learn-data-engineering/", "subreddit_subscribers": 141304, "created_utc": 1700753655.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}