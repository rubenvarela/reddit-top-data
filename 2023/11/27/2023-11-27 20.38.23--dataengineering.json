{"kind": "Listing", "data": {"after": "t3_1855a08", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been tasked with implementing some code-based tools/frameworks that will be critical to our company's data platform.  I am essentially the solo driver of this project since neither management/PM's nor engineers have any experience with this type of work.  Team is primarily SQL + GUI ETL focused on data warehousing.  Management has my back and trusts me, but I want to avoid a situation where I get enough pushback from engineers that causes management to second guess.    \nI've been slowly exposing the team to these tools/practices over the last few months, but it will become critical that devs are somewhat comfortable with:\n\n* Containers\n* Non-SQL programming (Python)\n* Version Control\n* Code quality/readability/reviews\n* Unit testing\n* CI/CD pipelines\n* Prod/Test/Dev environments  \n\n\nSome questions:\n\nRegarding devs, most are more adaptable and willing to learn.  However, some will strongly push back in favor of keeping their old practices, and I'm worried some won't even be able to upskill at all.  Any tips to make the transition and training smoother?  \n\n\nDo PM's/BA's need to be upskilled/informed as well? They're solely concerned with the business side of things, so I've been pretty much ignored, but I have a feeling they will come knocking once stuff starts really rolling out.  \n\n\nI'm not a senior, but management pretty much told me to do it, and they've given me free reign with no oversight. I'm doing my best to set a good precedent for initial development by keeping code as high quality as possible (writing tests, consistency, modular, documented, future-proof, and as simple + readable as possible).  Any general advice for handling a large initiative like this?  \n\n\nBonus question: it is very likely I will receive negative heat from some senior engineers, and I will most likely have to deal with a lot of complaining and criticism. I mostly just tune it out, but I do not need this making my life more difficult. Any advice for dealing with this?", "author_fullname": "t2_4rifsjav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to sell SWE best practices to team and management", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_184kvgi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701032837.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been tasked with implementing some code-based tools/frameworks that will be critical to our company&amp;#39;s data platform.  I am essentially the solo driver of this project since neither management/PM&amp;#39;s nor engineers have any experience with this type of work.  Team is primarily SQL + GUI ETL focused on data warehousing.  Management has my back and trusts me, but I want to avoid a situation where I get enough pushback from engineers that causes management to second guess.&lt;br/&gt;\nI&amp;#39;ve been slowly exposing the team to these tools/practices over the last few months, but it will become critical that devs are somewhat comfortable with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Containers&lt;/li&gt;\n&lt;li&gt;Non-SQL programming (Python)&lt;/li&gt;\n&lt;li&gt;Version Control&lt;/li&gt;\n&lt;li&gt;Code quality/readability/reviews&lt;/li&gt;\n&lt;li&gt;Unit testing&lt;/li&gt;\n&lt;li&gt;CI/CD pipelines&lt;/li&gt;\n&lt;li&gt;Prod/Test/Dev environments&lt;br/&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Some questions:&lt;/p&gt;\n\n&lt;p&gt;Regarding devs, most are more adaptable and willing to learn.  However, some will strongly push back in favor of keeping their old practices, and I&amp;#39;m worried some won&amp;#39;t even be able to upskill at all.  Any tips to make the transition and training smoother?  &lt;/p&gt;\n\n&lt;p&gt;Do PM&amp;#39;s/BA&amp;#39;s need to be upskilled/informed as well? They&amp;#39;re solely concerned with the business side of things, so I&amp;#39;ve been pretty much ignored, but I have a feeling they will come knocking once stuff starts really rolling out.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not a senior, but management pretty much told me to do it, and they&amp;#39;ve given me free reign with no oversight. I&amp;#39;m doing my best to set a good precedent for initial development by keeping code as high quality as possible (writing tests, consistency, modular, documented, future-proof, and as simple + readable as possible).  Any general advice for handling a large initiative like this?  &lt;/p&gt;\n\n&lt;p&gt;Bonus question: it is very likely I will receive negative heat from some senior engineers, and I will most likely have to deal with a lot of complaining and criticism. I mostly just tune it out, but I do not need this making my life more difficult. Any advice for dealing with this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "184kvgi", "is_robot_indexable": true, "report_reasons": null, "author": "Techthrowaway2222888", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/184kvgi/how_to_sell_swe_best_practices_to_team_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/184kvgi/how_to_sell_swe_best_practices_to_team_and/", "subreddit_subscribers": 142119, "created_utc": 1701032837.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are using dbt, but still, it's getting messy pretty fast with tons of models, sometimes overlapping logic. Is there a clear convention you're using to write models, architecture wise?", "author_fullname": "t2_85sq3osr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you avoid DWH mess?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_184xh8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701072748.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701070499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are using dbt, but still, it&amp;#39;s getting messy pretty fast with tons of models, sometimes overlapping logic. Is there a clear convention you&amp;#39;re using to write models, architecture wise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "184xh8c", "is_robot_indexable": true, "report_reasons": null, "author": "lirco_", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/184xh8c/how_do_you_avoid_dwh_mess/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/184xh8c/how_do_you_avoid_dwh_mess/", "subreddit_subscribers": 142119, "created_utc": 1701070499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7lvmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Me as an ETL engineer watching people build data tables with no regard to what goes in them.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18567aq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sz9-bBG2MqUFoLruoU0gIMIAlXGWn6fB4VywBKcnfiA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701100697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/9ZJkPvV", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/o4naRaSSLJjYgSUFgUtIDf2XEKB5v0QDL68efT-i0-8.jpg?auto=webp&amp;s=a0409ac9fe00d820e35ee11b30726619a53bb830", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/o4naRaSSLJjYgSUFgUtIDf2XEKB5v0QDL68efT-i0-8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d65aa08d31eb1ba26fb133f7935cf28ee170a83c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/o4naRaSSLJjYgSUFgUtIDf2XEKB5v0QDL68efT-i0-8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0483e77d4a72889ba5e28f6044032ec7de7afc88", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/o4naRaSSLJjYgSUFgUtIDf2XEKB5v0QDL68efT-i0-8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7bbe577c010686e5ab69bdf22a5b59c69b571fb", "width": 320, "height": 168}], "variants": {}, "id": "ZUFg4bSnyDR3YZyUibB0z7GMKsmGWJDSocIjUs10m9s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18567aq", "is_robot_indexable": true, "report_reasons": null, "author": "claytonjr", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18567aq/me_as_an_etl_engineer_watching_people_build_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/9ZJkPvV", "subreddit_subscribers": 142119, "created_utc": 1701100697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm designing a solution that includes one big table (OBT) as a consumption layer above our DW. Naturally, this includes looking for information about designing OBT. It seems like there are two definitions that people have in their head, but don't specifically call out when discussing the patterns and critiques. \n\n1. Simply add dimension columns to a fact table\n2. Create one table that represents multiple entities where rows can be at different grains. \n\nWhich definition do you consider?", "author_fullname": "t2_8ov8i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you define one big table (OBT)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_184oj6m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701041890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m designing a solution that includes one big table (OBT) as a consumption layer above our DW. Naturally, this includes looking for information about designing OBT. It seems like there are two definitions that people have in their head, but don&amp;#39;t specifically call out when discussing the patterns and critiques. &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Simply add dimension columns to a fact table&lt;/li&gt;\n&lt;li&gt;Create one table that represents multiple entities where rows can be at different grains. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Which definition do you consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "184oj6m", "is_robot_indexable": true, "report_reasons": null, "author": "leogodin217", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/184oj6m/how_do_you_define_one_big_table_obt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/184oj6m/how_do_you_define_one_big_table_obt/", "subreddit_subscribers": 142119, "created_utc": 1701041890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Packages: a Primer for Data People", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1854luu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WhzpsEWYeT_y_94hDBjHfAlyb3DKS85QlmsVLC2Jc8M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701096408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/python-packages-primer-1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?auto=webp&amp;s=6d2eb199f7c455f81e966bee40d7df02837744aa", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=440fde9802be891f6d4d7572c7d3fe51e2f49d15", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af4f21f8e36603776251933cb48393e3800615f2", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd90858425839b19c339231b7abe87e23db7d410", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fd3295b342dbdd8860fb89ad4bc0e4b9069b372", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e5a3acac644d062d829533502e4f92886a222cb", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=caaa6c497963a3b1aa7271ac3966b2283d867bd3", "width": 1080, "height": 567}], "variants": {}, "id": "HkPo8G4Z5BNccgNizTqmPsBKJEh2N7-tMWViJKnIl2s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1854luu", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1854luu/python_packages_a_primer_for_data_people/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/python-packages-primer-1", "subreddit_subscribers": 142119, "created_utc": 1701096408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am creating a Node JS web app - and have a requirement to text search up to 1 billion XML files. The XML files have text in them from PDFs (OCRd)...so the premise of the app is to search PDFs.\n\nI found [https://www.dtsearch.com/](https://www.dtsearch.com/) but not convinced it's the best solution. I explored other ideas like AWS Glue, but think the cost would be huge.  Has anyone come across anything similar that would be both quick (performance) and affordable (less than couple of hundred dollars per month)?\n\nThe goal would be to have data retrieved in less than 3seconds. I'd love to hear some feedback/ ideas/ experiences!", "author_fullname": "t2_i2wnfy6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web app to search 500 million to 1 billion XML files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1854v5u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701097120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am creating a Node JS web app - and have a requirement to text search up to 1 billion XML files. The XML files have text in them from PDFs (OCRd)...so the premise of the app is to search PDFs.&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://www.dtsearch.com/\"&gt;https://www.dtsearch.com/&lt;/a&gt; but not convinced it&amp;#39;s the best solution. I explored other ideas like AWS Glue, but think the cost would be huge.  Has anyone come across anything similar that would be both quick (performance) and affordable (less than couple of hundred dollars per month)?&lt;/p&gt;\n\n&lt;p&gt;The goal would be to have data retrieved in less than 3seconds. I&amp;#39;d love to hear some feedback/ ideas/ experiences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1854v5u", "is_robot_indexable": true, "report_reasons": null, "author": "Brave_Argument627", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1854v5u/web_app_to_search_500_million_to_1_billion_xml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1854v5u/web_app_to_search_500_million_to_1_billion_xml/", "subreddit_subscribers": 142119, "created_utc": 1701097120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to code a CLI that adds a layer above snapshoted metrics. And I want to open source it. The goal of that CLI is to generate a changelog report for a metric, in a dbt-native project (showing how it moved, opening issue for custom alerts, other stuff). \n\nGo: its new, fun, performant. There is a smaller community than python\n\nPython: Well everybody in data does python... And dbt is in python. And everything leads to python\n\n**If a fun open source project was in Go, would you contribute to it ? even if that meant learning go ?**\n\n[I have already started working on it](https://github.com/data-drift/data-drift), I started in Go for some part, but I needed python to deploy a Pypi lib. Now its hybrid, and I prefer working with go \ud83d\ude2c but the most rational thinking leads to python. ", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would learn Go to contribute to an OS project ? Or should I stick to python ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1853kfp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701093558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to code a CLI that adds a layer above snapshoted metrics. And I want to open source it. The goal of that CLI is to generate a changelog report for a metric, in a dbt-native project (showing how it moved, opening issue for custom alerts, other stuff). &lt;/p&gt;\n\n&lt;p&gt;Go: its new, fun, performant. There is a smaller community than python&lt;/p&gt;\n\n&lt;p&gt;Python: Well everybody in data does python... And dbt is in python. And everything leads to python&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;If a fun open source project was in Go, would you contribute to it ? even if that meant learning go ?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/data-drift/data-drift\"&gt;I have already started working on it&lt;/a&gt;, I started in Go for some part, but I needed python to deploy a Pypi lib. Now its hybrid, and I prefer working with go \ud83d\ude2c but the most rational thinking leads to python. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?auto=webp&amp;s=dd4bfd0f3ef1d90e367bbc5b3340444947cbaaee", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4b7f6f01a0d7b64cfd98a6395091b75f0fc1932", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0898d4f8d99b1aeedaa56ab08cc8a0aca9e65dac", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c6c27afd4a08250ef2d6d816ce59436ce207e9b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e4fbdf5012ddda5a8c5b1b392c7bc75b04ef4b8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ba3bd6ad7d2c4403d5eb16d02b80a5b6337ca6b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=537297000bcccec2a747cf9d26ce342a52a6d239", "width": 1080, "height": 540}], "variants": {}, "id": "jT7i9UbQy0olPDx7s6vkLISm41g17NGiI0pwgrS1YM8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1853kfp", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1853kfp/would_learn_go_to_contribute_to_an_os_project_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1853kfp/would_learn_go_to_contribute_to_an_os_project_or/", "subreddit_subscribers": 142119, "created_utc": 1701093558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to implement Apache Airflow to our data management systems, particularly focusing on integrating it with our existing infrastructure which includes a mix of cloud and on-premises data sources. The goal is to streamline our complex ETL tasks. However, I'm encountering issues with scaling Airflow to handle large datasets and ensuring integration with data sources and tools we use. \n\nHas anyone faced similar obstacles while using Apache Airflow? I'm looking for insights on best practices, tools, or strategies that could help in smoothing out these issues. Any experiences or advice you could share would be great!", "author_fullname": "t2_bcq4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Challenges with Implementing Apache Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1857dh5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701103664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to implement Apache Airflow to our data management systems, particularly focusing on integrating it with our existing infrastructure which includes a mix of cloud and on-premises data sources. The goal is to streamline our complex ETL tasks. However, I&amp;#39;m encountering issues with scaling Airflow to handle large datasets and ensuring integration with data sources and tools we use. &lt;/p&gt;\n\n&lt;p&gt;Has anyone faced similar obstacles while using Apache Airflow? I&amp;#39;m looking for insights on best practices, tools, or strategies that could help in smoothing out these issues. Any experiences or advice you could share would be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1857dh5", "is_robot_indexable": true, "report_reasons": null, "author": "xDarkOne", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1857dh5/challenges_with_implementing_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1857dh5/challenges_with_implementing_apache_airflow/", "subreddit_subscribers": 142119, "created_utc": 1701103664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Setting up a change-data-capture (CDC) pipeline can be a real pain.\n\nCompanies that use Lassoo Headless Analytics to collect first-party behavioral data, often stream it to destinations like Snowflake.\n\nAfter hearing David Yaffe speak at a data event, I learned about Estuary.\n\nSo, I thought I'd see if I could quickly set up a behavioral data pipeline using Estuary, as an alternative to some of the more complex setups I've encountered.\n\nMost importantly, I looked to do so without help from the technical folks on either team.\n\nHere was my experience. Interested in hearing about any you've had with CDC.\n\n  \n[https://lassoo.io/blog/2023/11/20/quick-cdc-pipeline-with-eestuary-lassoo/](https://lassoo.io/blog/2023/11/20/quick-cdc-pipeline-with-eestuary-lassoo/?ref=reddit_de)\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8s6trahv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a CDC pipeline with Estuary and Lassoo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1853qic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701094003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Setting up a change-data-capture (CDC) pipeline can be a real pain.&lt;/p&gt;\n\n&lt;p&gt;Companies that use Lassoo Headless Analytics to collect first-party behavioral data, often stream it to destinations like Snowflake.&lt;/p&gt;\n\n&lt;p&gt;After hearing David Yaffe speak at a data event, I learned about Estuary.&lt;/p&gt;\n\n&lt;p&gt;So, I thought I&amp;#39;d see if I could quickly set up a behavioral data pipeline using Estuary, as an alternative to some of the more complex setups I&amp;#39;ve encountered.&lt;/p&gt;\n\n&lt;p&gt;Most importantly, I looked to do so without help from the technical folks on either team.&lt;/p&gt;\n\n&lt;p&gt;Here was my experience. Interested in hearing about any you&amp;#39;ve had with CDC.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lassoo.io/blog/2023/11/20/quick-cdc-pipeline-with-eestuary-lassoo/?ref=reddit_de\"&gt;https://lassoo.io/blog/2023/11/20/quick-cdc-pipeline-with-eestuary-lassoo/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?auto=webp&amp;s=8f2cdf65ab04d656d09fa95f2010374b3a134977", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91bb2e1714752353ffc2154340b4d12ce21f556d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=182afcec5846bf0a5fdafacdc3f14ccba30179d0", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e46cb77288ea03f0125e5feb506b0383eac8db5", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c52d4b419821437165b6b1aa74043425cdbc15a", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7669a34964623e8ac15182b9718f3071ce5fc73e", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a622935fd526776af61ddd487acfb9838859bda", "width": 1080, "height": 607}], "variants": {}, "id": "h9abt165JkSxltaHAMtwAm6n0NuicYXDCGZoWpMk5vU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1853qic", "is_robot_indexable": true, "report_reasons": null, "author": "Crafty_Combination54", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1853qic/setting_up_a_cdc_pipeline_with_estuary_and_lassoo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1853qic/setting_up_a_cdc_pipeline_with_estuary_and_lassoo/", "subreddit_subscribers": 142119, "created_utc": 1701094003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The medallion architectural pattern partitions data as follows:\n\n* Bronze: raw data (e.g. events from the Windows Event Log)\n* Silver: transformed, normalized data (e.g. events from the Windows Event Log normalized to Elastic Common Schema (ECS) format)\n* Gold: enriched, joined, aggregated data - effectively materialized views (e.g. process trees constructed using events from the Windows Event Log that have been normalized to ECS format, events correlated to alerts or other telemetry)\n\nIn some cases, data in the silver layer is ready to be queried, and should be forwarded to another system for consumption by analysts (e.g. piped into a full-text search engine, imported into a relational database as-is, etc.).\n\nShould I copy any data that doesn't require further processing to the gold layer, even if it's unchanged from silver to gold?\n\nI'm looking to do as much in a serverless context as possible to minimize costs while allowing data to be analyzed in relational or graph format with ETL off of data in the gold layer.\n\nI'm still designing the stack, but am thinking of forming a Kappa architecture consisting of RabbitMQ / Kafka -&gt; S3 -&gt; \\[TimescaleDB | Neo4j | ElasticSearch\\] where data is immediately written to S3, a set of serverless functions is used to go from bronze, silver, gold, and then another set of serverless functions is used to import data in the gold layer into a given data store.\n\nThe serverless functions would be triggered by RabbitMQ, Kafka, SQS, etc.\n\nThe datasets aren't very big, so simply copying data from silver to gold and keeping the silver layer private seems like it'd make the most sense?", "author_fullname": "t2_lwr1wvz8q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Under the medallion pattern, what do you do when data doesn't change from bronze to gold, or silver to gold?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1853fe8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701095338.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701093119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The medallion architectural pattern partitions data as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Bronze: raw data (e.g. events from the Windows Event Log)&lt;/li&gt;\n&lt;li&gt;Silver: transformed, normalized data (e.g. events from the Windows Event Log normalized to Elastic Common Schema (ECS) format)&lt;/li&gt;\n&lt;li&gt;Gold: enriched, joined, aggregated data - effectively materialized views (e.g. process trees constructed using events from the Windows Event Log that have been normalized to ECS format, events correlated to alerts or other telemetry)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In some cases, data in the silver layer is ready to be queried, and should be forwarded to another system for consumption by analysts (e.g. piped into a full-text search engine, imported into a relational database as-is, etc.).&lt;/p&gt;\n\n&lt;p&gt;Should I copy any data that doesn&amp;#39;t require further processing to the gold layer, even if it&amp;#39;s unchanged from silver to gold?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to do as much in a serverless context as possible to minimize costs while allowing data to be analyzed in relational or graph format with ETL off of data in the gold layer.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still designing the stack, but am thinking of forming a Kappa architecture consisting of RabbitMQ / Kafka -&amp;gt; S3 -&amp;gt; [TimescaleDB | Neo4j | ElasticSearch] where data is immediately written to S3, a set of serverless functions is used to go from bronze, silver, gold, and then another set of serverless functions is used to import data in the gold layer into a given data store.&lt;/p&gt;\n\n&lt;p&gt;The serverless functions would be triggered by RabbitMQ, Kafka, SQS, etc.&lt;/p&gt;\n\n&lt;p&gt;The datasets aren&amp;#39;t very big, so simply copying data from silver to gold and keeping the silver layer private seems like it&amp;#39;d make the most sense?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1853fe8", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Importance-1605", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1853fe8/under_the_medallion_pattern_what_do_you_do_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1853fe8/under_the_medallion_pattern_what_do_you_do_when/", "subreddit_subscribers": 142119, "created_utc": 1701093119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am senior studying statistics/data science and I (like many others) are  freaking out about my prospects right out of college. As for  experience, all I have is that I am currently working as a software  developer (web) for a tiny startup (based at my university) part-time  which is also what I did this over last summer. I feel super  disadvantaged with my degree, but I will be taking as many CS classes as  I can.  \nSeeing that I am pretty late to the party and haven't sent in  a single application currently, I am not really sure where to start. I  am pretty slammed with my classes and work, so I haven't had much time  to build other projects (that are not work) for the resume or even apply  to anything. Where do you guys think I should start? For a non-CS major  what should my first step be? I could pivot to analytics as that makes  much more sense with my degree but I enjoy building things than  discovering I suppose. A lot of people I know and stories I have heard  have gone with pursuing a masters degree but I don't want to more school  just because everyone else is. I feel like learning on the job or with  projects would be much more cost-efficient and a better plan for me.\n\nThanks!", "author_fullname": "t2_vkg8wnuu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you were a senior in college and just realized that this career was the right choice for you, what you do?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_184vr3i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701063788.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am senior studying statistics/data science and I (like many others) are  freaking out about my prospects right out of college. As for  experience, all I have is that I am currently working as a software  developer (web) for a tiny startup (based at my university) part-time  which is also what I did this over last summer. I feel super  disadvantaged with my degree, but I will be taking as many CS classes as  I can.&lt;br/&gt;\nSeeing that I am pretty late to the party and haven&amp;#39;t sent in  a single application currently, I am not really sure where to start. I  am pretty slammed with my classes and work, so I haven&amp;#39;t had much time  to build other projects (that are not work) for the resume or even apply  to anything. Where do you guys think I should start? For a non-CS major  what should my first step be? I could pivot to analytics as that makes  much more sense with my degree but I enjoy building things than  discovering I suppose. A lot of people I know and stories I have heard  have gone with pursuing a masters degree but I don&amp;#39;t want to more school  just because everyone else is. I feel like learning on the job or with  projects would be much more cost-efficient and a better plan for me.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "184vr3i", "is_robot_indexable": true, "report_reasons": null, "author": "neshybear_", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/184vr3i/if_you_were_a_senior_in_college_and_just_realized/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/184vr3i/if_you_were_a_senior_in_college_and_just_realized/", "subreddit_subscribers": 142119, "created_utc": 1701063788.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All!\n\nI currently have 3.5 years experience in primarily Teradata Architecture/ Development and looking to transition into Data Engineering. My skillset is: Teradata SQL, Query Performance Tuning, writing Teradata BTEQ scripts, Unix, AutoSys, basic Python). I briefly dabbled little bit with Hive. \n\nI mainly work in a Teradata shop and want to expand into more diverse data engineering tech stack. I understand that most interviews are based on sql+python+data modeling plus questions around my experience. \n\nCan you point out which areas in Python to focus for interviews? I don\u2019t have much industry experience in Python in my current role. What courses would you suggest to get a solid recap of Python?\n\nAlso which data structures and algorithms you suggest to learn/practice? Any resources on here ?\n\nThanks!", "author_fullname": "t2_6msghwx8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning into Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_184oosh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701042309.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All!&lt;/p&gt;\n\n&lt;p&gt;I currently have 3.5 years experience in primarily Teradata Architecture/ Development and looking to transition into Data Engineering. My skillset is: Teradata SQL, Query Performance Tuning, writing Teradata BTEQ scripts, Unix, AutoSys, basic Python). I briefly dabbled little bit with Hive. &lt;/p&gt;\n\n&lt;p&gt;I mainly work in a Teradata shop and want to expand into more diverse data engineering tech stack. I understand that most interviews are based on sql+python+data modeling plus questions around my experience. &lt;/p&gt;\n\n&lt;p&gt;Can you point out which areas in Python to focus for interviews? I don\u2019t have much industry experience in Python in my current role. What courses would you suggest to get a solid recap of Python?&lt;/p&gt;\n\n&lt;p&gt;Also which data structures and algorithms you suggest to learn/practice? Any resources on here ?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "184oosh", "is_robot_indexable": true, "report_reasons": null, "author": "billytimmy123", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/184oosh/transitioning_into_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/184oosh/transitioning_into_data_engineering/", "subreddit_subscribers": 142119, "created_utc": 1701042309.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I am a Product Manager at a FinTech. We use Looker. I want to understand how I can analyze customer data. Currently I use Zendesk for ticketing. This flows to snowflake\u00a0+ looker for insights.   \n\n\nI need a better way to link feedback to impact metrics like CSAT, churn, expansion, referrals, etc. aka how important is this piece of feedback.\u00a0Do you have any suggestions on any tools that offer this or how I can build this in -house? ", "author_fullname": "t2_nvnticro", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Customer Success Data Analysis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_184vzeo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701064665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am a Product Manager at a FinTech. We use Looker. I want to understand how I can analyze customer data. Currently I use Zendesk for ticketing. This flows to snowflake\u00a0+ looker for insights.   &lt;/p&gt;\n\n&lt;p&gt;I need a better way to link feedback to impact metrics like CSAT, churn, expansion, referrals, etc. aka how important is this piece of feedback.\u00a0Do you have any suggestions on any tools that offer this or how I can build this in -house? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "184vzeo", "is_robot_indexable": true, "report_reasons": null, "author": "Scary-Swing2852", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/184vzeo/customer_success_data_analysis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/184vzeo/customer_success_data_analysis/", "subreddit_subscribers": 142119, "created_utc": 1701064665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks. In an effort to learn more about streaming I was planning to set up a small personal project using Washington, DC's transit api (https://developer.wmata.com/docs/services/). \n\nMy initial idea (knowing very little about streaming) would be using Kafka/Spark to stream the data, store it in affordable cloud storage/data warehouse, and transform and serve the data in either a dashboard or a streamlit app to provide near real-time updates on transit. \n\nHowever, since the APIs refresh their data every 10-20 seconds I imagine storage could get expensive quickly and I really don't want to wake up to a $1000 bill. \n\nDoes anyone have advice on how to structure this project to be affordable? I'm not opposed to some small amount of cost (e.g. ~$10/month).", "author_fullname": "t2_550fb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Affordable ways to set up a streaming personal project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18567u0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701100738.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks. In an effort to learn more about streaming I was planning to set up a small personal project using Washington, DC&amp;#39;s transit api (&lt;a href=\"https://developer.wmata.com/docs/services/\"&gt;https://developer.wmata.com/docs/services/&lt;/a&gt;). &lt;/p&gt;\n\n&lt;p&gt;My initial idea (knowing very little about streaming) would be using Kafka/Spark to stream the data, store it in affordable cloud storage/data warehouse, and transform and serve the data in either a dashboard or a streamlit app to provide near real-time updates on transit. &lt;/p&gt;\n\n&lt;p&gt;However, since the APIs refresh their data every 10-20 seconds I imagine storage could get expensive quickly and I really don&amp;#39;t want to wake up to a $1000 bill. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have advice on how to structure this project to be affordable? I&amp;#39;m not opposed to some small amount of cost (e.g. ~$10/month).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18567u0", "is_robot_indexable": true, "report_reasons": null, "author": "PureOhms", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18567u0/affordable_ways_to_set_up_a_streaming_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18567u0/affordable_ways_to_set_up_a_streaming_personal/", "subreddit_subscribers": 142119, "created_utc": 1701100738.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! \ud83d\udc4b\n\nI'm one of the folks behind Airbook. We're super excited about this project we've been working on \u2013 it's a data notebook designed for teams who need to pull information from a bunch of different places like Salesforce, Google Analytics, Hubspot, Amplitude ++ various databases (think MySQL, PostgreSQL, etc.), and data warehouses (Snowflake, Redshift, BigQuery, you name it).\n\nWhat's cool about Airbook is that it lets you connect with over 150 data sources natively and query using SQL or a no-code approach (and hey, Python is on the way!). The best part? You can do all this without the hassle of jumping between a million tools.\n\nWe've added some neat collaborative features, kind of like what you'd find in Google Docs, so teams can work together seamlessly.\n\nWe're riding high from ranking #1 on Product Hunt recently (yay us!), and now we're on the lookout for design partners. We want to make Airbook even better and validate our upcoming features.   \n\n\nIf you find value here and can spend some time giving us feedback, I'd love to chat with you!  \n\n\n&amp;#x200B;", "author_fullname": "t2_qand76u7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Notebook built for collaboration and cross-tool reporting across business apps, databases &amp; data warehouses- using SQL/Python or No-code- in one place.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18506t0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701081959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m one of the folks behind Airbook. We&amp;#39;re super excited about this project we&amp;#39;ve been working on \u2013 it&amp;#39;s a data notebook designed for teams who need to pull information from a bunch of different places like Salesforce, Google Analytics, Hubspot, Amplitude ++ various databases (think MySQL, PostgreSQL, etc.), and data warehouses (Snowflake, Redshift, BigQuery, you name it).&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s cool about Airbook is that it lets you connect with over 150 data sources natively and query using SQL or a no-code approach (and hey, Python is on the way!). The best part? You can do all this without the hassle of jumping between a million tools.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve added some neat collaborative features, kind of like what you&amp;#39;d find in Google Docs, so teams can work together seamlessly.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re riding high from ranking #1 on Product Hunt recently (yay us!), and now we&amp;#39;re on the lookout for design partners. We want to make Airbook even better and validate our upcoming features.   &lt;/p&gt;\n\n&lt;p&gt;If you find value here and can spend some time giving us feedback, I&amp;#39;d love to chat with you!  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18506t0", "is_robot_indexable": true, "report_reasons": null, "author": "AirbookIO", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18506t0/data_notebook_built_for_collaboration_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18506t0/data_notebook_built_for_collaboration_and/", "subreddit_subscribers": 142119, "created_utc": 1701081959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, we are in the process of recruiting a Data Engineer so please forgive my ignorance, and if this is not the right place to be asking, my apologies.\n\nAs a SQL DBA I am looking to build a date/calendar dimension table (Date, Qtr, Year, Financial Year etc.) in our DW but what is the best approach if there is an extra column in the fact table (Site) that affects a one of the date columns, e.g.\n\n\\-Fact table\n\n* RecordId (PK)\n* RecordDate \\[FK\\]\n* SiteId (FK) \\[this would be a school or office\\]\n\n\\-Calendar Dimension\n\n* Date (PK)\n* Year\n* Quarter\n* ...\n* SchoolTerm\n\nThe problem is, that for Site 1 a specific date may refer to term 1, for site 2 that may be term 2.\n\nMy thinking is that I should build a standard Calendar dimension for conformity and use cases where a fact may not be related to a site.\n\nI would then build a Term Dates dimension with a surrogate key and populate during ETL?  It feels a bit redundant to do this but feels more correct.  Also, would a surrogate like YYYYMMDD\\[SiteId\\] be the best idea, if not what is a better approach?\n\nThanks", "author_fullname": "t2_153iwm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Date dimension query for site specific column", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_184z704", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701077866.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, we are in the process of recruiting a Data Engineer so please forgive my ignorance, and if this is not the right place to be asking, my apologies.&lt;/p&gt;\n\n&lt;p&gt;As a SQL DBA I am looking to build a date/calendar dimension table (Date, Qtr, Year, Financial Year etc.) in our DW but what is the best approach if there is an extra column in the fact table (Site) that affects a one of the date columns, e.g.&lt;/p&gt;\n\n&lt;p&gt;-Fact table&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;RecordId (PK)&lt;/li&gt;\n&lt;li&gt;RecordDate [FK]&lt;/li&gt;\n&lt;li&gt;SiteId (FK) [this would be a school or office]&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;-Calendar Dimension&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Date (PK)&lt;/li&gt;\n&lt;li&gt;Year&lt;/li&gt;\n&lt;li&gt;Quarter&lt;/li&gt;\n&lt;li&gt;...&lt;/li&gt;\n&lt;li&gt;SchoolTerm&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The problem is, that for Site 1 a specific date may refer to term 1, for site 2 that may be term 2.&lt;/p&gt;\n\n&lt;p&gt;My thinking is that I should build a standard Calendar dimension for conformity and use cases where a fact may not be related to a site.&lt;/p&gt;\n\n&lt;p&gt;I would then build a Term Dates dimension with a surrogate key and populate during ETL?  It feels a bit redundant to do this but feels more correct.  Also, would a surrogate like YYYYMMDD[SiteId] be the best idea, if not what is a better approach?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "184z704", "is_robot_indexable": true, "report_reasons": null, "author": "Rahmorak", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/184z704/date_dimension_query_for_site_specific_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/184z704/date_dimension_query_for_site_specific_column/", "subreddit_subscribers": 142119, "created_utc": 1701077866.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anyone have any experience with these so-called Financial EDM such as GoldenSource, Matrix Rimes and Finbourne. Can someone shares some insights on the differences between these EDM and Snowflake/Databricks Lakehouse. ", "author_fullname": "t2_dgqi4197", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Financial EDM VS Snowflake/Databricks Lakehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_184tr6h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701056885.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone have any experience with these so-called Financial EDM such as GoldenSource, Matrix Rimes and Finbourne. Can someone shares some insights on the differences between these EDM and Snowflake/Databricks Lakehouse. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "184tr6h", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Criticism-8127", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/184tr6h/financial_edm_vs_snowflakedatabricks_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/184tr6h/financial_edm_vs_snowflakedatabricks_lakehouse/", "subreddit_subscribers": 142119, "created_utc": 1701056885.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Any thoughts welcome, expecially when linked to power bi or tableau", "author_fullname": "t2_xt5zb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with databricks sql? Opinions? Limitations? Positives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_184jfyj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701029241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any thoughts welcome, expecially when linked to power bi or tableau&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "184jfyj", "is_robot_indexable": true, "report_reasons": null, "author": "mister_patience", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/184jfyj/experience_with_databricks_sql_opinions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/184jfyj/experience_with_databricks_sql_opinions/", "subreddit_subscribers": 142119, "created_utc": 1701029241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How common is it to reuse spark batch function in spark streaming ? Specifically when migration to windowing and states. \n\nAnd what would be the best practice for doing that from experience ?", "author_fullname": "t2_j15inhpup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reusing spark batch and spark streaming functions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_185buch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701114620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How common is it to reuse spark batch function in spark streaming ? Specifically when migration to windowing and states. &lt;/p&gt;\n\n&lt;p&gt;And what would be the best practice for doing that from experience ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185buch", "is_robot_indexable": true, "report_reasons": null, "author": "springRock88", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185buch/reusing_spark_batch_and_spark_streaming_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185buch/reusing_spark_batch_and_spark_streaming_functions/", "subreddit_subscribers": 142119, "created_utc": 1701114620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This partnership with [Tonic.ai](https://Tonic.ai) means that all Databricks customers can now harness the full potential of their sensitive data without leaving the Databricks ecosystem. The connector leverages Databricks APIs\u2014supporting Unity Catalog, Delta Tables, and Delta Sharing\u2014to provide a comprehensive data protection and utility solution. Together, Tonic is helping make Databricks the Intelligent Data Platform by providing better and more efficient ways to share and test data as companies innovate to incorporate AI into their organizations.", "author_fullname": "t2_o9k5pf7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unlocking Secure Data Utility with Databricks and Tonic.ai", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_185bhzu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701113768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This partnership with &lt;a href=\"https://Tonic.ai\"&gt;Tonic.ai&lt;/a&gt; means that all Databricks customers can now harness the full potential of their sensitive data without leaving the Databricks ecosystem. The connector leverages Databricks APIs\u2014supporting Unity Catalog, Delta Tables, and Delta Sharing\u2014to provide a comprehensive data protection and utility solution. Together, Tonic is helping make Databricks the Intelligent Data Platform by providing better and more efficient ways to share and test data as companies innovate to incorporate AI into their organizations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nYzy_2SjzgtJbbot221H8Ds3x8u1jiPHzF2qUAeOoD8.jpg?auto=webp&amp;s=402ddf3b874eabd6d0276aefd323fcb103658b12", "width": 1033, "height": 470}, "resolutions": [{"url": "https://external-preview.redd.it/nYzy_2SjzgtJbbot221H8Ds3x8u1jiPHzF2qUAeOoD8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3a56d98f553879b84dc8a2c764cd1ef2e7891e5b", "width": 108, "height": 49}, {"url": "https://external-preview.redd.it/nYzy_2SjzgtJbbot221H8Ds3x8u1jiPHzF2qUAeOoD8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=711710427785fa1cad7167769a21ff351fc0cdae", "width": 216, "height": 98}, {"url": "https://external-preview.redd.it/nYzy_2SjzgtJbbot221H8Ds3x8u1jiPHzF2qUAeOoD8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf4adc594b3287ec9b0af20c33adb78524ac3f6f", "width": 320, "height": 145}, {"url": "https://external-preview.redd.it/nYzy_2SjzgtJbbot221H8Ds3x8u1jiPHzF2qUAeOoD8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c72bd0671222fa24034d05d90b183338a07ff48", "width": 640, "height": 291}, {"url": "https://external-preview.redd.it/nYzy_2SjzgtJbbot221H8Ds3x8u1jiPHzF2qUAeOoD8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=67f17ee875bdecb12e672b4115b6fcc193c5a956", "width": 960, "height": 436}], "variants": {}, "id": "a7kKZto6SC8Mmgen-CGJ8eD26fE4Razq-cUmNwJtPyo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185bhzu", "is_robot_indexable": true, "report_reasons": null, "author": "tombenom", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185bhzu/unlocking_secure_data_utility_with_databricks_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185bhzu/unlocking_secure_data_utility_with_databricks_and/", "subreddit_subscribers": 142119, "created_utc": 1701113768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nThe team i work in uses cloud functions (sometimes with workflows) to do ingestions from SFTPs back into BigQuery\n\n\nUsually they run out of compute and it fails. How do you ingest data in your teams in GCP?", "author_fullname": "t2_851if4wo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud functions for data ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_185agjf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701111184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;The team i work in uses cloud functions (sometimes with workflows) to do ingestions from SFTPs back into BigQuery&lt;/p&gt;\n\n&lt;p&gt;Usually they run out of compute and it fails. How do you ingest data in your teams in GCP?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185agjf", "is_robot_indexable": true, "report_reasons": null, "author": "No-Dress-3160", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185agjf/cloud_functions_for_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185agjf/cloud_functions_for_data_ingestion/", "subreddit_subscribers": 142119, "created_utc": 1701111184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we model the data and create data marts. Data from those tables need to be exposed to end users within a web app. \n\nIt strikes me as bad practice to have web traffic hit the DWH directly or even through an internal API that might be load balanced. At the nd of the day the DWH data mart tables are being queried once per user action. On top of broader analytics workloads.\n\nWould it be better to export those data marts to a secondary DWH/DB that only holds final tables? And choose it/tune it specifically for read heavy workloads?\n\nThis is more of a system design question I imagine but I'd appreciate any pointers.", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prod web app traffic directly through DWH, or move modeled data to prod web app DB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_185aela", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701111047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we model the data and create data marts. Data from those tables need to be exposed to end users within a web app. &lt;/p&gt;\n\n&lt;p&gt;It strikes me as bad practice to have web traffic hit the DWH directly or even through an internal API that might be load balanced. At the nd of the day the DWH data mart tables are being queried once per user action. On top of broader analytics workloads.&lt;/p&gt;\n\n&lt;p&gt;Would it be better to export those data marts to a secondary DWH/DB that only holds final tables? And choose it/tune it specifically for read heavy workloads?&lt;/p&gt;\n\n&lt;p&gt;This is more of a system design question I imagine but I&amp;#39;d appreciate any pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185aela", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185aela/prod_web_app_traffic_directly_through_dwh_or_move/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185aela/prod_web_app_traffic_directly_through_dwh_or_move/", "subreddit_subscribers": 142119, "created_utc": 1701111047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering community,\n\nI'm gearing up to tackle the Google Data Engineer Certification, and I'm torn between going for the official Google course or relying on resources from Udemy (specifically Dan Sullivan and/or Jose Portilla's). I plan on doing a lot to prepare, including exam practices etc, but I'm curious about the depth/material of the courses.\n\nFor those here that have taken the official Google course to prepare for the exam, and do you think it's a must? Alternatively, for those who went the Udemy or Coursera route, did you find it was enough to cover the exam?\n\nAlso a follow up question. Do the labs help much for the exam? Or is is more relevant for practical / real world experience.\n\nThanks in advance!", "author_fullname": "t2_6khra", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP Data Engineer Certification - Official Course vs. Udemy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1859zpn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701110033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; community,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m gearing up to tackle the Google Data Engineer Certification, and I&amp;#39;m torn between going for the official Google course or relying on resources from Udemy (specifically Dan Sullivan and/or Jose Portilla&amp;#39;s). I plan on doing a lot to prepare, including exam practices etc, but I&amp;#39;m curious about the depth/material of the courses.&lt;/p&gt;\n\n&lt;p&gt;For those here that have taken the official Google course to prepare for the exam, and do you think it&amp;#39;s a must? Alternatively, for those who went the Udemy or Coursera route, did you find it was enough to cover the exam?&lt;/p&gt;\n\n&lt;p&gt;Also a follow up question. Do the labs help much for the exam? Or is is more relevant for practical / real world experience.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1859zpn", "is_robot_indexable": true, "report_reasons": null, "author": "mmilli", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1859zpn/gcp_data_engineer_certification_official_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1859zpn/gcp_data_engineer_certification_official_course/", "subreddit_subscribers": 142119, "created_utc": 1701110033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Running Ceph on lots of hardware is a no-brainer. Scaling the system as big as you need it to store all of your data is the name of the game. But what if you need to run lean?\n\nYou can always run on a single server, although that's for learning and development. Maybe certain kinds of testing. If you want to run the smallest possible production environment, how many storage nodes do you need?\n\nWe ran some experiments, and the results will shock you. Okay, maybe not. Still, you will be interested to learn what happens at the lower limits of a Ceph cluster.\n\n[https://koor.tech/blog/2023/running-ceph-with-3-nodes/](https://koor.tech/blog/2023/running-ceph-with-3-nodes/)", "author_fullname": "t2_d321jhgdp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running Ceph in production with only 3 nodes may leave you at the brink", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1858tq0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701107226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Running Ceph on lots of hardware is a no-brainer. Scaling the system as big as you need it to store all of your data is the name of the game. But what if you need to run lean?&lt;/p&gt;\n\n&lt;p&gt;You can always run on a single server, although that&amp;#39;s for learning and development. Maybe certain kinds of testing. If you want to run the smallest possible production environment, how many storage nodes do you need?&lt;/p&gt;\n\n&lt;p&gt;We ran some experiments, and the results will shock you. Okay, maybe not. Still, you will be interested to learn what happens at the lower limits of a Ceph cluster.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://koor.tech/blog/2023/running-ceph-with-3-nodes/\"&gt;https://koor.tech/blog/2023/running-ceph-with-3-nodes/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OSIaMJVS-_REkeaYv1HrF-SpWM8P_SUjya9S0NKSYdQ.jpg?auto=webp&amp;s=1f5e05ca734553c2b5251ab7d6f7b207f8c13d1b", "width": 1000, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/OSIaMJVS-_REkeaYv1HrF-SpWM8P_SUjya9S0NKSYdQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b73e7fc9f4ea8179d9feebff948cd98ff6423cd", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/OSIaMJVS-_REkeaYv1HrF-SpWM8P_SUjya9S0NKSYdQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=de1278c4b96e6f6c67e18c4b34a99e4d0baa90b6", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/OSIaMJVS-_REkeaYv1HrF-SpWM8P_SUjya9S0NKSYdQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef29cbf986e420e28feeeb290c09025ba238acea", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/OSIaMJVS-_REkeaYv1HrF-SpWM8P_SUjya9S0NKSYdQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=63804b0511cacee074690156a8da1a33ebf8bc42", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/OSIaMJVS-_REkeaYv1HrF-SpWM8P_SUjya9S0NKSYdQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d390a9ab1f40527a456d43c84fb6fc5f4db47b37", "width": 960, "height": 960}], "variants": {}, "id": "_7DC-GerDjmy4oQLnqqAnd1LrQrWe1ECMX-zEjMfYXM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1858tq0", "is_robot_indexable": true, "report_reasons": null, "author": "Dave-at-Koor", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1858tq0/running_ceph_in_production_with_only_3_nodes_may/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1858tq0/running_ceph_in_production_with_only_3_nodes_may/", "subreddit_subscribers": 142119, "created_utc": 1701107226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. I've been using Pycharm for years. Lately, the professional version, which has some great features that I use almost daily: jupyter notebooks plugins and SQL integration.\n\nHowever, I am seeing some very cool plugins for VCS, like dbt, copilot and data-diff, that are make me thinking about start using VCS. The jupyter and SQL features are also available on VCS?\n\nBesides that, which one do you prefer? Any definite advantage from one over the other that makes you choose one?", "author_fullname": "t2_chl6zxlwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pycharm Professional vs VCS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1855a08", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701098220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I&amp;#39;ve been using Pycharm for years. Lately, the professional version, which has some great features that I use almost daily: jupyter notebooks plugins and SQL integration.&lt;/p&gt;\n\n&lt;p&gt;However, I am seeing some very cool plugins for VCS, like dbt, copilot and data-diff, that are make me thinking about start using VCS. The jupyter and SQL features are also available on VCS?&lt;/p&gt;\n\n&lt;p&gt;Besides that, which one do you prefer? Any definite advantage from one over the other that makes you choose one?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1855a08", "is_robot_indexable": true, "report_reasons": null, "author": "JLTDE", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1855a08/pycharm_professional_vs_vcs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1855a08/pycharm_professional_vs_vcs/", "subreddit_subscribers": 142119, "created_utc": 1701098220.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}