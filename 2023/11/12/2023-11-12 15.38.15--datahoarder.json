{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've seen many archiving projects of old versions of popular games (the best example is [Omniarchive](https://omniarchive.uk/) that's archiving old Minecraft versions), except Fortnite. When I see Fortnite Archiving projects, they're dead or very small.\n\nMore than 50% of old Fortnite versions are lost. Which is so unreal to me. Some of the lost versions we're talking about were released during when Fortnite was **very popular**. (For example: 4.4 build, released in 2018.) \"Most wanted\" builds are before Battle Royale.\n\nSo, I am asking here. If you have any build that isn't listed [here](https://github.com/simplyblk/Fortnitebuilds), please DM me here.\n\nEDIT: For all asking if it even makes sense to archive those, the anwser is yes. There are multiple projects for old Fortnite and you can play multiplayer (or singleplayer) games on old builds.", "author_fullname": "t2_t6ightw1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archiving old Fortnite Builds/Versions.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17sxb8h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699792048.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699718162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen many archiving projects of old versions of popular games (the best example is &lt;a href=\"https://omniarchive.uk/\"&gt;Omniarchive&lt;/a&gt; that&amp;#39;s archiving old Minecraft versions), except Fortnite. When I see Fortnite Archiving projects, they&amp;#39;re dead or very small.&lt;/p&gt;\n\n&lt;p&gt;More than 50% of old Fortnite versions are lost. Which is so unreal to me. Some of the lost versions we&amp;#39;re talking about were released during when Fortnite was &lt;strong&gt;very popular&lt;/strong&gt;. (For example: 4.4 build, released in 2018.) &amp;quot;Most wanted&amp;quot; builds are before Battle Royale.&lt;/p&gt;\n\n&lt;p&gt;So, I am asking here. If you have any build that isn&amp;#39;t listed &lt;a href=\"https://github.com/simplyblk/Fortnitebuilds\"&gt;here&lt;/a&gt;, please DM me here.&lt;/p&gt;\n\n&lt;p&gt;EDIT: For all asking if it even makes sense to archive those, the anwser is yes. There are multiple projects for old Fortnite and you can play multiplayer (or singleplayer) games on old builds.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/p7eYqjFaW_nyebDDVoJ_pUlZqErNiJmyFluJiZ6G38g.jpg?auto=webp&amp;s=ef65892b6d243ef61dfc7ff2aa54ec56a9c83659", "width": 128, "height": 128}, "resolutions": [{"url": "https://external-preview.redd.it/p7eYqjFaW_nyebDDVoJ_pUlZqErNiJmyFluJiZ6G38g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=42d10f00467df9786bc222da7512eb9efcb9381a", "width": 108, "height": 108}], "variants": {}, "id": "qmaQ5xBLoURVxRDUic9pTp-AskE4yQLfOXVpKpgRqiw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17sxb8h", "is_robot_indexable": true, "report_reasons": null, "author": "plvqr", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17sxb8h/archiving_old_fortnite_buildsversions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17sxb8h/archiving_old_fortnite_buildsversions/", "subreddit_subscribers": 711695, "created_utc": 1699718162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to start data hoarding ebooks and other educational media, this drive will live off the grid most of the time as my area gets a lot of storms and I've lost a previous setup to a surge (surge protectors did not help).\nThe drive will not see much reading, mainly writing so I was thinking maybe a surveillance drive might be best.\nI'm looking at around ~2tb of calculated storage I'll be needing so maybe a 4tb or 8tb or higher(if possible with my budget) might be better in order to have some more space for future data needs. What are my best options below 150 euro?", "author_fullname": "t2_69j5jfgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best 4tb to 8tb drive below 150 euros", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17tflov", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699776634.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699776089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to start data hoarding ebooks and other educational media, this drive will live off the grid most of the time as my area gets a lot of storms and I&amp;#39;ve lost a previous setup to a surge (surge protectors did not help).\nThe drive will not see much reading, mainly writing so I was thinking maybe a surveillance drive might be best.\nI&amp;#39;m looking at around ~2tb of calculated storage I&amp;#39;ll be needing so maybe a 4tb or 8tb or higher(if possible with my budget) might be better in order to have some more space for future data needs. What are my best options below 150 euro?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17tflov", "is_robot_indexable": true, "report_reasons": null, "author": "ComfyCore", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17tflov/best_4tb_to_8tb_drive_below_150_euros/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17tflov/best_4tb_to_8tb_drive_below_150_euros/", "subreddit_subscribers": 711695, "created_utc": 1699776089.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently had an indicdent where I discovered that some image folders on the drive that I did a backup of were damaged which resulted in an incomplete/broken backup. Fortunately I managed to recover the damaged pictures from an old computer. \n\n**old computer and other devices** (no damaged files ) **--&gt; backup** (had some damaged files) **--&gt; backup of backup** (also got the damaged files)\n\nI know that the drive shouldn't had been used the state where files got damaged in the first place, but I'd like to know if there's a tool that can scan for inconsistencies among the files and identify if there are any damaged ones? Preferably without having to compare with the original files. \n\nI discovered the broken images by pure luck (it was only a couple of folders), and the thought of not doing this and continuing to backup broken files scares me at night when I try to sleep. ", "author_fullname": "t2_r76xkp0n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unknown damaged files in backups scares me at night, is there a solution?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t8bop", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699749162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently had an indicdent where I discovered that some image folders on the drive that I did a backup of were damaged which resulted in an incomplete/broken backup. Fortunately I managed to recover the damaged pictures from an old computer. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;old computer and other devices&lt;/strong&gt; (no damaged files ) &lt;strong&gt;--&amp;gt; backup&lt;/strong&gt; (had some damaged files) &lt;strong&gt;--&amp;gt; backup of backup&lt;/strong&gt; (also got the damaged files)&lt;/p&gt;\n\n&lt;p&gt;I know that the drive shouldn&amp;#39;t had been used the state where files got damaged in the first place, but I&amp;#39;d like to know if there&amp;#39;s a tool that can scan for inconsistencies among the files and identify if there are any damaged ones? Preferably without having to compare with the original files. &lt;/p&gt;\n\n&lt;p&gt;I discovered the broken images by pure luck (it was only a couple of folders), and the thought of not doing this and continuing to backup broken files scares me at night when I try to sleep. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t8bop", "is_robot_indexable": true, "report_reasons": null, "author": "mediamystery", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t8bop/unknown_damaged_files_in_backups_scares_me_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t8bop/unknown_damaged_files_in_backups_scares_me_at/", "subreddit_subscribers": 711695, "created_utc": 1699749162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I will digitize a couple of thousand pictures for my family and some of them range from 40-50 years ago, which means they are discolored and blurred.\n\nThe scanner I have doesn't have a good auto processing software (Canon Imageformula R40), it increases the contrast way too much and doesn't seem to understand that old photos get red with time.\n\nSo I was looking at ways to bulk process these images. It doesn't need to be perfect, it just need to improve a little bit. Does anyone has any tip?", "author_fullname": "t2_eg24o7dd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any tips on how to bulk process digitized photos?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17tc9gy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699762286.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will digitize a couple of thousand pictures for my family and some of them range from 40-50 years ago, which means they are discolored and blurred.&lt;/p&gt;\n\n&lt;p&gt;The scanner I have doesn&amp;#39;t have a good auto processing software (Canon Imageformula R40), it increases the contrast way too much and doesn&amp;#39;t seem to understand that old photos get red with time.&lt;/p&gt;\n\n&lt;p&gt;So I was looking at ways to bulk process these images. It doesn&amp;#39;t need to be perfect, it just need to improve a little bit. Does anyone has any tip?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17tc9gy", "is_robot_indexable": true, "report_reasons": null, "author": "backwards_watch", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17tc9gy/any_tips_on_how_to_bulk_process_digitized_photos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17tc9gy/any_tips_on_how_to_bulk_process_digitized_photos/", "subreddit_subscribers": 711695, "created_utc": 1699762286.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "\nI know there are various tools that are supposed to make this easy, but I couldn't find anything that did everything I wanted, so I made this today for fun. The web-based offerings all take forever and seem flaky, and you need to process one video at a time, with no control over the transcription settings. In contrast, my script lets you convert a whole playlist in bulk with full control over everything.\n\nIt's truly easy to use-- you can clone the repo, install to a venv, and be generating a folder full of high quality transcript text files in under 5 minutes. All you need to do is supply the URL to a YouTube playlist or to an individual video file and this tool does the rest automatically. It uses faster-whisper with a high beam_size, so it's a bit slower than you might expect, but this does result in higher accuracy. The best way to use this is to take an existing playlist, or create a new one on YouTube, start this script up, and come back the next morning with all your finished transcripts. It attempts to \"upgrade\" the output of whisper by taking all the transcript segments, gluing them together, and then splitting them back into sentences (it uses Spacy for this, or a simpler regex-based function). You end up with a single text file with the full transcript all ready to go for each video in the playlist, with a sensible file name based on the title of the video.\n\nIf you have CUDA installed, it will try to use it, but as with all things CUDA, it's annoyingly fragile and picky, so don't be surprised if you get a CUDA error even if you know for a fact CUDA is installed on your system. If you're looking for reliability, disable CUDA. But if you need to transcribe a LOT of transcripts, it does go much, much faster on a GPU.\n\nEven if you don't have a GPU, if you have a powerful machine with a lot of RAM and cores, this script will fully saturate them and can download and process multiple videos at the same time. The default settings are pretty good for that situation. But if you have a slower machine, you might want to use a smaller Whisper model (like `base.en` or even `tiny.en`) and dial down the beam_size to 2.", "author_fullname": "t2_aod18", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bulk Creation of Transcripts from YouTube Playlists with Whisper", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17t9njk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/D-XXy6YrNXiUtVtN5bMxWdMP4sgukGjnQ30cIIMRo5A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699753392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know there are various tools that are supposed to make this easy, but I couldn&amp;#39;t find anything that did everything I wanted, so I made this today for fun. The web-based offerings all take forever and seem flaky, and you need to process one video at a time, with no control over the transcription settings. In contrast, my script lets you convert a whole playlist in bulk with full control over everything.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s truly easy to use-- you can clone the repo, install to a venv, and be generating a folder full of high quality transcript text files in under 5 minutes. All you need to do is supply the URL to a YouTube playlist or to an individual video file and this tool does the rest automatically. It uses faster-whisper with a high beam_size, so it&amp;#39;s a bit slower than you might expect, but this does result in higher accuracy. The best way to use this is to take an existing playlist, or create a new one on YouTube, start this script up, and come back the next morning with all your finished transcripts. It attempts to &amp;quot;upgrade&amp;quot; the output of whisper by taking all the transcript segments, gluing them together, and then splitting them back into sentences (it uses Spacy for this, or a simpler regex-based function). You end up with a single text file with the full transcript all ready to go for each video in the playlist, with a sensible file name based on the title of the video.&lt;/p&gt;\n\n&lt;p&gt;If you have CUDA installed, it will try to use it, but as with all things CUDA, it&amp;#39;s annoyingly fragile and picky, so don&amp;#39;t be surprised if you get a CUDA error even if you know for a fact CUDA is installed on your system. If you&amp;#39;re looking for reliability, disable CUDA. But if you need to transcribe a LOT of transcripts, it does go much, much faster on a GPU.&lt;/p&gt;\n\n&lt;p&gt;Even if you don&amp;#39;t have a GPU, if you have a powerful machine with a lot of RAM and cores, this script will fully saturate them and can download and process multiple videos at the same time. The default settings are pretty good for that situation. But if you have a slower machine, you might want to use a smaller Whisper model (like &lt;code&gt;base.en&lt;/code&gt; or even &lt;code&gt;tiny.en&lt;/code&gt;) and dial down the beam_size to 2.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/Dicklesworthstone/bulk_transcribe_youtube_videos_from_playlist", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MUozvfgo8u4muJQOTz314mQ1qADma4Ek07wZoyHV3vw.jpg?auto=webp&amp;s=a5c47fe51bbccf0042e5be1ac685c7e1ada6785b", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/MUozvfgo8u4muJQOTz314mQ1qADma4Ek07wZoyHV3vw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d74027886f21186594414c9e5442db3302b84319", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/MUozvfgo8u4muJQOTz314mQ1qADma4Ek07wZoyHV3vw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a68965eaa4d977135a9db6348742ecbf14ccb39", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/MUozvfgo8u4muJQOTz314mQ1qADma4Ek07wZoyHV3vw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=224f1d15fe382d62897ece3d3b0cbca3dc4ddc8d", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/MUozvfgo8u4muJQOTz314mQ1qADma4Ek07wZoyHV3vw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=12d4c43e9677640dd45453e7571e526344042af1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/MUozvfgo8u4muJQOTz314mQ1qADma4Ek07wZoyHV3vw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6458c4abe5f18282bdf72aae5e0bc6c2acd79ca9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/MUozvfgo8u4muJQOTz314mQ1qADma4Ek07wZoyHV3vw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96b0dc841672059c889a1b90c380b65ab5d9cb37", "width": 1080, "height": 540}], "variants": {}, "id": "v-Wj3rAYmAz36GP-eHRThTYuIed01ANmkIAj0Pls8HQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t9njk", "is_robot_indexable": true, "report_reasons": null, "author": "dicklesworth", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t9njk/bulk_creation_of_transcripts_from_youtube/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/Dicklesworthstone/bulk_transcribe_youtube_videos_from_playlist", "subreddit_subscribers": 711695, "created_utc": 1699753392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello all,I'm looking for some input into if I'm looking at the best options for an HBA card to expand my storage capacity for my Windows 10 machine.  Right now I've got 4 drives in my tower, and two drives externally hooked up via USB, as I don't have enough SATA ports on my motherboard to put everything internal.\n\nI have two PCIE x16 slots open, one with an x8 data lane and one with an x4 (as well as 3 x1 slots, but those I'm assuming are too slow to use for multiple HDDs).  I have a Fractal Design Define 7XL, so I can fit a lot of HDDs it into for my future needs, I just need to be sure that I'm set for at least 8 new HDDs before I consider adding either a second HBA card or upgrading to one with more ports.\n\nThe below table are the cards I've looked at that MIGHT work for me, but I'm honestly not certain on how good the brands are (aside from StarTech and LSI), or whether there are better options out there.\n\n|Manufacturer|PCIE Type|Speed|Architecture|\\# of Ports|Price|\n|:-|:-|:-|:-|:-|:-|\n|[BEYIMEI](https://www.amazon.com/BEYIMEI-Splitter-Profile-Controller-Expansion/dp/B09K5GLJ8D/ref=sr_1_19?crid=24FEGOPRRRXQR&amp;keywords=sas%2B3200&amp;qid=1699648061&amp;refinements=p_85%3A2470955011&amp;rnid=2470954011&amp;rps=1&amp;sprefix=sas%2B3200%2Caps%2C122&amp;sr=8-19&amp;th=1) |x16|6Gbps|6x ASM1064 - 1x ASM1812|24|$169.98|\n|[StarTech](https://www.amazon.com/StarTech-com-Port-SATA-PCIe-8P6G-PCIE-SATA-CARD/dp/B09KDLKYRN/ref=psdc_3012291011_t3_B0BVVDT4F1?th=1)|x4|6Gbps|ASM1062|10|$111.80|\n|[10GTek](https://www.amazon.com/dp/B07VV91L61/ref=as_li_ss_tl?ie=UTF8&amp;linkCode=ll1&amp;tag=xbit-20&amp;linkId=80524adb8573cebbfb5fcc10b46f18fd&amp;language=en_US&amp;th=1)|x3|12Gbps|9300-8i|8|$112.99|\n|[LSI](https://www.amazon.com/dp/B0B23S57ZS?tag=apcstart-20&amp;linkCode=ogi&amp;th=1&amp;psc=1)||12Gbps|9300-16i|16|$149.00|\n|[LSI Broadcom](https://www.amazon.com/LSI-Broadcom-9300-8i-PCI-Express-Profile/dp/B00DSURZYS/ref=sr_1_4?crid=HPZ2IMJ8HVQL&amp;keywords=LSI+SAS9207-8i+8-Port+Internal+HBA+PCIe&amp;qid=1699716834&amp;sprefix=lsi+sas9207-8i+8-port+internal+hba+pcie%2Caps%2C174&amp;sr=8-4&amp;ufe=app_do%3Aamzn1.fos.d977788f-1483-4f76-90a3-786e4cdc8f10)|x3|12Gbps|9300-8i|8|$80.00|\n|[LSI Logic](https://www.amazon.com/SAS9211-8I-8PORT-Int-Sata-Pcie/dp/B002RL8I7M/ref=sr_1_2?crid=HPZ2IMJ8HVQL&amp;keywords=LSI+SAS9207-8i+8-Port+Internal+HBA+PCIe&amp;qid=1699716834&amp;sprefix=lsi+sas9207-8i+8-port+internal+hba+pcie%2Caps%2C174&amp;sr=8-2&amp;ufe=app_do%3Aamzn1.fos.d977788f-1483-4f76-90a3-786e4cdc8f10)|x2|6Gbps|9211-8i|8|$64.90|\n\nI know that there is a lot of knowledge in this subreddit, and I really appreciate if anyone can steer me in the right/better direction on this.\n\nThanks in advance for any help!", "author_fullname": "t2_d8cma", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Proper HBA card for Windows 10 server", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t3t10", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699736558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,I&amp;#39;m looking for some input into if I&amp;#39;m looking at the best options for an HBA card to expand my storage capacity for my Windows 10 machine.  Right now I&amp;#39;ve got 4 drives in my tower, and two drives externally hooked up via USB, as I don&amp;#39;t have enough SATA ports on my motherboard to put everything internal.&lt;/p&gt;\n\n&lt;p&gt;I have two PCIE x16 slots open, one with an x8 data lane and one with an x4 (as well as 3 x1 slots, but those I&amp;#39;m assuming are too slow to use for multiple HDDs).  I have a Fractal Design Define 7XL, so I can fit a lot of HDDs it into for my future needs, I just need to be sure that I&amp;#39;m set for at least 8 new HDDs before I consider adding either a second HBA card or upgrading to one with more ports.&lt;/p&gt;\n\n&lt;p&gt;The below table are the cards I&amp;#39;ve looked at that MIGHT work for me, but I&amp;#39;m honestly not certain on how good the brands are (aside from StarTech and LSI), or whether there are better options out there.&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Manufacturer&lt;/th&gt;\n&lt;th align=\"left\"&gt;PCIE Type&lt;/th&gt;\n&lt;th align=\"left\"&gt;Speed&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;# of Ports&lt;/th&gt;\n&lt;th align=\"left\"&gt;Price&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.amazon.com/BEYIMEI-Splitter-Profile-Controller-Expansion/dp/B09K5GLJ8D/ref=sr_1_19?crid=24FEGOPRRRXQR&amp;amp;keywords=sas%2B3200&amp;amp;qid=1699648061&amp;amp;refinements=p_85%3A2470955011&amp;amp;rnid=2470954011&amp;amp;rps=1&amp;amp;sprefix=sas%2B3200%2Caps%2C122&amp;amp;sr=8-19&amp;amp;th=1\"&gt;BEYIMEI&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;x16&lt;/td&gt;\n&lt;td align=\"left\"&gt;6Gbps&lt;/td&gt;\n&lt;td align=\"left\"&gt;6x ASM1064 - 1x ASM1812&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;$169.98&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.amazon.com/StarTech-com-Port-SATA-PCIe-8P6G-PCIE-SATA-CARD/dp/B09KDLKYRN/ref=psdc_3012291011_t3_B0BVVDT4F1?th=1\"&gt;StarTech&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;x4&lt;/td&gt;\n&lt;td align=\"left\"&gt;6Gbps&lt;/td&gt;\n&lt;td align=\"left\"&gt;ASM1062&lt;/td&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;$111.80&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.amazon.com/dp/B07VV91L61/ref=as_li_ss_tl?ie=UTF8&amp;amp;linkCode=ll1&amp;amp;tag=xbit-20&amp;amp;linkId=80524adb8573cebbfb5fcc10b46f18fd&amp;amp;language=en_US&amp;amp;th=1\"&gt;10GTek&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;x3&lt;/td&gt;\n&lt;td align=\"left\"&gt;12Gbps&lt;/td&gt;\n&lt;td align=\"left\"&gt;9300-8i&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;$112.99&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.amazon.com/dp/B0B23S57ZS?tag=apcstart-20&amp;amp;linkCode=ogi&amp;amp;th=1&amp;amp;psc=1\"&gt;LSI&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;12Gbps&lt;/td&gt;\n&lt;td align=\"left\"&gt;9300-16i&lt;/td&gt;\n&lt;td align=\"left\"&gt;16&lt;/td&gt;\n&lt;td align=\"left\"&gt;$149.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.amazon.com/LSI-Broadcom-9300-8i-PCI-Express-Profile/dp/B00DSURZYS/ref=sr_1_4?crid=HPZ2IMJ8HVQL&amp;amp;keywords=LSI+SAS9207-8i+8-Port+Internal+HBA+PCIe&amp;amp;qid=1699716834&amp;amp;sprefix=lsi+sas9207-8i+8-port+internal+hba+pcie%2Caps%2C174&amp;amp;sr=8-4&amp;amp;ufe=app_do%3Aamzn1.fos.d977788f-1483-4f76-90a3-786e4cdc8f10\"&gt;LSI Broadcom&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;x3&lt;/td&gt;\n&lt;td align=\"left\"&gt;12Gbps&lt;/td&gt;\n&lt;td align=\"left\"&gt;9300-8i&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;$80.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;a href=\"https://www.amazon.com/SAS9211-8I-8PORT-Int-Sata-Pcie/dp/B002RL8I7M/ref=sr_1_2?crid=HPZ2IMJ8HVQL&amp;amp;keywords=LSI+SAS9207-8i+8-Port+Internal+HBA+PCIe&amp;amp;qid=1699716834&amp;amp;sprefix=lsi+sas9207-8i+8-port+internal+hba+pcie%2Caps%2C174&amp;amp;sr=8-2&amp;amp;ufe=app_do%3Aamzn1.fos.d977788f-1483-4f76-90a3-786e4cdc8f10\"&gt;LSI Logic&lt;/a&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;x2&lt;/td&gt;\n&lt;td align=\"left\"&gt;6Gbps&lt;/td&gt;\n&lt;td align=\"left\"&gt;9211-8i&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;$64.90&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I know that there is a lot of knowledge in this subreddit, and I really appreciate if anyone can steer me in the right/better direction on this.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t3t10", "is_robot_indexable": true, "report_reasons": null, "author": "dakar82", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t3t10/proper_hba_card_for_windows_10_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t3t10/proper_hba_card_for_windows_10_server/", "subreddit_subscribers": 711695, "created_utc": 1699736558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nI recently got a Synology DS923+ for someone as a gift, and installed the drives/ran initial setup so that it will be good to go when they open it.\n\nMy question is - i**s it safe to repackage the unit in the box that it arrived in now that the drives are installed?** It is almost all cardboard while the NAS itself came wrapped in a light cloth material \n\nI don't see why it wouldn't be , but I wanted to make sure before I bricked some 20 TB drives\n\nThank you in advance!", "author_fullname": "t2_kqraiwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it safe to store a NAS with installed HDDs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t4g5r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699738266.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I recently got a Synology DS923+ for someone as a gift, and installed the drives/ran initial setup so that it will be good to go when they open it.&lt;/p&gt;\n\n&lt;p&gt;My question is - i&lt;strong&gt;s it safe to repackage the unit in the box that it arrived in now that the drives are installed?&lt;/strong&gt; It is almost all cardboard while the NAS itself came wrapped in a light cloth material &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t see why it wouldn&amp;#39;t be , but I wanted to make sure before I bricked some 20 TB drives&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t4g5r", "is_robot_indexable": true, "report_reasons": null, "author": "FiziksMayMays", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t4g5r/is_it_safe_to_store_a_nas_with_installed_hdds/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t4g5r/is_it_safe_to_store_a_nas_with_installed_hdds/", "subreddit_subscribers": 711695, "created_utc": 1699738266.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey Everyone,\nI was just wondering what the most efficient and/or cost efficient way to transfer data from smaller external hard drives into a much larger external hard drive. I've definitely connected both to my pc but to be honest, that can take hours and days. What are some faster or just more efficient processes for doing so? \nThanks", "author_fullname": "t2_i7183", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best method or transferring multiple external HDDs to single external HDD (or SSD)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t2chu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699732508.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everyone,\nI was just wondering what the most efficient and/or cost efficient way to transfer data from smaller external hard drives into a much larger external hard drive. I&amp;#39;ve definitely connected both to my pc but to be honest, that can take hours and days. What are some faster or just more efficient processes for doing so? \nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t2chu", "is_robot_indexable": true, "report_reasons": null, "author": "OutlawJournalist", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t2chu/best_method_or_transferring_multiple_external/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t2chu/best_method_or_transferring_multiple_external/", "subreddit_subscribers": 711695, "created_utc": 1699732508.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hoping someone here has a better understanding of these tools than I do and might be willing to help me out - or perhaps can point me to an existing example.\n\nI have Macrium on my Win11 computer set up to backup my Windows drive (`C:\\`, 2TB) and my VMs drive (`D:\\` 2TB) to my onboard backups drive (`E:\\`, 8TB) using the default Grandfather, Father, Son strategy for both sets of backups. I also have two external SSDs (`A:\\` and `B:\\`, 4TB) which I would like to keep backups on as well. I can plug either (or both) in once a day, but I was thinking of rotating one offsite weekly and have them unplugged most of the time for some additional protection.\n\nAt the moment, I am thinking a reasonable strategy would be to automatically sync backup images to either of two USB SSD drives when plugged in. I have this sort of working with SyncFolder from the Microsoft Store, but I want to protect the drive with MacriumImageGuardian, which would I think would prevent SyncFolder from deleting files, and eventually the external drives will run out of space which will require a different retention strategy for `A:\\` and `B:\\` (external backups) than for `E:\\` (onboard backups).\n\n[This discussion](https://forum.macrium.com/38582/Sync-Options?PageIndex=1) explains how to make a powershell script within macrium to sync to two destinations *when a backup is completed*.\n\n[This discussion](https://forum.macrium.com/Topic332.aspx) explains how to make a powershell script that detects when a drive has been mounted and *copy* files over, but it also seems like it is doing a lot of other stuff  which I'm not sure I need - also not sure how \"the script is started at user logon\".\n\n[This discussion](https://forum.macrium.com/Topic42020.aspx) includes a link to a description of how to set up backups to run when a drive is plugged in, but it relies on another piece of software to trigger the script, which I would like to avoid. [Here's](https://techcommunity.microsoft.com/t5/windows-powershell/automaticlly-running-a-ps-script-when-the-usb-drive-is-plugged/m-p/3663653) an example of running a script automatically using just windows event log.\n\n**Questions:**\n\n1. Is there a smarter way to keep backups on the two external drives than what I'm asking for?\n2. if not... then is there a straightforward way to combine the script ideas from those two links to sync backups from `E:\\Macrium Reflect\\C Drive` to `A|B:\\Macrium Reflect\\C Drive` when `A:\\` or `B:\\` is plugged in and have it unmount `A:\\` or `B:\\`when the sync is completed?\n3. Is there a way to add a different retention strategy to `A:\\` and `B:\\` to keep this from breaking as they fill up, but while `E:\\` still has space?\n\n&amp;#x200B;", "author_fullname": "t2_4yuut", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Macrium sync images to USB SSDs when plugged in, or other strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t1n2x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699757208.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699730561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hoping someone here has a better understanding of these tools than I do and might be willing to help me out - or perhaps can point me to an existing example.&lt;/p&gt;\n\n&lt;p&gt;I have Macrium on my Win11 computer set up to backup my Windows drive (&lt;code&gt;C:\\&lt;/code&gt;, 2TB) and my VMs drive (&lt;code&gt;D:\\&lt;/code&gt; 2TB) to my onboard backups drive (&lt;code&gt;E:\\&lt;/code&gt;, 8TB) using the default Grandfather, Father, Son strategy for both sets of backups. I also have two external SSDs (&lt;code&gt;A:\\&lt;/code&gt; and &lt;code&gt;B:\\&lt;/code&gt;, 4TB) which I would like to keep backups on as well. I can plug either (or both) in once a day, but I was thinking of rotating one offsite weekly and have them unplugged most of the time for some additional protection.&lt;/p&gt;\n\n&lt;p&gt;At the moment, I am thinking a reasonable strategy would be to automatically sync backup images to either of two USB SSD drives when plugged in. I have this sort of working with SyncFolder from the Microsoft Store, but I want to protect the drive with MacriumImageGuardian, which would I think would prevent SyncFolder from deleting files, and eventually the external drives will run out of space which will require a different retention strategy for &lt;code&gt;A:\\&lt;/code&gt; and &lt;code&gt;B:\\&lt;/code&gt; (external backups) than for &lt;code&gt;E:\\&lt;/code&gt; (onboard backups).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forum.macrium.com/38582/Sync-Options?PageIndex=1\"&gt;This discussion&lt;/a&gt; explains how to make a powershell script within macrium to sync to two destinations &lt;em&gt;when a backup is completed&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forum.macrium.com/Topic332.aspx\"&gt;This discussion&lt;/a&gt; explains how to make a powershell script that detects when a drive has been mounted and &lt;em&gt;copy&lt;/em&gt; files over, but it also seems like it is doing a lot of other stuff  which I&amp;#39;m not sure I need - also not sure how &amp;quot;the script is started at user logon&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forum.macrium.com/Topic42020.aspx\"&gt;This discussion&lt;/a&gt; includes a link to a description of how to set up backups to run when a drive is plugged in, but it relies on another piece of software to trigger the script, which I would like to avoid. &lt;a href=\"https://techcommunity.microsoft.com/t5/windows-powershell/automaticlly-running-a-ps-script-when-the-usb-drive-is-plugged/m-p/3663653\"&gt;Here&amp;#39;s&lt;/a&gt; an example of running a script automatically using just windows event log.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is there a smarter way to keep backups on the two external drives than what I&amp;#39;m asking for?&lt;/li&gt;\n&lt;li&gt;if not... then is there a straightforward way to combine the script ideas from those two links to sync backups from &lt;code&gt;E:\\Macrium Reflect\\C Drive&lt;/code&gt; to &lt;code&gt;A|B:\\Macrium Reflect\\C Drive&lt;/code&gt; when &lt;code&gt;A:\\&lt;/code&gt; or &lt;code&gt;B:\\&lt;/code&gt; is plugged in and have it unmount &lt;code&gt;A:\\&lt;/code&gt; or &lt;code&gt;B:\\&lt;/code&gt;when the sync is completed?&lt;/li&gt;\n&lt;li&gt;Is there a way to add a different retention strategy to &lt;code&gt;A:\\&lt;/code&gt; and &lt;code&gt;B:\\&lt;/code&gt; to keep this from breaking as they fill up, but while &lt;code&gt;E:\\&lt;/code&gt; still has space?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t1n2x", "is_robot_indexable": true, "report_reasons": null, "author": "verticalfuzz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t1n2x/macrium_sync_images_to_usb_ssds_when_plugged_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t1n2x/macrium_sync_images_to_usb_ssds_when_plugged_in/", "subreddit_subscribers": 711695, "created_utc": 1699730561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone, new to this whole world and haven\u2019t jumped in on purchasing an NAS yet, but had a question. With the synology units I notice they have a USB port and an Ethernet port as well. I\u2019m wondering if I have the device connected to my home network via the Ethernet port, can I also have it connected to my PC with the USB port?\n\nBy doing this I\u2019m wondering if I can have the computer recognize it as an external attached storage device (therefore it will sync to Backblaze as an attached hard drive), but I would still be able to access files over the network from my laptop if the desktop computer its attached to (by usb) is turned off. Do you know if this is possible?", "author_fullname": "t2_a2ekvhrw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about synology USB port &amp; Ethernet ports\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t1if6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699730191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, new to this whole world and haven\u2019t jumped in on purchasing an NAS yet, but had a question. With the synology units I notice they have a USB port and an Ethernet port as well. I\u2019m wondering if I have the device connected to my home network via the Ethernet port, can I also have it connected to my PC with the USB port?&lt;/p&gt;\n\n&lt;p&gt;By doing this I\u2019m wondering if I can have the computer recognize it as an external attached storage device (therefore it will sync to Backblaze as an attached hard drive), but I would still be able to access files over the network from my laptop if the desktop computer its attached to (by usb) is turned off. Do you know if this is possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t1if6", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Abrocoma7460", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t1if6/question_about_synology_usb_port_ethernet_ports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t1if6/question_about_synology_usb_port_ethernet_ports/", "subreddit_subscribers": 711695, "created_utc": 1699730191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've followed some guides but this has now gone beyond my limited knowledge.\n\nI have a Dell R630 server running Proxmox V8.0.4 and I have installed a Dell H200e HAB flashed as per these instructions: -\n\n[https://techmattr.wordpress.com/2016/04/11/updated-sas-hba-crossflashing-or-flashing-to-it-mode-dell-perc-h200-and-h310/](https://techmattr.wordpress.com/2016/04/11/updated-sas-hba-crossflashing-or-flashing-to-it-mode-dell-perc-h200-and-h310/)\n\nAnd i've been trying to setup TrueNAS in a VM. Although I got it working the write performance was poor.\n\nIn an attempt to rule out issues with the hardware passthrough I have just done some read/write tests to a single drive in a MD1200 connected through the H200.\n\nThis is the read test:\n\n    root@proxmox:~# fio --ioengine=libaio --direct=1 --sync=1 --rw=read --bs=4K --numjobs=1 --iodepth=4 --runtime=60 --time_based --name seq_read_job --filename=/dev/sdd\n    seq_read_job: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=4\n    fio-3.33\n    Starting 1 process\n    Jobs: 1 (f=1): [R(1)][100.0%][r=168MiB/s][r=43.0k IOPS][eta 00m:00s]\n    seq_read_job: (groupid=0, jobs=1): err= 0: pid=10317: Sun Nov 12 14:12:12 2023\n      read: IOPS=41.5k, BW=162MiB/s (170MB/s)(9734MiB/60001msec)\n        slat (usec): min=3, max=675, avg=15.87, stdev= 5.57\n        clat (usec): min=30, max=28442, avg=78.73, stdev=77.27\n         lat (usec): min=41, max=28458, avg=94.60, stdev=77.14\n        clat percentiles (usec):\n         |  1.00th=[   43],  5.00th=[   54], 10.00th=[   63], 20.00th=[   70],\n         | 30.00th=[   72], 40.00th=[   74], 50.00th=[   74], 60.00th=[   76],\n         | 70.00th=[   79], 80.00th=[   81], 90.00th=[   85], 95.00th=[   89],\n         | 99.00th=[  143], 99.50th=[  660], 99.90th=[  865], 99.95th=[  873],\n         | 99.99th=[  955]\n       bw (  KiB/s): min=124528, max=175392, per=100.00%, avg=166278.55, stdev=5530.75, samples=119\n       iops        : min=31132, max=43848, avg=41569.62, stdev=1382.68, samples=119\n      lat (usec)   : 50=3.37%, 100=95.08%, 250=0.71%, 500=0.32%, 750=0.03%\n      lat (usec)   : 1000=0.48%\n      lat (msec)   : 2=0.01%, 4=0.01%, 10=0.01%, 20=0.01%, 50=0.01%\n      cpu          : usr=9.57%, sys=40.60%, ctx=2888210, majf=0, minf=42\n      IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\n         submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         issued rwts: total=2492009,0,0,0 short=0,0,0,0 dropped=0,0,0,0\n         latency   : target=0, window=0, percentile=100.00%, depth=4\n    \n    Run status group 0 (all jobs):\n       READ: bw=162MiB/s (170MB/s), 162MiB/s-162MiB/s (170MB/s-170MB/s), io=9734MiB (10.2GB), run=60001-60001msec\n    \n    Disk stats (read/write):\n      sdd: ios=2487331/0, merge=82/0, ticks=151526/0, in_queue=151526, util=99.95%\n\nAnd the write test:\n\n    root@proxmox:~# fio --ioengine=libaio --direct=1 --sync=1 --rw=write --bs=4K --numjobs=1 --iodepth=4 --runtime=60 --time_based --name seq_write --filename=/dev/sdd \n    seq_write: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=4\n    fio-3.33\n    Starting 1 process\n    Jobs: 1 (f=1): [W(1)][100.0%][w=624KiB/s][w=156 IOPS][eta 00m:00s]\n    seq_write: (groupid=0, jobs=1): err= 0: pid=8502: Sun Nov 12 14:00:37 2023\n      write: IOPS=152, BW=611KiB/s (626kB/s)(35.8MiB/60019msec); 0 zone resets\n        slat (usec): min=6, max=25638, avg=20.06, stdev=267.78\n        clat (usec): min=24487, max=50681, avg=26114.08, stdev=2881.03\n         lat (usec): min=24527, max=58414, avg=26134.15, stdev=2899.79\n        clat percentiles (usec):\n         |  1.00th=[25035],  5.00th=[25035], 10.00th=[25035], 20.00th=[25035],\n         | 30.00th=[25035], 40.00th=[25035], 50.00th=[25035], 60.00th=[25035],\n         | 70.00th=[25035], 80.00th=[25035], 90.00th=[33424], 95.00th=[33424],\n         | 99.00th=[33424], 99.50th=[33817], 99.90th=[41681], 99.95th=[50070],\n         | 99.99th=[50594]\n       bw (  KiB/s): min=  448, max=  640, per=99.96%, avg=611.76, stdev=23.26, samples=119\n       iops        : min=  112, max=  160, avg=152.94, stdev= 5.82, samples=119\n      lat (msec)   : 50=99.92%, 100=0.08%\n      cpu          : usr=0.05%, sys=0.37%, ctx=2341, majf=0, minf=12\n      IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\n         submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n         issued rwts: total=0,9172,0,0 short=0,0,0,0 dropped=0,0,0,0\n         latency   : target=0, window=0, percentile=100.00%, depth=4\n    \n    Run status group 0 (all jobs):\n      WRITE: bw=611KiB/s (626kB/s), 611KiB/s-611KiB/s (626kB/s-626kB/s), io=35.8MiB (37.6MB), run=60019-60019msec\n    \n    Disk stats (read/write):\n      sdd: ios=77/9152, merge=0/0, ticks=958/238391, in_queue=239348, util=99.80%\n\nAs you can see my write speed is very slow though I don't know if that is showing a problem with my hardware or an issue with the way I am testing it.\n\nThe HDD's are refurbished Dell branded SAS drives, model  HUS723030ALS640.\n\nAny pointers on what could be wrong would be greatly appreciated.\n\nThanks.", "author_fullname": "t2_9weqi6u3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slow write speeds with Dell H200e", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17tl3px", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699798561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve followed some guides but this has now gone beyond my limited knowledge.&lt;/p&gt;\n\n&lt;p&gt;I have a Dell R630 server running Proxmox V8.0.4 and I have installed a Dell H200e HAB flashed as per these instructions: -&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://techmattr.wordpress.com/2016/04/11/updated-sas-hba-crossflashing-or-flashing-to-it-mode-dell-perc-h200-and-h310/\"&gt;https://techmattr.wordpress.com/2016/04/11/updated-sas-hba-crossflashing-or-flashing-to-it-mode-dell-perc-h200-and-h310/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And i&amp;#39;ve been trying to setup TrueNAS in a VM. Although I got it working the write performance was poor.&lt;/p&gt;\n\n&lt;p&gt;In an attempt to rule out issues with the hardware passthrough I have just done some read/write tests to a single drive in a MD1200 connected through the H200.&lt;/p&gt;\n\n&lt;p&gt;This is the read test:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;root@proxmox:~# fio --ioengine=libaio --direct=1 --sync=1 --rw=read --bs=4K --numjobs=1 --iodepth=4 --runtime=60 --time_based --name seq_read_job --filename=/dev/sdd\nseq_read_job: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=4\nfio-3.33\nStarting 1 process\nJobs: 1 (f=1): [R(1)][100.0%][r=168MiB/s][r=43.0k IOPS][eta 00m:00s]\nseq_read_job: (groupid=0, jobs=1): err= 0: pid=10317: Sun Nov 12 14:12:12 2023\n  read: IOPS=41.5k, BW=162MiB/s (170MB/s)(9734MiB/60001msec)\n    slat (usec): min=3, max=675, avg=15.87, stdev= 5.57\n    clat (usec): min=30, max=28442, avg=78.73, stdev=77.27\n     lat (usec): min=41, max=28458, avg=94.60, stdev=77.14\n    clat percentiles (usec):\n     |  1.00th=[   43],  5.00th=[   54], 10.00th=[   63], 20.00th=[   70],\n     | 30.00th=[   72], 40.00th=[   74], 50.00th=[   74], 60.00th=[   76],\n     | 70.00th=[   79], 80.00th=[   81], 90.00th=[   85], 95.00th=[   89],\n     | 99.00th=[  143], 99.50th=[  660], 99.90th=[  865], 99.95th=[  873],\n     | 99.99th=[  955]\n   bw (  KiB/s): min=124528, max=175392, per=100.00%, avg=166278.55, stdev=5530.75, samples=119\n   iops        : min=31132, max=43848, avg=41569.62, stdev=1382.68, samples=119\n  lat (usec)   : 50=3.37%, 100=95.08%, 250=0.71%, 500=0.32%, 750=0.03%\n  lat (usec)   : 1000=0.48%\n  lat (msec)   : 2=0.01%, 4=0.01%, 10=0.01%, 20=0.01%, 50=0.01%\n  cpu          : usr=9.57%, sys=40.60%, ctx=2888210, majf=0, minf=42\n  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, &amp;gt;=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%\n     issued rwts: total=2492009,0,0,0 short=0,0,0,0 dropped=0,0,0,0\n     latency   : target=0, window=0, percentile=100.00%, depth=4\n\nRun status group 0 (all jobs):\n   READ: bw=162MiB/s (170MB/s), 162MiB/s-162MiB/s (170MB/s-170MB/s), io=9734MiB (10.2GB), run=60001-60001msec\n\nDisk stats (read/write):\n  sdd: ios=2487331/0, merge=82/0, ticks=151526/0, in_queue=151526, util=99.95%\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And the write test:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;root@proxmox:~# fio --ioengine=libaio --direct=1 --sync=1 --rw=write --bs=4K --numjobs=1 --iodepth=4 --runtime=60 --time_based --name seq_write --filename=/dev/sdd \nseq_write: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=4\nfio-3.33\nStarting 1 process\nJobs: 1 (f=1): [W(1)][100.0%][w=624KiB/s][w=156 IOPS][eta 00m:00s]\nseq_write: (groupid=0, jobs=1): err= 0: pid=8502: Sun Nov 12 14:00:37 2023\n  write: IOPS=152, BW=611KiB/s (626kB/s)(35.8MiB/60019msec); 0 zone resets\n    slat (usec): min=6, max=25638, avg=20.06, stdev=267.78\n    clat (usec): min=24487, max=50681, avg=26114.08, stdev=2881.03\n     lat (usec): min=24527, max=58414, avg=26134.15, stdev=2899.79\n    clat percentiles (usec):\n     |  1.00th=[25035],  5.00th=[25035], 10.00th=[25035], 20.00th=[25035],\n     | 30.00th=[25035], 40.00th=[25035], 50.00th=[25035], 60.00th=[25035],\n     | 70.00th=[25035], 80.00th=[25035], 90.00th=[33424], 95.00th=[33424],\n     | 99.00th=[33424], 99.50th=[33817], 99.90th=[41681], 99.95th=[50070],\n     | 99.99th=[50594]\n   bw (  KiB/s): min=  448, max=  640, per=99.96%, avg=611.76, stdev=23.26, samples=119\n   iops        : min=  112, max=  160, avg=152.94, stdev= 5.82, samples=119\n  lat (msec)   : 50=99.92%, 100=0.08%\n  cpu          : usr=0.05%, sys=0.37%, ctx=2341, majf=0, minf=12\n  IO depths    : 1=0.1%, 2=0.1%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, &amp;gt;=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &amp;gt;=64=0.0%\n     issued rwts: total=0,9172,0,0 short=0,0,0,0 dropped=0,0,0,0\n     latency   : target=0, window=0, percentile=100.00%, depth=4\n\nRun status group 0 (all jobs):\n  WRITE: bw=611KiB/s (626kB/s), 611KiB/s-611KiB/s (626kB/s-626kB/s), io=35.8MiB (37.6MB), run=60019-60019msec\n\nDisk stats (read/write):\n  sdd: ios=77/9152, merge=0/0, ticks=958/238391, in_queue=239348, util=99.80%\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;As you can see my write speed is very slow though I don&amp;#39;t know if that is showing a problem with my hardware or an issue with the way I am testing it.&lt;/p&gt;\n\n&lt;p&gt;The HDD&amp;#39;s are refurbished Dell branded SAS drives, model  HUS723030ALS640.&lt;/p&gt;\n\n&lt;p&gt;Any pointers on what could be wrong would be greatly appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ED_BoFZ-oqJzskFry5UfB3efoX0Y3vKixFQNvbVLUP4.jpg?auto=webp&amp;s=0f852826a73b5e616c15a908d70961c68d030874", "width": 1024, "height": 680}, "resolutions": [{"url": "https://external-preview.redd.it/ED_BoFZ-oqJzskFry5UfB3efoX0Y3vKixFQNvbVLUP4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef713438235887195b3ae52c917f35bdde89d2eb", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/ED_BoFZ-oqJzskFry5UfB3efoX0Y3vKixFQNvbVLUP4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2d4a4c929feb84932506285516126d73cc63ef12", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/ED_BoFZ-oqJzskFry5UfB3efoX0Y3vKixFQNvbVLUP4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5cb09d867f3853defc884812e675b9b479d82a8d", "width": 320, "height": 212}, {"url": "https://external-preview.redd.it/ED_BoFZ-oqJzskFry5UfB3efoX0Y3vKixFQNvbVLUP4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c95f6426da0d2ecd797dc4565ee59bab7ed9e97", "width": 640, "height": 425}, {"url": "https://external-preview.redd.it/ED_BoFZ-oqJzskFry5UfB3efoX0Y3vKixFQNvbVLUP4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cef25ecf7f014f1fee252ef07fde9c2ea9663d67", "width": 960, "height": 637}], "variants": {}, "id": "eqCyGN-VCjRNUS1qPIeNNsuCS7vzebvTtUvx69d9brY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17tl3px", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Environment_5368", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17tl3px/slow_write_speeds_with_dell_h200e/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17tl3px/slow_write_speeds_with_dell_h200e/", "subreddit_subscribers": 711695, "created_utc": 1699798561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.resauctions.com/auctions/24572-two-day-richard-and-mary-lou-taylor-lifetime-collection-absolute-auction?page=2](https://www.resauctions.com/auctions/24572-two-day-richard-and-mary-lou-taylor-lifetime-collection-absolute-auction?page=2)\n\nI want to archive just this auction, mainly the photos but the closing prices would be nice too. So my goal is either a folder of photos, or that and a browsable offline copy.  \n\nSimply right clicking and saving works decently enough but surely there is a faster way. There are 16 pages and I'm actually doing 2 separate auctions. \n\nMy usual script for a webpage is crawling the whole site and pulling various vendor information instead of just this webpage. My script usually doesn't do that so I'm thinking it's something to do with how the site is structured. \n\n    wget -p --convert-links -e robots=off -U mozilla --no-parent https://www.resauctions.com/auctions/24572-two-day-richard-and-mary-lou-taylor-lifetime-collection-absolute-auction?page=2\n\nAny advice would be appreciated. ", "author_fullname": "t2_ay7tp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I archive this particular auction?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17tkwvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699797954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.resauctions.com/auctions/24572-two-day-richard-and-mary-lou-taylor-lifetime-collection-absolute-auction?page=2\"&gt;https://www.resauctions.com/auctions/24572-two-day-richard-and-mary-lou-taylor-lifetime-collection-absolute-auction?page=2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I want to archive just this auction, mainly the photos but the closing prices would be nice too. So my goal is either a folder of photos, or that and a browsable offline copy.  &lt;/p&gt;\n\n&lt;p&gt;Simply right clicking and saving works decently enough but surely there is a faster way. There are 16 pages and I&amp;#39;m actually doing 2 separate auctions. &lt;/p&gt;\n\n&lt;p&gt;My usual script for a webpage is crawling the whole site and pulling various vendor information instead of just this webpage. My script usually doesn&amp;#39;t do that so I&amp;#39;m thinking it&amp;#39;s something to do with how the site is structured. &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;wget -p --convert-links -e robots=off -U mozilla --no-parent https://www.resauctions.com/auctions/24572-two-day-richard-and-mary-lou-taylor-lifetime-collection-absolute-auction?page=2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Any advice would be appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "244TB ZFS and Synology", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17tkwvv", "is_robot_indexable": true, "report_reasons": null, "author": "erik530195", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17tkwvv/how_can_i_archive_this_particular_auction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17tkwvv/how_can_i_archive_this_particular_auction/", "subreddit_subscribers": 711695, "created_utc": 1699797954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello! I've seen a few posts on how it can be done with FFMPEG, but I have zero clue on how to use it (And when I've tried it won't work) And I'm wondering if there's any other way to save these videos. These 2 videos are the only remaining snippets of these music videos anywhere on the internet, and I'd like to preserve them as [archive.org](https://archive.org) can't seem to archive them properly. Any help is very appreciated. \n\n[Video 1](https://players.brightcove.net/4838167534001/HJ6my7pN_default/index.html?videoId=ref:75000267_ESCL-2349_01VFL)\n\n[Video 2](https://players.brightcove.net/4838167534001/HJ6my7pN_default/index.html?videoId=ref:75000267_ESCL-2326_01VFL)", "author_fullname": "t2_2wu5lee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help downloading brightcove videos.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t80em", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699748211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I&amp;#39;ve seen a few posts on how it can be done with FFMPEG, but I have zero clue on how to use it (And when I&amp;#39;ve tried it won&amp;#39;t work) And I&amp;#39;m wondering if there&amp;#39;s any other way to save these videos. These 2 videos are the only remaining snippets of these music videos anywhere on the internet, and I&amp;#39;d like to preserve them as &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt; can&amp;#39;t seem to archive them properly. Any help is very appreciated. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://players.brightcove.net/4838167534001/HJ6my7pN_default/index.html?videoId=ref:75000267_ESCL-2349_01VFL\"&gt;Video 1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://players.brightcove.net/4838167534001/HJ6my7pN_default/index.html?videoId=ref:75000267_ESCL-2326_01VFL\"&gt;Video 2&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t80em", "is_robot_indexable": true, "report_reasons": null, "author": "Lunah05", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t80em/need_help_downloading_brightcove_videos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t80em/need_help_downloading_brightcove_videos/", "subreddit_subscribers": 711695, "created_utc": 1699748211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I'm looking to add some large Enterprise HDD's to my array and was wondering if a Long Smart Test would be sufficient before putting the drive into service?\n\nI use Windows/Snapraid and have/use HDD Sentinel and also could run Read and/or write tests.\n\nI'm curious what others testing methods are after a drive is shipped to them before putting it into service?", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your test method for new HDDs before adding to array?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t4ozw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699738915.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to add some large Enterprise HDD&amp;#39;s to my array and was wondering if a Long Smart Test would be sufficient before putting the drive into service?&lt;/p&gt;\n\n&lt;p&gt;I use Windows/Snapraid and have/use HDD Sentinel and also could run Read and/or write tests.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious what others testing methods are after a drive is shipped to them before putting it into service?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t4ozw", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17t4ozw/what_is_your_test_method_for_new_hdds_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t4ozw/what_is_your_test_method_for_new_hdds_before/", "subreddit_subscribers": 711695, "created_utc": 1699738915.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was recently gifted on old computer that I'm hoping to turn in to a NAS. The MOBO has 4 SATA 2.0 ports. I'm needing more ports than that, and also hoping to get some SATA 3.0 ports for the faster speed. The issue is the computer only has PCI (not express) and PCIe 1.0 slots on it. Does anyone know of any SATA cards that will work in these slots? And is SATA 3.0 even that much faster for hard drives over SATA 2.0?", "author_fullname": "t2_6obd67l2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old computer SATA 3.0 card", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t4j2s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699738469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was recently gifted on old computer that I&amp;#39;m hoping to turn in to a NAS. The MOBO has 4 SATA 2.0 ports. I&amp;#39;m needing more ports than that, and also hoping to get some SATA 3.0 ports for the faster speed. The issue is the computer only has PCI (not express) and PCIe 1.0 slots on it. Does anyone know of any SATA cards that will work in these slots? And is SATA 3.0 even that much faster for hard drives over SATA 2.0?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t4j2s", "is_robot_indexable": true, "report_reasons": null, "author": "spaghettigoedde", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t4j2s/old_computer_sata_30_card/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t4j2s/old_computer_sata_30_card/", "subreddit_subscribers": 711695, "created_utc": 1699738469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\nI've got two drives connected to a mini pc that's powered 24/7: a 6TB WD Blue (it's actually a WD Elements but the serial number is the same, guess it is SMR btw) and a 2TB WD Purple (this one is CMR I think).\n\nThese are the tasks I want to perform:\n\n- Back up my desktop computer (pictures, music, documents, etc.).\n- Store videos and music to be streamed through Navidrome and Jellyfin.\n- Back up those videos and music.\n- Download torrents.\n- Store and seed torrent files.\n- Back up those torrent files just in case (private trackers are hard).\n\nSo, the question is, how would you distribute this tasks between both drives?\n\nI'm thinking about keeping both plugged in and using the purple for backing up and leeching torrents, and the blue for storing music and video and seeding torrents (also making an additional desktop backup if there's free space); OR using the Purple to leech&amp;seed torrents and store the music and movies, and leaving the blue offline for backing up everything.\n\nNot planning to torrent more than 1.5tb in the next 5 years though.\n\nThanks!", "author_fullname": "t2_i70y3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Assigning roles to my HDD drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t2kgj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699733112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got two drives connected to a mini pc that&amp;#39;s powered 24/7: a 6TB WD Blue (it&amp;#39;s actually a WD Elements but the serial number is the same, guess it is SMR btw) and a 2TB WD Purple (this one is CMR I think).&lt;/p&gt;\n\n&lt;p&gt;These are the tasks I want to perform:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Back up my desktop computer (pictures, music, documents, etc.).&lt;/li&gt;\n&lt;li&gt;Store videos and music to be streamed through Navidrome and Jellyfin.&lt;/li&gt;\n&lt;li&gt;Back up those videos and music.&lt;/li&gt;\n&lt;li&gt;Download torrents.&lt;/li&gt;\n&lt;li&gt;Store and seed torrent files.&lt;/li&gt;\n&lt;li&gt;Back up those torrent files just in case (private trackers are hard).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So, the question is, how would you distribute this tasks between both drives?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking about keeping both plugged in and using the purple for backing up and leeching torrents, and the blue for storing music and video and seeding torrents (also making an additional desktop backup if there&amp;#39;s free space); OR using the Purple to leech&amp;amp;seed torrents and store the music and movies, and leaving the blue offline for backing up everything.&lt;/p&gt;\n\n&lt;p&gt;Not planning to torrent more than 1.5tb in the next 5 years though.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t2kgj", "is_robot_indexable": true, "report_reasons": null, "author": "alvarodel8", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t2kgj/assigning_roles_to_my_hdd_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t2kgj/assigning_roles_to_my_hdd_drives/", "subreddit_subscribers": 711695, "created_utc": 1699733112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On my old PC my secondary HDD would keep spinning when I restarted my PC, but on my new one it spins down and spins back up on restart which I don't want as it's not good for the drive.\n\nI have the B650 Tomahawk Motherboard - Is there perhaps some sort of setting I can change? Thanks.", "author_fullname": "t2_ag5e2mic0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any way to stop secondary HDD from spinning down on restart?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17tf2la", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699773728.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On my old PC my secondary HDD would keep spinning when I restarted my PC, but on my new one it spins down and spins back up on restart which I don&amp;#39;t want as it&amp;#39;s not good for the drive.&lt;/p&gt;\n\n&lt;p&gt;I have the B650 Tomahawk Motherboard - Is there perhaps some sort of setting I can change? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17tf2la", "is_robot_indexable": true, "report_reasons": null, "author": "ArmorOfAltair", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17tf2la/any_way_to_stop_secondary_hdd_from_spinning_down/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17tf2la/any_way_to_stop_secondary_hdd_from_spinning_down/", "subreddit_subscribers": 711695, "created_utc": 1699773728.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey, i am right now building a homeserver and i found a old 2TB HDD so i bought a equally old sas controller, some cables and connected everything how its supposed  to be. Now i am stuck at Initializing the HDD as it just say's \"Dataerror CRC-Check\" or something like this (have my server in german). It recognizes the HDD in the Bios(with name) and in the disk managment(without name).\n\n&amp;#x200B;\n\n[CRC-ERROR](https://preview.redd.it/x29t0tnc3xzb1.png?width=398&amp;format=png&amp;auto=webp&amp;s=3574f95b56cb0e47e26bd5f0f069667aa1ca8ae0)\n\n&amp;#x200B;\n\n[SAS CONTROLLER BIOS](https://preview.redd.it/mdbtstxe3xzb1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=b25777bf113d2fd7e641c2d0c574d0d5ee00167a)\n\n[SAS TOPOLOGY](https://preview.redd.it/1su7doxe3xzb1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=622f0a0d34c8d33a28e1f285b634b633aee93ec9)\n\n[ADAPTER PROPERTIES](https://preview.redd.it/t565cyxe3xzb1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=66dfec0d90518bf29064f00b0d73043b001b59bb)\n\nPlease tell me what im doing wrong. Im now just hammering my head trough a wall.  \n\n\nIve choosen this sub bcs you might know better about sas technology", "author_fullname": "t2_aljbln0f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2TB SAS HDD wont Initialize", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": 52, "top_awarded_type": null, "hide_score": false, "media_metadata": {"t565cyxe3xzb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/t565cyxe3xzb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=32d527d624c8f2031a05657a927cc85d4f52d1c4"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/t565cyxe3xzb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5c17f5d0feb13669ff6f856a9d5fb22de06ac37a"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/t565cyxe3xzb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=11d9bda2ecc7e39b58977e4a0743ebb4601bd729"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/t565cyxe3xzb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=150e29eb605335f340267aacdaa9b1a76b27a20c"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/t565cyxe3xzb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d0b3133277ccf2103baa417561af6995fa51eacf"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/t565cyxe3xzb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=004851e900484610d5b11412ea24f2d182589fb9"}], "s": {"y": 3000, "x": 4000, "u": "https://preview.redd.it/t565cyxe3xzb1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=66dfec0d90518bf29064f00b0d73043b001b59bb"}, "id": "t565cyxe3xzb1"}, "mdbtstxe3xzb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/mdbtstxe3xzb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=48f125c8a68d9fae04214c26c730d0176413651a"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/mdbtstxe3xzb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6e4e80ac51c9d6f41386c02f817eb6c73831cd3b"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/mdbtstxe3xzb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=55acf391650d4b14c1ffe4c39f4d4ac5347d1373"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/mdbtstxe3xzb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=69781a035c24d00e89253cd8bc59db446855c0f5"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/mdbtstxe3xzb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=95b7199c1a9b479158d3f9acd7f3726a7d626f82"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/mdbtstxe3xzb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c769695f86896659da2b04b126f9e3506288272d"}], "s": {"y": 3000, "x": 4000, "u": "https://preview.redd.it/mdbtstxe3xzb1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=b25777bf113d2fd7e641c2d0c574d0d5ee00167a"}, "id": "mdbtstxe3xzb1"}, "1su7doxe3xzb1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/1su7doxe3xzb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbccd8886c6eb334559ee982136aff1c893ea5dc"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/1su7doxe3xzb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6af977efc646333223b0f786ffe4148e5839101"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/1su7doxe3xzb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5dd22ac198ca6ff19b8406286bd47184efa60b72"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/1su7doxe3xzb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1767aa0bd235b332a6e678307c1360b38c2b9a8a"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/1su7doxe3xzb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=777f372e9e0a4c6364d0a45b10bc27bf5142a61d"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/1su7doxe3xzb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=68c6fc06bb6f8d50870519df236032ea4f4498d9"}], "s": {"y": 3000, "x": 4000, "u": "https://preview.redd.it/1su7doxe3xzb1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=622f0a0d34c8d33a28e1f285b634b633aee93ec9"}, "id": "1su7doxe3xzb1"}, "x29t0tnc3xzb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 40, "x": 108, "u": "https://preview.redd.it/x29t0tnc3xzb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=17f1268a0f4db13ec10651c8df0d1a4ccaef3293"}, {"y": 81, "x": 216, "u": "https://preview.redd.it/x29t0tnc3xzb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=24d78dbd0fb13ea53846c87daff2e522b80d2773"}, {"y": 120, "x": 320, "u": "https://preview.redd.it/x29t0tnc3xzb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8847dc0749fa9b2d3b98aa5cbc8293c7854295f1"}], "s": {"y": 150, "x": 398, "u": "https://preview.redd.it/x29t0tnc3xzb1.png?width=398&amp;format=png&amp;auto=webp&amp;s=3574f95b56cb0e47e26bd5f0f069667aa1ca8ae0"}, "id": "x29t0tnc3xzb1"}}, "name": "t3_17tk32f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/m_WgRQwdyy0Yhslp5H-lFvgUNh7qPpJk7RZR3cZEmRQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699795187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, i am right now building a homeserver and i found a old 2TB HDD so i bought a equally old sas controller, some cables and connected everything how its supposed  to be. Now i am stuck at Initializing the HDD as it just say&amp;#39;s &amp;quot;Dataerror CRC-Check&amp;quot; or something like this (have my server in german). It recognizes the HDD in the Bios(with name) and in the disk managment(without name).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/x29t0tnc3xzb1.png?width=398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3574f95b56cb0e47e26bd5f0f069667aa1ca8ae0\"&gt;CRC-ERROR&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mdbtstxe3xzb1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b25777bf113d2fd7e641c2d0c574d0d5ee00167a\"&gt;SAS CONTROLLER BIOS&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1su7doxe3xzb1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=622f0a0d34c8d33a28e1f285b634b633aee93ec9\"&gt;SAS TOPOLOGY&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/t565cyxe3xzb1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=66dfec0d90518bf29064f00b0d73043b001b59bb\"&gt;ADAPTER PROPERTIES&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Please tell me what im doing wrong. Im now just hammering my head trough a wall.  &lt;/p&gt;\n\n&lt;p&gt;Ive choosen this sub bcs you might know better about sas technology&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17tk32f", "is_robot_indexable": true, "report_reasons": null, "author": "ClownPhones", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17tk32f/2tb_sas_hdd_wont_initialize/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17tk32f/2tb_sas_hdd_wont_initialize/", "subreddit_subscribers": 711695, "created_utc": 1699795187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Posting here because r/techsupport does not allow my post.\n\nSo I'm using Ghidra on Octoparse and I don't know how to find the code that stores the data/spreadsheets from web scraping. Any advice?\n\n(Trying to open the files without paying the subscription)", "author_fullname": "t2_v8fspdy7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using Ghidra on Octoparse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17th34z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699782865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Posting here because &lt;a href=\"/r/techsupport\"&gt;r/techsupport&lt;/a&gt; does not allow my post.&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m using Ghidra on Octoparse and I don&amp;#39;t know how to find the code that stores the data/spreadsheets from web scraping. Any advice?&lt;/p&gt;\n\n&lt;p&gt;(Trying to open the files without paying the subscription)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17th34z", "is_robot_indexable": true, "report_reasons": null, "author": "Cheap_Apartment_9730", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17th34z/using_ghidra_on_octoparse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17th34z/using_ghidra_on_octoparse/", "subreddit_subscribers": 711695, "created_utc": 1699782865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\ni found a offering on Ebay for refurbished seagate enterprise capacity harddrives with a good price (12tb/120\u20ac).\n\nHas anyone experience using these drives in a nas and would recommend them ? Or should I stay away?\n\nI\u2019m running a 4 bay nas with raid5.\n\nThanks.", "author_fullname": "t2_18d60sm2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used Refurbished Seagate Enterprise Capacity for Nas?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_17tg4ul", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.38, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/i53F1Hnz0SHAvA6GsiwkP9Vu8oAZADbATdfixHZUe8M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699778560.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,\ni found a offering on Ebay for refurbished seagate enterprise capacity harddrives with a good price (12tb/120\u20ac).&lt;/p&gt;\n\n&lt;p&gt;Has anyone experience using these drives in a nas and would recommend them ? Or should I stay away?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m running a 4 bay nas with raid5.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/4e9lshqcqvzb1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/4e9lshqcqvzb1.jpg?auto=webp&amp;s=746162479822935741d51396cb1e9168f08ed7a2", "width": 1125, "height": 1637}, "resolutions": [{"url": "https://preview.redd.it/4e9lshqcqvzb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=27f91d59bab9bb41057dabb4fd965489477f7291", "width": 108, "height": 157}, {"url": "https://preview.redd.it/4e9lshqcqvzb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47458dc7a2c8d8207d3cebb6e6eb2ab9803747f6", "width": 216, "height": 314}, {"url": "https://preview.redd.it/4e9lshqcqvzb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d1f1f6664bc6389786529e606d40533a8fe4bbf7", "width": 320, "height": 465}, {"url": "https://preview.redd.it/4e9lshqcqvzb1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c333f31de9209635cda4e8174ffe5874008a1a1", "width": 640, "height": 931}, {"url": "https://preview.redd.it/4e9lshqcqvzb1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2632bf64cbfa9747aa157dee01ba798cbe2706a4", "width": 960, "height": 1396}, {"url": "https://preview.redd.it/4e9lshqcqvzb1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c01decaae033b33c1c4711a47c89a316fddd470", "width": 1080, "height": 1571}], "variants": {}, "id": "XYYwksRwZfPHxHbKXeBYXQwy5RGyXQ5W-RMNNxD6pXI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17tg4ul", "is_robot_indexable": true, "report_reasons": null, "author": "chaosys", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17tg4ul/has_anyone_used_refurbished_seagate_enterprise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/4e9lshqcqvzb1.jpg", "subreddit_subscribers": 711695, "created_utc": 1699778560.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just swapped some bigger drives into my Synology NAS and now have a spare 8TB and 4TB drive. I'd like to use them as an offsite backup. I tried to access the drives on my Windows 10 PC with a SATA reader and WinBTRFS, but apparently WinBTRFS can't access Synology drives because they use LVM.\n\nNew plan is to reformat the drives to NTFS so Windows can access them natively (I have multiple backups so I'm fine if they get wiped). Anyone have experience with reformatting BTRFS to NTFS?", "author_fullname": "t2_5vffa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reformat BTRFS drives to NTFS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17t1ipp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699730214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just swapped some bigger drives into my Synology NAS and now have a spare 8TB and 4TB drive. I&amp;#39;d like to use them as an offsite backup. I tried to access the drives on my Windows 10 PC with a SATA reader and WinBTRFS, but apparently WinBTRFS can&amp;#39;t access Synology drives because they use LVM.&lt;/p&gt;\n\n&lt;p&gt;New plan is to reformat the drives to NTFS so Windows can access them natively (I have multiple backups so I&amp;#39;m fine if they get wiped). Anyone have experience with reformatting BTRFS to NTFS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17t1ipp", "is_robot_indexable": true, "report_reasons": null, "author": "One_Eyed_Man", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17t1ipp/reformat_btrfs_drives_to_ntfs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17t1ipp/reformat_btrfs_drives_to_ntfs/", "subreddit_subscribers": 711695, "created_utc": 1699730214.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}