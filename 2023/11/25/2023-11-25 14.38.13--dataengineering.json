{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI'm interested what were your costliest mistakes in de? Mine was setting up databricks within a vpc, forgetting to set up a S3 endpoint and then working with tb of data. I wasted around 4k.", "author_fullname": "t2_bk0fqe9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Costliest Mistakes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182w1a0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700843617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m interested what were your costliest mistakes in de? Mine was setting up databricks within a vpc, forgetting to set up a S3 endpoint and then working with tb of data. I wasted around 4k.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182w1a0", "is_robot_indexable": true, "report_reasons": null, "author": "Agreeable_Bake_783", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182w1a0/costliest_mistakes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182w1a0/costliest_mistakes/", "subreddit_subscribers": 141580, "created_utc": 1700843617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My current DE job doesn't involve much coding at all. I don't program anything more than some occasional simple Python scripts. Feels like a deathblow for my coding skills, and I want to improve it, but there is simply no opportunity for it. Yet I just feel unemployable for the jobs/projects that would require higher proficiency in programming for a daily basis, so I'll need to brush it up on my own afterhours if I want to attain that.\n\nCurrently I feel intermediate. I am comfortable writing basic Python apps and more advanced scripts/data transformations. I also know some technical details and best practices that allow me to enhance my code. But other than that I feel that my current skillset wouldn't bring much value to more advanced projects in data platforms.\n\nI was consider contributing to some open-source libraries, but I really have no idea which and what. Codebases for libs I know are mostly overwhelming, and I don't use any particular one to the extent I would know some bugs, shortcomings that could be fixed. I have no idea for own library and/or extensions. Maybe start with some smaller ones, like some unofficial web scrapers or API clients?\n\nI was already creating some more advanced DE projects and I liked it, but I feel like I have enough of it, won't learn much new stuff anymore coding-wise, and I want to progress with my skills further.\n\nDoing Leetcode and some other stuff like \"resolve X puzzles\" just isn't what I am looking for. I know some companies love it, but I am looking for some real SWE-like experience.\n\nWhat would you suggest? Any other options?", "author_fullname": "t2_9e7m1qmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving coding/SWE skills as mid DE.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1834w3n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700870246.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700867272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My current DE job doesn&amp;#39;t involve much coding at all. I don&amp;#39;t program anything more than some occasional simple Python scripts. Feels like a deathblow for my coding skills, and I want to improve it, but there is simply no opportunity for it. Yet I just feel unemployable for the jobs/projects that would require higher proficiency in programming for a daily basis, so I&amp;#39;ll need to brush it up on my own afterhours if I want to attain that.&lt;/p&gt;\n\n&lt;p&gt;Currently I feel intermediate. I am comfortable writing basic Python apps and more advanced scripts/data transformations. I also know some technical details and best practices that allow me to enhance my code. But other than that I feel that my current skillset wouldn&amp;#39;t bring much value to more advanced projects in data platforms.&lt;/p&gt;\n\n&lt;p&gt;I was consider contributing to some open-source libraries, but I really have no idea which and what. Codebases for libs I know are mostly overwhelming, and I don&amp;#39;t use any particular one to the extent I would know some bugs, shortcomings that could be fixed. I have no idea for own library and/or extensions. Maybe start with some smaller ones, like some unofficial web scrapers or API clients?&lt;/p&gt;\n\n&lt;p&gt;I was already creating some more advanced DE projects and I liked it, but I feel like I have enough of it, won&amp;#39;t learn much new stuff anymore coding-wise, and I want to progress with my skills further.&lt;/p&gt;\n\n&lt;p&gt;Doing Leetcode and some other stuff like &amp;quot;resolve X puzzles&amp;quot; just isn&amp;#39;t what I am looking for. I know some companies love it, but I am looking for some real SWE-like experience.&lt;/p&gt;\n\n&lt;p&gt;What would you suggest? Any other options?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1834w3n", "is_robot_indexable": true, "report_reasons": null, "author": "LewWariat", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1834w3n/improving_codingswe_skills_as_mid_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1834w3n/improving_codingswe_skills_as_mid_de/", "subreddit_subscribers": 141580, "created_utc": 1700867272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to understand how ds algo is getting used at work.", "author_fullname": "t2_kfvc08j9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What real world DE problems have you solved applying ds algo or leetcode solutions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_183duwu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700895219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to understand how ds algo is getting used at work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "183duwu", "is_robot_indexable": true, "report_reasons": null, "author": "RepulsiveCry8412", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/183duwu/what_real_world_de_problems_have_you_solved/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/183duwu/what_real_world_de_problems_have_you_solved/", "subreddit_subscribers": 141580, "created_utc": 1700895219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hi,\n\nI've been applying for different de and analytics engineer roles around mid to senior level.. For the ones that I do get the interview, I seem to never make it past the technical interview. Sometimes it's a case study, sometimes it's a in person coding. Case study questions are sometimes like \"how do you design a dB for gold layer based on xy business case and this schema.\" Analyze the risk and data validation/ qa tests and stuff. \n\nIs there any recommendations on studying material to improve?", "author_fullname": "t2_6o6sl8n7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Failing technical interviews. Looking for suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182xcut", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700847172.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been applying for different de and analytics engineer roles around mid to senior level.. For the ones that I do get the interview, I seem to never make it past the technical interview. Sometimes it&amp;#39;s a case study, sometimes it&amp;#39;s a in person coding. Case study questions are sometimes like &amp;quot;how do you design a dB for gold layer based on xy business case and this schema.&amp;quot; Analyze the risk and data validation/ qa tests and stuff. &lt;/p&gt;\n\n&lt;p&gt;Is there any recommendations on studying material to improve?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "182xcut", "is_robot_indexable": true, "report_reasons": null, "author": "Tasty_Fold3012", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182xcut/failing_technical_interviews_looking_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182xcut/failing_technical_interviews_looking_for/", "subreddit_subscribers": 141580, "created_utc": 1700847172.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dnf1ebcu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oldcoderguy - The World\u2019s Most Secure Data Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_183hgs2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/zFNB-Y1-bLs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The World&amp;#39;s Most Secure Data Warehouse. #sql #dataengineer #programmerhumor #from\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The World's Most Secure Data Warehouse. #sql #dataengineer #programmerhumor #from", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/zFNB-Y1-bLs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The World&amp;#39;s Most Secure Data Warehouse. #sql #dataengineer #programmerhumor #from\"&gt;&lt;/iframe&gt;", "author_name": "OldCoderGuy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/zFNB-Y1-bLs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@oldcoderguy"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/zFNB-Y1-bLs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The World&amp;#39;s Most Secure Data Warehouse. #sql #dataengineer #programmerhumor #from\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/183hgs2", "height": 200}, "link_flair_text": "Meme", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rl9MTYsoyoBU1z-I0GkQ7y-WrqnAL8c11Ofod8Fpkts.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700910320.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/zFNB-Y1-bLs?si=8klUbx516LZQzfqa", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vot8pxO0tcvBBYYSfV9R2z9dWtKPU-LNa-E1C0xxqjo.jpg?auto=webp&amp;s=fec6c542511f7bfd49fbbf136d139917cdc8e4a9", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/vot8pxO0tcvBBYYSfV9R2z9dWtKPU-LNa-E1C0xxqjo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b81bcc9a0324b726808c3c2a8be9d8d8ca474ec", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/vot8pxO0tcvBBYYSfV9R2z9dWtKPU-LNa-E1C0xxqjo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=abd7121cd83acf81a021a964aee426c0ae8bfddc", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/vot8pxO0tcvBBYYSfV9R2z9dWtKPU-LNa-E1C0xxqjo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f51b0ed31a9e49e295ecd775349d68bb7c75c7b", "width": 320, "height": 240}], "variants": {}, "id": "Z5EYsiMJGCt_8vHttO6ALjOo5E0IB4n8ojbmTxS4MfM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "183hgs2", "is_robot_indexable": true, "report_reasons": null, "author": "Background-Head9233", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/183hgs2/oldcoderguy_the_worlds_most_secure_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/zFNB-Y1-bLs?si=8klUbx516LZQzfqa", "subreddit_subscribers": 141580, "created_utc": 1700910320.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "The World's Most Secure Data Warehouse. #sql #dataengineer #programmerhumor #from", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/zFNB-Y1-bLs?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"The World&amp;#39;s Most Secure Data Warehouse. #sql #dataengineer #programmerhumor #from\"&gt;&lt;/iframe&gt;", "author_name": "OldCoderGuy", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/zFNB-Y1-bLs/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@oldcoderguy"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "3 workflow improvements we wish dbt announced at Coalesce 2023", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182vxzq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1700843378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/3-workflow-improvements-we-wish-dbt-announced-at-coalesce-2023", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "182vxzq", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182vxzq/3_workflow_improvements_we_wish_dbt_announced_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/3-workflow-improvements-we-wish-dbt-announced-at-coalesce-2023", "subreddit_subscribers": 141580, "created_utc": 1700843378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Shopping around for experiences. In the past have obviously used Airflow since it was the only game in town. Of course I was also bitten by the numerous issues that Airflow has experienced with Schedulers falling over, deadlocks between worker threads and schedulers, etc. Now I have never run Airflow with a HA Scheduler environment and supposedly since the 2.6 update they have fixed some deadlocking issues. But will this scale to 1000s of DAGs scheduled at an hourly cadence?\n\nI have also poked around the edges of Dagster and Prefect but never used them in a production environment before, so I don't know how far those technologies scale in a real world environment.\n\nOr maybe there is another tool I am unaware of that better meets this usecase?", "author_fullname": "t2_dxegl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Scaling to N DAGs: Airflow vs. Prefect vs. Dagster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_183jgps", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700917920.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Shopping around for experiences. In the past have obviously used Airflow since it was the only game in town. Of course I was also bitten by the numerous issues that Airflow has experienced with Schedulers falling over, deadlocks between worker threads and schedulers, etc. Now I have never run Airflow with a HA Scheduler environment and supposedly since the 2.6 update they have fixed some deadlocking issues. But will this scale to 1000s of DAGs scheduled at an hourly cadence?&lt;/p&gt;\n\n&lt;p&gt;I have also poked around the edges of Dagster and Prefect but never used them in a production environment before, so I don&amp;#39;t know how far those technologies scale in a real world environment.&lt;/p&gt;\n\n&lt;p&gt;Or maybe there is another tool I am unaware of that better meets this usecase?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "183jgps", "is_robot_indexable": true, "report_reasons": null, "author": "nutso_muzz", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/183jgps/scaling_to_n_dags_airflow_vs_prefect_vs_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/183jgps/scaling_to_n_dags_airflow_vs_prefect_vs_dagster/", "subreddit_subscribers": 141580, "created_utc": 1700917920.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm new to Databricks Auto Loader and find myself a bit overwhelmed with all the information I've come across recently. I would really appreciate your help and input. Here\u2019s a little more about what I want to do and the data I have:\n\n&amp;#x200B;\n\n* I have large CSV files containing customer activities stored in an Amazon S3 bucket in the format \\`df\\_YYYYMMDD\\_xxxx.csv\\`. The \\`xxxx\\` indicates the version of the file. For example, on July 20, 2021, there are 5 different versions: \\`df\\_20210720\\_0001.csv\\`, \\`df\\_20210720\\_0002.csv\\`,..., and \\`df\\_20210720\\_0005.csv\\`. There are a lot of duplicated rows (for instance, version 0002 contains the same rows in version 0001 plus some new rows). I don\u2019t have any unique primary key.\n* I want to use `spark.readStream()` to read the data and `spark.writeStream()` to write into a Databricks table. My goal is to only write the unique rows. For example, let\u2019s say I have two files: \\`df\\_20210720\\_0001.csv\\` and \\`df\\_20210720\\_0002.csv\\` and there are 10 rows in version 0001 that are also in version 0002. I only want those 10 rows to appear once in my resulting table. \n\nThe Spark Documentation mentions the use of `dropDuplicates()` to \u201cdeduplicate records in data streams using a unique identifier\u201d. I read that Spark uses \u201ca *micro-batch processing* engine, which processes data streams as a series of small batch jobs\u201d. My understanding is that Spark reads a subset of a file as a micro-batch and performs `dropDuplicates()` to that micro-batch. So, each file is processed independently in its own micro-batch.\n\n* Does this mean that the `dropDuplicates()` operation is only designed to remove duplicates within the scope of each micro-batch and thus, I can\u2019t use it to deduplicate across the files? \n* Could you give me some suggestions on how I should go about dropping duplicated rows across files if I want to use Auto loader?\n* I would really appreciate it if you could point me to some readings/videos that will help me further understand how Spark Structured Streaming works \n\nThank you for your help and I apologize in advance if my understanding is incorrect and if my questions are silly", "author_fullname": "t2_8rwu4pz7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Auto Loader and deduplicating across files questions. TIA for your help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_183a6m0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700882197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to Databricks Auto Loader and find myself a bit overwhelmed with all the information I&amp;#39;ve come across recently. I would really appreciate your help and input. Here\u2019s a little more about what I want to do and the data I have:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I have large CSV files containing customer activities stored in an Amazon S3 bucket in the format `df_YYYYMMDD_xxxx.csv`. The `xxxx` indicates the version of the file. For example, on July 20, 2021, there are 5 different versions: `df_20210720_0001.csv`, `df_20210720_0002.csv`,..., and `df_20210720_0005.csv`. There are a lot of duplicated rows (for instance, version 0002 contains the same rows in version 0001 plus some new rows). I don\u2019t have any unique primary key.&lt;/li&gt;\n&lt;li&gt;I want to use &lt;code&gt;spark.readStream()&lt;/code&gt; to read the data and &lt;code&gt;spark.writeStream()&lt;/code&gt; to write into a Databricks table. My goal is to only write the unique rows. For example, let\u2019s say I have two files: `df_20210720_0001.csv` and `df_20210720_0002.csv` and there are 10 rows in version 0001 that are also in version 0002. I only want those 10 rows to appear once in my resulting table. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The Spark Documentation mentions the use of &lt;code&gt;dropDuplicates()&lt;/code&gt; to \u201cdeduplicate records in data streams using a unique identifier\u201d. I read that Spark uses \u201ca &lt;em&gt;micro-batch processing&lt;/em&gt; engine, which processes data streams as a series of small batch jobs\u201d. My understanding is that Spark reads a subset of a file as a micro-batch and performs &lt;code&gt;dropDuplicates()&lt;/code&gt; to that micro-batch. So, each file is processed independently in its own micro-batch.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Does this mean that the &lt;code&gt;dropDuplicates()&lt;/code&gt; operation is only designed to remove duplicates within the scope of each micro-batch and thus, I can\u2019t use it to deduplicate across the files? &lt;/li&gt;\n&lt;li&gt;Could you give me some suggestions on how I should go about dropping duplicated rows across files if I want to use Auto loader?&lt;/li&gt;\n&lt;li&gt;I would really appreciate it if you could point me to some readings/videos that will help me further understand how Spark Structured Streaming works &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you for your help and I apologize in advance if my understanding is incorrect and if my questions are silly&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "183a6m0", "is_robot_indexable": true, "report_reasons": null, "author": "lunalita_99", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/183a6m0/databricks_auto_loader_and_deduplicating_across/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/183a6m0/databricks_auto_loader_and_deduplicating_across/", "subreddit_subscribers": 141580, "created_utc": 1700882197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I'm relatively new to Airflow and encountering issues while attempting to read a CSV file within my function. Just to provide more context, the file I'm trying to read is located in my local storage, specifically in folder D, while the Airflow files are in folder C. I've also tried placing the file in the same folder as the DAGs, but unfortunately, the function still fails to read the input file. I would greatly appreciate any help or insights into what might be causing this issue. ChatGPT has provided assistance, but I'm still seeking further guidance.\n\nThe error in Logs is \" FileNotFoundError: \\[Errno 2\\] No such file or directory:  \"\n\n \n\n\\# Define the read\\_csv task   \ndef read\\_csv(\\*\\*kwargs):   \ninput\\_file = r'C:\\\\Users\\\\user name\\\\airflow-docker\\\\input\\_data.csv'  \ndf = pd.read\\_csv(input\\_file)   \nkwargs\\['ti'\\].xcom\\_push(key='data', value=df)   \nread\\_csv\\_task = PythonOperator(   \ntask\\_id='read\\_csv',   \npython\\_callable=read\\_csv,   \nprovide\\_context=True, dag=dag,   \n) ", "author_fullname": "t2_54vsm483", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with inserting a csv file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_183fgpi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700901855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;m relatively new to Airflow and encountering issues while attempting to read a CSV file within my function. Just to provide more context, the file I&amp;#39;m trying to read is located in my local storage, specifically in folder D, while the Airflow files are in folder C. I&amp;#39;ve also tried placing the file in the same folder as the DAGs, but unfortunately, the function still fails to read the input file. I would greatly appreciate any help or insights into what might be causing this issue. ChatGPT has provided assistance, but I&amp;#39;m still seeking further guidance.&lt;/p&gt;\n\n&lt;p&gt;The error in Logs is &amp;quot; FileNotFoundError: [Errno 2] No such file or directory:  &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;# Define the read_csv task&lt;br/&gt;\ndef read_csv(**kwargs):&lt;br/&gt;\ninput_file = r&amp;#39;C:\\Users\\user name\\airflow-docker\\input_data.csv&amp;#39;&lt;br/&gt;\ndf = pd.read_csv(input_file)&lt;br/&gt;\nkwargs[&amp;#39;ti&amp;#39;].xcom_push(key=&amp;#39;data&amp;#39;, value=df)&lt;br/&gt;\nread_csv_task = PythonOperator(&lt;br/&gt;\ntask_id=&amp;#39;read_csv&amp;#39;,&lt;br/&gt;\npython_callable=read_csv,&lt;br/&gt;\nprovide_context=True, dag=dag,&lt;br/&gt;\n) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "183fgpi", "is_robot_indexable": true, "report_reasons": null, "author": "slugabed123", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/183fgpi/need_help_with_inserting_a_csv_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/183fgpi/need_help_with_inserting_a_csv_file/", "subreddit_subscribers": 141580, "created_utc": 1700901855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \ud83d\ude80 Microsoft Azure AI Fundamentals Cloud Skill Challenge \ud83e\udd16\n\nThe     challenge has begun! Join now and unlock the power of Microsoft  Azure    AI resources for your future projects. The learning adventure   continues   until December 2nd! \ud83d\uddd3\ud83c\udf10\n\nIt will be an incentive-driven competition, where participants who rank highest on the leaderboard will qualify.\n\n\ud83c\udf81 Perks Await You:\n\n\ud83c\udf1f LinkedIn Premium Voucher (12 months)\n\n\ud83d\udcdc Certificates of Achievement\n\n\ud83c\udf96 Exclusive Badge\n\n\ud83d\udcdd Registration Form:\n\nPlease  take a moment to fill out this  form   and incentives will be sent to  the email you provide. The email  should   be you given one for the  event registration.\n\n[https://forms.office.com/r/TAr28SpwBE?origin=lprLink](https://forms.office.com/r/TAr28SpwBE?origin=lprLink)\n\n\ud83d\udd17 Challenge Link:\n\n[https://learn.microsoft.com/training/challenges?id=18cbff95-6b28-489f-b346-9b2648684292&amp;WT.mc\\_id=cloudskillschallenge\\_18cbff95-6b28-489f-b346-9b2648684292&amp;wt.mc\\_id=studentamb\\_209465](https://learn.microsoft.com/training/challenges?id=18cbff95-6b28-489f-b346-9b2648684292&amp;WT.mc_id=cloudskillschallenge_18cbff95-6b28-489f-b346-9b2648684292&amp;wt.mc_id=studentamb_209465)\n\n\ud83d\udce7 Event Support:\n\nEncountering challenges or have questions about the event incentives send an email to:\n\n[nirmal.chandrasiri@studentambassadors.com](mailto:nirmal.chandrasiri@studentambassadors.com)\n\n\ud83d\ude80 Don't miss out on this incredible opportunity to learn and earn fantastic rewards! \ud83d\ude4c\ud83c\udffb\n\n\\#Sweepstakes\u00a0#Azure\u00a0#AI", "author_fullname": "t2_v8te7qpv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure AI Cloud Skills Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_183k7nb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700920375.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;\ud83d\ude80 Microsoft Azure AI Fundamentals Cloud Skill Challenge \ud83e\udd16&lt;/p&gt;\n\n&lt;p&gt;The     challenge has begun! Join now and unlock the power of Microsoft  Azure    AI resources for your future projects. The learning adventure   continues   until December 2nd! \ud83d\uddd3\ud83c\udf10&lt;/p&gt;\n\n&lt;p&gt;It will be an incentive-driven competition, where participants who rank highest on the leaderboard will qualify.&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf81 Perks Await You:&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf1f LinkedIn Premium Voucher (12 months)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcdc Certificates of Achievement&lt;/p&gt;\n\n&lt;p&gt;\ud83c\udf96 Exclusive Badge&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcdd Registration Form:&lt;/p&gt;\n\n&lt;p&gt;Please  take a moment to fill out this  form   and incentives will be sent to  the email you provide. The email  should   be you given one for the  event registration.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://forms.office.com/r/TAr28SpwBE?origin=lprLink\"&gt;https://forms.office.com/r/TAr28SpwBE?origin=lprLink&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udd17 Challenge Link:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://learn.microsoft.com/training/challenges?id=18cbff95-6b28-489f-b346-9b2648684292&amp;amp;WT.mc_id=cloudskillschallenge_18cbff95-6b28-489f-b346-9b2648684292&amp;amp;wt.mc_id=studentamb_209465\"&gt;https://learn.microsoft.com/training/challenges?id=18cbff95-6b28-489f-b346-9b2648684292&amp;amp;WT.mc_id=cloudskillschallenge_18cbff95-6b28-489f-b346-9b2648684292&amp;amp;wt.mc_id=studentamb_209465&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udce7 Event Support:&lt;/p&gt;\n\n&lt;p&gt;Encountering challenges or have questions about the event incentives send an email to:&lt;/p&gt;\n\n&lt;p&gt;[&lt;a href=\"mailto:nirmal.chandrasiri@studentambassadors.com\"&gt;nirmal.chandrasiri@studentambassadors.com&lt;/a&gt;](mailto:&lt;a href=\"mailto:nirmal.chandrasiri@studentambassadors.com\"&gt;nirmal.chandrasiri@studentambassadors.com&lt;/a&gt;)&lt;/p&gt;\n\n&lt;p&gt;\ud83d\ude80 Don&amp;#39;t miss out on this incredible opportunity to learn and earn fantastic rewards! \ud83d\ude4c\ud83c\udffb&lt;/p&gt;\n\n&lt;p&gt;#Sweepstakes\u00a0#Azure\u00a0#AI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "183k7nb", "is_robot_indexable": true, "report_reasons": null, "author": "Code-Squad", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/183k7nb/azure_ai_cloud_skills_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/183k7nb/azure_ai_cloud_skills_challenge/", "subreddit_subscribers": 141580, "created_utc": 1700920375.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello\n\nI'm encountering an issue while attempting to read data from Elasticsearch using PySpark's Elasticsearch connector. Surprisingly, I can successfully retrieve data using the Python Elasticsearch client directly. The code snippets and configurations for both approaches are provided below:\n\nPython Elasticsearch Client (successful):\n\n    from elasticsearch import Elasticsearch\n    es = Elasticsearch(\n    f\"https://{CONN_ENV}:{PORT}\",\n    basic_auth=(USER, PASSWORD),\n    verify_certs=False,\n    request_timeout=150\n    )\n    response = es.search(index=INDEX_NAME_START, size=10000)\n    print(response[\"hits\"][\"hits\"][1])\n\nPySpark Elasticsearch Connector (encountering issues):\n\n    es_options = {\n    \"es.nodes\": CONN_ENV,\n    \"es.port\": PORT,\n    \"es.resource\": INDEX_NAME_START,\n    \"es.net.http.auth.user\": USER,\n    \"es.net.http.auth.pass\": PASSWORD,\n    \"es.net.ssl\": False,\n    \"es.nodes.wan.only\": \"true\",\n    }\n    \n    es_dataframe = spark.read.format(\"org.elasticsearch.spark.sql\") \\\n    .options(**es_options) \\\n    .load()\n\nError Encountered:\n\n    \n    org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'\n    Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings) - all nodes failed; tried\n\nNote: I'm using the Elasticsearch connector version org.elasticsearch:elasticsearch-spark-30\\_2.12:8.9.1, which matches the Elasticsearch version and i am using databricks\n\n&amp;#x200B;\n\nI'd like to understand why there's a discrepancy between successfully accessing Elasticsearch using the Python Elasticsearch client and encountering connection issues with PySpark's Elasticsearch connector, despite using the same Elasticsearch version. Any insights or troubleshooting suggestions would be greatly appreciated. Thank you!\n\n&amp;#x200B;", "author_fullname": "t2_d6bmxkhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Unable to Read from Elasticsearch Using PySpark Connector but Successful with Python Elasticsearch Client", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_183jhwm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700918035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m encountering an issue while attempting to read data from Elasticsearch using PySpark&amp;#39;s Elasticsearch connector. Surprisingly, I can successfully retrieve data using the Python Elasticsearch client directly. The code snippets and configurations for both approaches are provided below:&lt;/p&gt;\n\n&lt;p&gt;Python Elasticsearch Client (successful):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from elasticsearch import Elasticsearch\nes = Elasticsearch(\nf&amp;quot;https://{CONN_ENV}:{PORT}&amp;quot;,\nbasic_auth=(USER, PASSWORD),\nverify_certs=False,\nrequest_timeout=150\n)\nresponse = es.search(index=INDEX_NAME_START, size=10000)\nprint(response[&amp;quot;hits&amp;quot;][&amp;quot;hits&amp;quot;][1])\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;PySpark Elasticsearch Connector (encountering issues):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;es_options = {\n&amp;quot;es.nodes&amp;quot;: CONN_ENV,\n&amp;quot;es.port&amp;quot;: PORT,\n&amp;quot;es.resource&amp;quot;: INDEX_NAME_START,\n&amp;quot;es.net.http.auth.user&amp;quot;: USER,\n&amp;quot;es.net.http.auth.pass&amp;quot;: PASSWORD,\n&amp;quot;es.net.ssl&amp;quot;: False,\n&amp;quot;es.nodes.wan.only&amp;quot;: &amp;quot;true&amp;quot;,\n}\n\nes_dataframe = spark.read.format(&amp;quot;org.elasticsearch.spark.sql&amp;quot;) \\\n.options(**es_options) \\\n.load()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Error Encountered:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting &amp;#39;es.nodes.wan.only&amp;#39;\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings) - all nodes failed; tried\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Note: I&amp;#39;m using the Elasticsearch connector version org.elasticsearch:elasticsearch-spark-30_2.12:8.9.1, which matches the Elasticsearch version and i am using databricks&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to understand why there&amp;#39;s a discrepancy between successfully accessing Elasticsearch using the Python Elasticsearch client and encountering connection issues with PySpark&amp;#39;s Elasticsearch connector, despite using the same Elasticsearch version. Any insights or troubleshooting suggestions would be greatly appreciated. Thank you!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "183jhwm", "is_robot_indexable": true, "report_reasons": null, "author": "PinPrestigious2327", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/183jhwm/unable_to_read_from_elasticsearch_using_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/183jhwm/unable_to_read_from_elasticsearch_using_pyspark/", "subreddit_subscribers": 141580, "created_utc": 1700918035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "EMR is old 5.x and the spark installed in it is 2.4.x but I need to use the newer spark version to be compatible with certain libs, newer hudi. Is there anyway to run newer Spark [3.5.x] on the older cluster. Any help would be appreciated.\n\nP.S: Older MR jobs and spark jobs are running in the cluster, so the option of upgrading the cluster won't be possible.", "author_fullname": "t2_76srr1mpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there anyway to run newer Spark version on EMR [older spark version]?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_183j7yn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700917133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EMR is old 5.x and the spark installed in it is 2.4.x but I need to use the newer spark version to be compatible with certain libs, newer hudi. Is there anyway to run newer Spark [3.5.x] on the older cluster. Any help would be appreciated.&lt;/p&gt;\n\n&lt;p&gt;P.S: Older MR jobs and spark jobs are running in the cluster, so the option of upgrading the cluster won&amp;#39;t be possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "183j7yn", "is_robot_indexable": true, "report_reasons": null, "author": "pr6g_head", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/183j7yn/is_there_anyway_to_run_newer_spark_version_on_emr/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/183j7yn/is_there_anyway_to_run_newer_spark_version_on_emr/", "subreddit_subscribers": 141580, "created_utc": 1700917133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Our team is looking for an ETL tool (for Reverse ETL specifically, AWS Athena -&gt; HubSpot) and we've stumbled upon Polytomic, which seems to have a lot of integrations and looks promising overall. I couldn't however find any mentions on reddit. Does anyone have any experience with it?", "author_fullname": "t2_tp4bdcyr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience with Polytomic", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182wgsr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700844778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Our team is looking for an ETL tool (for Reverse ETL specifically, AWS Athena -&amp;gt; HubSpot) and we&amp;#39;ve stumbled upon Polytomic, which seems to have a lot of integrations and looks promising overall. I couldn&amp;#39;t however find any mentions on reddit. Does anyone have any experience with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182wgsr", "is_robot_indexable": true, "report_reasons": null, "author": "ichhabewyjebane", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182wgsr/experience_with_polytomic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182wgsr/experience_with_polytomic/", "subreddit_subscribers": 141580, "created_utc": 1700844778.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}