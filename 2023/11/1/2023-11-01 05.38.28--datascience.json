{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know a number of people express annoyance at interviews on this sub. I was raked over the coals a few months ago for apparently bad interview questions but my latest experience blows that out the water. I thought I'd give my experience from the other side of the desk which may go some way to showing why it can be so bad.\n\nI received a message last week saying that an online assessor for a Graduate Data Scientist role had dropped out and they needed volunteers to stand in. I volunteered to help.\n\nSomeone from HR sent me an email with a link to a training video and the interview platform. I watched the 30 min video at 1.5 speed which was mostly stuff like which buttons to press.\n\nThe day before I logged onto the assessment portal I reviewed the questions. I noticed that the questions were very generic but thought there might be some 'calibration' briefing before the interviews; it was too late to speak to HR.\n\nBefore the assessment day there was a HR call 30 mins before. It turned out to be just to check if anyone had technical issues. There was no 'calibration' brief. The call ended after 10 mins as the HR rep had to leave to chase no shows.\n\nI was dropped straight into a 'technical' interview 1 on 1 with the candidate. Although it was apparently technical most of the questions were very generic. E.g. Walk me through a project where you had to solve a problem.\n\nThere were criteria associated with the questions but there was no way you would answer them as the interviewee unless prompted. E.g in the above question a criterion might be 'The candidate readily accepts new ideas'. Given the short time (5 mins per question) it was not really possible to prompt for every criterion but I did try to enable the candidate to score highly but it meant the questioning was very disjointed.\n\nAfter a few of these there was the 'technical' section. These questions seemed to be totally left-field. E.g. you have two identical-size metal cubes how could you differentiate the material they are made of? Obviously this question is useless for the role and the CS-background interviewee needed lots of coaching to answer this.\n\nNext I had a soft skills interview with a different candidate. The questions again were vague and sensible answers would not meet the criteria.\n\nFinally there was a group activity and we were supposed to observe the 'teamwork' but the team just split the tasks and got on with them individually so there was hardly anything to observe.\n\nAfter this the HR bod asked us to complete all the assessments and submit them. Then we'd have a 'wash up'. The wash up was basically the place where scoring could be calibrated by discussing with the other assessors. Of course, the scores had already been submitted by then so this was entirely pointless.\n\nI also asked about the inappropriate technical questions and they said they didn't get the DS questions in time so had just used other technical questions (we were hiring other engineers/scientists at the same time).\n\nSo, as you can see, HR ruin everything they touch and hiring is a HR process so it's terrible. Sorry if you had to go through this.", "author_fullname": "t2_7q2ap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why some data science interviews suck, as an interviewer...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kvjmp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 92, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 92, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698786767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a number of people express annoyance at interviews on this sub. I was raked over the coals a few months ago for apparently bad interview questions but my latest experience blows that out the water. I thought I&amp;#39;d give my experience from the other side of the desk which may go some way to showing why it can be so bad.&lt;/p&gt;\n\n&lt;p&gt;I received a message last week saying that an online assessor for a Graduate Data Scientist role had dropped out and they needed volunteers to stand in. I volunteered to help.&lt;/p&gt;\n\n&lt;p&gt;Someone from HR sent me an email with a link to a training video and the interview platform. I watched the 30 min video at 1.5 speed which was mostly stuff like which buttons to press.&lt;/p&gt;\n\n&lt;p&gt;The day before I logged onto the assessment portal I reviewed the questions. I noticed that the questions were very generic but thought there might be some &amp;#39;calibration&amp;#39; briefing before the interviews; it was too late to speak to HR.&lt;/p&gt;\n\n&lt;p&gt;Before the assessment day there was a HR call 30 mins before. It turned out to be just to check if anyone had technical issues. There was no &amp;#39;calibration&amp;#39; brief. The call ended after 10 mins as the HR rep had to leave to chase no shows.&lt;/p&gt;\n\n&lt;p&gt;I was dropped straight into a &amp;#39;technical&amp;#39; interview 1 on 1 with the candidate. Although it was apparently technical most of the questions were very generic. E.g. Walk me through a project where you had to solve a problem.&lt;/p&gt;\n\n&lt;p&gt;There were criteria associated with the questions but there was no way you would answer them as the interviewee unless prompted. E.g in the above question a criterion might be &amp;#39;The candidate readily accepts new ideas&amp;#39;. Given the short time (5 mins per question) it was not really possible to prompt for every criterion but I did try to enable the candidate to score highly but it meant the questioning was very disjointed.&lt;/p&gt;\n\n&lt;p&gt;After a few of these there was the &amp;#39;technical&amp;#39; section. These questions seemed to be totally left-field. E.g. you have two identical-size metal cubes how could you differentiate the material they are made of? Obviously this question is useless for the role and the CS-background interviewee needed lots of coaching to answer this.&lt;/p&gt;\n\n&lt;p&gt;Next I had a soft skills interview with a different candidate. The questions again were vague and sensible answers would not meet the criteria.&lt;/p&gt;\n\n&lt;p&gt;Finally there was a group activity and we were supposed to observe the &amp;#39;teamwork&amp;#39; but the team just split the tasks and got on with them individually so there was hardly anything to observe.&lt;/p&gt;\n\n&lt;p&gt;After this the HR bod asked us to complete all the assessments and submit them. Then we&amp;#39;d have a &amp;#39;wash up&amp;#39;. The wash up was basically the place where scoring could be calibrated by discussing with the other assessors. Of course, the scores had already been submitted by then so this was entirely pointless.&lt;/p&gt;\n\n&lt;p&gt;I also asked about the inappropriate technical questions and they said they didn&amp;#39;t get the DS questions in time so had just used other technical questions (we were hiring other engineers/scientists at the same time).&lt;/p&gt;\n\n&lt;p&gt;So, as you can see, HR ruin everything they touch and hiring is a HR process so it&amp;#39;s terrible. Sorry if you had to go through this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17kvjmp", "is_robot_indexable": true, "report_reasons": null, "author": "nth_citizen", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17kvjmp/why_some_data_science_interviews_suck_as_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17kvjmp/why_some_data_science_interviews_suck_as_an/", "subreddit_subscribers": 1107809, "created_utc": 1698786767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Got assigned some TS projects at work and now have kind of carved out this niche at my company. It\u2019s great career-wise but I feel like I\u2019d enjoy working with other ML approaches more. \n\nTime series at the scale I\u2019m doing it is basically just lightweight software development; at the end of the day all we do is train a bunch of transformers and models and see which is best for each time series, then use that to make a forecast. \n\nIt also seems that the simplest models (ETS, Theta) perform at least on par with fancy unexplainable models, so there is not much reason to use or even learn about them in depth. \n\nAnyone else find time series somewhat uninteresting? What can I do to get more interested it in?", "author_fullname": "t2_g7jmnu5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone else find time series work a little dull?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17koo01", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 64, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 64, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698768668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Got assigned some TS projects at work and now have kind of carved out this niche at my company. It\u2019s great career-wise but I feel like I\u2019d enjoy working with other ML approaches more. &lt;/p&gt;\n\n&lt;p&gt;Time series at the scale I\u2019m doing it is basically just lightweight software development; at the end of the day all we do is train a bunch of transformers and models and see which is best for each time series, then use that to make a forecast. &lt;/p&gt;\n\n&lt;p&gt;It also seems that the simplest models (ETS, Theta) perform at least on par with fancy unexplainable models, so there is not much reason to use or even learn about them in depth. &lt;/p&gt;\n\n&lt;p&gt;Anyone else find time series somewhat uninteresting? What can I do to get more interested it in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17koo01", "is_robot_indexable": true, "report_reasons": null, "author": "_hairyberry_", "discussion_type": null, "num_comments": 47, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17koo01/anyone_else_find_time_series_work_a_little_dull/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17koo01/anyone_else_find_time_series_work_a_little_dull/", "subreddit_subscribers": 1107809, "created_utc": 1698768668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "From classwork, it seems like a lot of people choose the same number for input into a sample() or set.seed() function. \n\nI always assumed that it was 'bad form' to use the same number for multiple applications of a random seed.  So I actually use dice to generate random seeds, just to be over-detailed.  But is that necessary?  If I just use \"42\" or \"365\" or \"1234\" all the time, am I missing something?  Is there a cultural issue or tradition in communities to use a given number? ", "author_fullname": "t2_ghxga", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data folks of Reddit: How do you choose a random seed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kxd5s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698791634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From classwork, it seems like a lot of people choose the same number for input into a sample() or set.seed() function. &lt;/p&gt;\n\n&lt;p&gt;I always assumed that it was &amp;#39;bad form&amp;#39; to use the same number for multiple applications of a random seed.  So I actually use dice to generate random seeds, just to be over-detailed.  But is that necessary?  If I just use &amp;quot;42&amp;quot; or &amp;quot;365&amp;quot; or &amp;quot;1234&amp;quot; all the time, am I missing something?  Is there a cultural issue or tradition in communities to use a given number? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17kxd5s", "is_robot_indexable": true, "report_reasons": null, "author": "CatOfGrey", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17kxd5s/data_folks_of_reddit_how_do_you_choose_a_random/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17kxd5s/data_folks_of_reddit_how_do_you_choose_a_random/", "subreddit_subscribers": 1107809, "created_utc": 1698791634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "If I don\u2019t use LangChain or HuggingFace how can I build a chat box trained on my local data but using LLM like turbo etc..", "author_fullname": "t2_ayqufd5k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why should I learn LangChain? It\u2019s like learning a whole new tool set on top of LLM/Transformer models\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17l11nx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698802357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I don\u2019t use LangChain or HuggingFace how can I build a chat box trained on my local data but using LLM like turbo etc..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "17l11nx", "is_robot_indexable": true, "report_reasons": null, "author": "Dependent_Mushroom98", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17l11nx/why_should_i_learn_langchain_its_like_learning_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17l11nx/why_should_i_learn_langchain_its_like_learning_a/", "subreddit_subscribers": 1107809, "created_utc": 1698802357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Sorry if this is a dumb question. But how are you all analyzing your models after fitting it with the training? Or in general? \n\nMy coworkers only use GLR for binomial type data. And that allows you to print out a full statistical summary from there. They use the pvalues from this summary to pick the features that are most significant to go into the final model and then test the data. I like this method for GLR but other algorithms aren\u2019t able to print summaries like this and I don\u2019t think we should limit ourselves to GLR only for future projects. \n\nSo how are you all analyzing the data to get insight on what features to use into these types of models? Most of my courses in school taught us to use the correlation matrix against the target. So I am a bit lost on this. I\u2019m not even sure how I would suggest using other algorithms for future business projects if they don\u2019t agree with using a correlation matrix or features of importance to pick the features.", "author_fullname": "t2_5akq1mi3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you analyze your models?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "network", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kp0nu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Analysis", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698769598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a dumb question. But how are you all analyzing your models after fitting it with the training? Or in general? &lt;/p&gt;\n\n&lt;p&gt;My coworkers only use GLR for binomial type data. And that allows you to print out a full statistical summary from there. They use the pvalues from this summary to pick the features that are most significant to go into the final model and then test the data. I like this method for GLR but other algorithms aren\u2019t able to print summaries like this and I don\u2019t think we should limit ourselves to GLR only for future projects. &lt;/p&gt;\n\n&lt;p&gt;So how are you all analyzing the data to get insight on what features to use into these types of models? Most of my courses in school taught us to use the correlation matrix against the target. So I am a bit lost on this. I\u2019m not even sure how I would suggest using other algorithms for future business projects if they don\u2019t agree with using a correlation matrix or features of importance to pick the features.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8addf236-d780-11e7-932d-0e90af9dfe6e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17kp0nu", "is_robot_indexable": true, "report_reasons": null, "author": "Dapper-Economy", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17kp0nu/how_do_you_analyze_your_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17kp0nu/how_do_you_analyze_your_models/", "subreddit_subscribers": 1107809, "created_utc": 1698769598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " So I have been reading up on SHAP values. I get that it works on the principle of game theory . Basically, just like we would want to allocate a payoff among the participants fairly, the same could be done to a statistical model.\n\nFor e.g. if we have a Linear regression model and we have ice cream sales as dependent variable. The independent variables are weather, location of the ice cream store, cost of the ice creams, some marketing efforts (pamphlets, bill boards, sales person etc.) . The SHAP value would ideally attribute the sales to the IVs cited above in varying order of importance.\n\nNow we already get a coefficient associated with each IV through linear regression. Thus giving us the importance of that particular variable.\n\nMy question is : Would a SHAP value applied on top of the Linear regression model discover the same 'truth'. That is, would the SHAP value identify the magnitude of importance of variables exactly like the regression coefficients?\n\nWhat has been your experience? Has SHAP worked for you in case LM or GLM models?\n\nWhat are the pitfalls of using SHAP?", "author_fullname": "t2_1umdosna", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any utility in using SHAP values for feature attribution in cases of Linear models and GLMs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kfjr0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698737668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have been reading up on SHAP values. I get that it works on the principle of game theory . Basically, just like we would want to allocate a payoff among the participants fairly, the same could be done to a statistical model.&lt;/p&gt;\n\n&lt;p&gt;For e.g. if we have a Linear regression model and we have ice cream sales as dependent variable. The independent variables are weather, location of the ice cream store, cost of the ice creams, some marketing efforts (pamphlets, bill boards, sales person etc.) . The SHAP value would ideally attribute the sales to the IVs cited above in varying order of importance.&lt;/p&gt;\n\n&lt;p&gt;Now we already get a coefficient associated with each IV through linear regression. Thus giving us the importance of that particular variable.&lt;/p&gt;\n\n&lt;p&gt;My question is : Would a SHAP value applied on top of the Linear regression model discover the same &amp;#39;truth&amp;#39;. That is, would the SHAP value identify the magnitude of importance of variables exactly like the regression coefficients?&lt;/p&gt;\n\n&lt;p&gt;What has been your experience? Has SHAP worked for you in case LM or GLM models?&lt;/p&gt;\n\n&lt;p&gt;What are the pitfalls of using SHAP?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17kfjr0", "is_robot_indexable": true, "report_reasons": null, "author": "venkarafa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17kfjr0/is_there_any_utility_in_using_shap_values_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17kfjr0/is_there_any_utility_in_using_shap_values_for/", "subreddit_subscribers": 1107809, "created_utc": 1698737668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Would appreciate other website suggestions too.", "author_fullname": "t2_bmxqugb8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Upwork a good place to find data science freelance gigs in the UK ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kmu0e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698763882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would appreciate other website suggestions too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17kmu0e", "is_robot_indexable": true, "report_reasons": null, "author": "FreakedoutNeurotic98", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17kmu0e/is_upwork_a_good_place_to_find_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17kmu0e/is_upwork_a_good_place_to_find_data_science/", "subreddit_subscribers": 1107809, "created_utc": 1698763882.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey y'all, I made a [post](https://www.reddit.com/r/datascience/comments/16rvrrx/anyone_else_here_bogged_down_with_adhoc_sql/) here last month about my team spending too much time on ad-hoc SQL requests.\n\nSo I partnered up with a friend created an AI data assistant to automate ad-hoc SQL requests. It's basically a text to SQL interface for your users. We're looking for a design partner to use our product for free in exchange for feedback.\n\nIn the original [post](https://www.reddit.com/r/datascience/comments/16rvrrx/anyone_else_here_bogged_down_with_adhoc_sql/) there were concerns with trusting an LLM to produce accurate queries. We think there are too, it's not perfect yet. That's why we'd love to partner up with you guys to figure out a way to design a system that can be trusted and reliable, and at the very least, automates the 80% of ad-hoc questions that should be self-served\n\nDM or comment if you're interested and we'll set something up! Would love to hear some feedback, positive or negative, from y'all", "author_fullname": "t2_l386p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "automating ad-hoc SQL requests from stakeholders", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kpxml", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698772053.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all, I made a &lt;a href=\"https://www.reddit.com/r/datascience/comments/16rvrrx/anyone_else_here_bogged_down_with_adhoc_sql/\"&gt;post&lt;/a&gt; here last month about my team spending too much time on ad-hoc SQL requests.&lt;/p&gt;\n\n&lt;p&gt;So I partnered up with a friend created an AI data assistant to automate ad-hoc SQL requests. It&amp;#39;s basically a text to SQL interface for your users. We&amp;#39;re looking for a design partner to use our product for free in exchange for feedback.&lt;/p&gt;\n\n&lt;p&gt;In the original &lt;a href=\"https://www.reddit.com/r/datascience/comments/16rvrrx/anyone_else_here_bogged_down_with_adhoc_sql/\"&gt;post&lt;/a&gt; there were concerns with trusting an LLM to produce accurate queries. We think there are too, it&amp;#39;s not perfect yet. That&amp;#39;s why we&amp;#39;d love to partner up with you guys to figure out a way to design a system that can be trusted and reliable, and at the very least, automates the 80% of ad-hoc questions that should be self-served&lt;/p&gt;\n\n&lt;p&gt;DM or comment if you&amp;#39;re interested and we&amp;#39;ll set something up! Would love to hear some feedback, positive or negative, from y&amp;#39;all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17kpxml", "is_robot_indexable": true, "report_reasons": null, "author": "ruckrawjers", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17kpxml/automating_adhoc_sql_requests_from_stakeholders/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17kpxml/automating_adhoc_sql_requests_from_stakeholders/", "subreddit_subscribers": 1107809, "created_utc": 1698772053.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is it right to assume that the reason, the validation loss (inside the purple box) is fluctuating so much is due to small batch size? What are other reasons due to which loss validation could be fluctuating so much? All hyperparameter values are given in the bottom left of the image.\n\nI'm using BinaryCrossentropy loss function. The problem I'm trying to  solve is from the kaggle's titanic competition. Basically, it's tabular  structured data that has features 'TicketClass', 'Name', 'Sex', 'Age',  'SiblingsBoarded', 'ParentsBoarded', 'Fare', 'Embarked' and target is  'Survived'(1/0). Let me know if you need more info.\n\nhttps://preview.redd.it/gerrkzyzxjxb1.png?width=1087&amp;format=png&amp;auto=webp&amp;s=b20530593f527d138a190a33740e752692d984aa", "author_fullname": "t2_hcgjj0xo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the possible reasons for validation loss to fluctuate so much?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"gerrkzyzxjxb1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/gerrkzyzxjxb1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e2a5627af7b1c267249de3a3e7f0ff925f0b3aa"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/gerrkzyzxjxb1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a6b403d79031bd86f826c73bfa42c859981b8306"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/gerrkzyzxjxb1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd5e667d6c545449021465ebfd60cd8bf51b2ee9"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/gerrkzyzxjxb1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8330db62adf83621f9466ea9aec74bc7c388347c"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/gerrkzyzxjxb1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=911c8213651e7ab675c2811cc1cf28983a6ae038"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/gerrkzyzxjxb1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1ec96c1e6f9b25fc17e998f016ac3ffafd21c19b"}], "s": {"y": 816, "x": 1087, "u": "https://preview.redd.it/gerrkzyzxjxb1.png?width=1087&amp;format=png&amp;auto=webp&amp;s=b20530593f527d138a190a33740e752692d984aa"}, "id": "gerrkzyzxjxb1"}}, "name": "t3_17kmxnc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fpweJb1vOAfmPPotR_URI75nCKNS8GeENhE2Rtp6YPU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698764164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it right to assume that the reason, the validation loss (inside the purple box) is fluctuating so much is due to small batch size? What are other reasons due to which loss validation could be fluctuating so much? All hyperparameter values are given in the bottom left of the image.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using BinaryCrossentropy loss function. The problem I&amp;#39;m trying to  solve is from the kaggle&amp;#39;s titanic competition. Basically, it&amp;#39;s tabular  structured data that has features &amp;#39;TicketClass&amp;#39;, &amp;#39;Name&amp;#39;, &amp;#39;Sex&amp;#39;, &amp;#39;Age&amp;#39;,  &amp;#39;SiblingsBoarded&amp;#39;, &amp;#39;ParentsBoarded&amp;#39;, &amp;#39;Fare&amp;#39;, &amp;#39;Embarked&amp;#39; and target is  &amp;#39;Survived&amp;#39;(1/0). Let me know if you need more info.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gerrkzyzxjxb1.png?width=1087&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b20530593f527d138a190a33740e752692d984aa\"&gt;https://preview.redd.it/gerrkzyzxjxb1.png?width=1087&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b20530593f527d138a190a33740e752692d984aa&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "17kmxnc", "is_robot_indexable": true, "report_reasons": null, "author": "Total-Opposite-8396", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17kmxnc/what_are_the_possible_reasons_for_validation_loss/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17kmxnc/what_are_the_possible_reasons_for_validation_loss/", "subreddit_subscribers": 1107809, "created_utc": 1698764164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019ll compile answers and write an article with the summary", "author_fullname": "t2_dif6b393", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Describe the analytics tool of your dreams\u2026", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kvn2f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tools", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698787019.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ll compile answers and write an article with the summary&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#a06324", "id": "17kvn2f", "is_robot_indexable": true, "report_reasons": null, "author": "ExpressOcelot8977", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17kvn2f/describe_the_analytics_tool_of_your_dreams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17kvn2f/describe_the_analytics_tool_of_your_dreams/", "subreddit_subscribers": 1107809, "created_utc": 1698787019.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a technical interview coming up. The focus will be cleaning unstructured data with Pandas. Are there specific resources for this type of interviews other than just practicing with Kaggle?", "author_fullname": "t2_dc8euqz6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Resources for technical interviews with focus on cleaning unstructured data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kvm37", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698786949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a technical interview coming up. The focus will be cleaning unstructured data with Pandas. Are there specific resources for this type of interviews other than just practicing with Kaggle?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17kvm37", "is_robot_indexable": true, "report_reasons": null, "author": "Illustrious-Bed5587", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17kvm37/resources_for_technical_interviews_with_focus_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17kvm37/resources_for_technical_interviews_with_focus_on/", "subreddit_subscribers": 1107809, "created_utc": 1698786949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "My company offers tuition assistance and I'm thinking about going back for a formal degree, but it'd need to be online in a way I can do while working. I have a Bsc in statistics and an MSc in an unrelated field that I lucked out in being able to take quant-ier courses and leverage an internship into a job, but I feel like there's gaps in my math and experience with some of the newer ML methods &amp; neural networks in particular. \n\nI'm thinking of the Georgia Tech one but would be curious to hear about others.", "author_fullname": "t2_89ar1fajx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "If you did an online MSc in Stats and/or DS or something in that area &amp; liked it, what program was it and what did you like about it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ktlc5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698781586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company offers tuition assistance and I&amp;#39;m thinking about going back for a formal degree, but it&amp;#39;d need to be online in a way I can do while working. I have a Bsc in statistics and an MSc in an unrelated field that I lucked out in being able to take quant-ier courses and leverage an internship into a job, but I feel like there&amp;#39;s gaps in my math and experience with some of the newer ML methods &amp;amp; neural networks in particular. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of the Georgia Tech one but would be curious to hear about others.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "17ktlc5", "is_robot_indexable": true, "report_reasons": null, "author": "AnxiousEgg6284", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17ktlc5/if_you_did_an_online_msc_in_stats_andor_ds_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17ktlc5/if_you_did_an_online_msc_in_stats_andor_ds_or/", "subreddit_subscribers": 1107809, "created_utc": 1698781586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello folks! I\u2019m building a classification model using LSTM many-to-one because my data is sequential in nature. My input dimensions are (50000, 12, 100) to encompass 50000 users, 12 months, and 100 features. So far it\u2019s performing super poorly. How do I ensure that each input individually reads each user\u2019s 12 months data separate from the other. Meaning that 1:50000 are treated independently and ensure non-overlapping in model training.", "author_fullname": "t2_9nb63fwi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LSTM many-to-one classification question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17l0xaq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "ML", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698801978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks! I\u2019m building a classification model using LSTM many-to-one because my data is sequential in nature. My input dimensions are (50000, 12, 100) to encompass 50000 users, 12 months, and 100 features. So far it\u2019s performing super poorly. How do I ensure that each input individually reads each user\u2019s 12 months data separate from the other. Meaning that 1:50000 are treated independently and ensure non-overlapping in model training.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#878a8c", "id": "17l0xaq", "is_robot_indexable": true, "report_reasons": null, "author": "dogsdogsdogsdogswooo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17l0xaq/lstm_manytoone_classification_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17l0xaq/lstm_manytoone_classification_question/", "subreddit_subscribers": 1107809, "created_utc": 1698801978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been watching his videos for a while now and being a beginner, I assumed he was pretty good.\n\nHowever, I've seen a few people criticise him for not knowing what he's talking about, and that he's only good for absolute beginners.", "author_fullname": "t2_cs54hyd66", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on Krish Naik?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17l3gak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698810020.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been watching his videos for a while now and being a beginner, I assumed he was pretty good.&lt;/p&gt;\n\n&lt;p&gt;However, I&amp;#39;ve seen a few people criticise him for not knowing what he&amp;#39;s talking about, and that he&amp;#39;s only good for absolute beginners.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1b37a3ae-70eb-11ee-b5c7-7e3a672f3d51", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#00a6a5", "id": "17l3gak", "is_robot_indexable": true, "report_reasons": null, "author": "Mission-Language8789", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17l3gak/thoughts_on_krish_naik/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17l3gak/thoughts_on_krish_naik/", "subreddit_subscribers": 1107809, "created_utc": 1698810020.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Sorry if this is a dumb question, but I'm still learning!\n\nI have a dataframe with user traits and a second with user id's and target variables. Due to the dataframe that has user id's and target variables having duplicates (for example they viewed a viewed a product x5 then bought ) when I merge my dataframe grows exponentially. I understand why, but is there a way to get my user information attached to the dataframe that has the user id and target variable? I was thinking along the lines of a dictionary, but It has 1mil+ users and 30+ columns, so idk how feasible that is.\n\nThank you for any help!", "author_fullname": "t2_4d7im0v5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I merge user ids with user traits (duplicate id's)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17l2g9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698806731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry if this is a dumb question, but I&amp;#39;m still learning!&lt;/p&gt;\n\n&lt;p&gt;I have a dataframe with user traits and a second with user id&amp;#39;s and target variables. Due to the dataframe that has user id&amp;#39;s and target variables having duplicates (for example they viewed a viewed a product x5 then bought ) when I merge my dataframe grows exponentially. I understand why, but is there a way to get my user information attached to the dataframe that has the user id and target variable? I was thinking along the lines of a dictionary, but It has 1mil+ users and 30+ columns, so idk how feasible that is.&lt;/p&gt;\n\n&lt;p&gt;Thank you for any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#1a1a1b", "id": "17l2g9o", "is_robot_indexable": true, "report_reasons": null, "author": "KillYourFirstBorn", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/17l2g9o/how_do_i_merge_user_ids_with_user_traits/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/17l2g9o/how_do_i_merge_user_ids_with_user_traits/", "subreddit_subscribers": 1107809, "created_utc": 1698806731.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}