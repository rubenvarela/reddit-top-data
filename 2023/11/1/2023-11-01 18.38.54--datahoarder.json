{"kind": "Listing", "data": {"after": "t3_17labiv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4axt7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "House Speaker deleted his podcast. Hoarders to the rescue", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_17krv41", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 2040, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "BULLSHIT!", "can_mod_post": false, "score": 2040, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/uvTH9T91fGpH0ezEYyJiofKRjFdA-3IdaXllbRIhq9M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698777103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/jo7at0rg0lxb1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/jo7at0rg0lxb1.jpg?auto=webp&amp;s=4602c332cf9034751b0d8f19c8eb3bab8fe6b090", "width": 535, "height": 767}, "resolutions": [{"url": "https://preview.redd.it/jo7at0rg0lxb1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=465345bc3f725d4f06d8d86cf23550e448181673", "width": 108, "height": 154}, {"url": "https://preview.redd.it/jo7at0rg0lxb1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8b6bc686085d9403fd096bf7a7d03b00b5bac791", "width": 216, "height": 309}, {"url": "https://preview.redd.it/jo7at0rg0lxb1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a2b798e51e51d372bd8cd6b462fca2a99e0bca14", "width": 320, "height": 458}], "variants": {}, "id": "r0WGUPjd0FhehKIcUOJIb3YTx1NdTYp90vaOq7txZXg"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17krv41", "is_robot_indexable": true, "report_reasons": null, "author": "harrro", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17krv41/house_speaker_deleted_his_podcast_hoarders_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/jo7at0rg0lxb1.jpg", "subreddit_subscribers": 709822, "created_utc": 1698777103.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[https://www.reddit.com/r/DataHoarder/comments/n2w3dr/how\\_to\\_get\\_wd\\_firmware\\_updates/](https://www.reddit.com/r/DataHoarder/comments/n2w3dr/how_to_get_wd_firmware_updates/)\n\nThere was a thread about this, and besides what was said there, I wanted to add something relevant for this subreddit, because only after contacting WD I was able to solve it. After I received their 2 TB NVME SSD, DASHBOARD (their software) updated the firmware, it was an easy process, and after that, random BSODs stopped for good, with Windows 11. Also, the drive is said to be fine with CrystalDiskINFO.\n\nBut, after recent updates, DASHBOARD stopped detecting the SSD. Then, the ticket sent me a few steps which solved the issue:\n\n[https://community.wd.com/t/wd-dashboard-no-longer-recognizes-sn850x-2-tb-nvme-drive/286317/14](https://community.wd.com/t/wd-dashboard-no-longer-recognizes-sn850x-2-tb-nvme-drive/286317/14)\n\nIt's important to post this here, because updating these SSDs is really needed to avoid further problems. Also, if you own a Samsung SSD (like the 980 or 990 Pro), check this ASAP because a few of these will DIE soon if you don't update the FW:\n\n[https://www.tomshardware.com/news/samsung-980-pro-ssd-failures-firmware-update](https://www.tomshardware.com/news/samsung-980-pro-ssd-failures-firmware-update)\n\nI avoided buying NVME from Samsung for that reason. The 990 Pro was said here in DATAHOARDER to experience performance issues:\n\n[https://www.reddit.com/r/buildapc/comments/11p9svz/samsung\\_990\\_pro\\_2tb\\_performance\\_is\\_slow/](https://www.reddit.com/r/buildapc/comments/11p9svz/samsung_990_pro_2tb_performance_is_slow/)\n\nFor Samsung, install their MAGICIAN software.\n\nHere is a link for WD DASHBOARD:  \n[https://support-en.wd.com/app/answers/detailweb/a\\_id/31759/initiator/user](https://support-en.wd.com/app/answers/detailweb/a_id/31759/initiator/user)\n\nAnd MAGICIAN: [https://semiconductor.samsung.com/consumer-storage/magician/](https://semiconductor.samsung.com/consumer-storage/magician/)\n\nI also updated my motherboard (BIOS), in my case it's the MSI Z590-A Pro.\n\nFrom Samsung I only purchased their 4 TB regular SSD. The reason I didn't get the 4 TB NVME from WD was because the price was/still is more than twice what they are asking for the 2 TB version.", "author_fullname": "t2_1utoiwm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Important: How to get Western Digital firmware updates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17krvhp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698777130.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/DataHoarder/comments/n2w3dr/how_to_get_wd_firmware_updates/\"&gt;https://www.reddit.com/r/DataHoarder/comments/n2w3dr/how_to_get_wd_firmware_updates/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There was a thread about this, and besides what was said there, I wanted to add something relevant for this subreddit, because only after contacting WD I was able to solve it. After I received their 2 TB NVME SSD, DASHBOARD (their software) updated the firmware, it was an easy process, and after that, random BSODs stopped for good, with Windows 11. Also, the drive is said to be fine with CrystalDiskINFO.&lt;/p&gt;\n\n&lt;p&gt;But, after recent updates, DASHBOARD stopped detecting the SSD. Then, the ticket sent me a few steps which solved the issue:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://community.wd.com/t/wd-dashboard-no-longer-recognizes-sn850x-2-tb-nvme-drive/286317/14\"&gt;https://community.wd.com/t/wd-dashboard-no-longer-recognizes-sn850x-2-tb-nvme-drive/286317/14&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s important to post this here, because updating these SSDs is really needed to avoid further problems. Also, if you own a Samsung SSD (like the 980 or 990 Pro), check this ASAP because a few of these will DIE soon if you don&amp;#39;t update the FW:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.tomshardware.com/news/samsung-980-pro-ssd-failures-firmware-update\"&gt;https://www.tomshardware.com/news/samsung-980-pro-ssd-failures-firmware-update&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I avoided buying NVME from Samsung for that reason. The 990 Pro was said here in DATAHOARDER to experience performance issues:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/buildapc/comments/11p9svz/samsung_990_pro_2tb_performance_is_slow/\"&gt;https://www.reddit.com/r/buildapc/comments/11p9svz/samsung_990_pro_2tb_performance_is_slow/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For Samsung, install their MAGICIAN software.&lt;/p&gt;\n\n&lt;p&gt;Here is a link for WD DASHBOARD:&lt;br/&gt;\n&lt;a href=\"https://support-en.wd.com/app/answers/detailweb/a_id/31759/initiator/user\"&gt;https://support-en.wd.com/app/answers/detailweb/a_id/31759/initiator/user&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And MAGICIAN: &lt;a href=\"https://semiconductor.samsung.com/consumer-storage/magician/\"&gt;https://semiconductor.samsung.com/consumer-storage/magician/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I also updated my motherboard (BIOS), in my case it&amp;#39;s the MSI Z590-A Pro.&lt;/p&gt;\n\n&lt;p&gt;From Samsung I only purchased their 4 TB regular SSD. The reason I didn&amp;#39;t get the 4 TB NVME from WD was because the price was/still is more than twice what they are asking for the 2 TB version.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17krvhp", "is_robot_indexable": true, "report_reasons": null, "author": "Maratocarde", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17krvhp/important_how_to_get_western_digital_firmware/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17krvhp/important_how_to_get_western_digital_firmware/", "subreddit_subscribers": 709822, "created_utc": 1698777130.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I found this list: https://github.com/meichthys/foss_photo_libraries/tree/main\n\nWhich seems to be only a year old. I've also installed PhotoPrism and the jury is still out because there's gaping holes and issues I don't know if I can resolve.\n\nMy focus is not on storage or I'd just do NextCloud. I want all the bells and whistles. Good face detection, scene and object detection / TensorFlow, reverse geocoding with an API like MapBox, automatic album and filenames using machine learning.\n\nI was really hoping there would be one obvious front-runner.", "author_fullname": "t2_nv9mx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bleeding-edge Self-Hosted Photo software in 2023? I'm talking AI-powered, Docker-friendly, open source, etc...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17l7230", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698825271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found this list: &lt;a href=\"https://github.com/meichthys/foss_photo_libraries/tree/main\"&gt;https://github.com/meichthys/foss_photo_libraries/tree/main&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Which seems to be only a year old. I&amp;#39;ve also installed PhotoPrism and the jury is still out because there&amp;#39;s gaping holes and issues I don&amp;#39;t know if I can resolve.&lt;/p&gt;\n\n&lt;p&gt;My focus is not on storage or I&amp;#39;d just do NextCloud. I want all the bells and whistles. Good face detection, scene and object detection / TensorFlow, reverse geocoding with an API like MapBox, automatic album and filenames using machine learning.&lt;/p&gt;\n\n&lt;p&gt;I was really hoping there would be one obvious front-runner.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17l7230", "is_robot_indexable": true, "report_reasons": null, "author": "malachi347", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17l7230/bleedingedge_selfhosted_photo_software_in_2023_im/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17l7230/bleedingedge_selfhosted_photo_software_in_2023_im/", "subreddit_subscribers": 709822, "created_utc": 1698825271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am planning a Christmas gift for someone who loves having physical media.  She likes the packaging and knowing that she owns the movie without having to pay any type of subscription or logging into an account.  \n\nWe have a PS5 that can play the Blu-ray discs, but I worry about them getting scratched up after continual use.  We have an LG TV that runs webOS which has a USB port so if I can rip the Blu-ray to USB in a supported format, we can play them that way.  She also likes to watch the movies on her iPad while away from home so I am hoping the rip can also be played on it.  \n\nI have a [full-size desktop computer](https://psref.lenovo.com/Product/Legion/Lenovo_Legion_T7_34IMZ5) (i9-11900K / 64GB DDR4 / RTX 3080 4.0 GB) but it doesn't have a Blu-ray player.  I don't see an internal bay to add one so I imagine my options are to either use a USB Blu-ray reader (probably extremely slow) or just temporarily open the case and have an internal SATA Blu-ray player connected while I rip them.  Are there any models people recommend that will do the job quickly?\n\nIt looks like [MakeMKV](https://www.makemkv.com/) is the gold standard for converting a physical Blu-ray to digital media.  Is the digital file compatible with iOS and webOS?  If not, what converters would you recommend? \n\nThe end goal is to have a USB drive with all of the movies digitally on it which can be plugged into the LG TV to play directly.  The files will also be copied to the iPad preferably through iTunes but I could also install the VLC app and copy them that way if iTunes doesn't support it.", "author_fullname": "t2_a4gfzgya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wanting to purchase ~20 Blu-ray musicals and rip them to a format playable on iOS and webOS. What do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17lgn0a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698857740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning a Christmas gift for someone who loves having physical media.  She likes the packaging and knowing that she owns the movie without having to pay any type of subscription or logging into an account.  &lt;/p&gt;\n\n&lt;p&gt;We have a PS5 that can play the Blu-ray discs, but I worry about them getting scratched up after continual use.  We have an LG TV that runs webOS which has a USB port so if I can rip the Blu-ray to USB in a supported format, we can play them that way.  She also likes to watch the movies on her iPad while away from home so I am hoping the rip can also be played on it.  &lt;/p&gt;\n\n&lt;p&gt;I have a &lt;a href=\"https://psref.lenovo.com/Product/Legion/Lenovo_Legion_T7_34IMZ5\"&gt;full-size desktop computer&lt;/a&gt; (i9-11900K / 64GB DDR4 / RTX 3080 4.0 GB) but it doesn&amp;#39;t have a Blu-ray player.  I don&amp;#39;t see an internal bay to add one so I imagine my options are to either use a USB Blu-ray reader (probably extremely slow) or just temporarily open the case and have an internal SATA Blu-ray player connected while I rip them.  Are there any models people recommend that will do the job quickly?&lt;/p&gt;\n\n&lt;p&gt;It looks like &lt;a href=\"https://www.makemkv.com/\"&gt;MakeMKV&lt;/a&gt; is the gold standard for converting a physical Blu-ray to digital media.  Is the digital file compatible with iOS and webOS?  If not, what converters would you recommend? &lt;/p&gt;\n\n&lt;p&gt;The end goal is to have a USB drive with all of the movies digitally on it which can be plugged into the LG TV to play directly.  The files will also be copied to the iPad preferably through iTunes but I could also install the VLC app and copy them that way if iTunes doesn&amp;#39;t support it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lgn0a", "is_robot_indexable": true, "report_reasons": null, "author": "FatherLiamFinnegan", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lgn0a/wanting_to_purchase_20_bluray_musicals_and_rip/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lgn0a/wanting_to_purchase_20_bluray_musicals_and_rip/", "subreddit_subscribers": 709822, "created_utc": 1698857740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The single biggest factor in SSD performance is the presence and quantity of DRAM cache.  A drive with a GB or two of DRAM will write at several hundred MB/s all day long, while a DRAM-less drive will slow to a crawl after the first couple GB written.  \n\nThis being the case, why does nobody list this spec?  Neither amazon nor newegg have cache as a filterable item.  95% of drives don't even mention it one way or the other in the product description.  It's fucking maddening.\n\nAre there any web shops that will let me search for drives with DRAM cache specifically, or am I going to have to resort to googling full reviews of each model individually?", "author_fullname": "t2_4wc8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSD shopping - how to find drives with DRAM cache?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17lgjrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698857499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The single biggest factor in SSD performance is the presence and quantity of DRAM cache.  A drive with a GB or two of DRAM will write at several hundred MB/s all day long, while a DRAM-less drive will slow to a crawl after the first couple GB written.  &lt;/p&gt;\n\n&lt;p&gt;This being the case, why does nobody list this spec?  Neither amazon nor newegg have cache as a filterable item.  95% of drives don&amp;#39;t even mention it one way or the other in the product description.  It&amp;#39;s fucking maddening.&lt;/p&gt;\n\n&lt;p&gt;Are there any web shops that will let me search for drives with DRAM cache specifically, or am I going to have to resort to googling full reviews of each model individually?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "190TB Drivepool/Snapraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17lgjrj", "is_robot_indexable": true, "report_reasons": null, "author": "candre23", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17lgjrj/ssd_shopping_how_to_find_drives_with_dram_cache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lgjrj/ssd_shopping_how_to_find_drives_with_dram_cache/", "subreddit_subscribers": 709822, "created_utc": 1698857499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The drive in question is a Samsung 500 GB Samsung EVO 850. Samsung Magician says the drive is good. There have been 354 TB written. I believe the drive is at least 8 years old.\n\nLately HD Sentinel has been reporting consistent sectors being reallocated. For the month of October 80 sectors were reallocated for a total of 145 that have been reallocated.\n\n(Sorry about the line breaks creating the extra space. I am not sure how to get rid of that.)\n\nHere is a brief history from the log.\n\n11/1/2023 10:45:34 AM,#5   Reallocated Sectors Count  144 -&gt; 145\n\n11/1/2023 1:15:11 AM,#5   Reallocated Sectors Count  143 -&gt; 144\n\n10/31/2023 7:28:19 PM,#5   Reallocated Sectors Count  142 -&gt; 143\n\n10/31/2023 7:46:46 AM,#5   Reallocated Sectors Count  141 -&gt; 142\n\n10/31/2023 3:40:42 AM,#5   Reallocated Sectors Count  140 -&gt; 141\n\n10/31/2023 3:30:40 AM,#5   Reallocated Sectors Count  139 -&gt; 140\n\n10/31/2023 1:45:16 AM,#5   Reallocated Sectors Count  138 -&gt; 139\n\n10/31/2023 1:40:16 AM,#5   Reallocated Sectors Count  137 -&gt; 138\n\n10/30/2023 8:13:11 PM,#5   Reallocated Sectors Count  136 -&gt; 137\n\n10/30/2023 10:31:28 AM,#5   Reallocated Sectors Count  135 -&gt; 136\n\n10/29/2023 11:32:55 PM,#5   Reallocated Sectors Count  134 -&gt; 135\n\n10/29/2023 3:01:36 PM,#5   Reallocated Sectors Count  133 -&gt; 134\n\n10/29/2023 11:20:46 AM,#5   Reallocated Sectors Count  132 -&gt; 133\n\n10/28/2023 12:42:09 PM,#5   Reallocated Sectors Count  130 -&gt; 132\n\n10/28/2023 8:31:29 AM,#5   Reallocated Sectors Count  129 -&gt; 130\n\n10/28/2023 8:21:27 AM,#5   Reallocated Sectors Count  128 -&gt; 129\n\n10/27/2023 5:22:07 PM,#5   Reallocated Sectors Count  127 -&gt; 128\n\n10/27/2023 2:26:43 PM,#5   Reallocated Sectors Count  126 -&gt; 127\n\n10/26/2023 10:34:41 PM,#5   Reallocated Sectors Count  125 -&gt; 126\n\n10/26/2023 7:39:15 PM,#5   Reallocated Sectors Count  124 -&gt; 125\n\n10/26/2023 5:48:58 PM,#5   Reallocated Sectors Count  123 -&gt; 124\n\n10/26/2023 12:48:08 PM,#5   Reallocated Sectors Count  122 -&gt; 123\n\n10/26/2023 9:23:56 AM,#5   Reallocated Sectors Count  121 -&gt; 122\n\n10/26/2023 9:18:53 AM,#5   Reallocated Sectors Count  120 -&gt; 121", "author_fullname": "t2_hxgex", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When Do Reallocated Sectors Become a Problem on an SSD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17le5f1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698851046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The drive in question is a Samsung 500 GB Samsung EVO 850. Samsung Magician says the drive is good. There have been 354 TB written. I believe the drive is at least 8 years old.&lt;/p&gt;\n\n&lt;p&gt;Lately HD Sentinel has been reporting consistent sectors being reallocated. For the month of October 80 sectors were reallocated for a total of 145 that have been reallocated.&lt;/p&gt;\n\n&lt;p&gt;(Sorry about the line breaks creating the extra space. I am not sure how to get rid of that.)&lt;/p&gt;\n\n&lt;p&gt;Here is a brief history from the log.&lt;/p&gt;\n\n&lt;p&gt;11/1/2023 10:45:34 AM,#5   Reallocated Sectors Count  144 -&amp;gt; 145&lt;/p&gt;\n\n&lt;p&gt;11/1/2023 1:15:11 AM,#5   Reallocated Sectors Count  143 -&amp;gt; 144&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 7:28:19 PM,#5   Reallocated Sectors Count  142 -&amp;gt; 143&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 7:46:46 AM,#5   Reallocated Sectors Count  141 -&amp;gt; 142&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 3:40:42 AM,#5   Reallocated Sectors Count  140 -&amp;gt; 141&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 3:30:40 AM,#5   Reallocated Sectors Count  139 -&amp;gt; 140&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 1:45:16 AM,#5   Reallocated Sectors Count  138 -&amp;gt; 139&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 1:40:16 AM,#5   Reallocated Sectors Count  137 -&amp;gt; 138&lt;/p&gt;\n\n&lt;p&gt;10/30/2023 8:13:11 PM,#5   Reallocated Sectors Count  136 -&amp;gt; 137&lt;/p&gt;\n\n&lt;p&gt;10/30/2023 10:31:28 AM,#5   Reallocated Sectors Count  135 -&amp;gt; 136&lt;/p&gt;\n\n&lt;p&gt;10/29/2023 11:32:55 PM,#5   Reallocated Sectors Count  134 -&amp;gt; 135&lt;/p&gt;\n\n&lt;p&gt;10/29/2023 3:01:36 PM,#5   Reallocated Sectors Count  133 -&amp;gt; 134&lt;/p&gt;\n\n&lt;p&gt;10/29/2023 11:20:46 AM,#5   Reallocated Sectors Count  132 -&amp;gt; 133&lt;/p&gt;\n\n&lt;p&gt;10/28/2023 12:42:09 PM,#5   Reallocated Sectors Count  130 -&amp;gt; 132&lt;/p&gt;\n\n&lt;p&gt;10/28/2023 8:31:29 AM,#5   Reallocated Sectors Count  129 -&amp;gt; 130&lt;/p&gt;\n\n&lt;p&gt;10/28/2023 8:21:27 AM,#5   Reallocated Sectors Count  128 -&amp;gt; 129&lt;/p&gt;\n\n&lt;p&gt;10/27/2023 5:22:07 PM,#5   Reallocated Sectors Count  127 -&amp;gt; 128&lt;/p&gt;\n\n&lt;p&gt;10/27/2023 2:26:43 PM,#5   Reallocated Sectors Count  126 -&amp;gt; 127&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 10:34:41 PM,#5   Reallocated Sectors Count  125 -&amp;gt; 126&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 7:39:15 PM,#5   Reallocated Sectors Count  124 -&amp;gt; 125&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 5:48:58 PM,#5   Reallocated Sectors Count  123 -&amp;gt; 124&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 12:48:08 PM,#5   Reallocated Sectors Count  122 -&amp;gt; 123&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 9:23:56 AM,#5   Reallocated Sectors Count  121 -&amp;gt; 122&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 9:18:53 AM,#5   Reallocated Sectors Count  120 -&amp;gt; 121&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17le5f1", "is_robot_indexable": true, "report_reasons": null, "author": "waynomo", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17le5f1/when_do_reallocated_sectors_become_a_problem_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17le5f1/when_do_reallocated_sectors_become_a_problem_on/", "subreddit_subscribers": 709822, "created_utc": 1698851046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "SSDs self-encrypting drives can be bought with two sorts of Opal standard: Opalite and Pyrite. The further is obvious: it implies the data is encrypted but Pyrite has no such requirement.\n\nWhat is your practice? Is the data actually at least partially \u201esafe\u201d from people trying to breach the local stuff by a screwdriver?", "author_fullname": "t2_2wyahov1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSD Opalite vs. Pyrite", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17krmeh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698776490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;SSDs self-encrypting drives can be bought with two sorts of Opal standard: Opalite and Pyrite. The further is obvious: it implies the data is encrypted but Pyrite has no such requirement.&lt;/p&gt;\n\n&lt;p&gt;What is your practice? Is the data actually at least partially \u201esafe\u201d from people trying to breach the local stuff by a screwdriver?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17krmeh", "is_robot_indexable": true, "report_reasons": null, "author": "eRIZpl", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17krmeh/ssd_opalite_vs_pyrite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17krmeh/ssd_opalite_vs_pyrite/", "subreddit_subscribers": 709822, "created_utc": 1698776490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,\n\nI just flashed a 9300-8i to IT mode, but it only shows one disk properly (sda), and one disk as 0B (sdb) (in proxmox).  This is the same/similar regardless of which combination of 8 drives, 2 cables, 2 ports I use - always only one disk reporting a size, up to three additional disks reporting 0B, and no disks ever showing under sas3ircu.\n\nAnyone have any ideas what's going on?\n\n\\# lsblk:\n\n    NAME               MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\n    sda                  8:0    0   1.5T  0 disk\n    \u251c\u2500sda1               8:1    0   1.5T  0 part\n    \u2514\u2500sda9               8:9    0     8M  0 part\n    sdb                  8:16   0     0B  0 disk\n    nvme3n1            259:0    0 931.5G  0 disk\n    \u251c\u2500nvme3n1p1        259:1    0  1007K  0 part\n    \u251c\u2500nvme3n1p2        259:2    0     1G  0 part /boot/efi\n    \u2514\u2500nvme3n1p3        259:3    0 930.5G  0 part\n      \u251c\u2500pve-swap       253:0    0     8G  0 lvm  [SWAP]\n      \u251c\u2500pve-root       253:1    0    96G  0 lvm  /\n      \u251c\u2500pve-data_tmeta 253:2    0   8.1G  0 lvm \n      \u2502 \u2514\u2500pve-data     253:4    0 794.3G  0 lvm \n      \u2514\u2500pve-data_tdata 253:3    0 794.3G  0 lvm \n        \u2514\u2500pve-data     253:4    0 794.3G  0 lvm \n    nvme0n1            259:4    0 931.5G  0 disk\n    \u251c\u2500nvme0n1p1        259:5    0 931.5G  0 part\n    \u2514\u2500nvme0n1p9        259:6    0     8M  0 part\n    nvme1n1            259:7    0 931.5G  0 disk\n    \u251c\u2500nvme1n1p1        259:8    0 931.5G  0 part\n    \u2514\u2500nvme1n1p9        259:9    0     8M  0 part\n    nvme2n1            259:10   0   1.5T  0 disk\n\n\\# sas3ircu 0 display\n\n    Avago Technologies SAS3 IR Configuration Utility.\n    Version 17.00.00.00 (2018.04.02)\n    Copyright (c) 2009-2018 Avago Technologies. All rights reserved.\n    \n    Read configuration has been initiated for controller 0\n    ------------------------------------------------------------------------\n    Controller information\n    ------------------------------------------------------------------------\n      Controller type                         : SAS3008\n      BIOS version                            : 8.37.00.00\n      Firmware version                        : 16.00.10.00\n      Channel description                     : 1 Serial Attached SCSI\n      Initiator ID                            : 0\n      Maximum physical devices                : 1023\n      Concurrent commands supported           : 9856\n      Slot                                    : 24\n      Segment                                 : 0\n      Bus                                     : 3\n      Device                                  : 0\n      Function                                : 0\n      RAID Support                            : No\n    ------------------------------------------------------------------------\n    IR Volume information\n    ------------------------------------------------------------------------\n    ------------------------------------------------------------------------\n    Physical device information\n    ------------------------------------------------------------------------\n    ------------------------------------------------------------------------\n    Enclosure information\n    ------------------------------------------------------------------------\n      Enclosure#                              : 1\n      Logical ID                              : 500605b0:0b2c9b80\n      Numslots                                : 8\n      StartSlot                               : 0\n    ------------------------------------------------------------------------\n    SAS3IRCU: Command DISPLAY Completed Successfully.\n    SAS3IRCU: Utility Completed Successfully.\n\n\\# sas3flash -c 0 -list\n\n    Avago Technologies SAS3 Flash Utility\n    Version 16.00.00.00 (2017.05.02) \n    Copyright 2008-2017 Avago Technologies. All rights reserved.\n    \n            Adapter Selected is a Avago SAS: SAS3008(C0)\n    \n            Controller Number              : 0\n            Controller                     : SAS3008(C0)\n            PCI Address                    : 00:03:00:00\n            SAS Address                    : 500605b-0-0b2c-9b80\n            NVDATA Version (Default)       : 0e.01.00.07\n            NVDATA Version (Persistent)    : 0e.01.00.07\n            Firmware Product ID            : 0x2221 (IT)\n            Firmware Version               : 16.00.10.00\n            NVDATA Vendor                  : LSI\n            NVDATA Product ID              : SAS9300-8i\n            BIOS Version                   : 08.37.00.00\n            UEFI BSD Version               : 18.00.00.00\n            FCODE Version                  : N/A\n            Board Name                     : SAS9300-8i\n            Board Assembly                 : N/A\n            Board Tracer Number            : N/A\n    \n            Finished Processing Commands Successfully.\n            Exiting SAS3Flash.\n\n\\# lspci\n\n    03:00.0 Serial Attached SCSI controller: Broadcom / LSI SAS3008 PCI-Express Fusion-MPT SAS-3 (rev 02)\n\n&amp;#x200B;", "author_fullname": "t2_qz8xb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with HBA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17la0xl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698840023.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698838408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I just flashed a 9300-8i to IT mode, but it only shows one disk properly (sda), and one disk as 0B (sdb) (in proxmox).  This is the same/similar regardless of which combination of 8 drives, 2 cables, 2 ports I use - always only one disk reporting a size, up to three additional disks reporting 0B, and no disks ever showing under sas3ircu.&lt;/p&gt;\n\n&lt;p&gt;Anyone have any ideas what&amp;#39;s going on?&lt;/p&gt;\n\n&lt;p&gt;# lsblk:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;NAME               MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nsda                  8:0    0   1.5T  0 disk\n\u251c\u2500sda1               8:1    0   1.5T  0 part\n\u2514\u2500sda9               8:9    0     8M  0 part\nsdb                  8:16   0     0B  0 disk\nnvme3n1            259:0    0 931.5G  0 disk\n\u251c\u2500nvme3n1p1        259:1    0  1007K  0 part\n\u251c\u2500nvme3n1p2        259:2    0     1G  0 part /boot/efi\n\u2514\u2500nvme3n1p3        259:3    0 930.5G  0 part\n  \u251c\u2500pve-swap       253:0    0     8G  0 lvm  [SWAP]\n  \u251c\u2500pve-root       253:1    0    96G  0 lvm  /\n  \u251c\u2500pve-data_tmeta 253:2    0   8.1G  0 lvm \n  \u2502 \u2514\u2500pve-data     253:4    0 794.3G  0 lvm \n  \u2514\u2500pve-data_tdata 253:3    0 794.3G  0 lvm \n    \u2514\u2500pve-data     253:4    0 794.3G  0 lvm \nnvme0n1            259:4    0 931.5G  0 disk\n\u251c\u2500nvme0n1p1        259:5    0 931.5G  0 part\n\u2514\u2500nvme0n1p9        259:6    0     8M  0 part\nnvme1n1            259:7    0 931.5G  0 disk\n\u251c\u2500nvme1n1p1        259:8    0 931.5G  0 part\n\u2514\u2500nvme1n1p9        259:9    0     8M  0 part\nnvme2n1            259:10   0   1.5T  0 disk\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;# sas3ircu 0 display&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Avago Technologies SAS3 IR Configuration Utility.\nVersion 17.00.00.00 (2018.04.02)\nCopyright (c) 2009-2018 Avago Technologies. All rights reserved.\n\nRead configuration has been initiated for controller 0\n------------------------------------------------------------------------\nController information\n------------------------------------------------------------------------\n  Controller type                         : SAS3008\n  BIOS version                            : 8.37.00.00\n  Firmware version                        : 16.00.10.00\n  Channel description                     : 1 Serial Attached SCSI\n  Initiator ID                            : 0\n  Maximum physical devices                : 1023\n  Concurrent commands supported           : 9856\n  Slot                                    : 24\n  Segment                                 : 0\n  Bus                                     : 3\n  Device                                  : 0\n  Function                                : 0\n  RAID Support                            : No\n------------------------------------------------------------------------\nIR Volume information\n------------------------------------------------------------------------\n------------------------------------------------------------------------\nPhysical device information\n------------------------------------------------------------------------\n------------------------------------------------------------------------\nEnclosure information\n------------------------------------------------------------------------\n  Enclosure#                              : 1\n  Logical ID                              : 500605b0:0b2c9b80\n  Numslots                                : 8\n  StartSlot                               : 0\n------------------------------------------------------------------------\nSAS3IRCU: Command DISPLAY Completed Successfully.\nSAS3IRCU: Utility Completed Successfully.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;# sas3flash -c 0 -list&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Avago Technologies SAS3 Flash Utility\nVersion 16.00.00.00 (2017.05.02) \nCopyright 2008-2017 Avago Technologies. All rights reserved.\n\n        Adapter Selected is a Avago SAS: SAS3008(C0)\n\n        Controller Number              : 0\n        Controller                     : SAS3008(C0)\n        PCI Address                    : 00:03:00:00\n        SAS Address                    : 500605b-0-0b2c-9b80\n        NVDATA Version (Default)       : 0e.01.00.07\n        NVDATA Version (Persistent)    : 0e.01.00.07\n        Firmware Product ID            : 0x2221 (IT)\n        Firmware Version               : 16.00.10.00\n        NVDATA Vendor                  : LSI\n        NVDATA Product ID              : SAS9300-8i\n        BIOS Version                   : 08.37.00.00\n        UEFI BSD Version               : 18.00.00.00\n        FCODE Version                  : N/A\n        Board Name                     : SAS9300-8i\n        Board Assembly                 : N/A\n        Board Tracer Number            : N/A\n\n        Finished Processing Commands Successfully.\n        Exiting SAS3Flash.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;# lspci&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;03:00.0 Serial Attached SCSI controller: Broadcom / LSI SAS3008 PCI-Express Fusion-MPT SAS-3 (rev 02)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17la0xl", "is_robot_indexable": true, "report_reasons": null, "author": "SunRoyal", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17la0xl/help_with_hba/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17la0xl/help_with_hba/", "subreddit_subscribers": 709822, "created_utc": 1698838408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nWhats your experience with the Samsung 870 QVO 8TB drive? I plan to buy an Intel NUC tall version, install Jellyfin and use this drive as main storage for my media. I don't expect I will write and delete data too much off of it in this context ...\n\nThanks", "author_fullname": "t2_155ml1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Samsung 870 QVO 8TB - What's your experience with this drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17l9nzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698837011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;Whats your experience with the Samsung 870 QVO 8TB drive? I plan to buy an Intel NUC tall version, install Jellyfin and use this drive as main storage for my media. I don&amp;#39;t expect I will write and delete data too much off of it in this context ...&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17l9nzl", "is_robot_indexable": true, "report_reasons": null, "author": "mariusmoga_2005", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17l9nzl/samsung_870_qvo_8tb_whats_your_experience_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17l9nzl/samsung_870_qvo_8tb_whats_your_experience_with/", "subreddit_subscribers": 709822, "created_utc": 1698837011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I am switching from a Synology with 8 drive bays to a server with 24 drive bays to up my data hoarding game. [This](https://www.servercase.co.uk/shop/server-cases/rackmount/4u-chassis/4u-server-case-w-24x-35-hot-swappable-satasas-drive-bays-12g-backplane-sc-4324/) is the chassis I will be using. There appear to be 6 backplanes, each of which drive 4 drives. What I am unsure of is how to connect that to my system. I understand I should probably be using an HBA card, I see a lot of recommendations for the LSI 9211-8i, but I only see 2 ports on that, is that sufficient? If not which hba card should I be going for? Also which cables will I need to connect it to the backplanes? Oh also I am using the [Gigabyte Z790 Aorus Elite AX, Intel Z790 Motherboard](https://www.amazon.co.uk/dp/B0BH28M64J?psc=1&amp;ref=ppx_yo2ov_dt_b_product_details) if that matters for this question. Thanks in advance!", "author_fullname": "t2_417t2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some advice with HBA card and cables.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17l92dm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698834941.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1698834598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I am switching from a Synology with 8 drive bays to a server with 24 drive bays to up my data hoarding game. &lt;a href=\"https://www.servercase.co.uk/shop/server-cases/rackmount/4u-chassis/4u-server-case-w-24x-35-hot-swappable-satasas-drive-bays-12g-backplane-sc-4324/\"&gt;This&lt;/a&gt; is the chassis I will be using. There appear to be 6 backplanes, each of which drive 4 drives. What I am unsure of is how to connect that to my system. I understand I should probably be using an HBA card, I see a lot of recommendations for the LSI 9211-8i, but I only see 2 ports on that, is that sufficient? If not which hba card should I be going for? Also which cables will I need to connect it to the backplanes? Oh also I am using the &lt;a href=\"https://www.amazon.co.uk/dp/B0BH28M64J?psc=1&amp;amp;ref=ppx_yo2ov_dt_b_product_details\"&gt;Gigabyte Z790 Aorus Elite AX, Intel Z790 Motherboard&lt;/a&gt; if that matters for this question. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/S15efRRRd-wxIoPCijhJ4k6KZo8xMwCP4mq1uTf_DAc.jpg?auto=webp&amp;s=cc1290301fa7f3bd493ccc284409c3e5f30ab017", "width": 1024, "height": 768}, "resolutions": [{"url": "https://external-preview.redd.it/S15efRRRd-wxIoPCijhJ4k6KZo8xMwCP4mq1uTf_DAc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6fdfbe029bf8446131197eabcc9b2ac7fa3f13b7", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/S15efRRRd-wxIoPCijhJ4k6KZo8xMwCP4mq1uTf_DAc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd493e717e05880dc5683f5c2545724752620188", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/S15efRRRd-wxIoPCijhJ4k6KZo8xMwCP4mq1uTf_DAc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b437b86b3d759167b06eec0bd7852d5c99252f5f", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/S15efRRRd-wxIoPCijhJ4k6KZo8xMwCP4mq1uTf_DAc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3fe7c3d62562f65eb4b0fa3c37011aa9970ad68e", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/S15efRRRd-wxIoPCijhJ4k6KZo8xMwCP4mq1uTf_DAc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a2f43b6d3bae8d99be957d758ddafa61eeb5c86", "width": 960, "height": 720}], "variants": {}, "id": "KycaFLy5iFQEXScOKN6TrtxSMGr4MVNhNdnzL2-2lRE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17l92dm", "is_robot_indexable": true, "report_reasons": null, "author": "Spaztic_monkey", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17l92dm/need_some_advice_with_hba_card_and_cables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17l92dm/need_some_advice_with_hba_card_and_cables/", "subreddit_subscribers": 709822, "created_utc": 1698834598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey so, I just passed the 30TB mark but I need more storage. Can anyone reccomend me a SATA hub or docking bay for more HDDs? I already maxed out the slots in my server case. And specifically NOT USB 3.0 bays. I want them to act like actual drives and I feel like USB would slow them down way too much for my liking.", "author_fullname": "t2_g14at", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can someone reccomend me a SATA HDD HUB/BAY?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17l88yp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698830981.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey so, I just passed the 30TB mark but I need more storage. Can anyone reccomend me a SATA hub or docking bay for more HDDs? I already maxed out the slots in my server case. And specifically NOT USB 3.0 bays. I want them to act like actual drives and I feel like USB would slow them down way too much for my liking.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17l88yp", "is_robot_indexable": true, "report_reasons": null, "author": "Firefoxray", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17l88yp/can_someone_reccomend_me_a_sata_hdd_hubbay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17l88yp/can_someone_reccomend_me_a_sata_hdd_hubbay/", "subreddit_subscribers": 709822, "created_utc": 1698830981.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi everyone.  Figured this would be the best place for this seemingly simple question.  I have a home arcade and I was wondering where best to store my ROMs?  I find myself constantly trying new OS\u2019s and don\u2019t mind spending a weekend going all in, but eventually I will find exactly what I want and will settle.  So my question is, in terms of just ROM storage, does it matter between an SSD or a USB?", "author_fullname": "t2_m3v42azt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSD vs. USB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kx4cl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698790963.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone.  Figured this would be the best place for this seemingly simple question.  I have a home arcade and I was wondering where best to store my ROMs?  I find myself constantly trying new OS\u2019s and don\u2019t mind spending a weekend going all in, but eventually I will find exactly what I want and will settle.  So my question is, in terms of just ROM storage, does it matter between an SSD or a USB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17kx4cl", "is_robot_indexable": true, "report_reasons": null, "author": "Meechiemon76", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17kx4cl/ssd_vs_usb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17kx4cl/ssd_vs_usb/", "subreddit_subscribers": 709822, "created_utc": 1698790963.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just got an external HDD for the first time. Paying 60$ for it was painful but probably worth it. I want to move a lot of things from my current pc to a pc im about to build. Whats the best way to go about it? Do i just drag and drop and wait? As far as i recall i did something like that with a usb once and it just stopped moving files if i tried moving too many files at once. Its going to be hundreds of gigabytes of data.", "author_fullname": "t2_i8glk74", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do i go about backing my ENTIRE pc to an external HDD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17lhxmd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698861121.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just got an external HDD for the first time. Paying 60$ for it was painful but probably worth it. I want to move a lot of things from my current pc to a pc im about to build. Whats the best way to go about it? Do i just drag and drop and wait? As far as i recall i did something like that with a usb once and it just stopped moving files if i tried moving too many files at once. Its going to be hundreds of gigabytes of data.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lhxmd", "is_robot_indexable": true, "report_reasons": null, "author": "ShadowDevil123", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lhxmd/how_do_i_go_about_backing_my_entire_pc_to_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lhxmd/how_do_i_go_about_backing_my_entire_pc_to_an/", "subreddit_subscribers": 709822, "created_utc": 1698861121.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1zols3gr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "(youtube) Storing data for thousands of years | Microsoft Project Silica", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_17lhstu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-rfEYd4NGQg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Storing data for thousands of years | Microsoft Project Silica\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Storing data for thousands of years | Microsoft Project Silica", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-rfEYd4NGQg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Storing data for thousands of years | Microsoft Project Silica\"&gt;&lt;/iframe&gt;", "author_name": "Microsoft", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/-rfEYd4NGQg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Microsoft"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-rfEYd4NGQg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Storing data for thousands of years | Microsoft Project Silica\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17lhstu", "height": 200}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/qfgAl9K2siFv9YZh0m5o5ro90sPN9tqmh3bMQ6wwzMQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698860772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=-rfEYd4NGQg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GTY_frZgMFu1LX2AOQNmqT7I8muJecVcpkqn5se8uso.jpg?auto=webp&amp;s=f2daddc2b52a1b838662205ddc5e3109e4ef08d5", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/GTY_frZgMFu1LX2AOQNmqT7I8muJecVcpkqn5se8uso.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b98d0216da75bc3255d7ffd32916bab1e1be8aa0", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/GTY_frZgMFu1LX2AOQNmqT7I8muJecVcpkqn5se8uso.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=68688aa5084f178c1c90c5e945b1f9d0188af742", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/GTY_frZgMFu1LX2AOQNmqT7I8muJecVcpkqn5se8uso.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6efe5bc715e66852276945b6fb7eb501a64a6c41", "width": 320, "height": 240}], "variants": {}, "id": "k1ITqqVc_YlwsuCT0OFqC1N1mYLdjRJvZ97cQrZ79hU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lhstu", "is_robot_indexable": true, "report_reasons": null, "author": "ZYinMD", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lhstu/youtube_storing_data_for_thousands_of_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=-rfEYd4NGQg", "subreddit_subscribers": 709822, "created_utc": 1698860772.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Storing data for thousands of years | Microsoft Project Silica", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-rfEYd4NGQg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Storing data for thousands of years | Microsoft Project Silica\"&gt;&lt;/iframe&gt;", "author_name": "Microsoft", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/-rfEYd4NGQg/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@Microsoft"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to store a backup of recordings we've been making for the past three years. It's currently at less than 3 TB and these are 5 - 9 GB files each, as mp4s. It will continue to grow, as we generate 6 recordings a month. I don't need to access the backup frequently, as the files are also on my local machine, on archival M-discs, and on a separate HDD that I keep as a physical backup and sync two regularly. I also have Backblaze in the background. So when I go back to edit the recordings, I'll be using the local files rather than the ones in the cloud.\n\nHowever I really want someplace to drop a recording immediately after it's done that's off-site in the cloud, just in case of an immediate disaster.\n\nThey are currently on [Sync.com](https://sync.com/), which offers unlimited space for $30/mo, but the service's stopped providing support *of any kind* recently (despite advertising phone support for their higher tiers) so I'm worried they're about to go under or that something is up with their company.\n\nI had considered AWS Deep Archive, but their egress cost seems insane as the service is really meant for \"archives\" as far as I understand it, not as a backup I might very infrequently access. \n\nWhat other options are out there to consider?", "author_fullname": "t2_7r7o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use case for my long term video storage and potential cloud options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17lh46i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698858960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to store a backup of recordings we&amp;#39;ve been making for the past three years. It&amp;#39;s currently at less than 3 TB and these are 5 - 9 GB files each, as mp4s. It will continue to grow, as we generate 6 recordings a month. I don&amp;#39;t need to access the backup frequently, as the files are also on my local machine, on archival M-discs, and on a separate HDD that I keep as a physical backup and sync two regularly. I also have Backblaze in the background. So when I go back to edit the recordings, I&amp;#39;ll be using the local files rather than the ones in the cloud.&lt;/p&gt;\n\n&lt;p&gt;However I really want someplace to drop a recording immediately after it&amp;#39;s done that&amp;#39;s off-site in the cloud, just in case of an immediate disaster.&lt;/p&gt;\n\n&lt;p&gt;They are currently on &lt;a href=\"https://sync.com/\"&gt;Sync.com&lt;/a&gt;, which offers unlimited space for $30/mo, but the service&amp;#39;s stopped providing support &lt;em&gt;of any kind&lt;/em&gt; recently (despite advertising phone support for their higher tiers) so I&amp;#39;m worried they&amp;#39;re about to go under or that something is up with their company.&lt;/p&gt;\n\n&lt;p&gt;I had considered AWS Deep Archive, but their egress cost seems insane as the service is really meant for &amp;quot;archives&amp;quot; as far as I understand it, not as a backup I might very infrequently access. &lt;/p&gt;\n\n&lt;p&gt;What other options are out there to consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lh46i", "is_robot_indexable": true, "report_reasons": null, "author": "mccoypauley", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lh46i/use_case_for_my_long_term_video_storage_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lh46i/use_case_for_my_long_term_video_storage_and/", "subreddit_subscribers": 709822, "created_utc": 1698858960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! I know I can write a script for this, but I was wondering if there is some sort of solution that is already made for this.\n\nThanks!", "author_fullname": "t2_uy2zp43z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a tool to automatically archive a folder, transfer via scp/rsync and decompress on the other side automatically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lfukx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698855651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I know I can write a script for this, but I was wondering if there is some sort of solution that is already made for this.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lfukx", "is_robot_indexable": true, "report_reasons": null, "author": "angel__-__-", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lfukx/is_there_a_tool_to_automatically_archive_a_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lfukx/is_there_a_tool_to_automatically_archive_a_folder/", "subreddit_subscribers": 709822, "created_utc": 1698855651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, first time posting on this subreddit. \n\nSo currently I have over 200,000 bookmarks on my Windows laptop split across these browsers - Chrome, Brave, Edge, Firefox, Opera, Vivaldi and Chrome Beta (Chrome Dev seems to be quite unstable).\n\nDue to privacy and security reasons, I DON'T want to use ANY online/cloud service, and I use all browsers on my laptop SIGNED OUT so there is NO online/cloud syncing, NO multiple profiles, and ALL the bookmarks are stored LOCALLY on the laptop's hard drive. I also keep exporting them from time-to-time in HTML formats as backups. \n\nI have NO duplicates or waste bookmarks, they are well organized into dozens of different folders and it has been my habit to collect various links, websites and URLs since nearly a decade now. \n\nGenuinely curious about whether or not this process can scale to potentially MILLIONS of bookmarks in the future, because its part of my habit to store bookmarks while browsing the web. ", "author_fullname": "t2_986czkvnk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can web browsers on computers store and manage millions of bookmarks offline and still be functional without lagging if there are NO online/cloud syncing at all?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lejtu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698852140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, first time posting on this subreddit. &lt;/p&gt;\n\n&lt;p&gt;So currently I have over 200,000 bookmarks on my Windows laptop split across these browsers - Chrome, Brave, Edge, Firefox, Opera, Vivaldi and Chrome Beta (Chrome Dev seems to be quite unstable).&lt;/p&gt;\n\n&lt;p&gt;Due to privacy and security reasons, I DON&amp;#39;T want to use ANY online/cloud service, and I use all browsers on my laptop SIGNED OUT so there is NO online/cloud syncing, NO multiple profiles, and ALL the bookmarks are stored LOCALLY on the laptop&amp;#39;s hard drive. I also keep exporting them from time-to-time in HTML formats as backups. &lt;/p&gt;\n\n&lt;p&gt;I have NO duplicates or waste bookmarks, they are well organized into dozens of different folders and it has been my habit to collect various links, websites and URLs since nearly a decade now. &lt;/p&gt;\n\n&lt;p&gt;Genuinely curious about whether or not this process can scale to potentially MILLIONS of bookmarks in the future, because its part of my habit to store bookmarks while browsing the web. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lejtu", "is_robot_indexable": true, "report_reasons": null, "author": "ZyxWvuO", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lejtu/can_web_browsers_on_computers_store_and_manage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lejtu/can_web_browsers_on_computers_store_and_manage/", "subreddit_subscribers": 709822, "created_utc": 1698852140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Where can i learn facts about HDD head movement?\n\nMost information I get is clueless consumers scaring each other and telling stories of failing drives (which could very well be the case i'm in)\n\nWe just moved offices and our main NAS (RAID1 of WD 10TB gold drives) started to display constant head movement that i can hear easily. Almost like clicking of death, but more \"contained\".  not so loud and just under 1s interval.\n\nWe do have daily/biweekly backup besides the RAID1. But since we were semi-offline for the office move we have some data there that is not on the (paused) daily snapshots. Which i don't want to tell users they lost. This sound very amateur, because it is :) non-profit and all.\n\nThe host is linux 6.5. I'm using only two disks in read-only mode for now.\n\nEach disk is 10TB but only have 2TB partitions for the linux soft raid (mdadm). In the raid there's a LUKS volume with encryption and inside that a single ext4 partition.\n\neverything shows up perfect. SMART pass with flying colors on short tests. RAID shows healthy status. kernel complains of nothing. But the head movement is driving me crazy scared!\n\nThe drives are mostly idle, but vmstat does show some IO happening. Which i suspect is journal being checked? Is there anyway i can tell the kernel to pause all journal activity to make sure the noise goes away? Or maybe it is a firmware thing?\n\nThe drives are not too old. They are exact same model but different lots. always lived in that raid1. And they were transported in different methods for the move. They are both connected to the same PSU (can this be bad power? would it not show up anywhere?). Data reading bandwidth seems the same as before the move (i.e. during the time without the weird head movement sound).\n\nWhat i'm trying to understand now is if I can take my time getting data out of the drivers which are offline, threat it as a imminent failure case, or don't even bother buying new drives for now and use them as regular till they die as they all do and just keep dropping in new spares in the raid. it's only the fact that all of them started the sound at the same time that leads me to believe it might be something in software/firmware/journal thanks to the time offline instead of hardware failure.", "author_fullname": "t2_vjutt7ni", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD head movement information", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ldhba", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698849188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where can i learn facts about HDD head movement?&lt;/p&gt;\n\n&lt;p&gt;Most information I get is clueless consumers scaring each other and telling stories of failing drives (which could very well be the case i&amp;#39;m in)&lt;/p&gt;\n\n&lt;p&gt;We just moved offices and our main NAS (RAID1 of WD 10TB gold drives) started to display constant head movement that i can hear easily. Almost like clicking of death, but more &amp;quot;contained&amp;quot;.  not so loud and just under 1s interval.&lt;/p&gt;\n\n&lt;p&gt;We do have daily/biweekly backup besides the RAID1. But since we were semi-offline for the office move we have some data there that is not on the (paused) daily snapshots. Which i don&amp;#39;t want to tell users they lost. This sound very amateur, because it is :) non-profit and all.&lt;/p&gt;\n\n&lt;p&gt;The host is linux 6.5. I&amp;#39;m using only two disks in read-only mode for now.&lt;/p&gt;\n\n&lt;p&gt;Each disk is 10TB but only have 2TB partitions for the linux soft raid (mdadm). In the raid there&amp;#39;s a LUKS volume with encryption and inside that a single ext4 partition.&lt;/p&gt;\n\n&lt;p&gt;everything shows up perfect. SMART pass with flying colors on short tests. RAID shows healthy status. kernel complains of nothing. But the head movement is driving me crazy scared!&lt;/p&gt;\n\n&lt;p&gt;The drives are mostly idle, but vmstat does show some IO happening. Which i suspect is journal being checked? Is there anyway i can tell the kernel to pause all journal activity to make sure the noise goes away? Or maybe it is a firmware thing?&lt;/p&gt;\n\n&lt;p&gt;The drives are not too old. They are exact same model but different lots. always lived in that raid1. And they were transported in different methods for the move. They are both connected to the same PSU (can this be bad power? would it not show up anywhere?). Data reading bandwidth seems the same as before the move (i.e. during the time without the weird head movement sound).&lt;/p&gt;\n\n&lt;p&gt;What i&amp;#39;m trying to understand now is if I can take my time getting data out of the drivers which are offline, threat it as a imminent failure case, or don&amp;#39;t even bother buying new drives for now and use them as regular till they die as they all do and just keep dropping in new spares in the raid. it&amp;#39;s only the fact that all of them started the sound at the same time that leads me to believe it might be something in software/firmware/journal thanks to the time offline instead of hardware failure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ldhba", "is_robot_indexable": true, "report_reasons": null, "author": "DecentTone876", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ldhba/hdd_head_movement_information/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ldhba/hdd_head_movement_information/", "subreddit_subscribers": 709822, "created_utc": 1698849188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I say \"large\" because it's probably crumbs to folks on here but I have roughly 2-3TB of movies and tv shows for my Plex server. Problem is, I have copies of mainly movies scattered between 3-4 computers and I'm finally setting up a dedicated server to consolidate everything. Once I have everything on a single computer, I want to be able to find duplicate movies and shows and keep the higher quality ones. I guess the simplest way would be to toss everything into the same folder and when Windows notifies me of a duplicate, I would just keep which ever file size is bigger which would assume higher quality. I'm hoping to find an application that lets me export video metadata so I can see sizes, bitrate, and resolution so I can 1. delete low quality version then 2. see which of my favorite movies/shows are of low quality so I can eventually get higher quality replacements.  \n  \nI came across MediaInfo but can't tell if it only lets you view video data individually or if it can do a mass export to a csv or something of the like.", "author_fullname": "t2_ceda1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exporting video metadata of \"large\" collection of movies and tv shows.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lbwxz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698844631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I say &amp;quot;large&amp;quot; because it&amp;#39;s probably crumbs to folks on here but I have roughly 2-3TB of movies and tv shows for my Plex server. Problem is, I have copies of mainly movies scattered between 3-4 computers and I&amp;#39;m finally setting up a dedicated server to consolidate everything. Once I have everything on a single computer, I want to be able to find duplicate movies and shows and keep the higher quality ones. I guess the simplest way would be to toss everything into the same folder and when Windows notifies me of a duplicate, I would just keep which ever file size is bigger which would assume higher quality. I&amp;#39;m hoping to find an application that lets me export video metadata so I can see sizes, bitrate, and resolution so I can 1. delete low quality version then 2. see which of my favorite movies/shows are of low quality so I can eventually get higher quality replacements.  &lt;/p&gt;\n\n&lt;p&gt;I came across MediaInfo but can&amp;#39;t tell if it only lets you view video data individually or if it can do a mass export to a csv or something of the like.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lbwxz", "is_robot_indexable": true, "report_reasons": null, "author": "inthemix8080", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lbwxz/exporting_video_metadata_of_large_collection_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lbwxz/exporting_video_metadata_of_large_collection_of/", "subreddit_subscribers": 709822, "created_utc": 1698844631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Dear members,\n\nI have a collection of low to medium capacity HDDs (int. &amp; ext.) and SSDs (ext) between 480 GB and 4 TB. Currently, I'm considering building a NAS with those components that does not need to perform exceptionally. Thus, I was considering a RPi 5 with a Quad SATA HAT (currently available only for RPi 4) and a powered USB Hub for powering the HDDs and as a temporary solution for the SATA drives, until a SATA HAT is released for the RPi 5.\n\nMy goal is to have the lowest idle consumption possible and even turn the NAS off for the most part of the day, make use of the components, invest less than 350 bucks and fit it under my bookshelf with a height constraint of 9cm.\n\nWould this be viable at all, or have I not considered some solutions that are less advertised than the RPi articles i.e. DIY solutions?\n\nMuch appreciated!", "author_fullname": "t2_z560y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "4x 2.5 SATA HDD, 4x 2.5 USB HDD, 2x 2.5 SATA SSD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17l7rha", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698836894.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698828709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear members,&lt;/p&gt;\n\n&lt;p&gt;I have a collection of low to medium capacity HDDs (int. &amp;amp; ext.) and SSDs (ext) between 480 GB and 4 TB. Currently, I&amp;#39;m considering building a NAS with those components that does not need to perform exceptionally. Thus, I was considering a RPi 5 with a Quad SATA HAT (currently available only for RPi 4) and a powered USB Hub for powering the HDDs and as a temporary solution for the SATA drives, until a SATA HAT is released for the RPi 5.&lt;/p&gt;\n\n&lt;p&gt;My goal is to have the lowest idle consumption possible and even turn the NAS off for the most part of the day, make use of the components, invest less than 350 bucks and fit it under my bookshelf with a height constraint of 9cm.&lt;/p&gt;\n\n&lt;p&gt;Would this be viable at all, or have I not considered some solutions that are less advertised than the RPi articles i.e. DIY solutions?&lt;/p&gt;\n\n&lt;p&gt;Much appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17l7rha", "is_robot_indexable": true, "report_reasons": null, "author": "mhmilo24", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17l7rha/4x_25_sata_hdd_4x_25_usb_hdd_2x_25_sata_ssd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17l7rha/4x_25_sata_hdd_4x_25_usb_hdd_2x_25_sata_ssd/", "subreddit_subscribers": 709822, "created_utc": 1698828709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking to up my unraid array. Do they usually do good Black Friday deals. How many of you are satisfied with their refurb drives?", "author_fullname": "t2_5j15m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ServerPartsDeals good?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kypgv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698795388.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking to up my unraid array. Do they usually do good Black Friday deals. How many of you are satisfied with their refurb drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17kypgv", "is_robot_indexable": true, "report_reasons": null, "author": "halfam", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17kypgv/serverpartsdeals_good/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17kypgv/serverpartsdeals_good/", "subreddit_subscribers": 709822, "created_utc": 1698795388.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've read that changing the name of data disks in Snapraid is tolerated. However to my understanding changing the order of data drives in the Snapraid config will cause issues with parity.\n\nFor example when replacing two smaller disks for one larger 18TB HDD, I run into the issue:\n\nData Disks:\n\n&gt;*d1-Exos10tb (this drive to be replaced with larger HDD)*  \n&gt;  \n&gt;*d2-Exos-8tb  (drive also to be replaced with same large HDD from above)*  \n&gt;  \n&gt;*d3-Exos20tb*  \n&gt;  \n&gt;*d4-Exos16tb*  \n&gt;  \n&gt;*d5-Exos14tb*\n\nI wish to replace d1 and d2 with one large HDD. How would I go about this operation?\n\nIf I just assign the new larger data disk as d1, what would happen to d2 and the the drives listed after it? Should I comment out the d2 line until I utilize that position with another drive, preserving the following disk's order? Or is there a better way to go about this?", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reordering Data Drives in Snapraid config?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kv0ov", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698785409.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read that changing the name of data disks in Snapraid is tolerated. However to my understanding changing the order of data drives in the Snapraid config will cause issues with parity.&lt;/p&gt;\n\n&lt;p&gt;For example when replacing two smaller disks for one larger 18TB HDD, I run into the issue:&lt;/p&gt;\n\n&lt;p&gt;Data Disks:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;em&gt;d1-Exos10tb (this drive to be replaced with larger HDD)&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;d2-Exos-8tb  (drive also to be replaced with same large HDD from above)&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;d3-Exos20tb&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;d4-Exos16tb&lt;/em&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;d5-Exos14tb&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I wish to replace d1 and d2 with one large HDD. How would I go about this operation?&lt;/p&gt;\n\n&lt;p&gt;If I just assign the new larger data disk as d1, what would happen to d2 and the the drives listed after it? Should I comment out the d2 line until I utilize that position with another drive, preserving the following disk&amp;#39;s order? Or is there a better way to go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17kv0ov", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17kv0ov/reordering_data_drives_in_snapraid_config/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17kv0ov/reordering_data_drives_in_snapraid_config/", "subreddit_subscribers": 709822, "created_utc": 1698785409.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, was wondering if there was an app or convenient way to batch download saved and upvoted images from my profile. I saw some Python scripts floating around but unfortunately I\u2019m a neanderthal and don\u2019t know how to use Python", "author_fullname": "t2_2w622ddn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apps that batch download saved/upvoted images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lfohj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698855180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, was wondering if there was an app or convenient way to batch download saved and upvoted images from my profile. I saw some Python scripts floating around but unfortunately I\u2019m a neanderthal and don\u2019t know how to use Python&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lfohj", "is_robot_indexable": true, "report_reasons": null, "author": "PickleInTheSun", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lfohj/apps_that_batch_download_savedupvoted_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lfohj/apps_that_batch_download_savedupvoted_images/", "subreddit_subscribers": 709822, "created_utc": 1698855180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "this might be a silly question, but during the twitter link search, am i able to click confirm and start downloading before it's done, or will that cut off any of the posts that haven't been grabbed yet? \n\ni saw a post about manual crawling that said it's fine to do and that it'll add the rest of the media into a second batch later - but i've never used this before and i don't wanna waste the last half hour i've spent waiting for the search to be over just to have to do it all over again lol", "author_fullname": "t2_121h3iip", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confirming WFDownloader Link Search Before Complete?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17kzxxr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698798997.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;this might be a silly question, but during the twitter link search, am i able to click confirm and start downloading before it&amp;#39;s done, or will that cut off any of the posts that haven&amp;#39;t been grabbed yet? &lt;/p&gt;\n\n&lt;p&gt;i saw a post about manual crawling that said it&amp;#39;s fine to do and that it&amp;#39;ll add the rest of the media into a second batch later - but i&amp;#39;ve never used this before and i don&amp;#39;t wanna waste the last half hour i&amp;#39;ve spent waiting for the search to be over just to have to do it all over again lol&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17kzxxr", "is_robot_indexable": true, "report_reasons": null, "author": "food_WHOREder", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17kzxxr/confirming_wfdownloader_link_search_before/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17kzxxr/confirming_wfdownloader_link_search_before/", "subreddit_subscribers": 709822, "created_utc": 1698798997.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello there. \nI'm thinking of getting/making a home NAS/Server/Jellyfin. \nI already have an old pc with a i5-6600k, GTX1060, 550w PSU just laying around and I'm trying to understand if it will be enough for it. \nThe other options I saw was getting a Synology or making a Pi_NAS. \nI'm afraid that the PC will consume much more energy than the other 2 options. \nWhat do you guys think?", "author_fullname": "t2_5syc5zdl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old PC vs NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17labiv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698839483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there. \nI&amp;#39;m thinking of getting/making a home NAS/Server/Jellyfin. \nI already have an old pc with a i5-6600k, GTX1060, 550w PSU just laying around and I&amp;#39;m trying to understand if it will be enough for it. \nThe other options I saw was getting a Synology or making a Pi_NAS. \nI&amp;#39;m afraid that the PC will consume much more energy than the other 2 options. \nWhat do you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17labiv", "is_robot_indexable": true, "report_reasons": null, "author": "Serigaita", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17labiv/old_pc_vs_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17labiv/old_pc_vs_nas/", "subreddit_subscribers": 709822, "created_utc": 1698839483.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}