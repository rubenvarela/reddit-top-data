{"kind": "Listing", "data": {"after": "t3_17lfohj", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The single biggest factor in SSD performance is the presence and quantity of DRAM cache.  A drive with a GB or two of DRAM will write at several hundred MB/s all day long, while a DRAM-less drive will slow to a crawl after the first couple GB written.  \n\nThis being the case, why does nobody list this spec?  Neither amazon nor newegg have cache as a filterable item.  95% of drives don't even mention it one way or the other in the product description.  It's fucking maddening.\n\nAre there any web shops that will let me search for drives with DRAM cache specifically, or am I going to have to resort to googling full reviews of each model individually?", "author_fullname": "t2_4wc8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSD shopping - how to find drives with DRAM cache?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lgjrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698857499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The single biggest factor in SSD performance is the presence and quantity of DRAM cache.  A drive with a GB or two of DRAM will write at several hundred MB/s all day long, while a DRAM-less drive will slow to a crawl after the first couple GB written.  &lt;/p&gt;\n\n&lt;p&gt;This being the case, why does nobody list this spec?  Neither amazon nor newegg have cache as a filterable item.  95% of drives don&amp;#39;t even mention it one way or the other in the product description.  It&amp;#39;s fucking maddening.&lt;/p&gt;\n\n&lt;p&gt;Are there any web shops that will let me search for drives with DRAM cache specifically, or am I going to have to resort to googling full reviews of each model individually?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "210TB Drivepool/Snapraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17lgjrj", "is_robot_indexable": true, "report_reasons": null, "author": "candre23", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17lgjrj/ssd_shopping_how_to_find_drives_with_dram_cache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lgjrj/ssd_shopping_how_to_find_drives_with_dram_cache/", "subreddit_subscribers": 709950, "created_utc": 1698857499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "@storagereview tiktok, Instagram and YouTube ", "author_fullname": "t2_45mfx1w8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "btw my birthday after few days ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_17m15ik", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 1200, "fallback_url": "https://v.redd.it/y2xvvj9d4xxb1/DASH_480.mp4?source=fallback", "has_audio": true, "height": 854, "width": 480, "scrubber_media_url": "https://v.redd.it/y2xvvj9d4xxb1/DASH_96.mp4", "dash_url": "https://v.redd.it/y2xvvj9d4xxb1/DASHPlaylist.mpd?a=1701521492%2CMWM2OGUwMTVmZWFjNWIzOTdjMjM3NWE1NWFmNzc4NTZkYjI2ZTQ1OGM3MWZjZmI0MmYwZWFlODRkMmExZjBiNg%3D%3D&amp;v=1&amp;f=sd", "duration": 68, "hls_url": "https://v.redd.it/y2xvvj9d4xxb1/HLSPlaylist.m3u8?a=1701521492%2CMmY1MGFiODY1M2M4MDMwZTQyODBkMDUyM2JlOTllYzAzNjQ3NjNlOTQ3NjQyNzExYTJjN2I0ZTJkMzgwN2Q0Zg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=0ad44f8580400f68a7a98b7603bd0433493df335", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698923687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;@storagereview tiktok, Instagram and YouTube &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/y2xvvj9d4xxb1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?format=pjpg&amp;auto=webp&amp;s=79ef61972a167c7bdb4109e5889ea12296f70aed", "width": 1208, "height": 2148}, "resolutions": [{"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b92b942c6f821034c2051bc7d6a1d11eaca0a9d4", "width": 108, "height": 192}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3ba49c21ba43017a0c97988b49808d3abc930532", "width": 216, "height": 384}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2d73ee987c11a630adcc30829d83af771bd07d4a", "width": 320, "height": 569}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cdf900c875d85f00692beaef27abcf3d30c2d53b", "width": 640, "height": 1138}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=784cca675c8fa8f8ff00febc2875d1f5ee0ed54f", "width": 960, "height": 1707}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=89ae3dcce5e9f18556a22ade153245c62e476751", "width": 1080, "height": 1920}], "variants": {}, "id": "NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m15ik", "is_robot_indexable": true, "report_reasons": null, "author": "Champion-Dapper", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17m15ik/btw_my_birthday_after_few_days/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/y2xvvj9d4xxb1", "subreddit_subscribers": 709950, "created_utc": 1698923687.0, "num_crossposts": 2, "media": {"reddit_video": {"bitrate_kbps": 1200, "fallback_url": "https://v.redd.it/y2xvvj9d4xxb1/DASH_480.mp4?source=fallback", "has_audio": true, "height": 854, "width": 480, "scrubber_media_url": "https://v.redd.it/y2xvvj9d4xxb1/DASH_96.mp4", "dash_url": "https://v.redd.it/y2xvvj9d4xxb1/DASHPlaylist.mpd?a=1701521492%2CMWM2OGUwMTVmZWFjNWIzOTdjMjM3NWE1NWFmNzc4NTZkYjI2ZTQ1OGM3MWZjZmI0MmYwZWFlODRkMmExZjBiNg%3D%3D&amp;v=1&amp;f=sd", "duration": 68, "hls_url": "https://v.redd.it/y2xvvj9d4xxb1/HLSPlaylist.m3u8?a=1701521492%2CMmY1MGFiODY1M2M4MDMwZTQyODBkMDUyM2JlOTllYzAzNjQ3NjNlOTQ3NjQyNzExYTJjN2I0ZTJkMzgwN2Q0Zg%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am planning a Christmas gift for someone who loves having physical media.  She likes the packaging and knowing that she owns the movie without having to pay any type of subscription or logging into an account.  \n\nWe have a PS5 that can play the Blu-ray discs, but I worry about them getting scratched up after continual use.  We have an LG TV that runs webOS which has a USB port so if I can rip the Blu-ray to USB in a supported format, we can play them that way.  She also likes to watch the movies on her iPad while away from home so I am hoping the rip can also be played on it.  \n\nI have a [full-size desktop computer](https://psref.lenovo.com/Product/Legion/Lenovo_Legion_T7_34IMZ5) (i9-11900K / 64GB DDR4 / RTX 3080 4.0 GB) but it doesn't have a Blu-ray player.  I don't see an internal bay to add one so I imagine my options are to either use a USB Blu-ray reader (probably extremely slow) or just temporarily open the case and have an internal SATA Blu-ray player connected while I rip them.  Are there any models people recommend that will do the job quickly?\n\nIt looks like [MakeMKV](https://www.makemkv.com/) is the gold standard for converting a physical Blu-ray to digital media.  Is the digital file compatible with iOS and webOS?  If not, what converters would you recommend? \n\nThe end goal is to have a USB drive with all of the movies digitally on it which can be plugged into the LG TV to play directly.  The files will also be copied to the iPad preferably through iTunes but I could also install the VLC app and copy them that way if iTunes doesn't support it.", "author_fullname": "t2_a4gfzgya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wanting to purchase ~20 Blu-ray musicals and rip them to a format playable on iOS and webOS. What do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lgn0a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698857740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning a Christmas gift for someone who loves having physical media.  She likes the packaging and knowing that she owns the movie without having to pay any type of subscription or logging into an account.  &lt;/p&gt;\n\n&lt;p&gt;We have a PS5 that can play the Blu-ray discs, but I worry about them getting scratched up after continual use.  We have an LG TV that runs webOS which has a USB port so if I can rip the Blu-ray to USB in a supported format, we can play them that way.  She also likes to watch the movies on her iPad while away from home so I am hoping the rip can also be played on it.  &lt;/p&gt;\n\n&lt;p&gt;I have a &lt;a href=\"https://psref.lenovo.com/Product/Legion/Lenovo_Legion_T7_34IMZ5\"&gt;full-size desktop computer&lt;/a&gt; (i9-11900K / 64GB DDR4 / RTX 3080 4.0 GB) but it doesn&amp;#39;t have a Blu-ray player.  I don&amp;#39;t see an internal bay to add one so I imagine my options are to either use a USB Blu-ray reader (probably extremely slow) or just temporarily open the case and have an internal SATA Blu-ray player connected while I rip them.  Are there any models people recommend that will do the job quickly?&lt;/p&gt;\n\n&lt;p&gt;It looks like &lt;a href=\"https://www.makemkv.com/\"&gt;MakeMKV&lt;/a&gt; is the gold standard for converting a physical Blu-ray to digital media.  Is the digital file compatible with iOS and webOS?  If not, what converters would you recommend? &lt;/p&gt;\n\n&lt;p&gt;The end goal is to have a USB drive with all of the movies digitally on it which can be plugged into the LG TV to play directly.  The files will also be copied to the iPad preferably through iTunes but I could also install the VLC app and copy them that way if iTunes doesn&amp;#39;t support it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lgn0a", "is_robot_indexable": true, "report_reasons": null, "author": "FatherLiamFinnegan", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lgn0a/wanting_to_purchase_20_bluray_musicals_and_rip/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lgn0a/wanting_to_purchase_20_bluray_musicals_and_rip/", "subreddit_subscribers": 709950, "created_utc": 1698857740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, first time posting on this subreddit. \n\nSo currently I have over 200,000 bookmarks on my Windows laptop split across these browsers - Chrome, Brave, Edge, Firefox, Opera, Vivaldi and Chrome Beta (Chrome Dev seems to be quite unstable).\n\nDue to privacy and security reasons, I DON'T want to use ANY online/cloud service, and I use all browsers on my laptop SIGNED OUT so there is NO online/cloud syncing, NO multiple profiles, and ALL the bookmarks are stored LOCALLY on the laptop's hard drive. I also keep exporting them from time-to-time in HTML formats as backups. \n\nI have NO duplicates or waste bookmarks, they are well organized into dozens of different folders and it has been my habit to collect various links, websites and URLs since nearly a decade now. \n\nGenuinely curious about whether or not this process can scale to potentially MILLIONS of bookmarks in the future, because its part of my habit to store bookmarks while browsing the web. ", "author_fullname": "t2_986czkvnk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can web browsers on computers store and manage millions of bookmarks offline and still be functional without lagging if there are NO online/cloud syncing at all?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lejtu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698852140.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, first time posting on this subreddit. &lt;/p&gt;\n\n&lt;p&gt;So currently I have over 200,000 bookmarks on my Windows laptop split across these browsers - Chrome, Brave, Edge, Firefox, Opera, Vivaldi and Chrome Beta (Chrome Dev seems to be quite unstable).&lt;/p&gt;\n\n&lt;p&gt;Due to privacy and security reasons, I DON&amp;#39;T want to use ANY online/cloud service, and I use all browsers on my laptop SIGNED OUT so there is NO online/cloud syncing, NO multiple profiles, and ALL the bookmarks are stored LOCALLY on the laptop&amp;#39;s hard drive. I also keep exporting them from time-to-time in HTML formats as backups. &lt;/p&gt;\n\n&lt;p&gt;I have NO duplicates or waste bookmarks, they are well organized into dozens of different folders and it has been my habit to collect various links, websites and URLs since nearly a decade now. &lt;/p&gt;\n\n&lt;p&gt;Genuinely curious about whether or not this process can scale to potentially MILLIONS of bookmarks in the future, because its part of my habit to store bookmarks while browsing the web. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lejtu", "is_robot_indexable": true, "report_reasons": null, "author": "ZyxWvuO", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lejtu/can_web_browsers_on_computers_store_and_manage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lejtu/can_web_browsers_on_computers_store_and_manage/", "subreddit_subscribers": 709950, "created_utc": 1698852140.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have a ATX gaming setup with roughly 9x 2.5\" SSD drives that I have come to realization that it is simply not practical to squeeze all the drives and high-end GPUs and CPUs into 1 system. Not to mention, even most common ATX boards will start to run into difficulties trying to support that many SATA ports without affecting PCIe bandwidth. After coming to this conclusion, I am leaning towards building a separate ITX or mATX system dedicated to just housing 12+ 2.5\" SSDs and having it be on 24/7 and it only being accessed offline via LAN. Since I manually backup my data on external 3.5\" drives every week, the storage solution will not need any RAID mirroring.\n\nSince this is my first time looking into a 24/7 server setup, I am unsure if there are certain things I need to consider for system reliability. Some questions I have are:\n\n1. Between Intel LGA1700 and AMD AM5, which would be a better platform? For example, I just learned not all LGA1700 chips support ECC memory while AM5 chips have higher idle power consumption that low-end LGA1700.\n2. Would a consumer-grade ITX or mATX motherboard be good enough for 24/7 operation? Beside ECC support, are there other factors I should consider?\n3. Also what HBAs should I be considering? I know it is probably crude, but I was considering using a x16 to x8/x8 or x4/x4/x4/x4 m.2 card to hook up with multiple JMB585 5-port SATA cards and use 3-4 of them on a motherboard to get up to 20 SATA devices. \n\nI was leaning towards Intel LGA1700 for the lower idle power consumption despite the limited x8/x8 PCie bifurcation options, but this latest discovery of ECC memory not being supported on 1x100 and 1x400 chips does make me change my mind.", "author_fullname": "t2_jjarg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best means of accessing 12+ 2.5\" SSD drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lv0vn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698897990.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698897352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have a ATX gaming setup with roughly 9x 2.5&amp;quot; SSD drives that I have come to realization that it is simply not practical to squeeze all the drives and high-end GPUs and CPUs into 1 system. Not to mention, even most common ATX boards will start to run into difficulties trying to support that many SATA ports without affecting PCIe bandwidth. After coming to this conclusion, I am leaning towards building a separate ITX or mATX system dedicated to just housing 12+ 2.5&amp;quot; SSDs and having it be on 24/7 and it only being accessed offline via LAN. Since I manually backup my data on external 3.5&amp;quot; drives every week, the storage solution will not need any RAID mirroring.&lt;/p&gt;\n\n&lt;p&gt;Since this is my first time looking into a 24/7 server setup, I am unsure if there are certain things I need to consider for system reliability. Some questions I have are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Between Intel LGA1700 and AMD AM5, which would be a better platform? For example, I just learned not all LGA1700 chips support ECC memory while AM5 chips have higher idle power consumption that low-end LGA1700.&lt;/li&gt;\n&lt;li&gt;Would a consumer-grade ITX or mATX motherboard be good enough for 24/7 operation? Beside ECC support, are there other factors I should consider?&lt;/li&gt;\n&lt;li&gt;Also what HBAs should I be considering? I know it is probably crude, but I was considering using a x16 to x8/x8 or x4/x4/x4/x4 m.2 card to hook up with multiple JMB585 5-port SATA cards and use 3-4 of them on a motherboard to get up to 20 SATA devices. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I was leaning towards Intel LGA1700 for the lower idle power consumption despite the limited x8/x8 PCie bifurcation options, but this latest discovery of ECC memory not being supported on 1x100 and 1x400 chips does make me change my mind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lv0vn", "is_robot_indexable": true, "report_reasons": null, "author": "imaginary_num6er", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lv0vn/best_means_of_accessing_12_25_ssd_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lv0vn/best_means_of_accessing_12_25_ssd_drives/", "subreddit_subscribers": 709950, "created_utc": 1698897352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to store a backup of recordings we've been making for the past three years. It's currently at less than 3 TB and these are 5 - 9 GB files each, as mp4s. It will continue to grow, as we generate 6 recordings a month. I don't need to access the backup frequently, as the files are also on my local machine, on archival M-discs, and on a separate HDD that I keep as a physical backup and sync two regularly. I also have Backblaze in the background. So when I go back to edit the recordings, I'll be using the local files rather than the ones in the cloud.\n\nHowever I really want someplace to drop a recording immediately after it's done that's off-site in the cloud, just in case of an immediate disaster.\n\nThey are currently on [Sync.com](https://sync.com/), which offers unlimited space for $30/mo, but the service's stopped providing support *of any kind* recently (despite advertising phone support for their higher tiers) so I'm worried they're about to go under or that something is up with their company.\n\nI had considered AWS Deep Archive, but their egress cost seems insane as the service is really meant for \"archives\" as far as I understand it, not as a backup I might very infrequently access. \n\nWhat other options are out there to consider?", "author_fullname": "t2_7r7o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use case for my long term video storage and potential cloud options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lh46i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698858960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to store a backup of recordings we&amp;#39;ve been making for the past three years. It&amp;#39;s currently at less than 3 TB and these are 5 - 9 GB files each, as mp4s. It will continue to grow, as we generate 6 recordings a month. I don&amp;#39;t need to access the backup frequently, as the files are also on my local machine, on archival M-discs, and on a separate HDD that I keep as a physical backup and sync two regularly. I also have Backblaze in the background. So when I go back to edit the recordings, I&amp;#39;ll be using the local files rather than the ones in the cloud.&lt;/p&gt;\n\n&lt;p&gt;However I really want someplace to drop a recording immediately after it&amp;#39;s done that&amp;#39;s off-site in the cloud, just in case of an immediate disaster.&lt;/p&gt;\n\n&lt;p&gt;They are currently on &lt;a href=\"https://sync.com/\"&gt;Sync.com&lt;/a&gt;, which offers unlimited space for $30/mo, but the service&amp;#39;s stopped providing support &lt;em&gt;of any kind&lt;/em&gt; recently (despite advertising phone support for their higher tiers) so I&amp;#39;m worried they&amp;#39;re about to go under or that something is up with their company.&lt;/p&gt;\n\n&lt;p&gt;I had considered AWS Deep Archive, but their egress cost seems insane as the service is really meant for &amp;quot;archives&amp;quot; as far as I understand it, not as a backup I might very infrequently access. &lt;/p&gt;\n\n&lt;p&gt;What other options are out there to consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lh46i", "is_robot_indexable": true, "report_reasons": null, "author": "mccoypauley", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lh46i/use_case_for_my_long_term_video_storage_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lh46i/use_case_for_my_long_term_video_storage_and/", "subreddit_subscribers": 709950, "created_utc": 1698858960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The drive in question is a Samsung 500 GB Samsung EVO 850. Samsung Magician says the drive is good. There have been 354 TB written. I believe the drive is at least 8 years old.\n\nLately HD Sentinel has been reporting consistent sectors being reallocated. For the month of October 80 sectors were reallocated for a total of 145 that have been reallocated.\n\n(Sorry about the line breaks creating the extra space. I am not sure how to get rid of that.)\n\nHere is a brief history from the log.\n\n11/1/2023 10:45:34 AM,#5   Reallocated Sectors Count  144 -&gt; 145\n\n11/1/2023 1:15:11 AM,#5   Reallocated Sectors Count  143 -&gt; 144\n\n10/31/2023 7:28:19 PM,#5   Reallocated Sectors Count  142 -&gt; 143\n\n10/31/2023 7:46:46 AM,#5   Reallocated Sectors Count  141 -&gt; 142\n\n10/31/2023 3:40:42 AM,#5   Reallocated Sectors Count  140 -&gt; 141\n\n10/31/2023 3:30:40 AM,#5   Reallocated Sectors Count  139 -&gt; 140\n\n10/31/2023 1:45:16 AM,#5   Reallocated Sectors Count  138 -&gt; 139\n\n10/31/2023 1:40:16 AM,#5   Reallocated Sectors Count  137 -&gt; 138\n\n10/30/2023 8:13:11 PM,#5   Reallocated Sectors Count  136 -&gt; 137\n\n10/30/2023 10:31:28 AM,#5   Reallocated Sectors Count  135 -&gt; 136\n\n10/29/2023 11:32:55 PM,#5   Reallocated Sectors Count  134 -&gt; 135\n\n10/29/2023 3:01:36 PM,#5   Reallocated Sectors Count  133 -&gt; 134\n\n10/29/2023 11:20:46 AM,#5   Reallocated Sectors Count  132 -&gt; 133\n\n10/28/2023 12:42:09 PM,#5   Reallocated Sectors Count  130 -&gt; 132\n\n10/28/2023 8:31:29 AM,#5   Reallocated Sectors Count  129 -&gt; 130\n\n10/28/2023 8:21:27 AM,#5   Reallocated Sectors Count  128 -&gt; 129\n\n10/27/2023 5:22:07 PM,#5   Reallocated Sectors Count  127 -&gt; 128\n\n10/27/2023 2:26:43 PM,#5   Reallocated Sectors Count  126 -&gt; 127\n\n10/26/2023 10:34:41 PM,#5   Reallocated Sectors Count  125 -&gt; 126\n\n10/26/2023 7:39:15 PM,#5   Reallocated Sectors Count  124 -&gt; 125\n\n10/26/2023 5:48:58 PM,#5   Reallocated Sectors Count  123 -&gt; 124\n\n10/26/2023 12:48:08 PM,#5   Reallocated Sectors Count  122 -&gt; 123\n\n10/26/2023 9:23:56 AM,#5   Reallocated Sectors Count  121 -&gt; 122\n\n10/26/2023 9:18:53 AM,#5   Reallocated Sectors Count  120 -&gt; 121", "author_fullname": "t2_hxgex", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When Do Reallocated Sectors Become a Problem on an SSD?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17le5f1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698851046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The drive in question is a Samsung 500 GB Samsung EVO 850. Samsung Magician says the drive is good. There have been 354 TB written. I believe the drive is at least 8 years old.&lt;/p&gt;\n\n&lt;p&gt;Lately HD Sentinel has been reporting consistent sectors being reallocated. For the month of October 80 sectors were reallocated for a total of 145 that have been reallocated.&lt;/p&gt;\n\n&lt;p&gt;(Sorry about the line breaks creating the extra space. I am not sure how to get rid of that.)&lt;/p&gt;\n\n&lt;p&gt;Here is a brief history from the log.&lt;/p&gt;\n\n&lt;p&gt;11/1/2023 10:45:34 AM,#5   Reallocated Sectors Count  144 -&amp;gt; 145&lt;/p&gt;\n\n&lt;p&gt;11/1/2023 1:15:11 AM,#5   Reallocated Sectors Count  143 -&amp;gt; 144&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 7:28:19 PM,#5   Reallocated Sectors Count  142 -&amp;gt; 143&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 7:46:46 AM,#5   Reallocated Sectors Count  141 -&amp;gt; 142&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 3:40:42 AM,#5   Reallocated Sectors Count  140 -&amp;gt; 141&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 3:30:40 AM,#5   Reallocated Sectors Count  139 -&amp;gt; 140&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 1:45:16 AM,#5   Reallocated Sectors Count  138 -&amp;gt; 139&lt;/p&gt;\n\n&lt;p&gt;10/31/2023 1:40:16 AM,#5   Reallocated Sectors Count  137 -&amp;gt; 138&lt;/p&gt;\n\n&lt;p&gt;10/30/2023 8:13:11 PM,#5   Reallocated Sectors Count  136 -&amp;gt; 137&lt;/p&gt;\n\n&lt;p&gt;10/30/2023 10:31:28 AM,#5   Reallocated Sectors Count  135 -&amp;gt; 136&lt;/p&gt;\n\n&lt;p&gt;10/29/2023 11:32:55 PM,#5   Reallocated Sectors Count  134 -&amp;gt; 135&lt;/p&gt;\n\n&lt;p&gt;10/29/2023 3:01:36 PM,#5   Reallocated Sectors Count  133 -&amp;gt; 134&lt;/p&gt;\n\n&lt;p&gt;10/29/2023 11:20:46 AM,#5   Reallocated Sectors Count  132 -&amp;gt; 133&lt;/p&gt;\n\n&lt;p&gt;10/28/2023 12:42:09 PM,#5   Reallocated Sectors Count  130 -&amp;gt; 132&lt;/p&gt;\n\n&lt;p&gt;10/28/2023 8:31:29 AM,#5   Reallocated Sectors Count  129 -&amp;gt; 130&lt;/p&gt;\n\n&lt;p&gt;10/28/2023 8:21:27 AM,#5   Reallocated Sectors Count  128 -&amp;gt; 129&lt;/p&gt;\n\n&lt;p&gt;10/27/2023 5:22:07 PM,#5   Reallocated Sectors Count  127 -&amp;gt; 128&lt;/p&gt;\n\n&lt;p&gt;10/27/2023 2:26:43 PM,#5   Reallocated Sectors Count  126 -&amp;gt; 127&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 10:34:41 PM,#5   Reallocated Sectors Count  125 -&amp;gt; 126&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 7:39:15 PM,#5   Reallocated Sectors Count  124 -&amp;gt; 125&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 5:48:58 PM,#5   Reallocated Sectors Count  123 -&amp;gt; 124&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 12:48:08 PM,#5   Reallocated Sectors Count  122 -&amp;gt; 123&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 9:23:56 AM,#5   Reallocated Sectors Count  121 -&amp;gt; 122&lt;/p&gt;\n\n&lt;p&gt;10/26/2023 9:18:53 AM,#5   Reallocated Sectors Count  120 -&amp;gt; 121&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17le5f1", "is_robot_indexable": true, "report_reasons": null, "author": "waynomo", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17le5f1/when_do_reallocated_sectors_become_a_problem_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17le5f1/when_do_reallocated_sectors_become_a_problem_on/", "subreddit_subscribers": 709950, "created_utc": 1698851046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_rcb9rik6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storing data for thousands of years | Microsoft Project Silica", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17lzywn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1M_n3N_FzHS-7rRF0hUWIRTXTshijtAeV3xiXxpAQtY.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698919091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "m.youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://m.youtube.com/watch?v=-rfEYd4NGQg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?auto=webp&amp;s=168f4e4186b95a842fd888e6bc77a35bfa645789", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c46fdf3c69a23003d3d3203bd839db8a6f2c3490", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6176347747a6a03683d319c0374e89bf04ba445", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1587f2a3f4f07a86cecffff956f0830f7385e56", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea5e5928c66fdec8e98231e84e871aabac914e48", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c71cb2f1a6f4a52498a68152b7411a77b0fa1c5a", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1e0b2af0444b78af8513ab098be3be21a9c72a92", "width": 1080, "height": 607}], "variants": {}, "id": "fGf6q9pJHBEJ0KNmqU9nNiTn79MFHO8GMcShGXtDdPE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lzywn", "is_robot_indexable": true, "report_reasons": null, "author": "albanianspy", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17lzywn/storing_data_for_thousands_of_years_microsoft/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://m.youtube.com/watch?v=-rfEYd4NGQg", "subreddit_subscribers": 709950, "created_utc": 1698919091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I\u2019m having an issue with an external HDD I use as a backup drive. It\u2019s starting to fail and I am wondering if I should use an SSD for a backup instead (I have a spare SATA one from Micron).\n\nI have seen people mention that SSDs are not good for long-term storage when they are left powered off. If I power on the SSD once a month when I do my backups will I be fine or should I just use an HDD?", "author_fullname": "t2_13bt8084", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSD for External Backup Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lm1ya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698872718.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698872219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I\u2019m having an issue with an external HDD I use as a backup drive. It\u2019s starting to fail and I am wondering if I should use an SSD for a backup instead (I have a spare SATA one from Micron).&lt;/p&gt;\n\n&lt;p&gt;I have seen people mention that SSDs are not good for long-term storage when they are left powered off. If I power on the SSD once a month when I do my backups will I be fine or should I just use an HDD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lm1ya", "is_robot_indexable": true, "report_reasons": null, "author": "xxBLVCKMVGICxx", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lm1ya/ssd_for_external_backup_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lm1ya/ssd_for_external_backup_drive/", "subreddit_subscribers": 709950, "created_utc": 1698872219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I wanted to do it with an excel file and just do it manually. Every session is recorded with OBS, uploaded to YT, and then redownloaded so it's compressed. You may disagree with that method, but I just care about the moments and such, so the quality is irrelevant as long as it's 720+ and you can tell what's going on, and Youtube does the compression for me so I don't gotta be running handbrake 24/7.\n\nI have over a thousand sessions since 2017, probably way more than that, just know that it's a lot. I mostly play League and mostly with the same people, and it's almost all organized with dates and time. Some sessions (moreso edited stuff) don't have dates. Some stuff is not gaming related.\n\nNow the reason I'm making this post is that I'm wondering what data you'd include. You know how excel has those columns, well I thought the first would be some sort of hex id just in case I needed it for something in the future, the second would be the date, and third would be title. I'd like to include file path and YT url/id. Maybe YT upload date as well. File size and upload date of course.\n\nNow the more annoying stuff that I'd like to include: People I played with, what games I played, how long I played each game, the scorelines or lengths of said games. Timestamps to funny or notable moments. Now I'm not sure how I'd include that if every session only has a single row. Maybe a second sheet? Idk.\n\nI'm not asking you for nitty gritty details, but just suggestions and such if you have any.", "author_fullname": "t2_bede0mtsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I wanna create a database for all my gaming sessions. Asking for some ideas, as it's a long term project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lld37", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698870311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I wanted to do it with an excel file and just do it manually. Every session is recorded with OBS, uploaded to YT, and then redownloaded so it&amp;#39;s compressed. You may disagree with that method, but I just care about the moments and such, so the quality is irrelevant as long as it&amp;#39;s 720+ and you can tell what&amp;#39;s going on, and Youtube does the compression for me so I don&amp;#39;t gotta be running handbrake 24/7.&lt;/p&gt;\n\n&lt;p&gt;I have over a thousand sessions since 2017, probably way more than that, just know that it&amp;#39;s a lot. I mostly play League and mostly with the same people, and it&amp;#39;s almost all organized with dates and time. Some sessions (moreso edited stuff) don&amp;#39;t have dates. Some stuff is not gaming related.&lt;/p&gt;\n\n&lt;p&gt;Now the reason I&amp;#39;m making this post is that I&amp;#39;m wondering what data you&amp;#39;d include. You know how excel has those columns, well I thought the first would be some sort of hex id just in case I needed it for something in the future, the second would be the date, and third would be title. I&amp;#39;d like to include file path and YT url/id. Maybe YT upload date as well. File size and upload date of course.&lt;/p&gt;\n\n&lt;p&gt;Now the more annoying stuff that I&amp;#39;d like to include: People I played with, what games I played, how long I played each game, the scorelines or lengths of said games. Timestamps to funny or notable moments. Now I&amp;#39;m not sure how I&amp;#39;d include that if every session only has a single row. Maybe a second sheet? Idk.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not asking you for nitty gritty details, but just suggestions and such if you have any.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lld37", "is_robot_indexable": true, "report_reasons": null, "author": "spanspan3213", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lld37/i_wanna_create_a_database_for_all_my_gaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lld37/i_wanna_create_a_database_for_all_my_gaming/", "subreddit_subscribers": 709950, "created_utc": 1698870311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I want to completely mirror a website, but only get a specific level of length when accessing external websites. How can I do this in wget? The help section isn't so clear about this", "author_fullname": "t2_ld87l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Only getting a specific depth of an external link with wget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lk835", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698867232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I want to completely mirror a website, but only get a specific level of length when accessing external websites. How can I do this in wget? The help section isn&amp;#39;t so clear about this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "37 TB local + 2.1 TB cloud", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lk835", "is_robot_indexable": true, "report_reasons": null, "author": "Sai22", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17lk835/only_getting_a_specific_depth_of_an_external_link/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lk835/only_getting_a_specific_depth_of_an_external_link/", "subreddit_subscribers": 709950, "created_utc": 1698867232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "title, need to download multiple  youtube channels and have them download any new videos uploaded", "author_fullname": "t2_4av7osji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how can i download a whole youtube channel and have it automatically download new videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ljq1t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698865895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title, need to download multiple  youtube channels and have them download any new videos uploaded&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ljq1t", "is_robot_indexable": true, "report_reasons": null, "author": "alex_square6", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ljq1t/how_can_i_download_a_whole_youtube_channel_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ljq1t/how_can_i_download_a_whole_youtube_channel_and/", "subreddit_subscribers": 709950, "created_utc": 1698865895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone, \n\nMy parents have a bunch of data just sitting on external drives, and that data is not copied anywhere. They have lost family photos to a failing drive before, and I want to prevent something like that from happening again.\n\nIn this vein, **I want to get them a complete NAS solution** for a Christmas present. I work in technology, although not in IT, so I am competent at understanding things of this nature but do not have the knowledge myself currently.\n\nIdeally, **this is my target system**:\n- an easy-to-use NAS that they will find intuitive (and cannot easily break)\n- a automated backup solution (ideally one on-site and one in the cloud, I use Backblaze B2 personally)\n- perhaps a UPS to really mitigate the possibility of error (do you all think this is necessary?)\n\n**I will provide**\n- a budget to get this done (I am comfortable spending between 1-2k USD)\n- the initial setup for them\n- support in the future should something go wrong\n\n**Any advice is greatly appreciated!** I'm not currently aware of how much storage will actually be required, but I think 5 or so TB to start would be sufficient.\n\nI've been looking at synology NAS options and am really just looking for advice on whether I've made some sort of mistake in reasoning, companies/products to avoid (or use), and any other advice you all would think is valuable :) Thanks again!", "author_fullname": "t2_kqraiwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Complete NAS solution for parents for Christmas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ls9w0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698888964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt;\n\n&lt;p&gt;My parents have a bunch of data just sitting on external drives, and that data is not copied anywhere. They have lost family photos to a failing drive before, and I want to prevent something like that from happening again.&lt;/p&gt;\n\n&lt;p&gt;In this vein, &lt;strong&gt;I want to get them a complete NAS solution&lt;/strong&gt; for a Christmas present. I work in technology, although not in IT, so I am competent at understanding things of this nature but do not have the knowledge myself currently.&lt;/p&gt;\n\n&lt;p&gt;Ideally, &lt;strong&gt;this is my target system&lt;/strong&gt;:\n- an easy-to-use NAS that they will find intuitive (and cannot easily break)\n- a automated backup solution (ideally one on-site and one in the cloud, I use Backblaze B2 personally)\n- perhaps a UPS to really mitigate the possibility of error (do you all think this is necessary?)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I will provide&lt;/strong&gt;\n- a budget to get this done (I am comfortable spending between 1-2k USD)\n- the initial setup for them\n- support in the future should something go wrong&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Any advice is greatly appreciated!&lt;/strong&gt; I&amp;#39;m not currently aware of how much storage will actually be required, but I think 5 or so TB to start would be sufficient.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking at synology NAS options and am really just looking for advice on whether I&amp;#39;ve made some sort of mistake in reasoning, companies/products to avoid (or use), and any other advice you all would think is valuable :) Thanks again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ls9w0", "is_robot_indexable": true, "report_reasons": null, "author": "FiziksMayMays", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ls9w0/complete_nas_solution_for_parents_for_christmas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ls9w0/complete_nas_solution_for_parents_for_christmas/", "subreddit_subscribers": 709950, "created_utc": 1698888964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I say \"large\" because it's probably crumbs to folks on here but I have roughly 2-3TB of movies and tv shows for my Plex server. Problem is, I have copies of mainly movies scattered between 3-4 computers and I'm finally setting up a dedicated server to consolidate everything. Once I have everything on a single computer, I want to be able to find duplicate movies and shows and keep the higher quality ones. I guess the simplest way would be to toss everything into the same folder and when Windows notifies me of a duplicate, I would just keep which ever file size is bigger which would assume higher quality. I'm hoping to find an application that lets me export video metadata so I can see sizes, bitrate, and resolution so I can 1. delete low quality version then 2. see which of my favorite movies/shows are of low quality so I can eventually get higher quality replacements.  \n  \nI came across MediaInfo but can't tell if it only lets you view video data individually or if it can do a mass export to a csv or something of the like.", "author_fullname": "t2_ceda1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exporting video metadata of \"large\" collection of movies and tv shows.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lbwxz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698844631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I say &amp;quot;large&amp;quot; because it&amp;#39;s probably crumbs to folks on here but I have roughly 2-3TB of movies and tv shows for my Plex server. Problem is, I have copies of mainly movies scattered between 3-4 computers and I&amp;#39;m finally setting up a dedicated server to consolidate everything. Once I have everything on a single computer, I want to be able to find duplicate movies and shows and keep the higher quality ones. I guess the simplest way would be to toss everything into the same folder and when Windows notifies me of a duplicate, I would just keep which ever file size is bigger which would assume higher quality. I&amp;#39;m hoping to find an application that lets me export video metadata so I can see sizes, bitrate, and resolution so I can 1. delete low quality version then 2. see which of my favorite movies/shows are of low quality so I can eventually get higher quality replacements.  &lt;/p&gt;\n\n&lt;p&gt;I came across MediaInfo but can&amp;#39;t tell if it only lets you view video data individually or if it can do a mass export to a csv or something of the like.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lbwxz", "is_robot_indexable": true, "report_reasons": null, "author": "inthemix8080", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lbwxz/exporting_video_metadata_of_large_collection_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lbwxz/exporting_video_metadata_of_large_collection_of/", "subreddit_subscribers": 709950, "created_utc": 1698844631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just installed a ssd on my pc and installed windows on the it. my old hdd still have windows on it and I'm planning to completely remove the windows. my hdd has 2 partitions. C for windows and D for personal data. After formatting the C drive(system drive on hdd) i want to merge it with other D drive. I want to know if I can merge them as one single drive and if yes then how. thank you.\nedit: I also would like to do this without any data loss so let me know if the merger will lead to data loss on the personal drive (of old hdd)", "author_fullname": "t2_17eahs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about disk partition.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lk470", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698867413.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698866955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just installed a ssd on my pc and installed windows on the it. my old hdd still have windows on it and I&amp;#39;m planning to completely remove the windows. my hdd has 2 partitions. C for windows and D for personal data. After formatting the C drive(system drive on hdd) i want to merge it with other D drive. I want to know if I can merge them as one single drive and if yes then how. thank you.\nedit: I also would like to do this without any data loss so let me know if the merger will lead to data loss on the personal drive (of old hdd)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lk470", "is_robot_indexable": true, "report_reasons": null, "author": "thetushar7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lk470/question_about_disk_partition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lk470/question_about_disk_partition/", "subreddit_subscribers": 709950, "created_utc": 1698866955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! I know I can write a script for this, but I was wondering if there is some sort of solution that is already made for this.\n\nThanks!", "author_fullname": "t2_uy2zp43z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a tool to automatically archive a folder, transfer via scp/rsync and decompress on the other side automatically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lfukx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698855651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I know I can write a script for this, but I was wondering if there is some sort of solution that is already made for this.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lfukx", "is_robot_indexable": true, "report_reasons": null, "author": "angel__-__-", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lfukx/is_there_a_tool_to_automatically_archive_a_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lfukx/is_there_a_tool_to_automatically_archive_a_folder/", "subreddit_subscribers": 709950, "created_utc": 1698855651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Where can i learn facts about HDD head movement?\n\nMost information I get is clueless consumers scaring each other and telling stories of failing drives (which could very well be the case i'm in)\n\nWe just moved offices and our main NAS (RAID1 of WD 10TB gold drives) started to display constant head movement that i can hear easily. Almost like clicking of death, but more \"contained\".  not so loud and just under 1s interval.\n\nWe do have daily/biweekly backup besides the RAID1. But since we were semi-offline for the office move we have some data there that is not on the (paused) daily snapshots. Which i don't want to tell users they lost. This sound very amateur, because it is :) non-profit and all.\n\nThe host is linux 6.5. I'm using only two disks in read-only mode for now.\n\nEach disk is 10TB but only have 2TB partitions for the linux soft raid (mdadm). In the raid there's a LUKS volume with encryption and inside that a single ext4 partition.\n\neverything shows up perfect. SMART pass with flying colors on short tests. RAID shows healthy status. kernel complains of nothing. But the head movement is driving me crazy scared!\n\nThe drives are mostly idle, but vmstat does show some IO happening. Which i suspect is journal being checked? Is there anyway i can tell the kernel to pause all journal activity to make sure the noise goes away? Or maybe it is a firmware thing?\n\nThe drives are not too old. They are exact same model but different lots. always lived in that raid1. And they were transported in different methods for the move. They are both connected to the same PSU (can this be bad power? would it not show up anywhere?). Data reading bandwidth seems the same as before the move (i.e. during the time without the weird head movement sound).\n\nWhat i'm trying to understand now is if I can take my time getting data out of the drivers which are offline, threat it as a imminent failure case, or don't even bother buying new drives for now and use them as regular till they die as they all do and just keep dropping in new spares in the raid. it's only the fact that all of them started the sound at the same time that leads me to believe it might be something in software/firmware/journal thanks to the time offline instead of hardware failure.", "author_fullname": "t2_vjutt7ni", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HDD head movement information", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ldhba", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698849188.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where can i learn facts about HDD head movement?&lt;/p&gt;\n\n&lt;p&gt;Most information I get is clueless consumers scaring each other and telling stories of failing drives (which could very well be the case i&amp;#39;m in)&lt;/p&gt;\n\n&lt;p&gt;We just moved offices and our main NAS (RAID1 of WD 10TB gold drives) started to display constant head movement that i can hear easily. Almost like clicking of death, but more &amp;quot;contained&amp;quot;.  not so loud and just under 1s interval.&lt;/p&gt;\n\n&lt;p&gt;We do have daily/biweekly backup besides the RAID1. But since we were semi-offline for the office move we have some data there that is not on the (paused) daily snapshots. Which i don&amp;#39;t want to tell users they lost. This sound very amateur, because it is :) non-profit and all.&lt;/p&gt;\n\n&lt;p&gt;The host is linux 6.5. I&amp;#39;m using only two disks in read-only mode for now.&lt;/p&gt;\n\n&lt;p&gt;Each disk is 10TB but only have 2TB partitions for the linux soft raid (mdadm). In the raid there&amp;#39;s a LUKS volume with encryption and inside that a single ext4 partition.&lt;/p&gt;\n\n&lt;p&gt;everything shows up perfect. SMART pass with flying colors on short tests. RAID shows healthy status. kernel complains of nothing. But the head movement is driving me crazy scared!&lt;/p&gt;\n\n&lt;p&gt;The drives are mostly idle, but vmstat does show some IO happening. Which i suspect is journal being checked? Is there anyway i can tell the kernel to pause all journal activity to make sure the noise goes away? Or maybe it is a firmware thing?&lt;/p&gt;\n\n&lt;p&gt;The drives are not too old. They are exact same model but different lots. always lived in that raid1. And they were transported in different methods for the move. They are both connected to the same PSU (can this be bad power? would it not show up anywhere?). Data reading bandwidth seems the same as before the move (i.e. during the time without the weird head movement sound).&lt;/p&gt;\n\n&lt;p&gt;What i&amp;#39;m trying to understand now is if I can take my time getting data out of the drivers which are offline, threat it as a imminent failure case, or don&amp;#39;t even bother buying new drives for now and use them as regular till they die as they all do and just keep dropping in new spares in the raid. it&amp;#39;s only the fact that all of them started the sound at the same time that leads me to believe it might be something in software/firmware/journal thanks to the time offline instead of hardware failure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ldhba", "is_robot_indexable": true, "report_reasons": null, "author": "DecentTone876", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ldhba/hdd_head_movement_information/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ldhba/hdd_head_movement_information/", "subreddit_subscribers": 709950, "created_utc": 1698849188.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My HDDs which are attached to motherboard SATA show up when running Snapraid Smart commands without issue.\n\nHowever the two HDDs I have attached to my Dell Perc H310 HBA in IT mode don't report Smart results from Snapraid. \n\nSmartctl only shows these two drives as SCSI devices when I do a \"smartctl --scan\" command.\n\nBut these drives that are attached via HBA don't appear when running \"Snapraid smart\" \n\nDoes anyone know how to get Snapraid to see the drives when running Snapraid smart command?\n\nI'll be adding many more drives to my HBA soon, and would like to figure out this issue.\n\n&amp;#x200B;", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snapraid \"Smart\" cannot see drives attached via HBA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17m14d3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698923556.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My HDDs which are attached to motherboard SATA show up when running Snapraid Smart commands without issue.&lt;/p&gt;\n\n&lt;p&gt;However the two HDDs I have attached to my Dell Perc H310 HBA in IT mode don&amp;#39;t report Smart results from Snapraid. &lt;/p&gt;\n\n&lt;p&gt;Smartctl only shows these two drives as SCSI devices when I do a &amp;quot;smartctl --scan&amp;quot; command.&lt;/p&gt;\n\n&lt;p&gt;But these drives that are attached via HBA don&amp;#39;t appear when running &amp;quot;Snapraid smart&amp;quot; &lt;/p&gt;\n\n&lt;p&gt;Does anyone know how to get Snapraid to see the drives when running Snapraid smart command?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll be adding many more drives to my HBA soon, and would like to figure out this issue.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "74TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m14d3", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17m14d3/snapraid_smart_cannot_see_drives_attached_via_hba/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17m14d3/snapraid_smart_cannot_see_drives_attached_via_hba/", "subreddit_subscribers": 709950, "created_utc": 1698923556.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've recently came into possession of a few HGST HUH721010AL4200 drives (10TB SAS 4kn) and some LSI/Broadcom/Avago 9211-8i and 9305-16i HBAs.\n\nI've been setting them up in packs of 12 drives in a ZFS RAIDZ2 on Debian 12 with a 9305-16i HBA. Driver is mpt3sas (v43.100.00.00 according to dmesg).\n\nInitial checks of the drives turned out just fine, around 45k to 50k hours of power-on time, no bad sectors reported. I've moved some files (around 20TB) on them and then a drive was reported as faulted by ZFS due to read errors. Not to worry, I have loads of spares from the bunch, but that spare almost immediately faulted due to read errors when the resilver was running. Bad luck I thought, but then a third disk also had the same issue. Yet, SMART didn't record any bad sectors or other defects. The issue reported in dmesg was a SCSI command time-out (longer than 30 seconds, which is the default). Raising the time-out to 60 seconds made the issue go away, but made ZFS slow as hell.\n\nNow I started suspecting the HBA, replaced it by the same model, newest firmware. Same issues. While testing other drives faulted as well. Each time I would recreate the RAIDZ2 pool from scratch, fill it with garbage data and start scrubbing while writing to create additional stress.\n\nBad cables maybe? For the 9305-16i I had to buy new cables, SFF8643 to SFF8087. It would be really bad luck to have bought 4 faulty cables, so I switched the HBAs to two 9211-8i and put back the SFF8087 cables which worked for years and years. Same issues, same drives, again.\n\nCould the backplanes be faulty? These also worked for years and years without any issues. Nonetheless, I plugged those drives directly to SFF8643-&gt;4xSATA and SFF8087-&gt;4xSATA cables, same issues.\n\nNow I've also swapped the mainboard to a Supermicro X10SDV-F just to rule that out: same issues. Also I updated the drives' firmware to the most recent one to no avail.\n\nAnother box with a 12 drive RAIDZ2 pool I have built started showing the same symptoms, but this is another different mainboard, case, backplanes and PSU. Only similarities are the OS, drive model and the HBAs, thus the same driver.\n\nI dropped Debian on the larger box and installed TrueNAS Core, it's FreeBSD with a different driver for those HBAs. Lo and behold, it ran the stress tests for days without so much as a hickup. So it's the driver? I reinstalled Debian and ZFS and updated the driver to the newest one available from Broadcom (47.00.00.00). Everything worked just fine from there.\n\n&amp;#x200B;\n\nHas anyone encountered this (recently)? I searched everywhere for similar cases and found nothing fitting my situation. I would think my combination of hardware is not that special to cause such an edge case of driver issues that goes unnoticed by others, especially when the 9211-8i HBA is one of the most popular models out there.\n\nAll in all, I would've prevented all this headache and work by just swapping the driver, but I went down the hardware road.\n\n&amp;#x200B;\n\nLarge Storage:\n\n* Intel Xeon E3-1240Lv3\n* Supermicro X10SLL-F\n* 16GB DDR3 ECC RAM\n* 550W PSU\n* 1x Areca ARC1280ML RAID Controller\n* Norco 4224 Case\n* 2x LSI 9211-8i / 1x LSI 9305-16i\n* 12x HGST HUH721010AL4200 on LSI and ZFS RAIDZ2\n* 12x WD Red 6TB (Pre SMR era) on Areca as RAID6, to be replaced by 12x HGST HUH721212AL4200\n\n&amp;#x200B;\n\nSmall Storage:\n\n* Intel Xeon E3-1230Lv2\n* Supermicro X9SCM\n* 16GB DDR3 ECC RAM\n* 500W PSU\n* some Fantec 12-bay case\n* 2x LSI 9211-8i\n* 12x HGST HUH721010AL4200 on LSI and ZFS RAIDZ2\n\n&amp;#x200B;", "author_fullname": "t2_tnsdm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Avago/LSI/Broadcom mpt3sas driver issues on Debian 12", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17m0afd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698922724.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698920400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently came into possession of a few HGST HUH721010AL4200 drives (10TB SAS 4kn) and some LSI/Broadcom/Avago 9211-8i and 9305-16i HBAs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been setting them up in packs of 12 drives in a ZFS RAIDZ2 on Debian 12 with a 9305-16i HBA. Driver is mpt3sas (v43.100.00.00 according to dmesg).&lt;/p&gt;\n\n&lt;p&gt;Initial checks of the drives turned out just fine, around 45k to 50k hours of power-on time, no bad sectors reported. I&amp;#39;ve moved some files (around 20TB) on them and then a drive was reported as faulted by ZFS due to read errors. Not to worry, I have loads of spares from the bunch, but that spare almost immediately faulted due to read errors when the resilver was running. Bad luck I thought, but then a third disk also had the same issue. Yet, SMART didn&amp;#39;t record any bad sectors or other defects. The issue reported in dmesg was a SCSI command time-out (longer than 30 seconds, which is the default). Raising the time-out to 60 seconds made the issue go away, but made ZFS slow as hell.&lt;/p&gt;\n\n&lt;p&gt;Now I started suspecting the HBA, replaced it by the same model, newest firmware. Same issues. While testing other drives faulted as well. Each time I would recreate the RAIDZ2 pool from scratch, fill it with garbage data and start scrubbing while writing to create additional stress.&lt;/p&gt;\n\n&lt;p&gt;Bad cables maybe? For the 9305-16i I had to buy new cables, SFF8643 to SFF8087. It would be really bad luck to have bought 4 faulty cables, so I switched the HBAs to two 9211-8i and put back the SFF8087 cables which worked for years and years. Same issues, same drives, again.&lt;/p&gt;\n\n&lt;p&gt;Could the backplanes be faulty? These also worked for years and years without any issues. Nonetheless, I plugged those drives directly to SFF8643-&amp;gt;4xSATA and SFF8087-&amp;gt;4xSATA cables, same issues.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;ve also swapped the mainboard to a Supermicro X10SDV-F just to rule that out: same issues. Also I updated the drives&amp;#39; firmware to the most recent one to no avail.&lt;/p&gt;\n\n&lt;p&gt;Another box with a 12 drive RAIDZ2 pool I have built started showing the same symptoms, but this is another different mainboard, case, backplanes and PSU. Only similarities are the OS, drive model and the HBAs, thus the same driver.&lt;/p&gt;\n\n&lt;p&gt;I dropped Debian on the larger box and installed TrueNAS Core, it&amp;#39;s FreeBSD with a different driver for those HBAs. Lo and behold, it ran the stress tests for days without so much as a hickup. So it&amp;#39;s the driver? I reinstalled Debian and ZFS and updated the driver to the newest one available from Broadcom (47.00.00.00). Everything worked just fine from there.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone encountered this (recently)? I searched everywhere for similar cases and found nothing fitting my situation. I would think my combination of hardware is not that special to cause such an edge case of driver issues that goes unnoticed by others, especially when the 9211-8i HBA is one of the most popular models out there.&lt;/p&gt;\n\n&lt;p&gt;All in all, I would&amp;#39;ve prevented all this headache and work by just swapping the driver, but I went down the hardware road.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Large Storage:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intel Xeon E3-1240Lv3&lt;/li&gt;\n&lt;li&gt;Supermicro X10SLL-F&lt;/li&gt;\n&lt;li&gt;16GB DDR3 ECC RAM&lt;/li&gt;\n&lt;li&gt;550W PSU&lt;/li&gt;\n&lt;li&gt;1x Areca ARC1280ML RAID Controller&lt;/li&gt;\n&lt;li&gt;Norco 4224 Case&lt;/li&gt;\n&lt;li&gt;2x LSI 9211-8i / 1x LSI 9305-16i&lt;/li&gt;\n&lt;li&gt;12x HGST HUH721010AL4200 on LSI and ZFS RAIDZ2&lt;/li&gt;\n&lt;li&gt;12x WD Red 6TB (Pre SMR era) on Areca as RAID6, to be replaced by 12x HGST HUH721212AL4200&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Small Storage:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intel Xeon E3-1230Lv2&lt;/li&gt;\n&lt;li&gt;Supermicro X9SCM&lt;/li&gt;\n&lt;li&gt;16GB DDR3 ECC RAM&lt;/li&gt;\n&lt;li&gt;500W PSU&lt;/li&gt;\n&lt;li&gt;some Fantec 12-bay case&lt;/li&gt;\n&lt;li&gt;2x LSI 9211-8i&lt;/li&gt;\n&lt;li&gt;12x HGST HUH721010AL4200 on LSI and ZFS RAIDZ2&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "140TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m0afd", "is_robot_indexable": true, "report_reasons": null, "author": "_mrplow", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17m0afd/avagolsibroadcom_mpt3sas_driver_issues_on_debian/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17m0afd/avagolsibroadcom_mpt3sas_driver_issues_on_debian/", "subreddit_subscribers": 709950, "created_utc": 1698920400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On Linux, I  use RAID  mirror (mdadm - software reaid). How to configure it so that data is  read from both underlying devices and compared, to detect data bitrot  and fail in such case?\n\nIs that at all supported?\n\nWould be good option to have.\n\n(Of course as one of layers of protection, for important data)", "author_fullname": "t2_12iebe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux software RAID:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lzg31", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": "", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698916659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On Linux, I  use RAID  mirror (mdadm - software reaid). How to configure it so that data is  read from both underlying devices and compared, to detect data bitrot  and fail in such case?&lt;/p&gt;\n\n&lt;p&gt;Is that at all supported?&lt;/p&gt;\n\n&lt;p&gt;Would be good option to have.&lt;/p&gt;\n\n&lt;p&gt;(Of course as one of layers of protection, for important data)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lzg31", "is_robot_indexable": true, "report_reasons": null, "author": "metalzip", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17lzg31/linux_software_raid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lzg31/linux_software_raid/", "subreddit_subscribers": 709950, "created_utc": 1698916659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there,\n\nI bought two WD RED 14 TB drives (7200 RPM) recently for mass storage, and added all of my data to both of them which are now synced via 'rsync'.\n\nMy issue is this:\n\n\\- Every 5 secs, I can hear like a 'data read' type noise --&gt; Not necessarily as bad as a hard drive failure click, but something noticeable. Its kind of slight loud, and hard to ignore. It doesn't sound awful, but I can't like 'not hear it' which is getting annoying.\n\nI notice them in tandem since they are the same drive aka as soon as 5 secs go by --&gt; one makes the read noise, and the other makes a read noise shortly there after.\n\nI am using Linux so I observed this issue on the same machine via Ubuntu Server, and Linux Mint.\n\n&amp;#x200B;\n\nAnyone else notice this? Is this normal? Asking since I can probably just zero out the drives and return them, but would need to know if I can maybe force it to do this kind of idle read every few minutes or something.\n\n&amp;#x200B;\n\nFigured I would ask you guys, thanks.", "author_fullname": "t2_9kx87h0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question Regarding Default 'Read' Click For WD RED 14 TB Drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lt0kf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698891101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;I bought two WD RED 14 TB drives (7200 RPM) recently for mass storage, and added all of my data to both of them which are now synced via &amp;#39;rsync&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;My issue is this:&lt;/p&gt;\n\n&lt;p&gt;- Every 5 secs, I can hear like a &amp;#39;data read&amp;#39; type noise --&amp;gt; Not necessarily as bad as a hard drive failure click, but something noticeable. Its kind of slight loud, and hard to ignore. It doesn&amp;#39;t sound awful, but I can&amp;#39;t like &amp;#39;not hear it&amp;#39; which is getting annoying.&lt;/p&gt;\n\n&lt;p&gt;I notice them in tandem since they are the same drive aka as soon as 5 secs go by --&amp;gt; one makes the read noise, and the other makes a read noise shortly there after.&lt;/p&gt;\n\n&lt;p&gt;I am using Linux so I observed this issue on the same machine via Ubuntu Server, and Linux Mint.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyone else notice this? Is this normal? Asking since I can probably just zero out the drives and return them, but would need to know if I can maybe force it to do this kind of idle read every few minutes or something.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Figured I would ask you guys, thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lt0kf", "is_robot_indexable": true, "report_reasons": null, "author": "BackToPlebbit69", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lt0kf/question_regarding_default_read_click_for_wd_red/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lt0kf/question_regarding_default_read_click_for_wd_red/", "subreddit_subscribers": 709950, "created_utc": 1698891101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to archive the content of a twitter account with its tweets and images. Is this possible? I was able to download just the images with gallery-dl but as it is an art history account, I'd like to have the text of the tweet associated with it because it contains the source of the art piece. \nIs there a way to do this?", "author_fullname": "t2_sjvvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Utility to download all third party twitter account tweets and images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17loke2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698878770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to archive the content of a twitter account with its tweets and images. Is this possible? I was able to download just the images with gallery-dl but as it is an art history account, I&amp;#39;d like to have the text of the tweet associated with it because it contains the source of the art piece. \nIs there a way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17loke2", "is_robot_indexable": true, "report_reasons": null, "author": "aManIsNoOneEither", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17loke2/utility_to_download_all_third_party_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17loke2/utility_to_download_all_third_party_twitter/", "subreddit_subscribers": 709950, "created_utc": 1698878770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been a pretty faithful with backups for decades, changing tools and services with the flow, and I'm in the process of reviewing my current practices to see if they are a good fit for my needs. Would love to hear suggestions. I'm running an M1 Macbook Pro with 1TB internal drive.\n\nCurrent strategy:\n\n1. Time Machine for local backup of laptop\n2. iDrive for remote backup of laptop plus a couple of external drives (currently have 5TB storage plan)\n\nMy goals:\n\n1. Have a good local backup in case of drive failure\n2. Have a good remote backup of laptop\n3. Long-term archival storage of photos, videos, documents, etc., both local and remote. These are not working files and are not stored on the laptop. Current volume is in the 2TB range.\n4. Backup including iPhone stuff (photos and whatnot)\n\nTools I have on hand:\n\n1. Various external hard drives, mostly USB3, in the 2-5 TB range\n2. Chronosync, Carbon Copy Cloner\n\nI'm not willing to put in the time and energy to build a NAS.\n\nI used to store my archive files on Amazon Drive but that's going away and Amazon Photos is HORRIBLE so I don't even consider it an option, plus I have a lot of files that aren't photo/video.\n\nUsed to have Crashplan for years until they got rid of personal accounts back when. Switched to iDrive. I've played with Backblaze a bit but am alarmed by what I've read here about the viability of the company.\n\nShould I look into some type of cloud storage (not backup) for the archive stuff? Or try to hobble along with it as part of a personal backup service (having it back up as an external drive)? Or just get cloud storage and use something like Chronosync to back up to disk images and upload those to the cloud?\n\nThanks for your suggestions.", "author_fullname": "t2_d7hme", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for suggestions to create a good backup plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lksdo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698868775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been a pretty faithful with backups for decades, changing tools and services with the flow, and I&amp;#39;m in the process of reviewing my current practices to see if they are a good fit for my needs. Would love to hear suggestions. I&amp;#39;m running an M1 Macbook Pro with 1TB internal drive.&lt;/p&gt;\n\n&lt;p&gt;Current strategy:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Time Machine for local backup of laptop&lt;/li&gt;\n&lt;li&gt;iDrive for remote backup of laptop plus a couple of external drives (currently have 5TB storage plan)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My goals:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Have a good local backup in case of drive failure&lt;/li&gt;\n&lt;li&gt;Have a good remote backup of laptop&lt;/li&gt;\n&lt;li&gt;Long-term archival storage of photos, videos, documents, etc., both local and remote. These are not working files and are not stored on the laptop. Current volume is in the 2TB range.&lt;/li&gt;\n&lt;li&gt;Backup including iPhone stuff (photos and whatnot)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Tools I have on hand:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Various external hard drives, mostly USB3, in the 2-5 TB range&lt;/li&gt;\n&lt;li&gt;Chronosync, Carbon Copy Cloner&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m not willing to put in the time and energy to build a NAS.&lt;/p&gt;\n\n&lt;p&gt;I used to store my archive files on Amazon Drive but that&amp;#39;s going away and Amazon Photos is HORRIBLE so I don&amp;#39;t even consider it an option, plus I have a lot of files that aren&amp;#39;t photo/video.&lt;/p&gt;\n\n&lt;p&gt;Used to have Crashplan for years until they got rid of personal accounts back when. Switched to iDrive. I&amp;#39;ve played with Backblaze a bit but am alarmed by what I&amp;#39;ve read here about the viability of the company.&lt;/p&gt;\n\n&lt;p&gt;Should I look into some type of cloud storage (not backup) for the archive stuff? Or try to hobble along with it as part of a personal backup service (having it back up as an external drive)? Or just get cloud storage and use something like Chronosync to back up to disk images and upload those to the cloud?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lksdo", "is_robot_indexable": true, "report_reasons": null, "author": "debit72", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lksdo/looking_for_suggestions_to_create_a_good_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lksdo/looking_for_suggestions_to_create_a_good_backup/", "subreddit_subscribers": 709950, "created_utc": 1698868775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "See link - price is at $119.99 for 10TB drive; what am I missing? That seems ridiculously good.\n\n&amp;#x200B;\n\n[https://www.bhphotovideo.com/c/product/1759582-REG/toshiba\\_hdwg51axzstb\\_n300\\_nas\\_pro\\_10tb.html](https://www.bhphotovideo.com/c/product/1759582-REG/toshiba_hdwg51axzstb_n300_nas_pro_10tb.html)", "author_fullname": "t2_bzazl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Really good deal on Toshiba 10TB N300 Pro NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lic3q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698862186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;See link - price is at $119.99 for 10TB drive; what am I missing? That seems ridiculously good.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.bhphotovideo.com/c/product/1759582-REG/toshiba_hdwg51axzstb_n300_nas_pro_10tb.html\"&gt;https://www.bhphotovideo.com/c/product/1759582-REG/toshiba_hdwg51axzstb_n300_nas_pro_10tb.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lic3q", "is_robot_indexable": true, "report_reasons": null, "author": "Tempestshade", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lic3q/really_good_deal_on_toshiba_10tb_n300_pro_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lic3q/really_good_deal_on_toshiba_10tb_n300_pro_nas/", "subreddit_subscribers": 709950, "created_utc": 1698862186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, was wondering if there was an app or convenient way to batch download saved and upvoted images from my profile. I saw some Python scripts floating around but unfortunately I\u2019m a neanderthal and don\u2019t know how to use Python", "author_fullname": "t2_2w622ddn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apps that batch download saved/upvoted images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lfohj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698855180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, was wondering if there was an app or convenient way to batch download saved and upvoted images from my profile. I saw some Python scripts floating around but unfortunately I\u2019m a neanderthal and don\u2019t know how to use Python&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lfohj", "is_robot_indexable": true, "report_reasons": null, "author": "PickleInTheSun", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lfohj/apps_that_batch_download_savedupvoted_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lfohj/apps_that_batch_download_savedupvoted_images/", "subreddit_subscribers": 709950, "created_utc": 1698855180.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}