{"kind": "Listing", "data": {"after": "t3_17m0afd", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "@storagereview tiktok, Instagram and YouTube ", "author_fullname": "t2_45mfx1w8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "btw my birthday after few days ", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_17m15ik", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 215, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 1200, "fallback_url": "https://v.redd.it/y2xvvj9d4xxb1/DASH_480.mp4?source=fallback", "has_audio": true, "height": 854, "width": 480, "scrubber_media_url": "https://v.redd.it/y2xvvj9d4xxb1/DASH_96.mp4", "dash_url": "https://v.redd.it/y2xvvj9d4xxb1/DASHPlaylist.mpd?a=1701535340%2CZWFmZjVlZDdkNTcwMWQ3NDg3NjE3MDAyZjRhMWY2Njk3OWEzMWYxYTZlMjgzNjdhZDVlNmI4ODhmOGI2MGQwMw%3D%3D&amp;v=1&amp;f=sd", "duration": 68, "hls_url": "https://v.redd.it/y2xvvj9d4xxb1/HLSPlaylist.m3u8?a=1701535340%2CNjNhNWUyOTRmODA4MWM5ZTlkOWE3NWJhMTczYjQ0ODRiMzMyYTE5MjJiMGU2ZDUxMzRhZmJjNTRiYjNhMGI0ZA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 215, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=0ad44f8580400f68a7a98b7603bd0433493df335", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698923687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;@storagereview tiktok, Instagram and YouTube &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/y2xvvj9d4xxb1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?format=pjpg&amp;auto=webp&amp;s=79ef61972a167c7bdb4109e5889ea12296f70aed", "width": 1208, "height": 2148}, "resolutions": [{"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b92b942c6f821034c2051bc7d6a1d11eaca0a9d4", "width": 108, "height": 192}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3ba49c21ba43017a0c97988b49808d3abc930532", "width": 216, "height": 384}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2d73ee987c11a630adcc30829d83af771bd07d4a", "width": 320, "height": 569}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cdf900c875d85f00692beaef27abcf3d30c2d53b", "width": 640, "height": 1138}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=784cca675c8fa8f8ff00febc2875d1f5ee0ed54f", "width": 960, "height": 1707}, {"url": "https://external-preview.redd.it/NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=89ae3dcce5e9f18556a22ade153245c62e476751", "width": 1080, "height": 1920}], "variants": {}, "id": "NHFqc3pveGM0eHhiMYKhhk_pxfTy3xhkaub6V2IKZw0kWpWYFMsJSHYNKxtK"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m15ik", "is_robot_indexable": true, "report_reasons": null, "author": "Champion-Dapper", "discussion_type": null, "num_comments": 74, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17m15ik/btw_my_birthday_after_few_days/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/y2xvvj9d4xxb1", "subreddit_subscribers": 709971, "created_utc": 1698923687.0, "num_crossposts": 2, "media": {"reddit_video": {"bitrate_kbps": 1200, "fallback_url": "https://v.redd.it/y2xvvj9d4xxb1/DASH_480.mp4?source=fallback", "has_audio": true, "height": 854, "width": 480, "scrubber_media_url": "https://v.redd.it/y2xvvj9d4xxb1/DASH_96.mp4", "dash_url": "https://v.redd.it/y2xvvj9d4xxb1/DASHPlaylist.mpd?a=1701535340%2CZWFmZjVlZDdkNTcwMWQ3NDg3NjE3MDAyZjRhMWY2Njk3OWEzMWYxYTZlMjgzNjdhZDVlNmI4ODhmOGI2MGQwMw%3D%3D&amp;v=1&amp;f=sd", "duration": 68, "hls_url": "https://v.redd.it/y2xvvj9d4xxb1/HLSPlaylist.m3u8?a=1701535340%2CNjNhNWUyOTRmODA4MWM5ZTlkOWE3NWJhMTczYjQ0ODRiMzMyYTE5MjJiMGU2ZDUxMzRhZmJjNTRiYjNhMGI0ZA%3D%3D&amp;v=1&amp;f=sd", "is_gif": false, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "The single biggest factor in SSD performance is the presence and quantity of DRAM cache.  A drive with a GB or two of DRAM will write at several hundred MB/s all day long, while a DRAM-less drive will slow to a crawl after the first couple GB written.  \n\nThis being the case, why does nobody list this spec?  Neither amazon nor newegg have cache as a filterable item.  95% of drives don't even mention it one way or the other in the product description.  It's fucking maddening.\n\nAre there any web shops that will let me search for drives with DRAM cache specifically, or am I going to have to resort to googling full reviews of each model individually?", "author_fullname": "t2_4wc8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSD shopping - how to find drives with DRAM cache?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lgjrj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698857499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The single biggest factor in SSD performance is the presence and quantity of DRAM cache.  A drive with a GB or two of DRAM will write at several hundred MB/s all day long, while a DRAM-less drive will slow to a crawl after the first couple GB written.  &lt;/p&gt;\n\n&lt;p&gt;This being the case, why does nobody list this spec?  Neither amazon nor newegg have cache as a filterable item.  95% of drives don&amp;#39;t even mention it one way or the other in the product description.  It&amp;#39;s fucking maddening.&lt;/p&gt;\n\n&lt;p&gt;Are there any web shops that will let me search for drives with DRAM cache specifically, or am I going to have to resort to googling full reviews of each model individually?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "210TB Drivepool/Snapraid", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "17lgjrj", "is_robot_indexable": true, "report_reasons": null, "author": "candre23", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17lgjrj/ssd_shopping_how_to_find_drives_with_dram_cache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lgjrj/ssd_shopping_how_to_find_drives_with_dram_cache/", "subreddit_subscribers": 709971, "created_utc": 1698857499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am planning a Christmas gift for someone who loves having physical media.  She likes the packaging and knowing that she owns the movie without having to pay any type of subscription or logging into an account.  \n\nWe have a PS5 that can play the Blu-ray discs, but I worry about them getting scratched up after continual use.  We have an LG TV that runs webOS which has a USB port so if I can rip the Blu-ray to USB in a supported format, we can play them that way.  She also likes to watch the movies on her iPad while away from home so I am hoping the rip can also be played on it.  \n\nI have a [full-size desktop computer](https://psref.lenovo.com/Product/Legion/Lenovo_Legion_T7_34IMZ5) (i9-11900K / 64GB DDR4 / RTX 3080 4.0 GB) but it doesn't have a Blu-ray player.  I don't see an internal bay to add one so I imagine my options are to either use a USB Blu-ray reader (probably extremely slow) or just temporarily open the case and have an internal SATA Blu-ray player connected while I rip them.  Are there any models people recommend that will do the job quickly?\n\nIt looks like [MakeMKV](https://www.makemkv.com/) is the gold standard for converting a physical Blu-ray to digital media.  Is the digital file compatible with iOS and webOS?  If not, what converters would you recommend? \n\nThe end goal is to have a USB drive with all of the movies digitally on it which can be plugged into the LG TV to play directly.  The files will also be copied to the iPad preferably through iTunes but I could also install the VLC app and copy them that way if iTunes doesn't support it.", "author_fullname": "t2_a4gfzgya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Wanting to purchase ~20 Blu-ray musicals and rip them to a format playable on iOS and webOS. What do I need?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lgn0a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698857740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am planning a Christmas gift for someone who loves having physical media.  She likes the packaging and knowing that she owns the movie without having to pay any type of subscription or logging into an account.  &lt;/p&gt;\n\n&lt;p&gt;We have a PS5 that can play the Blu-ray discs, but I worry about them getting scratched up after continual use.  We have an LG TV that runs webOS which has a USB port so if I can rip the Blu-ray to USB in a supported format, we can play them that way.  She also likes to watch the movies on her iPad while away from home so I am hoping the rip can also be played on it.  &lt;/p&gt;\n\n&lt;p&gt;I have a &lt;a href=\"https://psref.lenovo.com/Product/Legion/Lenovo_Legion_T7_34IMZ5\"&gt;full-size desktop computer&lt;/a&gt; (i9-11900K / 64GB DDR4 / RTX 3080 4.0 GB) but it doesn&amp;#39;t have a Blu-ray player.  I don&amp;#39;t see an internal bay to add one so I imagine my options are to either use a USB Blu-ray reader (probably extremely slow) or just temporarily open the case and have an internal SATA Blu-ray player connected while I rip them.  Are there any models people recommend that will do the job quickly?&lt;/p&gt;\n\n&lt;p&gt;It looks like &lt;a href=\"https://www.makemkv.com/\"&gt;MakeMKV&lt;/a&gt; is the gold standard for converting a physical Blu-ray to digital media.  Is the digital file compatible with iOS and webOS?  If not, what converters would you recommend? &lt;/p&gt;\n\n&lt;p&gt;The end goal is to have a USB drive with all of the movies digitally on it which can be plugged into the LG TV to play directly.  The files will also be copied to the iPad preferably through iTunes but I could also install the VLC app and copy them that way if iTunes doesn&amp;#39;t support it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lgn0a", "is_robot_indexable": true, "report_reasons": null, "author": "FatherLiamFinnegan", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lgn0a/wanting_to_purchase_20_bluray_musicals_and_rip/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lgn0a/wanting_to_purchase_20_bluray_musicals_and_rip/", "subreddit_subscribers": 709971, "created_utc": 1698857740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I currently have a ATX gaming setup with roughly 9x 2.5\" SSD drives that I have come to realization that it is simply not practical to squeeze all the drives and high-end GPUs and CPUs into 1 system. Not to mention, even most common ATX boards will start to run into difficulties trying to support that many SATA ports without affecting PCIe bandwidth. After coming to this conclusion, I am leaning towards building a separate ITX or mATX system dedicated to just housing 12+ 2.5\" SSDs and having it be on 24/7 and it only being accessed offline via LAN. Since I manually backup my data on external 3.5\" drives every week, the storage solution will not need any RAID mirroring.\n\nSince this is my first time looking into a 24/7 server setup, I am unsure if there are certain things I need to consider for system reliability. Some questions I have are:\n\n1. Between Intel LGA1700 and AMD AM5, which would be a better platform? For example, I just learned not all LGA1700 chips support ECC memory while AM5 chips have higher idle power consumption that low-end LGA1700.\n2. Would a consumer-grade ITX or mATX motherboard be good enough for 24/7 operation? Beside ECC support, are there other factors I should consider?\n3. Also what HBAs should I be considering? I know it is probably crude, but I was considering using a x16 to x8/x8 or x4/x4/x4/x4 m.2 card to hook up with multiple JMB585 5-port SATA cards and use 3-4 of them on a motherboard to get up to 20 SATA devices. \n\nI was leaning towards Intel LGA1700 for the lower idle power consumption despite the limited x8/x8 PCie bifurcation options, but this latest discovery of ECC memory not being supported on 1x100 and 1x400 chips does make me change my mind.", "author_fullname": "t2_jjarg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best means of accessing 12+ 2.5\" SSD drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lv0vn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698897990.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698897352.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have a ATX gaming setup with roughly 9x 2.5&amp;quot; SSD drives that I have come to realization that it is simply not practical to squeeze all the drives and high-end GPUs and CPUs into 1 system. Not to mention, even most common ATX boards will start to run into difficulties trying to support that many SATA ports without affecting PCIe bandwidth. After coming to this conclusion, I am leaning towards building a separate ITX or mATX system dedicated to just housing 12+ 2.5&amp;quot; SSDs and having it be on 24/7 and it only being accessed offline via LAN. Since I manually backup my data on external 3.5&amp;quot; drives every week, the storage solution will not need any RAID mirroring.&lt;/p&gt;\n\n&lt;p&gt;Since this is my first time looking into a 24/7 server setup, I am unsure if there are certain things I need to consider for system reliability. Some questions I have are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Between Intel LGA1700 and AMD AM5, which would be a better platform? For example, I just learned not all LGA1700 chips support ECC memory while AM5 chips have higher idle power consumption that low-end LGA1700.&lt;/li&gt;\n&lt;li&gt;Would a consumer-grade ITX or mATX motherboard be good enough for 24/7 operation? Beside ECC support, are there other factors I should consider?&lt;/li&gt;\n&lt;li&gt;Also what HBAs should I be considering? I know it is probably crude, but I was considering using a x16 to x8/x8 or x4/x4/x4/x4 m.2 card to hook up with multiple JMB585 5-port SATA cards and use 3-4 of them on a motherboard to get up to 20 SATA devices. &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I was leaning towards Intel LGA1700 for the lower idle power consumption despite the limited x8/x8 PCie bifurcation options, but this latest discovery of ECC memory not being supported on 1x100 and 1x400 chips does make me change my mind.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lv0vn", "is_robot_indexable": true, "report_reasons": null, "author": "imaginary_num6er", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lv0vn/best_means_of_accessing_12_25_ssd_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lv0vn/best_means_of_accessing_12_25_ssd_drives/", "subreddit_subscribers": 709971, "created_utc": 1698897352.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to store a backup of recordings we've been making for the past three years. It's currently at less than 3 TB and these are 5 - 9 GB files each, as mp4s. It will continue to grow, as we generate 6 recordings a month. I don't need to access the backup frequently, as the files are also on my local machine, on archival M-discs, and on a separate HDD that I keep as a physical backup and sync two regularly. I also have Backblaze in the background. So when I go back to edit the recordings, I'll be using the local files rather than the ones in the cloud.\n\nHowever I really want someplace to drop a recording immediately after it's done that's off-site in the cloud, just in case of an immediate disaster.\n\nThey are currently on [Sync.com](https://sync.com/), which offers unlimited space for $30/mo, but the service's stopped providing support *of any kind* recently (despite advertising phone support for their higher tiers) so I'm worried they're about to go under or that something is up with their company.\n\nI had considered AWS Deep Archive, but their egress cost seems insane as the service is really meant for \"archives\" as far as I understand it, not as a backup I might very infrequently access. \n\nWhat other options are out there to consider?", "author_fullname": "t2_7r7o3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use case for my long term video storage and potential cloud options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lh46i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698858960.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to store a backup of recordings we&amp;#39;ve been making for the past three years. It&amp;#39;s currently at less than 3 TB and these are 5 - 9 GB files each, as mp4s. It will continue to grow, as we generate 6 recordings a month. I don&amp;#39;t need to access the backup frequently, as the files are also on my local machine, on archival M-discs, and on a separate HDD that I keep as a physical backup and sync two regularly. I also have Backblaze in the background. So when I go back to edit the recordings, I&amp;#39;ll be using the local files rather than the ones in the cloud.&lt;/p&gt;\n\n&lt;p&gt;However I really want someplace to drop a recording immediately after it&amp;#39;s done that&amp;#39;s off-site in the cloud, just in case of an immediate disaster.&lt;/p&gt;\n\n&lt;p&gt;They are currently on &lt;a href=\"https://sync.com/\"&gt;Sync.com&lt;/a&gt;, which offers unlimited space for $30/mo, but the service&amp;#39;s stopped providing support &lt;em&gt;of any kind&lt;/em&gt; recently (despite advertising phone support for their higher tiers) so I&amp;#39;m worried they&amp;#39;re about to go under or that something is up with their company.&lt;/p&gt;\n\n&lt;p&gt;I had considered AWS Deep Archive, but their egress cost seems insane as the service is really meant for &amp;quot;archives&amp;quot; as far as I understand it, not as a backup I might very infrequently access. &lt;/p&gt;\n\n&lt;p&gt;What other options are out there to consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lh46i", "is_robot_indexable": true, "report_reasons": null, "author": "mccoypauley", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lh46i/use_case_for_my_long_term_video_storage_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lh46i/use_case_for_my_long_term_video_storage_and/", "subreddit_subscribers": 709971, "created_utc": 1698858960.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey everyone, \n\nMy parents have a bunch of data just sitting on external drives, and that data is not copied anywhere. They have lost family photos to a failing drive before, and I want to prevent something like that from happening again.\n\nIn this vein, **I want to get them a complete NAS solution** for a Christmas present. I work in technology, although not in IT, so I am competent at understanding things of this nature but do not have the knowledge myself currently.\n\nIdeally, **this is my target system**:\n- an easy-to-use NAS that they will find intuitive (and cannot easily break)\n- a automated backup solution (ideally one on-site and one in the cloud, I use Backblaze B2 personally)\n- perhaps a UPS to really mitigate the possibility of error (do you all think this is necessary?)\n\n**I will provide**\n- a budget to get this done (I am comfortable spending between 1-2k USD)\n- the initial setup for them\n- support in the future should something go wrong\n\n**Any advice is greatly appreciated!** I'm not currently aware of how much storage will actually be required, but I think 5 or so TB to start would be sufficient.\n\nI've been looking at synology NAS options and am really just looking for advice on whether I've made some sort of mistake in reasoning, companies/products to avoid (or use), and any other advice you all would think is valuable :) Thanks again!", "author_fullname": "t2_kqraiwk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Complete NAS solution for parents for Christmas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ls9w0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698888964.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt;\n\n&lt;p&gt;My parents have a bunch of data just sitting on external drives, and that data is not copied anywhere. They have lost family photos to a failing drive before, and I want to prevent something like that from happening again.&lt;/p&gt;\n\n&lt;p&gt;In this vein, &lt;strong&gt;I want to get them a complete NAS solution&lt;/strong&gt; for a Christmas present. I work in technology, although not in IT, so I am competent at understanding things of this nature but do not have the knowledge myself currently.&lt;/p&gt;\n\n&lt;p&gt;Ideally, &lt;strong&gt;this is my target system&lt;/strong&gt;:\n- an easy-to-use NAS that they will find intuitive (and cannot easily break)\n- a automated backup solution (ideally one on-site and one in the cloud, I use Backblaze B2 personally)\n- perhaps a UPS to really mitigate the possibility of error (do you all think this is necessary?)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;I will provide&lt;/strong&gt;\n- a budget to get this done (I am comfortable spending between 1-2k USD)\n- the initial setup for them\n- support in the future should something go wrong&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Any advice is greatly appreciated!&lt;/strong&gt; I&amp;#39;m not currently aware of how much storage will actually be required, but I think 5 or so TB to start would be sufficient.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking at synology NAS options and am really just looking for advice on whether I&amp;#39;ve made some sort of mistake in reasoning, companies/products to avoid (or use), and any other advice you all would think is valuable :) Thanks again!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ls9w0", "is_robot_indexable": true, "report_reasons": null, "author": "FiziksMayMays", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ls9w0/complete_nas_solution_for_parents_for_christmas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ls9w0/complete_nas_solution_for_parents_for_christmas/", "subreddit_subscribers": 709971, "created_utc": 1698888964.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "title, need to download multiple  youtube channels and have them download any new videos uploaded", "author_fullname": "t2_4av7osji", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how can i download a whole youtube channel and have it automatically download new videos", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ljq1t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698865895.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title, need to download multiple  youtube channels and have them download any new videos uploaded&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17ljq1t", "is_robot_indexable": true, "report_reasons": null, "author": "alex_square6", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17ljq1t/how_can_i_download_a_whole_youtube_channel_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17ljq1t/how_can_i_download_a_whole_youtube_channel_and/", "subreddit_subscribers": 709971, "created_utc": 1698865895.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_rcb9rik6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storing data for thousands of years | Microsoft Project Silica", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17lzywn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.59, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1M_n3N_FzHS-7rRF0hUWIRTXTshijtAeV3xiXxpAQtY.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1698919091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "m.youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://m.youtube.com/watch?v=-rfEYd4NGQg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?auto=webp&amp;s=168f4e4186b95a842fd888e6bc77a35bfa645789", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c46fdf3c69a23003d3d3203bd839db8a6f2c3490", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6176347747a6a03683d319c0374e89bf04ba445", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e1587f2a3f4f07a86cecffff956f0830f7385e56", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea5e5928c66fdec8e98231e84e871aabac914e48", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c71cb2f1a6f4a52498a68152b7411a77b0fa1c5a", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/AGfIwIdzFcue70Y-rwpoPIvN1Rm_wa58FUPJqsXvmzs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1e0b2af0444b78af8513ab098be3be21a9c72a92", "width": 1080, "height": 607}], "variants": {}, "id": "fGf6q9pJHBEJ0KNmqU9nNiTn79MFHO8GMcShGXtDdPE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lzywn", "is_robot_indexable": true, "report_reasons": null, "author": "albanianspy", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17lzywn/storing_data_for_thousands_of_years_microsoft/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://m.youtube.com/watch?v=-rfEYd4NGQg", "subreddit_subscribers": 709971, "created_utc": 1698919091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, I\u2019m having an issue with an external HDD I use as a backup drive. It\u2019s starting to fail and I am wondering if I should use an SSD for a backup instead (I have a spare SATA one from Micron).\n\nI have seen people mention that SSDs are not good for long-term storage when they are left powered off. If I power on the SSD once a month when I do my backups will I be fine or should I just use an HDD?", "author_fullname": "t2_13bt8084", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSD for External Backup Drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lm1ya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.61, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698872718.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698872219.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I\u2019m having an issue with an external HDD I use as a backup drive. It\u2019s starting to fail and I am wondering if I should use an SSD for a backup instead (I have a spare SATA one from Micron).&lt;/p&gt;\n\n&lt;p&gt;I have seen people mention that SSDs are not good for long-term storage when they are left powered off. If I power on the SSD once a month when I do my backups will I be fine or should I just use an HDD?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lm1ya", "is_robot_indexable": true, "report_reasons": null, "author": "xxBLVCKMVGICxx", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lm1ya/ssd_for_external_backup_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lm1ya/ssd_for_external_backup_drive/", "subreddit_subscribers": 709971, "created_utc": 1698872219.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I wanted to do it with an excel file and just do it manually. Every session is recorded with OBS, uploaded to YT, and then redownloaded so it's compressed. You may disagree with that method, but I just care about the moments and such, so the quality is irrelevant as long as it's 720+ and you can tell what's going on, and Youtube does the compression for me so I don't gotta be running handbrake 24/7.\n\nI have over a thousand sessions since 2017, probably way more than that, just know that it's a lot. I mostly play League and mostly with the same people, and it's almost all organized with dates and time. Some sessions (moreso edited stuff) don't have dates. Some stuff is not gaming related.\n\nNow the reason I'm making this post is that I'm wondering what data you'd include. You know how excel has those columns, well I thought the first would be some sort of hex id just in case I needed it for something in the future, the second would be the date, and third would be title. I'd like to include file path and YT url/id. Maybe YT upload date as well. File size and upload date of course.\n\nNow the more annoying stuff that I'd like to include: People I played with, what games I played, how long I played each game, the scorelines or lengths of said games. Timestamps to funny or notable moments. Now I'm not sure how I'd include that if every session only has a single row. Maybe a second sheet? Idk.\n\nI'm not asking you for nitty gritty details, but just suggestions and such if you have any.", "author_fullname": "t2_bede0mtsb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I wanna create a database for all my gaming sessions. Asking for some ideas, as it's a long term project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lld37", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698870311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I wanted to do it with an excel file and just do it manually. Every session is recorded with OBS, uploaded to YT, and then redownloaded so it&amp;#39;s compressed. You may disagree with that method, but I just care about the moments and such, so the quality is irrelevant as long as it&amp;#39;s 720+ and you can tell what&amp;#39;s going on, and Youtube does the compression for me so I don&amp;#39;t gotta be running handbrake 24/7.&lt;/p&gt;\n\n&lt;p&gt;I have over a thousand sessions since 2017, probably way more than that, just know that it&amp;#39;s a lot. I mostly play League and mostly with the same people, and it&amp;#39;s almost all organized with dates and time. Some sessions (moreso edited stuff) don&amp;#39;t have dates. Some stuff is not gaming related.&lt;/p&gt;\n\n&lt;p&gt;Now the reason I&amp;#39;m making this post is that I&amp;#39;m wondering what data you&amp;#39;d include. You know how excel has those columns, well I thought the first would be some sort of hex id just in case I needed it for something in the future, the second would be the date, and third would be title. I&amp;#39;d like to include file path and YT url/id. Maybe YT upload date as well. File size and upload date of course.&lt;/p&gt;\n\n&lt;p&gt;Now the more annoying stuff that I&amp;#39;d like to include: People I played with, what games I played, how long I played each game, the scorelines or lengths of said games. Timestamps to funny or notable moments. Now I&amp;#39;m not sure how I&amp;#39;d include that if every session only has a single row. Maybe a second sheet? Idk.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not asking you for nitty gritty details, but just suggestions and such if you have any.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lld37", "is_robot_indexable": true, "report_reasons": null, "author": "spanspan3213", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lld37/i_wanna_create_a_database_for_all_my_gaming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lld37/i_wanna_create_a_database_for_all_my_gaming/", "subreddit_subscribers": 709971, "created_utc": 1698870311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I want to completely mirror a website, but only get a specific level of length when accessing external websites. How can I do this in wget? The help section isn't so clear about this", "author_fullname": "t2_ld87l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Only getting a specific depth of an external link with wget", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lk835", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698867232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I want to completely mirror a website, but only get a specific level of length when accessing external websites. How can I do this in wget? The help section isn&amp;#39;t so clear about this&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "37 TB local + 2.1 TB cloud", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lk835", "is_robot_indexable": true, "report_reasons": null, "author": "Sai22", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17lk835/only_getting_a_specific_depth_of_an_external_link/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lk835/only_getting_a_specific_depth_of_an_external_link/", "subreddit_subscribers": 709971, "created_utc": 1698867232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I got a large collection of DVD disks from my father as he passed. This includes a lot of disks that he burned especially for me for sentimental reasons, like the final match of the 1958 Soccer World Cup. I'd like to preserve it but have no place for the huge amount of disks. So i purchased a simple external DVD drive.\n\nI'm having problems to use the device to image some of the disks as it seemingly stops working or is turned off by the computer after some time. I've already set it to not be powered off and it does the same thing on different computers. I am never able to image the disk to the end.\n\nIs there a way of imaging it in parts? Something like batches of sectors each time and them just concatenate the resulting files somehow?", "author_fullname": "t2_o83sz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DVD imaging in parts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17m4tlr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698935548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got a large collection of DVD disks from my father as he passed. This includes a lot of disks that he burned especially for me for sentimental reasons, like the final match of the 1958 Soccer World Cup. I&amp;#39;d like to preserve it but have no place for the huge amount of disks. So i purchased a simple external DVD drive.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m having problems to use the device to image some of the disks as it seemingly stops working or is turned off by the computer after some time. I&amp;#39;ve already set it to not be powered off and it does the same thing on different computers. I am never able to image the disk to the end.&lt;/p&gt;\n\n&lt;p&gt;Is there a way of imaging it in parts? Something like batches of sectors each time and them just concatenate the resulting files somehow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m4tlr", "is_robot_indexable": true, "report_reasons": null, "author": "fernandodandrea", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17m4tlr/dvd_imaging_in_parts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17m4tlr/dvd_imaging_in_parts/", "subreddit_subscribers": 709971, "created_utc": 1698935548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just installed a ssd on my pc and installed windows on the it. my old hdd still have windows on it and I'm planning to completely remove the windows. my hdd has 2 partitions. C for windows and D for personal data. After formatting the C drive(system drive on hdd) i want to merge it with other D drive. I want to know if I can merge them as one single drive and if yes then how. thank you.\nedit: I also would like to do this without any data loss so let me know if the merger will lead to data loss on the personal drive (of old hdd)", "author_fullname": "t2_17eahs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about disk partition.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lk470", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698867413.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698866955.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just installed a ssd on my pc and installed windows on the it. my old hdd still have windows on it and I&amp;#39;m planning to completely remove the windows. my hdd has 2 partitions. C for windows and D for personal data. After formatting the C drive(system drive on hdd) i want to merge it with other D drive. I want to know if I can merge them as one single drive and if yes then how. thank you.\nedit: I also would like to do this without any data loss so let me know if the merger will lead to data loss on the personal drive (of old hdd)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lk470", "is_robot_indexable": true, "report_reasons": null, "author": "thetushar7", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lk470/question_about_disk_partition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lk470/question_about_disk_partition/", "subreddit_subscribers": 709971, "created_utc": 1698866955.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi! I know I can write a script for this, but I was wondering if there is some sort of solution that is already made for this.\n\nThanks!", "author_fullname": "t2_uy2zp43z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a tool to automatically archive a folder, transfer via scp/rsync and decompress on the other side automatically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lfukx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698855651.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I know I can write a script for this, but I was wondering if there is some sort of solution that is already made for this.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lfukx", "is_robot_indexable": true, "report_reasons": null, "author": "angel__-__-", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lfukx/is_there_a_tool_to_automatically_archive_a_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lfukx/is_there_a_tool_to_automatically_archive_a_folder/", "subreddit_subscribers": 709971, "created_utc": 1698855651.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nI am looking for a simple (I'd love it open source) bandwidth monitoring tool that can show how much data I downloaded during a specific period, last 24 hours etc.\n\nDo you use one?\n\nThank you!", "author_fullname": "t2_vc7clehs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm looking for a simple bandwidth monitoring tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17m5bs7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698936953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I am looking for a simple (I&amp;#39;d love it open source) bandwidth monitoring tool that can show how much data I downloaded during a specific period, last 24 hours etc.&lt;/p&gt;\n\n&lt;p&gt;Do you use one?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m5bs7", "is_robot_indexable": true, "report_reasons": null, "author": "Feeling_Usual1541", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17m5bs7/im_looking_for_a_simple_bandwidth_monitoring_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17m5bs7/im_looking_for_a_simple_bandwidth_monitoring_tool/", "subreddit_subscribers": 709971, "created_utc": 1698936953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My 20 TB Exos drive was throwing I/O errors in a Synology NAS. Support asked me to replace it. The drive is only 4 months old. \n\nWould anyone know if the SMART values are bad?\n\n(03) ST20000NM007D-3DJ103\n\n\\----------------------------------------------------------------------------\n\nModel : ST20000NM007D-3DJ103\n\nFirmware : SN03\n\n   Serial Number : \\*\\*\\*\\*\\*\n\nDisk Size : 20000.5 GB (8.4/137.4/20000.5/20000.5)\n\nBuffer Size : Unknown\n\nQueue Depth : 32\n\n\\# of Sectors : 39063650304\n\n   Rotation Rate : 7200 RPM\n\nInterface : UASP (Serial ATA)\n\n   Major Version : ACS-4\n\n   Minor Version : ----\n\n   Transfer Mode : SATA/600 | SATA/600\n\n  Power On Hours : 3407 hours\n\n  Power On Count : 1510 count\n\nTemperature : 40 C (104 F)\n\n   Health Status : Good\n\nFeatures : S.M.A.R.T., NCQ, Streaming, GPL\n\nAPM Level : ----\n\nAAM Level : ----\n\nDrive Letter : X:\n\n&amp;#x200B;\n\n\\-- S.M.A.R.T. --------------------------------------------------------------\n\nID Cur Wor Thr RawValues(6) Attribute Name\n\n01 \\_77 \\_64 \\_44 000002ADC4B2 Read Error Rate\n\n03 \\_91 \\_90 \\_\\_0 000000000000 Spin-Up Time\n\n04 \\_99 \\_99 \\_20 0000000006BD Start/Stop Count\n\n05 100 100 \\_10 000000000000 Reallocated Sectors Count\n\n07 \\_84 \\_60 \\_45 00000EDEBE70 Seek Error Rate\n\n09 \\_97 \\_97 \\_\\_0 000000000D4F Power-On Hours\n\n0A 100 100 \\_97 000000000000 Spin Retry Count\n\n0C \\_99 \\_99 \\_20 0000000005E6 Power Cycle Count\n\n12 100 100 \\_50 000000000000 Vendor Specific\n\nBB 100 100 \\_\\_0 000000000000 Reported Uncorrectable Errors\n\nBC 100 100 \\_\\_0 000000000000 Command Timeout\n\nBE \\_60 \\_53 \\_\\_0 000028240028 Airflow Temperature\n\nC0 100 100 \\_\\_0 000000000004 Power-off Retract Count\n\nC1 100 100 \\_\\_0 0000000006F6 Load/Unload Cycle Count\n\nC2 \\_40 \\_47 \\_\\_0 001400000028 Temperature\n\nC5 100 100 \\_\\_0 000000000000 Current Pending Sector Count\n\nC6 100 100 \\_\\_0 000000000000 Uncorrectable Sector Count\n\nC7 200 200 \\_\\_0 000000000000 UltraDMA CRC Error Count\n\nC8 100 100 \\_\\_1 000000000000 Write Error Rate\n\nF0 100 100 \\_\\_0 ABCA00000D43 Head Flying Hours\n\nF1 100 253 \\_\\_0 000F7F12072B Total Host Writes\n\nF2 100 253 \\_\\_0 00263DACBF21 Total Host Reads\n\n&amp;#x200B;\n\n\\-- IDENTIFY\\_DEVICE ---------------------------------------------------------\n\n0    1    2    3    4    5    6    7    8    9\n\n000: 0C5A 3FFF C837 0010 0000 0000 003F 0000 0000 0000\n\n010: 2020 2020 2020 2020 2020 2020 5A56 5438 4554 5457\n\n020: 0000 0000 0000 534E 3033 2020 2020 5354 3230 3030\n\n030: 304E 4D30 3037 442D 3344 4A31 3033 2020 2020 2020\n\n040: 2020 2020 2020 2020 2020 2020 2020 8010 4000 2F00\n\n050: 4000 0200 0200 0007 3FFF 0010 003F FC10 00FB 5D10\n\n060: FFFF 0FFF 0000 0407 0003 0078 0078 0078 0078 000C\n\n070: 0000 0000 0000 0000 0000 001F 8D0E 0046 00CC 0000\n\n080: 0FE0 FFFF 306B 7561 6173 3069 B441 6173 007F 8350\n\n090: 8350 0000 FFFE 0000 FE00 1000 0000 0000 2710 0000\n\n100: 0000 1860 0009 0000 0000 0000 6003 0000 5000 C500\n\n110: E664 8865 0000 0000 0000 0000 0000 0000 0000 43DE\n\n120: 401C 0000 0000 0000 0000 0000 0000 0000 0021 0000\n\n130: 1860 0000 1860 2020 0002 0140 0100 5000 3C06 3C0A\n\n140: 0000 003C 0000 0008 0000 0000 FDFF 0280 0000 0470\n\n150: 0008 0000 0000 1E33 0000 8008 0000 02F7 D400 8048\n\n160: 0000 0000 0000 0000 0000 0000 0000 0000 0002 0000\n\n170: 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000\n\n180: 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000\n\n190: 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000\n\n200: 0000 0000 0000 0000 0000 0000 70BD 0000 0000 4000\n\n210: 0000 0000 0000 0000 0000 0000 0000 1C20 0000 0000\n\n220: 0000 0000 11FF 0000 0000 0000 0000 0000 0000 0000\n\n230: 0000 1860 0009 0000 0000 0000 0000 0000 0000 0000\n\n240: 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000\n\n250: 0000 0000 0000 0000 0000 BAA5\n\n&amp;#x200B;\n\n\\-- SMART\\_READ\\_DATA ---------------------------------------------------------\n\n\\+0 +1 +2 +3 +4 +5 +6 +7 +8 +9 +A +B +C +D +E +F\n\n000: 0A 00 01 0F 00 4D 40 B2 C4 AD 02 00 00 00 03 03\n\n010: 00 5B 5A 00 00 00 00 00 00 00 04 32 00 63 63 BD\n\n020: 06 00 00 00 00 00 05 33 00 64 64 00 00 00 00 00\n\n030: 00 00 07 0F 00 54 3C 70 BE DE 0E 00 00 00 09 32\n\n040: 00 61 61 4F 0D 00 00 00 00 00 0A 13 00 64 64 00\n\n050: 00 00 00 00 00 00 0C 32 00 63 63 E6 05 00 00 00\n\n060: 00 00 12 0B 00 64 64 00 00 00 00 00 00 00 BB 32\n\n070: 00 64 64 00 00 00 00 00 00 00 BC 32 00 64 64 00\n\n080: 00 00 00 00 00 00 BE 22 00 3C 35 28 00 24 28 00\n\n090: 00 00 C0 32 00 64 64 04 00 00 00 00 00 00 C1 32\n\n0A0: 00 64 64 F6 06 00 00 00 00 00 C2 22 00 28 2F 28\n\n0B0: 00 00 00 14 00 00 C5 12 00 64 64 00 00 00 00 00\n\n0C0: 00 00 C6 10 00 64 64 00 00 00 00 00 00 00 C7 3E\n\n0D0: 00 C8 C8 00 00 00 00 00 00 00 C8 23 00 64 64 00\n\n0E0: 00 00 00 00 00 00 F0 00 00 64 64 43 0D 00 00 CA\n\n0F0: AB 1D F1 00 00 64 FD 2B 07 12 7F 0F 00 00 F2 00\n\n100: 00 64 FD 21 BF AC 3D 26 00 00 00 00 00 00 00 00\n\n110: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n120: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n130: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n140: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n150: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n160: 00 00 00 00 00 00 00 00 00 00 82 00 2F 02 00 7B\n\n170: 03 00 01 00 01 FF 02 9F 06 00 00 00 00 00 00 00\n\n180: 00 00 05 00 7D 05 00 00 05 09 09 0A 0A 0A 0A 0A\n\n190: 0A 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00\n\n1A0: 00 00 00 00 C6 07 00 00 F8 8A DE 27 28 0B 00 00\n\n1B0: 00 00 00 00 01 00 10 00 2B 07 12 7F 0F 00 00 00\n\n1C0: 21 BF AC 3D 26 00 00 00 00 00 00 00 00 00 00 00\n\n1D0: 01 00 01 00 00 00 00 00 29 25 00 00 01 00 00 00\n\n1E0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 03 03\n\n1F0: 00 00 00 00 00 00 00 00 00 00 14 19 00 00 00 B6\n\n&amp;#x200B;\n\n\\-- SMART\\_READ\\_THRESHOLD ----------------------------------------------------\n\n\\+0 +1 +2 +3 +4 +5 +6 +7 +8 +9 +A +B +C +D +E +F\n\n000: 01 00 01 2C 00 00 00 00 00 00 00 00 00 00 03 00\n\n010: 00 00 00 00 00 00 00 00 00 00 04 14 00 00 00 00\n\n020: 00 00 00 00 00 00 05 0A 00 00 00 00 00 00 00 00\n\n030: 00 00 07 2D 00 00 00 00 00 00 00 00 00 00 09 00\n\n040: 00 00 00 00 00 00 00 00 00 00 0A 61 00 00 00 00\n\n050: 00 00 00 00 00 00 0C 14 00 00 00 00 00 00 00 00\n\n060: 00 00 12 32 00 00 00 00 00 00 00 00 00 00 BB 00\n\n070: 00 00 00 00 00 00 00 00 00 00 BC 00 00 00 00 00\n\n080: 00 00 00 00 00 00 BE 00 00 00 00 00 00 00 00 00\n\n090: 00 00 C0 00 00 00 00 00 00 00 00 00 00 00 C1 00\n\n0A0: 00 00 00 00 00 00 00 00 00 00 C2 00 00 00 00 00\n\n0B0: 00 00 00 00 00 00 C5 00 00 00 00 00 00 00 00 00\n\n0C0: 00 00 C6 00 00 00 00 00 00 00 00 00 00 00 C7 00\n\n0D0: 00 00 00 00 00 00 00 00 00 00 C8 01 00 00 00 00\n\n0E0: 00 00 00 00 00 00 F0 00 00 00 00 00 00 00 00 00\n\n0F0: 00 00 F1 00 00 00 00 00 00 00 00 00 00 00 F2 00\n\n100: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n110: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n120: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n130: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n140: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n150: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n160: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n170: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n180: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n190: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n1A0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n1B0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n1C0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n1D0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n1E0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n1F0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 36", "author_fullname": "t2_54jotbjm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SMART values of Exos drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17m4inc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698934713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My 20 TB Exos drive was throwing I/O errors in a Synology NAS. Support asked me to replace it. The drive is only 4 months old. &lt;/p&gt;\n\n&lt;p&gt;Would anyone know if the SMART values are bad?&lt;/p&gt;\n\n&lt;p&gt;(03) ST20000NM007D-3DJ103&lt;/p&gt;\n\n&lt;p&gt;----------------------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;Model : ST20000NM007D-3DJ103&lt;/p&gt;\n\n&lt;p&gt;Firmware : SN03&lt;/p&gt;\n\n&lt;p&gt;Serial Number : *****&lt;/p&gt;\n\n&lt;p&gt;Disk Size : 20000.5 GB (8.4/137.4/20000.5/20000.5)&lt;/p&gt;\n\n&lt;p&gt;Buffer Size : Unknown&lt;/p&gt;\n\n&lt;p&gt;Queue Depth : 32&lt;/p&gt;\n\n&lt;p&gt;# of Sectors : 39063650304&lt;/p&gt;\n\n&lt;p&gt;Rotation Rate : 7200 RPM&lt;/p&gt;\n\n&lt;p&gt;Interface : UASP (Serial ATA)&lt;/p&gt;\n\n&lt;p&gt;Major Version : ACS-4&lt;/p&gt;\n\n&lt;p&gt;Minor Version : ----&lt;/p&gt;\n\n&lt;p&gt;Transfer Mode : SATA/600 | SATA/600&lt;/p&gt;\n\n&lt;p&gt;Power On Hours : 3407 hours&lt;/p&gt;\n\n&lt;p&gt;Power On Count : 1510 count&lt;/p&gt;\n\n&lt;p&gt;Temperature : 40 C (104 F)&lt;/p&gt;\n\n&lt;p&gt;Health Status : Good&lt;/p&gt;\n\n&lt;p&gt;Features : S.M.A.R.T., NCQ, Streaming, GPL&lt;/p&gt;\n\n&lt;p&gt;APM Level : ----&lt;/p&gt;\n\n&lt;p&gt;AAM Level : ----&lt;/p&gt;\n\n&lt;p&gt;Drive Letter : X:&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;-- S.M.A.R.T. --------------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;ID Cur Wor Thr RawValues(6) Attribute Name&lt;/p&gt;\n\n&lt;p&gt;01 _77 _64 _44 000002ADC4B2 Read Error Rate&lt;/p&gt;\n\n&lt;p&gt;03 _91 _90 __0 000000000000 Spin-Up Time&lt;/p&gt;\n\n&lt;p&gt;04 _99 _99 _20 0000000006BD Start/Stop Count&lt;/p&gt;\n\n&lt;p&gt;05 100 100 _10 000000000000 Reallocated Sectors Count&lt;/p&gt;\n\n&lt;p&gt;07 _84 _60 _45 00000EDEBE70 Seek Error Rate&lt;/p&gt;\n\n&lt;p&gt;09 _97 _97 __0 000000000D4F Power-On Hours&lt;/p&gt;\n\n&lt;p&gt;0A 100 100 _97 000000000000 Spin Retry Count&lt;/p&gt;\n\n&lt;p&gt;0C _99 _99 _20 0000000005E6 Power Cycle Count&lt;/p&gt;\n\n&lt;p&gt;12 100 100 _50 000000000000 Vendor Specific&lt;/p&gt;\n\n&lt;p&gt;BB 100 100 __0 000000000000 Reported Uncorrectable Errors&lt;/p&gt;\n\n&lt;p&gt;BC 100 100 __0 000000000000 Command Timeout&lt;/p&gt;\n\n&lt;p&gt;BE _60 _53 __0 000028240028 Airflow Temperature&lt;/p&gt;\n\n&lt;p&gt;C0 100 100 __0 000000000004 Power-off Retract Count&lt;/p&gt;\n\n&lt;p&gt;C1 100 100 __0 0000000006F6 Load/Unload Cycle Count&lt;/p&gt;\n\n&lt;p&gt;C2 _40 _47 __0 001400000028 Temperature&lt;/p&gt;\n\n&lt;p&gt;C5 100 100 __0 000000000000 Current Pending Sector Count&lt;/p&gt;\n\n&lt;p&gt;C6 100 100 __0 000000000000 Uncorrectable Sector Count&lt;/p&gt;\n\n&lt;p&gt;C7 200 200 __0 000000000000 UltraDMA CRC Error Count&lt;/p&gt;\n\n&lt;p&gt;C8 100 100 __1 000000000000 Write Error Rate&lt;/p&gt;\n\n&lt;p&gt;F0 100 100 __0 ABCA00000D43 Head Flying Hours&lt;/p&gt;\n\n&lt;p&gt;F1 100 253 __0 000F7F12072B Total Host Writes&lt;/p&gt;\n\n&lt;p&gt;F2 100 253 __0 00263DACBF21 Total Host Reads&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;-- IDENTIFY_DEVICE ---------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;0    1    2    3    4    5    6    7    8    9&lt;/p&gt;\n\n&lt;p&gt;000: 0C5A 3FFF C837 0010 0000 0000 003F 0000 0000 0000&lt;/p&gt;\n\n&lt;p&gt;010: 2020 2020 2020 2020 2020 2020 5A56 5438 4554 5457&lt;/p&gt;\n\n&lt;p&gt;020: 0000 0000 0000 534E 3033 2020 2020 5354 3230 3030&lt;/p&gt;\n\n&lt;p&gt;030: 304E 4D30 3037 442D 3344 4A31 3033 2020 2020 2020&lt;/p&gt;\n\n&lt;p&gt;040: 2020 2020 2020 2020 2020 2020 2020 8010 4000 2F00&lt;/p&gt;\n\n&lt;p&gt;050: 4000 0200 0200 0007 3FFF 0010 003F FC10 00FB 5D10&lt;/p&gt;\n\n&lt;p&gt;060: FFFF 0FFF 0000 0407 0003 0078 0078 0078 0078 000C&lt;/p&gt;\n\n&lt;p&gt;070: 0000 0000 0000 0000 0000 001F 8D0E 0046 00CC 0000&lt;/p&gt;\n\n&lt;p&gt;080: 0FE0 FFFF 306B 7561 6173 3069 B441 6173 007F 8350&lt;/p&gt;\n\n&lt;p&gt;090: 8350 0000 FFFE 0000 FE00 1000 0000 0000 2710 0000&lt;/p&gt;\n\n&lt;p&gt;100: 0000 1860 0009 0000 0000 0000 6003 0000 5000 C500&lt;/p&gt;\n\n&lt;p&gt;110: E664 8865 0000 0000 0000 0000 0000 0000 0000 43DE&lt;/p&gt;\n\n&lt;p&gt;120: 401C 0000 0000 0000 0000 0000 0000 0000 0021 0000&lt;/p&gt;\n\n&lt;p&gt;130: 1860 0000 1860 2020 0002 0140 0100 5000 3C06 3C0A&lt;/p&gt;\n\n&lt;p&gt;140: 0000 003C 0000 0008 0000 0000 FDFF 0280 0000 0470&lt;/p&gt;\n\n&lt;p&gt;150: 0008 0000 0000 1E33 0000 8008 0000 02F7 D400 8048&lt;/p&gt;\n\n&lt;p&gt;160: 0000 0000 0000 0000 0000 0000 0000 0000 0002 0000&lt;/p&gt;\n\n&lt;p&gt;170: 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000&lt;/p&gt;\n\n&lt;p&gt;180: 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000&lt;/p&gt;\n\n&lt;p&gt;190: 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000&lt;/p&gt;\n\n&lt;p&gt;200: 0000 0000 0000 0000 0000 0000 70BD 0000 0000 4000&lt;/p&gt;\n\n&lt;p&gt;210: 0000 0000 0000 0000 0000 0000 0000 1C20 0000 0000&lt;/p&gt;\n\n&lt;p&gt;220: 0000 0000 11FF 0000 0000 0000 0000 0000 0000 0000&lt;/p&gt;\n\n&lt;p&gt;230: 0000 1860 0009 0000 0000 0000 0000 0000 0000 0000&lt;/p&gt;\n\n&lt;p&gt;240: 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000&lt;/p&gt;\n\n&lt;p&gt;250: 0000 0000 0000 0000 0000 BAA5&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;-- SMART_READ_DATA ---------------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;+0 +1 +2 +3 +4 +5 +6 +7 +8 +9 +A +B +C +D +E +F&lt;/p&gt;\n\n&lt;p&gt;000: 0A 00 01 0F 00 4D 40 B2 C4 AD 02 00 00 00 03 03&lt;/p&gt;\n\n&lt;p&gt;010: 00 5B 5A 00 00 00 00 00 00 00 04 32 00 63 63 BD&lt;/p&gt;\n\n&lt;p&gt;020: 06 00 00 00 00 00 05 33 00 64 64 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;030: 00 00 07 0F 00 54 3C 70 BE DE 0E 00 00 00 09 32&lt;/p&gt;\n\n&lt;p&gt;040: 00 61 61 4F 0D 00 00 00 00 00 0A 13 00 64 64 00&lt;/p&gt;\n\n&lt;p&gt;050: 00 00 00 00 00 00 0C 32 00 63 63 E6 05 00 00 00&lt;/p&gt;\n\n&lt;p&gt;060: 00 00 12 0B 00 64 64 00 00 00 00 00 00 00 BB 32&lt;/p&gt;\n\n&lt;p&gt;070: 00 64 64 00 00 00 00 00 00 00 BC 32 00 64 64 00&lt;/p&gt;\n\n&lt;p&gt;080: 00 00 00 00 00 00 BE 22 00 3C 35 28 00 24 28 00&lt;/p&gt;\n\n&lt;p&gt;090: 00 00 C0 32 00 64 64 04 00 00 00 00 00 00 C1 32&lt;/p&gt;\n\n&lt;p&gt;0A0: 00 64 64 F6 06 00 00 00 00 00 C2 22 00 28 2F 28&lt;/p&gt;\n\n&lt;p&gt;0B0: 00 00 00 14 00 00 C5 12 00 64 64 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;0C0: 00 00 C6 10 00 64 64 00 00 00 00 00 00 00 C7 3E&lt;/p&gt;\n\n&lt;p&gt;0D0: 00 C8 C8 00 00 00 00 00 00 00 C8 23 00 64 64 00&lt;/p&gt;\n\n&lt;p&gt;0E0: 00 00 00 00 00 00 F0 00 00 64 64 43 0D 00 00 CA&lt;/p&gt;\n\n&lt;p&gt;0F0: AB 1D F1 00 00 64 FD 2B 07 12 7F 0F 00 00 F2 00&lt;/p&gt;\n\n&lt;p&gt;100: 00 64 FD 21 BF AC 3D 26 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;110: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;120: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;130: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;140: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;150: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;160: 00 00 00 00 00 00 00 00 00 00 82 00 2F 02 00 7B&lt;/p&gt;\n\n&lt;p&gt;170: 03 00 01 00 01 FF 02 9F 06 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;180: 00 00 05 00 7D 05 00 00 05 09 09 0A 0A 0A 0A 0A&lt;/p&gt;\n\n&lt;p&gt;190: 0A 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1A0: 00 00 00 00 C6 07 00 00 F8 8A DE 27 28 0B 00 00&lt;/p&gt;\n\n&lt;p&gt;1B0: 00 00 00 00 01 00 10 00 2B 07 12 7F 0F 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1C0: 21 BF AC 3D 26 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1D0: 01 00 01 00 00 00 00 00 29 25 00 00 01 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1E0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 03 03&lt;/p&gt;\n\n&lt;p&gt;1F0: 00 00 00 00 00 00 00 00 00 00 14 19 00 00 00 B6&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;-- SMART_READ_THRESHOLD ----------------------------------------------------&lt;/p&gt;\n\n&lt;p&gt;+0 +1 +2 +3 +4 +5 +6 +7 +8 +9 +A +B +C +D +E +F&lt;/p&gt;\n\n&lt;p&gt;000: 01 00 01 2C 00 00 00 00 00 00 00 00 00 00 03 00&lt;/p&gt;\n\n&lt;p&gt;010: 00 00 00 00 00 00 00 00 00 00 04 14 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;020: 00 00 00 00 00 00 05 0A 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;030: 00 00 07 2D 00 00 00 00 00 00 00 00 00 00 09 00&lt;/p&gt;\n\n&lt;p&gt;040: 00 00 00 00 00 00 00 00 00 00 0A 61 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;050: 00 00 00 00 00 00 0C 14 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;060: 00 00 12 32 00 00 00 00 00 00 00 00 00 00 BB 00&lt;/p&gt;\n\n&lt;p&gt;070: 00 00 00 00 00 00 00 00 00 00 BC 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;080: 00 00 00 00 00 00 BE 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;090: 00 00 C0 00 00 00 00 00 00 00 00 00 00 00 C1 00&lt;/p&gt;\n\n&lt;p&gt;0A0: 00 00 00 00 00 00 00 00 00 00 C2 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;0B0: 00 00 00 00 00 00 C5 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;0C0: 00 00 C6 00 00 00 00 00 00 00 00 00 00 00 C7 00&lt;/p&gt;\n\n&lt;p&gt;0D0: 00 00 00 00 00 00 00 00 00 00 C8 01 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;0E0: 00 00 00 00 00 00 F0 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;0F0: 00 00 F1 00 00 00 00 00 00 00 00 00 00 00 F2 00&lt;/p&gt;\n\n&lt;p&gt;100: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;110: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;120: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;130: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;140: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;150: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;160: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;170: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;180: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;190: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1A0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1B0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1C0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1D0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1E0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00&lt;/p&gt;\n\n&lt;p&gt;1F0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 36&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m4inc", "is_robot_indexable": true, "report_reasons": null, "author": "kp86uphai", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17m4inc/smart_values_of_exos_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17m4inc/smart_values_of_exos_drive/", "subreddit_subscribers": 709971, "created_utc": 1698934713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have the movie \"The Prince of Egypt\" on my computer but it's not in my native language. I've been able to find the movie dubbed but can't really download it.  \n\n\nSo my question is: Can I record the audio from a web page while the movie plays and then use handbrake to add the audio as another track? What software would I use?", "author_fullname": "t2_148bvb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Merge audio and video files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17m3m3c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698932081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have the movie &amp;quot;The Prince of Egypt&amp;quot; on my computer but it&amp;#39;s not in my native language. I&amp;#39;ve been able to find the movie dubbed but can&amp;#39;t really download it.  &lt;/p&gt;\n\n&lt;p&gt;So my question is: Can I record the audio from a web page while the movie plays and then use handbrake to add the audio as another track? What software would I use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m3m3c", "is_robot_indexable": true, "report_reasons": null, "author": "japgcf", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17m3m3c/merge_audio_and_video_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17m3m3c/merge_audio_and_video_files/", "subreddit_subscribers": 709971, "created_utc": 1698932081.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I managed to get a 14\" MacBook Pro on the cheap and think it's time to finally sort out a decent automated backup solution:  \n\n\n\u2022 Syncthing between this MBP and an older 2015 MBP for occasional backups (both have 1TB SSDs).\n\n\u2022 KopiaUI for direct backup to Scaleway Glacier (hopefully never needed, but you never know...)\n\nI have KopiaUI working okay so far, but it doesn't seem to support multiple backup destinations...\n\nI'd like to have an encrypted remote backup at a relative's house and maybe something like Backblaze B2 for essential files.  \nShould I be looking at Restic instead of KopiaUI or maybe in addition to it?\n\nThanks for your time!", "author_fullname": "t2_fqbiepzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Remote automated backup options for Laptop (MBP)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17m3jbe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698932086.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698931847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I managed to get a 14&amp;quot; MacBook Pro on the cheap and think it&amp;#39;s time to finally sort out a decent automated backup solution:  &lt;/p&gt;\n\n&lt;p&gt;\u2022 Syncthing between this MBP and an older 2015 MBP for occasional backups (both have 1TB SSDs).&lt;/p&gt;\n\n&lt;p&gt;\u2022 KopiaUI for direct backup to Scaleway Glacier (hopefully never needed, but you never know...)&lt;/p&gt;\n\n&lt;p&gt;I have KopiaUI working okay so far, but it doesn&amp;#39;t seem to support multiple backup destinations...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to have an encrypted remote backup at a relative&amp;#39;s house and maybe something like Backblaze B2 for essential files.&lt;br/&gt;\nShould I be looking at Restic instead of KopiaUI or maybe in addition to it?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your time!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m3jbe", "is_robot_indexable": true, "report_reasons": null, "author": "aaronsarginson", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17m3jbe/remote_automated_backup_options_for_laptop_mbp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17m3jbe/remote_automated_backup_options_for_laptop_mbp/", "subreddit_subscribers": 709971, "created_utc": 1698931847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "On Linux, I  use RAID  mirror (mdadm - software reaid). How to configure it so that data is  read from both underlying devices and compared, to detect data bitrot  and fail in such case?\n\nIs that at all supported?\n\nWould be good option to have.\n\n(Of course as one of layers of protection, for important data)", "author_fullname": "t2_12iebe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Linux software RAID:", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lzg31", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": "", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698916659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On Linux, I  use RAID  mirror (mdadm - software reaid). How to configure it so that data is  read from both underlying devices and compared, to detect data bitrot  and fail in such case?&lt;/p&gt;\n\n&lt;p&gt;Is that at all supported?&lt;/p&gt;\n\n&lt;p&gt;Would be good option to have.&lt;/p&gt;\n\n&lt;p&gt;(Of course as one of layers of protection, for important data)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lzg31", "is_robot_indexable": true, "report_reasons": null, "author": "metalzip", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17lzg31/linux_software_raid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lzg31/linux_software_raid/", "subreddit_subscribers": 709971, "created_utc": 1698916659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there,\n\nI bought two WD RED 14 TB drives (7200 RPM) recently for mass storage, and added all of my data to both of them which are now synced via 'rsync'.\n\nMy issue is this:\n\n\\- Every 5 secs, I can hear like a 'data read' type noise --&gt; Not necessarily as bad as a hard drive failure click, but something noticeable. Its kind of slight loud, and hard to ignore. It doesn't sound awful, but I can't like 'not hear it' which is getting annoying.\n\nI notice them in tandem since they are the same drive aka as soon as 5 secs go by --&gt; one makes the read noise, and the other makes a read noise shortly there after.\n\nI am using Linux so I observed this issue on the same machine via Ubuntu Server, and Linux Mint.\n\n&amp;#x200B;\n\nAnyone else notice this? Is this normal? Asking since I can probably just zero out the drives and return them, but would need to know if I can maybe force it to do this kind of idle read every few minutes or something.\n\n&amp;#x200B;\n\nFigured I would ask you guys, thanks.", "author_fullname": "t2_9kx87h0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question Regarding Default 'Read' Click For WD RED 14 TB Drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lt0kf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698891101.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there,&lt;/p&gt;\n\n&lt;p&gt;I bought two WD RED 14 TB drives (7200 RPM) recently for mass storage, and added all of my data to both of them which are now synced via &amp;#39;rsync&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;My issue is this:&lt;/p&gt;\n\n&lt;p&gt;- Every 5 secs, I can hear like a &amp;#39;data read&amp;#39; type noise --&amp;gt; Not necessarily as bad as a hard drive failure click, but something noticeable. Its kind of slight loud, and hard to ignore. It doesn&amp;#39;t sound awful, but I can&amp;#39;t like &amp;#39;not hear it&amp;#39; which is getting annoying.&lt;/p&gt;\n\n&lt;p&gt;I notice them in tandem since they are the same drive aka as soon as 5 secs go by --&amp;gt; one makes the read noise, and the other makes a read noise shortly there after.&lt;/p&gt;\n\n&lt;p&gt;I am using Linux so I observed this issue on the same machine via Ubuntu Server, and Linux Mint.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyone else notice this? Is this normal? Asking since I can probably just zero out the drives and return them, but would need to know if I can maybe force it to do this kind of idle read every few minutes or something.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Figured I would ask you guys, thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lt0kf", "is_robot_indexable": true, "report_reasons": null, "author": "BackToPlebbit69", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lt0kf/question_regarding_default_read_click_for_wd_red/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lt0kf/question_regarding_default_read_click_for_wd_red/", "subreddit_subscribers": 709971, "created_utc": 1698891101.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'd like to archive the content of a twitter account with its tweets and images. Is this possible? I was able to download just the images with gallery-dl but as it is an art history account, I'd like to have the text of the tweet associated with it because it contains the source of the art piece. \nIs there a way to do this?", "author_fullname": "t2_sjvvz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Utility to download all third party twitter account tweets and images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17loke2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698878770.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to archive the content of a twitter account with its tweets and images. Is this possible? I was able to download just the images with gallery-dl but as it is an art history account, I&amp;#39;d like to have the text of the tweet associated with it because it contains the source of the art piece. \nIs there a way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17loke2", "is_robot_indexable": true, "report_reasons": null, "author": "aManIsNoOneEither", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17loke2/utility_to_download_all_third_party_twitter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17loke2/utility_to_download_all_third_party_twitter/", "subreddit_subscribers": 709971, "created_utc": 1698878770.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been a pretty faithful with backups for decades, changing tools and services with the flow, and I'm in the process of reviewing my current practices to see if they are a good fit for my needs. Would love to hear suggestions. I'm running an M1 Macbook Pro with 1TB internal drive.\n\nCurrent strategy:\n\n1. Time Machine for local backup of laptop\n2. iDrive for remote backup of laptop plus a couple of external drives (currently have 5TB storage plan)\n\nMy goals:\n\n1. Have a good local backup in case of drive failure\n2. Have a good remote backup of laptop\n3. Long-term archival storage of photos, videos, documents, etc., both local and remote. These are not working files and are not stored on the laptop. Current volume is in the 2TB range.\n4. Backup including iPhone stuff (photos and whatnot)\n\nTools I have on hand:\n\n1. Various external hard drives, mostly USB3, in the 2-5 TB range\n2. Chronosync, Carbon Copy Cloner\n\nI'm not willing to put in the time and energy to build a NAS.\n\nI used to store my archive files on Amazon Drive but that's going away and Amazon Photos is HORRIBLE so I don't even consider it an option, plus I have a lot of files that aren't photo/video.\n\nUsed to have Crashplan for years until they got rid of personal accounts back when. Switched to iDrive. I've played with Backblaze a bit but am alarmed by what I've read here about the viability of the company.\n\nShould I look into some type of cloud storage (not backup) for the archive stuff? Or try to hobble along with it as part of a personal backup service (having it back up as an external drive)? Or just get cloud storage and use something like Chronosync to back up to disk images and upload those to the cloud?\n\nThanks for your suggestions.", "author_fullname": "t2_d7hme", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for suggestions to create a good backup plan", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lksdo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698868775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been a pretty faithful with backups for decades, changing tools and services with the flow, and I&amp;#39;m in the process of reviewing my current practices to see if they are a good fit for my needs. Would love to hear suggestions. I&amp;#39;m running an M1 Macbook Pro with 1TB internal drive.&lt;/p&gt;\n\n&lt;p&gt;Current strategy:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Time Machine for local backup of laptop&lt;/li&gt;\n&lt;li&gt;iDrive for remote backup of laptop plus a couple of external drives (currently have 5TB storage plan)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My goals:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Have a good local backup in case of drive failure&lt;/li&gt;\n&lt;li&gt;Have a good remote backup of laptop&lt;/li&gt;\n&lt;li&gt;Long-term archival storage of photos, videos, documents, etc., both local and remote. These are not working files and are not stored on the laptop. Current volume is in the 2TB range.&lt;/li&gt;\n&lt;li&gt;Backup including iPhone stuff (photos and whatnot)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Tools I have on hand:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Various external hard drives, mostly USB3, in the 2-5 TB range&lt;/li&gt;\n&lt;li&gt;Chronosync, Carbon Copy Cloner&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;m not willing to put in the time and energy to build a NAS.&lt;/p&gt;\n\n&lt;p&gt;I used to store my archive files on Amazon Drive but that&amp;#39;s going away and Amazon Photos is HORRIBLE so I don&amp;#39;t even consider it an option, plus I have a lot of files that aren&amp;#39;t photo/video.&lt;/p&gt;\n\n&lt;p&gt;Used to have Crashplan for years until they got rid of personal accounts back when. Switched to iDrive. I&amp;#39;ve played with Backblaze a bit but am alarmed by what I&amp;#39;ve read here about the viability of the company.&lt;/p&gt;\n\n&lt;p&gt;Should I look into some type of cloud storage (not backup) for the archive stuff? Or try to hobble along with it as part of a personal backup service (having it back up as an external drive)? Or just get cloud storage and use something like Chronosync to back up to disk images and upload those to the cloud?&lt;/p&gt;\n\n&lt;p&gt;Thanks for your suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lksdo", "is_robot_indexable": true, "report_reasons": null, "author": "debit72", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lksdo/looking_for_suggestions_to_create_a_good_backup/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lksdo/looking_for_suggestions_to_create_a_good_backup/", "subreddit_subscribers": 709971, "created_utc": 1698868775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "See link - price is at $119.99 for 10TB drive; what am I missing? That seems ridiculously good.\n\n&amp;#x200B;\n\n[https://www.bhphotovideo.com/c/product/1759582-REG/toshiba\\_hdwg51axzstb\\_n300\\_nas\\_pro\\_10tb.html](https://www.bhphotovideo.com/c/product/1759582-REG/toshiba_hdwg51axzstb_n300_nas_pro_10tb.html)", "author_fullname": "t2_bzazl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Really good deal on Toshiba 10TB N300 Pro NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lic3q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698862186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;See link - price is at $119.99 for 10TB drive; what am I missing? That seems ridiculously good.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.bhphotovideo.com/c/product/1759582-REG/toshiba_hdwg51axzstb_n300_nas_pro_10tb.html\"&gt;https://www.bhphotovideo.com/c/product/1759582-REG/toshiba_hdwg51axzstb_n300_nas_pro_10tb.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lic3q", "is_robot_indexable": true, "report_reasons": null, "author": "Tempestshade", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lic3q/really_good_deal_on_toshiba_10tb_n300_pro_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lic3q/really_good_deal_on_toshiba_10tb_n300_pro_nas/", "subreddit_subscribers": 709971, "created_utc": 1698862186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all, was wondering if there was an app or convenient way to batch download saved and upvoted images from my profile. I saw some Python scripts floating around but unfortunately I\u2019m a neanderthal and don\u2019t know how to use Python", "author_fullname": "t2_2w622ddn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apps that batch download saved/upvoted images?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17lfohj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698855180.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, was wondering if there was an app or convenient way to batch download saved and upvoted images from my profile. I saw some Python scripts floating around but unfortunately I\u2019m a neanderthal and don\u2019t know how to use Python&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17lfohj", "is_robot_indexable": true, "report_reasons": null, "author": "PickleInTheSun", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/17lfohj/apps_that_batch_download_savedupvoted_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17lfohj/apps_that_batch_download_savedupvoted_images/", "subreddit_subscribers": 709971, "created_utc": 1698855180.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've recently came into possession of a few HGST HUH721010AL4200 drives (10TB SAS 4kn) and some LSI/Broadcom/Avago 9211-8i and 9305-16i HBAs.\n\nI've been setting them up in packs of 12 drives in a ZFS RAIDZ2 on Debian 12 with a 9305-16i HBA. Driver is mpt3sas (v43.100.00.00 according to dmesg).\n\nInitial checks of the drives turned out just fine, around 45k to 50k hours of power-on time, no bad sectors reported. I've moved some files (around 20TB) on them and then a drive was reported as faulted by ZFS due to read errors. Not to worry, I have loads of spares from the bunch, but that spare almost immediately faulted due to read errors when the resilver was running. Bad luck I thought, but then a third disk also had the same issue. Yet, SMART didn't record any bad sectors or other defects. The issue reported in dmesg was a SCSI command time-out (longer than 30 seconds, which is the default). Raising the time-out to 60 seconds made the issue go away, but made ZFS slow as hell.\n\nNow I started suspecting the HBA, replaced it by the same model, newest firmware. Same issues. While testing other drives faulted as well. Each time I would recreate the RAIDZ2 pool from scratch, fill it with garbage data and start scrubbing while writing to create additional stress.\n\nBad cables maybe? For the 9305-16i I had to buy new cables, SFF8643 to SFF8087. It would be really bad luck to have bought 4 faulty cables, so I switched the HBAs to two 9211-8i and put back the SFF8087 cables which worked for years and years. Same issues, same drives, again.\n\nCould the backplanes be faulty? These also worked for years and years without any issues. Nonetheless, I plugged those drives directly to SFF8643-&gt;4xSATA and SFF8087-&gt;4xSATA cables, same issues.\n\nNow I've also swapped the mainboard to a Supermicro X10SDV-F just to rule that out: same issues. Also I updated the drives' firmware to the most recent one to no avail.\n\nAnother box with a 12 drive RAIDZ2 pool I have built started showing the same symptoms, but this is another different mainboard, case, backplanes and PSU. Only similarities are the OS, drive model and the HBAs, thus the same driver.\n\nI dropped Debian on the larger box and installed TrueNAS Core, it's FreeBSD with a different driver for those HBAs. Lo and behold, it ran the stress tests for days without so much as a hickup. So it's the driver? I reinstalled Debian and ZFS and updated the driver to the newest one available from Broadcom (47.00.00.00). Everything worked just fine from there.\n\n&amp;#x200B;\n\nHas anyone encountered this (recently)? I searched everywhere for similar cases and found nothing fitting my situation. I would think my combination of hardware is not that special to cause such an edge case of driver issues that goes unnoticed by others, especially when the 9211-8i HBA is one of the most popular models out there.\n\nAll in all, I would've prevented all this headache and work by just swapping the driver, but I went down the hardware road.\n\n&amp;#x200B;\n\nLarge Storage:\n\n* Intel Xeon E3-1240Lv3\n* Supermicro X10SLL-F\n* 16GB DDR3 ECC RAM\n* 550W PSU\n* 1x Areca ARC1280ML RAID Controller\n* Norco 4224 Case\n* 2x LSI 9211-8i / 1x LSI 9305-16i\n* 12x HGST HUH721010AL4200 on LSI and ZFS RAIDZ2\n* 12x WD Red 6TB (Pre SMR era) on Areca as RAID6, to be replaced by 12x HGST HUH721212AL4200\n\n&amp;#x200B;\n\nSmall Storage:\n\n* Intel Xeon E3-1230Lv2\n* Supermicro X9SCM\n* 16GB DDR3 ECC RAM\n* 500W PSU\n* some Fantec 12-bay case\n* 2x LSI 9211-8i\n* 12x HGST HUH721010AL4200 on LSI and ZFS RAIDZ2\n\n&amp;#x200B;", "author_fullname": "t2_tnsdm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Avago/LSI/Broadcom mpt3sas driver issues on Debian 12", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17m0afd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1698922724.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1698920400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently came into possession of a few HGST HUH721010AL4200 drives (10TB SAS 4kn) and some LSI/Broadcom/Avago 9211-8i and 9305-16i HBAs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been setting them up in packs of 12 drives in a ZFS RAIDZ2 on Debian 12 with a 9305-16i HBA. Driver is mpt3sas (v43.100.00.00 according to dmesg).&lt;/p&gt;\n\n&lt;p&gt;Initial checks of the drives turned out just fine, around 45k to 50k hours of power-on time, no bad sectors reported. I&amp;#39;ve moved some files (around 20TB) on them and then a drive was reported as faulted by ZFS due to read errors. Not to worry, I have loads of spares from the bunch, but that spare almost immediately faulted due to read errors when the resilver was running. Bad luck I thought, but then a third disk also had the same issue. Yet, SMART didn&amp;#39;t record any bad sectors or other defects. The issue reported in dmesg was a SCSI command time-out (longer than 30 seconds, which is the default). Raising the time-out to 60 seconds made the issue go away, but made ZFS slow as hell.&lt;/p&gt;\n\n&lt;p&gt;Now I started suspecting the HBA, replaced it by the same model, newest firmware. Same issues. While testing other drives faulted as well. Each time I would recreate the RAIDZ2 pool from scratch, fill it with garbage data and start scrubbing while writing to create additional stress.&lt;/p&gt;\n\n&lt;p&gt;Bad cables maybe? For the 9305-16i I had to buy new cables, SFF8643 to SFF8087. It would be really bad luck to have bought 4 faulty cables, so I switched the HBAs to two 9211-8i and put back the SFF8087 cables which worked for years and years. Same issues, same drives, again.&lt;/p&gt;\n\n&lt;p&gt;Could the backplanes be faulty? These also worked for years and years without any issues. Nonetheless, I plugged those drives directly to SFF8643-&amp;gt;4xSATA and SFF8087-&amp;gt;4xSATA cables, same issues.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;ve also swapped the mainboard to a Supermicro X10SDV-F just to rule that out: same issues. Also I updated the drives&amp;#39; firmware to the most recent one to no avail.&lt;/p&gt;\n\n&lt;p&gt;Another box with a 12 drive RAIDZ2 pool I have built started showing the same symptoms, but this is another different mainboard, case, backplanes and PSU. Only similarities are the OS, drive model and the HBAs, thus the same driver.&lt;/p&gt;\n\n&lt;p&gt;I dropped Debian on the larger box and installed TrueNAS Core, it&amp;#39;s FreeBSD with a different driver for those HBAs. Lo and behold, it ran the stress tests for days without so much as a hickup. So it&amp;#39;s the driver? I reinstalled Debian and ZFS and updated the driver to the newest one available from Broadcom (47.00.00.00). Everything worked just fine from there.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Has anyone encountered this (recently)? I searched everywhere for similar cases and found nothing fitting my situation. I would think my combination of hardware is not that special to cause such an edge case of driver issues that goes unnoticed by others, especially when the 9211-8i HBA is one of the most popular models out there.&lt;/p&gt;\n\n&lt;p&gt;All in all, I would&amp;#39;ve prevented all this headache and work by just swapping the driver, but I went down the hardware road.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Large Storage:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intel Xeon E3-1240Lv3&lt;/li&gt;\n&lt;li&gt;Supermicro X10SLL-F&lt;/li&gt;\n&lt;li&gt;16GB DDR3 ECC RAM&lt;/li&gt;\n&lt;li&gt;550W PSU&lt;/li&gt;\n&lt;li&gt;1x Areca ARC1280ML RAID Controller&lt;/li&gt;\n&lt;li&gt;Norco 4224 Case&lt;/li&gt;\n&lt;li&gt;2x LSI 9211-8i / 1x LSI 9305-16i&lt;/li&gt;\n&lt;li&gt;12x HGST HUH721010AL4200 on LSI and ZFS RAIDZ2&lt;/li&gt;\n&lt;li&gt;12x WD Red 6TB (Pre SMR era) on Areca as RAID6, to be replaced by 12x HGST HUH721212AL4200&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Small Storage:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Intel Xeon E3-1230Lv2&lt;/li&gt;\n&lt;li&gt;Supermicro X9SCM&lt;/li&gt;\n&lt;li&gt;16GB DDR3 ECC RAM&lt;/li&gt;\n&lt;li&gt;500W PSU&lt;/li&gt;\n&lt;li&gt;some Fantec 12-bay case&lt;/li&gt;\n&lt;li&gt;2x LSI 9211-8i&lt;/li&gt;\n&lt;li&gt;12x HGST HUH721010AL4200 on LSI and ZFS RAIDZ2&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "140TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "17m0afd", "is_robot_indexable": true, "report_reasons": null, "author": "_mrplow", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/17m0afd/avagolsibroadcom_mpt3sas_driver_issues_on_debian/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/17m0afd/avagolsibroadcom_mpt3sas_driver_issues_on_debian/", "subreddit_subscribers": 709971, "created_utc": 1698920400.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}