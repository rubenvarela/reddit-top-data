{"kind": "Listing", "data": {"after": null, "dist": 24, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering,\n\nFor anyone that says this is my fault for specializing in Microsoft stack - you're absolutely, 100% correct. I blame only myself. \n\nThe incessant cycle of \"progress\". I'm reaching my wit's end with how we're handling tech debt. It seems like every other year, there's a new 'bright new day' in the Microsoft analytics stack, and it's driving me nuts.\n\nFirst off, let's address the myth of avoiding tech debt. Spoiler alert: it's a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year's innovation is this year's digital paperweight.\n\nIt's a merry-go-round of mediocrity So, what do we do? We slap a new 'notebook' GUI over Spark clusters and pat ourselves on the back for 'innovation.' It's a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever's been rebranded this week, with awards handed out for sales volume, not product quality. \n\nWe've all heard the mantras: \"ADF is the way,\" \"Databricks is the way,\" \"Synapse is the way,\" \"Fabric is the way.\" It's just a parade of platforms, each hailed as the messiah of data engineering, but they're not, they're very naughty boys, only to be replaced by the next shiny thing in a year or two.\n\nI (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and 'platnum's to it.", "author_fullname": "t2_17e8xe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft data products - merry-go-round of mediocrity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vxmth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 179, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 179, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700066411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;For anyone that says this is my fault for specializing in Microsoft stack - you&amp;#39;re absolutely, 100% correct. I blame only myself. &lt;/p&gt;\n\n&lt;p&gt;The incessant cycle of &amp;quot;progress&amp;quot;. I&amp;#39;m reaching my wit&amp;#39;s end with how we&amp;#39;re handling tech debt. It seems like every other year, there&amp;#39;s a new &amp;#39;bright new day&amp;#39; in the Microsoft analytics stack, and it&amp;#39;s driving me nuts.&lt;/p&gt;\n\n&lt;p&gt;First off, let&amp;#39;s address the myth of avoiding tech debt. Spoiler alert: it&amp;#39;s a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year&amp;#39;s innovation is this year&amp;#39;s digital paperweight.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a merry-go-round of mediocrity So, what do we do? We slap a new &amp;#39;notebook&amp;#39; GUI over Spark clusters and pat ourselves on the back for &amp;#39;innovation.&amp;#39; It&amp;#39;s a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever&amp;#39;s been rebranded this week, with awards handed out for sales volume, not product quality. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve all heard the mantras: &amp;quot;ADF is the way,&amp;quot; &amp;quot;Databricks is the way,&amp;quot; &amp;quot;Synapse is the way,&amp;quot; &amp;quot;Fabric is the way.&amp;quot; It&amp;#39;s just a parade of platforms, each hailed as the messiah of data engineering, but they&amp;#39;re not, they&amp;#39;re very naughty boys, only to be replaced by the next shiny thing in a year or two.&lt;/p&gt;\n\n&lt;p&gt;I (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and &amp;#39;platnum&amp;#39;s to it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vxmth", "is_robot_indexable": true, "report_reasons": null, "author": "biowl", "discussion_type": null, "num_comments": 95, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vxmth/microsoft_data_products_merrygoround_of_mediocrity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vxmth/microsoft_data_products_merrygoround_of_mediocrity/", "subreddit_subscribers": 139965, "created_utc": 1700066411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! \ud83d\udc4b\n\nAs a junior data engineer still finding my footing in this vast field, I've been pondering over the trajectory of our tools and platforms. I'm eager to hear from the more seasoned data engineers, leads, and others about your take on this.\n\nI've observed a growing trend towards centralized solutions, particularly with platforms like DataBricks and Microsoft Azure. They are continually unveiling new features and tools, which is undoubtedly a sign of progress. However, this leads me to question if we're heading towards an overly centralized future in data engineering. \n\nThe heart of my concern is whether these platforms might be turning into a sort of never-ending learning treadmill and we data professionals end up 'trapped' inside the ecosystems of Azure, DataBricks, etc. On one hand, it's fantastic to have constant innovations, but on the other, it feels overwhelming at times. As we delve deeper into ecosystems like DataBricks and Azure, are we risking losing sight of the broader landscape? Is there a danger of becoming too specialised in these environments to the detriment of our versatility? Doesn't it give DataBricks and Azure more control over your decision-making and data?\n\nBeing relatively new to this field, I'm completely open to the idea that my concerns might be misplaced, and I'm really keen on understanding different perspectives. I'm all for specialisation, but I also value adaptability and a diverse skill set.\n\nI'm looking forward to hearing from the veterans and experts in this community. Do you think these concerns are valid, or is this simply the nature of our evolving field? How do you balance the need to stay current with specific platforms while maintaining a broad skill set?\n\nThanks for taking the time to share your wisdom and insights! Really looking forward to this discussion!\n\nEdit: sorry, had a duplicate paragraph in there somehow...", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Junior's perspective on centralised tools like DataBricks and Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w0ml8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 43, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 43, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700139602.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700074166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;As a junior data engineer still finding my footing in this vast field, I&amp;#39;ve been pondering over the trajectory of our tools and platforms. I&amp;#39;m eager to hear from the more seasoned data engineers, leads, and others about your take on this.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve observed a growing trend towards centralized solutions, particularly with platforms like DataBricks and Microsoft Azure. They are continually unveiling new features and tools, which is undoubtedly a sign of progress. However, this leads me to question if we&amp;#39;re heading towards an overly centralized future in data engineering. &lt;/p&gt;\n\n&lt;p&gt;The heart of my concern is whether these platforms might be turning into a sort of never-ending learning treadmill and we data professionals end up &amp;#39;trapped&amp;#39; inside the ecosystems of Azure, DataBricks, etc. On one hand, it&amp;#39;s fantastic to have constant innovations, but on the other, it feels overwhelming at times. As we delve deeper into ecosystems like DataBricks and Azure, are we risking losing sight of the broader landscape? Is there a danger of becoming too specialised in these environments to the detriment of our versatility? Doesn&amp;#39;t it give DataBricks and Azure more control over your decision-making and data?&lt;/p&gt;\n\n&lt;p&gt;Being relatively new to this field, I&amp;#39;m completely open to the idea that my concerns might be misplaced, and I&amp;#39;m really keen on understanding different perspectives. I&amp;#39;m all for specialisation, but I also value adaptability and a diverse skill set.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking forward to hearing from the veterans and experts in this community. Do you think these concerns are valid, or is this simply the nature of our evolving field? How do you balance the need to stay current with specific platforms while maintaining a broad skill set?&lt;/p&gt;\n\n&lt;p&gt;Thanks for taking the time to share your wisdom and insights! Really looking forward to this discussion!&lt;/p&gt;\n\n&lt;p&gt;Edit: sorry, had a duplicate paragraph in there somehow...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w0ml8", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17w0ml8/a_juniors_perspective_on_centralised_tools_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w0ml8/a_juniors_perspective_on_centralised_tools_like/", "subreddit_subscribers": 139965, "created_utc": 1700074166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I built a tool to make it faster/easier to write python scripts that will clean up Excel files. It's mostly targeted towards people who are less technical, or people like me who can never remember the best practice keyword arguments for pd.read\\_csv() lol. \n\nI called it [Computron](https://app.squack.io/?utm_content=dataengineering&amp;utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=v0p3_uifix).   \n\nHere's how it works: \n\n* Upload any messy csv, xlsx, xls, or xlsm file\n* Type out commands for how you want to clean it up\n* Computron builds and executes Python code to follow the command using GPT-4\n* Once you're done, the code can compiled into a stand-alone automation and reused for other files\n* API support for the hosted automations is coming soon \n\nThe thing is I don't want this to be another bullshit AI tool. I'm posting this on a few data-related subreddits, so you guys can [try it](https://app.squack.io/?utm_content=dataengineering&amp;utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=v0p3_uifix) and be brutally honest about how to make it better.   \n\nAs a token of my appreciation for helping, anybody who makes an account at this early stage will have access to all of the paid features forever. I'm also happy to answer any questions, or give anybody a more in depth tutorial.", "author_fullname": "t2_898csv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Data Roomba\" for cleaning up data before onboarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vuaxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700057010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built a tool to make it faster/easier to write python scripts that will clean up Excel files. It&amp;#39;s mostly targeted towards people who are less technical, or people like me who can never remember the best practice keyword arguments for pd.read_csv() lol. &lt;/p&gt;\n\n&lt;p&gt;I called it &lt;a href=\"https://app.squack.io/?utm_content=dataengineering&amp;amp;utm_medium=social&amp;amp;utm_source=reddit&amp;amp;utm_campaign=v0p3_uifix\"&gt;Computron&lt;/a&gt;.   &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upload any messy csv, xlsx, xls, or xlsm file&lt;/li&gt;\n&lt;li&gt;Type out commands for how you want to clean it up&lt;/li&gt;\n&lt;li&gt;Computron builds and executes Python code to follow the command using GPT-4&lt;/li&gt;\n&lt;li&gt;Once you&amp;#39;re done, the code can compiled into a stand-alone automation and reused for other files&lt;/li&gt;\n&lt;li&gt;API support for the hosted automations is coming soon &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The thing is I don&amp;#39;t want this to be another bullshit AI tool. I&amp;#39;m posting this on a few data-related subreddits, so you guys can &lt;a href=\"https://app.squack.io/?utm_content=dataengineering&amp;amp;utm_medium=social&amp;amp;utm_source=reddit&amp;amp;utm_campaign=v0p3_uifix\"&gt;try it&lt;/a&gt; and be brutally honest about how to make it better.   &lt;/p&gt;\n\n&lt;p&gt;As a token of my appreciation for helping, anybody who makes an account at this early stage will have access to all of the paid features forever. I&amp;#39;m also happy to answer any questions, or give anybody a more in depth tutorial.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vuaxr", "is_robot_indexable": true, "report_reasons": null, "author": "evilredpanda", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vuaxr/data_roomba_for_cleaning_up_data_before_onboarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vuaxr/data_roomba_for_cleaning_up_data_before_onboarding/", "subreddit_subscribers": 139965, "created_utc": 1700057010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at an SME and am building out our DBT infra. Currently a full DBT run on 60 models takes about 12 minutes without any incremental models. Interested to hear just how big these projects get, and potentially some ways to optimise as it grows in size.", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long are your DBT Runs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wdvh8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700108467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at an SME and am building out our DBT infra. Currently a full DBT run on 60 models takes about 12 minutes without any incremental models. Interested to hear just how big these projects get, and potentially some ways to optimise as it grows in size.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17wdvh8", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wdvh8/how_long_are_your_dbt_runs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wdvh8/how_long_are_your_dbt_runs/", "subreddit_subscribers": 139965, "created_utc": 1700108467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm very new to engineering but am working in the Microsoft stack...\n\nWhat I am trying to solve - \n\nDaily at 9 am, I need to load a pipe-delimited file from an SFTP server with about 300k lines into a SQL database and then execute a series of SPs on the data. \n\nOnce finished processing, I need to export a response file and drop it to the same SFTP server.\n\nWhat would be the best Azure tools to explore to perform the following activities?\n\nThanks.", "author_fullname": "t2_lqwsjkf1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Azure tools should I explore to ingest a pipe-delimited file from an SFTP server, bulk load it into a SQL database, and then run a few stored procedures on the data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wb46u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700100622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m very new to engineering but am working in the Microsoft stack...&lt;/p&gt;\n\n&lt;p&gt;What I am trying to solve - &lt;/p&gt;\n\n&lt;p&gt;Daily at 9 am, I need to load a pipe-delimited file from an SFTP server with about 300k lines into a SQL database and then execute a series of SPs on the data. &lt;/p&gt;\n\n&lt;p&gt;Once finished processing, I need to export a response file and drop it to the same SFTP server.&lt;/p&gt;\n\n&lt;p&gt;What would be the best Azure tools to explore to perform the following activities?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17wb46u", "is_robot_indexable": true, "report_reasons": null, "author": "busythread", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wb46u/what_azure_tools_should_i_explore_to_ingest_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wb46u/what_azure_tools_should_i_explore_to_ingest_a/", "subreddit_subscribers": 139965, "created_utc": 1700100622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I see a lot of recommendations to never join two fact tables, but I'm not seeing anything for the situation I'm running into where two facts are \"semi-related\" -- call them applications and sales.\n\n1. We have a fact\\_applications and a fact\\_sales.\n2. *Some* applications lead to a sale (customer gets approved and immediately makes a purchase)\n3. *Some* sales have an application (customer with existing count didn't need to re-apply)\n4. So the question arises -- \"Which application (if any) led to this sale?\" or \"Which sale (if any) resulted from this application?\"\n\nTo me, the natural way to reporting on this is to left join one table to the other (by calculating which sale, if any, resulted from an app, and vice versa) -- so we can answer questions like \"X% of approved applications for product A led to a sale, with an average of $$$, vs. Y% for product B with an average of $$$\"\n\nBut, I'm having a hard time finding best practices on dealing with this scenario. These seem clearly not related enough to consolidate into a single fact table, and people have mixed opinions about \"bridge tables\" or whatever we would call them. Does anyone have recommendations?", "author_fullname": "t2_ikd9g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Joining semi-related fact tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w07kw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700073077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see a lot of recommendations to never join two fact tables, but I&amp;#39;m not seeing anything for the situation I&amp;#39;m running into where two facts are &amp;quot;semi-related&amp;quot; -- call them applications and sales.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We have a fact_applications and a fact_sales.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Some&lt;/em&gt; applications lead to a sale (customer gets approved and immediately makes a purchase)&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Some&lt;/em&gt; sales have an application (customer with existing count didn&amp;#39;t need to re-apply)&lt;/li&gt;\n&lt;li&gt;So the question arises -- &amp;quot;Which application (if any) led to this sale?&amp;quot; or &amp;quot;Which sale (if any) resulted from this application?&amp;quot;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To me, the natural way to reporting on this is to left join one table to the other (by calculating which sale, if any, resulted from an app, and vice versa) -- so we can answer questions like &amp;quot;X% of approved applications for product A led to a sale, with an average of $$$, vs. Y% for product B with an average of $$$&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But, I&amp;#39;m having a hard time finding best practices on dealing with this scenario. These seem clearly not related enough to consolidate into a single fact table, and people have mixed opinions about &amp;quot;bridge tables&amp;quot; or whatever we would call them. Does anyone have recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w07kw", "is_robot_indexable": true, "report_reasons": null, "author": "dlb8685", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w07kw/joining_semirelated_fact_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w07kw/joining_semirelated_fact_tables/", "subreddit_subscribers": 139965, "created_utc": 1700073077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone,\n\nThis is the first time I'm designing a data model for an analytics project. I would love to hear some suggestions and guidance. We are using Snowflake + dbt. \n\nThere are collectors which query source systems using API's and do fresh dumps everyday (There could potentially be no change in records other than arrival timestamps). A simple use case right now is we collect list of instances running across all AWS accounts and check for some tag compliance. From receiving the dumps, how do should I plan it? What does dim and fact look like for this? My biggest struggle has been figuring out how to use the timestamp.   \n\n\n&amp;#x200B;", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modelling Suggestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wguez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700118723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt;\n\n&lt;p&gt;This is the first time I&amp;#39;m designing a data model for an analytics project. I would love to hear some suggestions and guidance. We are using Snowflake + dbt. &lt;/p&gt;\n\n&lt;p&gt;There are collectors which query source systems using API&amp;#39;s and do fresh dumps everyday (There could potentially be no change in records other than arrival timestamps). A simple use case right now is we collect list of instances running across all AWS accounts and check for some tag compliance. From receiving the dumps, how do should I plan it? What does dim and fact look like for this? My biggest struggle has been figuring out how to use the timestamp.   &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17wguez", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wguez/data_modelling_suggestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wguez/data_modelling_suggestion/", "subreddit_subscribers": 139965, "created_utc": 1700118723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \nI am working in a start up and we are looking for tools which would help collect data into place from which it could be queried and visualised. \nHowever, when it comes data sources- we have explored a couple of tools such as Grafana and Datadog \n\nOur main sources of data can range from google sheets to salesforce. \n\nI am looking for suggestions which can help with the same. \nThank you!", "author_fullname": "t2_6qhb8q3z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lake- tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wi9kk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700124577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, \nI am working in a start up and we are looking for tools which would help collect data into place from which it could be queried and visualised. \nHowever, when it comes data sources- we have explored a couple of tools such as Grafana and Datadog &lt;/p&gt;\n\n&lt;p&gt;Our main sources of data can range from google sheets to salesforce. &lt;/p&gt;\n\n&lt;p&gt;I am looking for suggestions which can help with the same. \nThank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17wi9kk", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-79605", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wi9kk/data_lake_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wi9kk/data_lake_tools/", "subreddit_subscribers": 139965, "created_utc": 1700124577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nWelcome back to the third episode of our Google Cloud Storage series. Today, I am delving deep into an advanced and vital aspect of storage management on GCP: Retention Policies. Ensure the safety and integrity of your data like never before!\n\n\ud83d\udccc What's Unpacked in this Video:\n\n\u2705 Retention Policy Basics: A brief refresher and deep dive into what retention policies are and why they're crucial.\n\n\u2705 Adding Retention Policies: Learn the step-by-step process of defining and applying retention policies to your data.\n\n\u2705 Locked vs. Unlocked Retention Policies: Understand the differences, benefits, and use cases of both locked and unlocked policies.\n\n\u2705 The Temporary Hold Feature: Grasp the nuances of the temporary hold, its applications, and how it interacts with your set policies.\n\n\u2705 Interactive Demo: A comprehensive hands-on demonstration, making it easier to understand and implement the discussed concepts.\n\n\ud83d\udccc [https://youtu.be/vqlg1gqW9ts](https://youtu.be/vqlg1gqW9ts)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c", "author_fullname": "t2_lbvus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\ude80 \ud83d\udd25 Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"pzw2tn8r3o0c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2d4ed14974c7e7cadd8807c474a68da9bdb36a0"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8bf4fb40cfb4c2c8a25839c1bbc5dc01710958a"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0d6675f08fd0631eef63ec806e1258f1f149a76"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=428b6debd32ab0ce99bae8734a85840ae02eddcd"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7eec6f0eafe2909341d927c4e88165d3eec256f"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6982827dc89892c0c5d73eedde8bfa4b0999c92a"}], "s": {"y": 1080, "x": 1920, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c"}, "id": "pzw2tn8r3o0c1"}}, "name": "t3_17wholn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqlg1gqW9ts?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqlg1gqW9ts?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!\"&gt;&lt;/iframe&gt;", "author_name": "Technical Potpourri", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/vqlg1gqW9ts/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TechnicalPotpourri"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqlg1gqW9ts?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17wholn", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Gcer_uuQphoyKkLywRH9T2pRsIfo5hSMsOqyDitteNY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1700122077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome back to the third episode of our Google Cloud Storage series. Today, I am delving deep into an advanced and vital aspect of storage management on GCP: Retention Policies. Ensure the safety and integrity of your data like never before!&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udccc What&amp;#39;s Unpacked in this Video:&lt;/p&gt;\n\n&lt;p&gt;\u2705 Retention Policy Basics: A brief refresher and deep dive into what retention policies are and why they&amp;#39;re crucial.&lt;/p&gt;\n\n&lt;p&gt;\u2705 Adding Retention Policies: Learn the step-by-step process of defining and applying retention policies to your data.&lt;/p&gt;\n\n&lt;p&gt;\u2705 Locked vs. Unlocked Retention Policies: Understand the differences, benefits, and use cases of both locked and unlocked policies.&lt;/p&gt;\n\n&lt;p&gt;\u2705 The Temporary Hold Feature: Grasp the nuances of the temporary hold, its applications, and how it interacts with your set policies.&lt;/p&gt;\n\n&lt;p&gt;\u2705 Interactive Demo: A comprehensive hands-on demonstration, making it easier to understand and implement the discussed concepts.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udccc &lt;a href=\"https://youtu.be/vqlg1gqW9ts\"&gt;https://youtu.be/vqlg1gqW9ts&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c\"&gt;https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/P_Q1RKy0PZJAvDtHfw5-uomHNk8jLrBZSjX57ILA8c4.jpg?auto=webp&amp;s=0652f395b21761011792c98e7816f735c6d457ee", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/P_Q1RKy0PZJAvDtHfw5-uomHNk8jLrBZSjX57ILA8c4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2315685ef4198dc5b2db596fa80fcda3552c30da", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/P_Q1RKy0PZJAvDtHfw5-uomHNk8jLrBZSjX57ILA8c4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a87bf5d5c4be1fe30904b18677f7a93a3770b90", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/P_Q1RKy0PZJAvDtHfw5-uomHNk8jLrBZSjX57ILA8c4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=417bc55067166501f6b7541105d43da187e14984", "width": 320, "height": 240}], "variants": {}, "id": "wBnYNUoDuJjylFRz-RE-mZHu0PX9jAEYqjoauSFqYGg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17wholn", "is_robot_indexable": true, "report_reasons": null, "author": "suddeb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wholn/mastering_google_cloud_storage_part_3_advanced/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wholn/mastering_google_cloud_storage_part_3_advanced/", "subreddit_subscribers": 139965, "created_utc": 1700122077.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqlg1gqW9ts?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!\"&gt;&lt;/iframe&gt;", "author_name": "Technical Potpourri", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/vqlg1gqW9ts/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TechnicalPotpourri"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2cbhndmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the semantics of streaming SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17w767w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rUps2sFUDPc3RV9N5E2lcSezRTL-oRqmKWOjY_9CYVE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700090431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arroyo.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.arroyo.dev/blog/what-is-streaming-sql", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?auto=webp&amp;s=a10a0e40248bd3a517dae78743e4b7d21bce37ea", "width": 2688, "height": 1792}, "resolutions": [{"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c028cf653aa21af66b426c5e0828a73b6adc593f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3217260728733976a48f0ecb5d63189c21501195", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=30ffef5304b9a5611cd13348b158734444837790", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=66af6aac96a7f10b82b00b73572d504da7520d02", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af926a872160d74a2148fd0cc062e9d6bed27552", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5fdae08319f5b54aed967865a159d7999865c7f", "width": 1080, "height": 720}], "variants": {}, "id": "3MUnRrRKbrvJfv2xkXe_FMbMBDhSS76KEXLCfLUK47M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17w767w", "is_robot_indexable": true, "report_reasons": null, "author": "mwylde_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w767w/what_are_the_semantics_of_streaming_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.arroyo.dev/blog/what-is-streaming-sql", "subreddit_subscribers": 139965, "created_utc": 1700090431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey all,\n\nIn my company we have many jobs running on Powercenter and I was thinking what would be the easiest way to migrate the logics from Powercenter into SQL statements or BTEQs.\n\nAny inputs and experiences of migirating away from Powercenter are highly appreciated!", "author_fullname": "t2_8edxjw39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to replace Informatica Powercenter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w1htc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700076448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey all,&lt;/p&gt;\n\n&lt;p&gt;In my company we have many jobs running on Powercenter and I was thinking what would be the easiest way to migrate the logics from Powercenter into SQL statements or BTEQs.&lt;/p&gt;\n\n&lt;p&gt;Any inputs and experiences of migirating away from Powercenter are highly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w1htc", "is_robot_indexable": true, "report_reasons": null, "author": "Net_Net_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w1htc/how_to_replace_informatica_powercenter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w1htc/how_to_replace_informatica_powercenter/", "subreddit_subscribers": 139965, "created_utc": 1700076448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey team! Hope you're doing fine. Quick question about segment and snowflake integration. I've landed in a company where they use multiple segment projects and data from those projects is loaded hourly to our snowflake datawarehouse. We then use dbt to transform and load this data into the warehouse.   \n\n\nI've noticed segment loads three standard tables (pages, tracks &amp; identifies) for each project, and then for each tracked event generates a new table with the data that already exists in tracks for each event plus the tracked custom data sent in the event payload. \u00bfIs there a way to avoid segment sending all the context fields in the event custom table? I think it's not good to have this data duplicated in both the custom and tracks table, I'm I wrong?\n\nSecondly, I would like to automate the dbt transform step and avoid doing it myself every time a new event is needed for analysis in the DW. Did anybody do that before? What would be the best approach?. Also, I would like to run the whole segment sync as a whole and avoid generating airflow dags for each synchronization. That would save me a lot of time as well.\n\nThanks for reading!", "author_fullname": "t2_36w05yox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Segment &amp; Snowflake Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vvweu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700061621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey team! Hope you&amp;#39;re doing fine. Quick question about segment and snowflake integration. I&amp;#39;ve landed in a company where they use multiple segment projects and data from those projects is loaded hourly to our snowflake datawarehouse. We then use dbt to transform and load this data into the warehouse.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed segment loads three standard tables (pages, tracks &amp;amp; identifies) for each project, and then for each tracked event generates a new table with the data that already exists in tracks for each event plus the tracked custom data sent in the event payload. \u00bfIs there a way to avoid segment sending all the context fields in the event custom table? I think it&amp;#39;s not good to have this data duplicated in both the custom and tracks table, I&amp;#39;m I wrong?&lt;/p&gt;\n\n&lt;p&gt;Secondly, I would like to automate the dbt transform step and avoid doing it myself every time a new event is needed for analysis in the DW. Did anybody do that before? What would be the best approach?. Also, I would like to run the whole segment sync as a whole and avoid generating airflow dags for each synchronization. That would save me a lot of time as well.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vvweu", "is_robot_indexable": true, "report_reasons": null, "author": "asnopm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vvweu/segment_snowflake_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vvweu/segment_snowflake_integration/", "subreddit_subscribers": 139965, "created_utc": 1700061621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking at Datahub, Amundsen and Atlan.\nEach looks great and Im looking to get some reviews from personal experience. \nIf you have experience with more than one tool - even better! \nMy end goal here is to let new employees a great head start and lower dependancy on power users and other sources. \nMainly an holistic view of the data stream", "author_fullname": "t2_9wdo6orq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data catalog tool - reviews needed!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wj6dc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700128430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at Datahub, Amundsen and Atlan.\nEach looks great and Im looking to get some reviews from personal experience. \nIf you have experience with more than one tool - even better! \nMy end goal here is to let new employees a great head start and lower dependancy on power users and other sources. \nMainly an holistic view of the data stream&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17wj6dc", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Abalone703", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wj6dc/data_catalog_tool_reviews_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wj6dc/data_catalog_tool_reviews_needed/", "subreddit_subscribers": 139965, "created_utc": 1700128430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone!\n\n&amp;#x200B;\n\nI work for a non-profit and we recently built a data management system to support our monitoring and evaluation efforts.\n\n&amp;#x200B;\n\nWe have been using Airtable as our data storage solution as its simplicity is very attractive. However, it starts to struggle when datasets get too large, and more so when there are multiple linkages built in. We are now looking for an alternative data storage solution.\n\n&amp;#x200B;\n\nRequirements for the solution:\n\n\\- Fairly simple to use\n\n\\- Relatively low code\n\n\\- Cost effective\n\n\\- Relational database\n\n\\- Cloud based and secure\n\nWe have looked into Google SQL - any thoughts on this?\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_t1mvr7oy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data storage solutions that are low code, secure, cloud based", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wipt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700126477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I work for a non-profit and we recently built a data management system to support our monitoring and evaluation efforts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We have been using Airtable as our data storage solution as its simplicity is very attractive. However, it starts to struggle when datasets get too large, and more so when there are multiple linkages built in. We are now looking for an alternative data storage solution.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Requirements for the solution:&lt;/p&gt;\n\n&lt;p&gt;- Fairly simple to use&lt;/p&gt;\n\n&lt;p&gt;- Relatively low code&lt;/p&gt;\n\n&lt;p&gt;- Cost effective&lt;/p&gt;\n\n&lt;p&gt;- Relational database&lt;/p&gt;\n\n&lt;p&gt;- Cloud based and secure&lt;/p&gt;\n\n&lt;p&gt;We have looked into Google SQL - any thoughts on this?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17wipt9", "is_robot_indexable": true, "report_reasons": null, "author": "yaksurf", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wipt9/data_storage_solutions_that_are_low_code_secure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wipt9/data_storage_solutions_that_are_low_code_secure/", "subreddit_subscribers": 139965, "created_utc": 1700126477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you want audit trails of DB records, is it better to use server logs (e.g. DataDog) or build your own audit\\_logs table? Why would you do one or the other? \n\nIn addition, do business users need access to audit logs? I'm curious if you'd want to build an audit table so that your business teams can query it, if needed. ", "author_fullname": "t2_cqdfu7nf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you audit the history of a database record? Why do you audit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w27za", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700078398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you want audit trails of DB records, is it better to use server logs (e.g. DataDog) or build your own audit_logs table? Why would you do one or the other? &lt;/p&gt;\n\n&lt;p&gt;In addition, do business users need access to audit logs? I&amp;#39;m curious if you&amp;#39;d want to build an audit table so that your business teams can query it, if needed. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w27za", "is_robot_indexable": true, "report_reasons": null, "author": "Different-General700", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w27za/how_do_you_audit_the_history_of_a_database_record/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w27za/how_do_you_audit_the_history_of_a_database_record/", "subreddit_subscribers": 139965, "created_utc": 1700078398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering what other orgs look like for DE teams. We currently have to handle all of our QA as a DE team while our app dev team has a dedicated team of a handful of resources. ", "author_fullname": "t2_81ywblydd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you have a dedicated QA team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vxubk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700066992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering what other orgs look like for DE teams. We currently have to handle all of our QA as a DE team while our app dev team has a dedicated team of a handful of resources. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vxubk", "is_robot_indexable": true, "report_reasons": null, "author": "DataDoyle", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vxubk/do_you_have_a_dedicated_qa_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vxubk/do_you_have_a_dedicated_qa_team/", "subreddit_subscribers": 139965, "created_utc": 1700066992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nAre you concerned about sending your sensitive data to OpenAI?\n\nThen on-premise AI is the solution: deploy an AI model on your own servers instead of sending your data to the cloud.\n\nHere's an article we just wrote at NLP Cloud, explaining what on-premise AI is about, how to implement it, and what the potential challenges are: [https://nlpcloud.com/on-premise-ai-models-edge-ai-for-sensitive-applications.html](https://nlpcloud.com/on-premise-ai-models-edge-ai-for-sensitive-applications.html?utm_source=reddit&amp;utm_campaign=bbcdw625-3816-81ed-a26450242ac140019)\n\nIf you have questions about an on-premise strategy for your project, please let me know, I'll be happy to advise!\n\nJulien", "author_fullname": "t2_4z4m2qcs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Edge AI / On-Premise AI Models For Sensitive Applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17wncmv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700142653.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Are you concerned about sending your sensitive data to OpenAI?&lt;/p&gt;\n\n&lt;p&gt;Then on-premise AI is the solution: deploy an AI model on your own servers instead of sending your data to the cloud.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an article we just wrote at NLP Cloud, explaining what on-premise AI is about, how to implement it, and what the potential challenges are: &lt;a href=\"https://nlpcloud.com/on-premise-ai-models-edge-ai-for-sensitive-applications.html?utm_source=reddit&amp;amp;utm_campaign=bbcdw625-3816-81ed-a26450242ac140019\"&gt;https://nlpcloud.com/on-premise-ai-models-edge-ai-for-sensitive-applications.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you have questions about an on-premise strategy for your project, please let me know, I&amp;#39;ll be happy to advise!&lt;/p&gt;\n\n&lt;p&gt;Julien&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oswAZMo60339FwwkD3PK87jRAlNkQxpbGMkEYAVJbcQ.jpg?auto=webp&amp;s=94dbc709adba202f448112f80e7684c2a9b8d951", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/oswAZMo60339FwwkD3PK87jRAlNkQxpbGMkEYAVJbcQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e7f7ae936b0f2c78ed77797035dd6754d676f3f", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/oswAZMo60339FwwkD3PK87jRAlNkQxpbGMkEYAVJbcQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d247cf760108bdf102c956885db91c1194f0386d", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/oswAZMo60339FwwkD3PK87jRAlNkQxpbGMkEYAVJbcQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=42bc9b7771c28245ca98cae88525f0880e7916b4", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/oswAZMo60339FwwkD3PK87jRAlNkQxpbGMkEYAVJbcQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d36192afc75e5d61cfddea8ac5b3a6dd0ee9feee", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/oswAZMo60339FwwkD3PK87jRAlNkQxpbGMkEYAVJbcQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9ea437a6fe8b792a905aa38cd44659721e971f0", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/oswAZMo60339FwwkD3PK87jRAlNkQxpbGMkEYAVJbcQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=02c4d2a70f8302753987bafca20024eb400e4079", "width": 1080, "height": 607}], "variants": {}, "id": "gk9hmYu8E1DwdN3hjKXUKZQXLUM5X-Ut0Oh1pZnW4Nw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17wncmv", "is_robot_indexable": true, "report_reasons": null, "author": "juliensalinas", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wncmv/edge_ai_onpremise_ai_models_for_sensitive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wncmv/edge_ai_onpremise_ai_models_for_sensitive/", "subreddit_subscribers": 139965, "created_utc": 1700142653.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, organizing a free instructor-led workshop about reaching sub-second analytics over TB-scale datasets using Firebolt (which I work for :)). \n\nWe'll cover:\n\n1. Data modeling and ingestion with performance in mind\n2. Reaching fast query performance using indexes and partitions while requiring less compute\n3. Working with semi-structured data efficiently using JSON functions and dealing with arrays\n4. Q&amp;A and knowledge exchange\n\nJoin here: [https://hi.firebolt.io/lp/hands-on-firebolt-workshop](https://hi.firebolt.io/lp/hands-on-firebolt-workshop)", "author_fullname": "t2_jg3w8gbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hands-on workshop for faster query performance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wkzcu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700135282.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, organizing a free instructor-led workshop about reaching sub-second analytics over TB-scale datasets using Firebolt (which I work for :)). &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ll cover:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Data modeling and ingestion with performance in mind&lt;/li&gt;\n&lt;li&gt;Reaching fast query performance using indexes and partitions while requiring less compute&lt;/li&gt;\n&lt;li&gt;Working with semi-structured data efficiently using JSON functions and dealing with arrays&lt;/li&gt;\n&lt;li&gt;Q&amp;amp;A and knowledge exchange&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Join here: &lt;a href=\"https://hi.firebolt.io/lp/hands-on-firebolt-workshop\"&gt;https://hi.firebolt.io/lp/hands-on-firebolt-workshop&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/wK2crd-bgwiTVOJyn2TfQzruU3yPVpRfNhJfFkHS9gI.jpg?auto=webp&amp;s=a23b766867387c27da7803c1b560fb26497069ff", "width": 1201, "height": 627}, "resolutions": [{"url": "https://external-preview.redd.it/wK2crd-bgwiTVOJyn2TfQzruU3yPVpRfNhJfFkHS9gI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=900d09d18c0c8e887e2bd2354743c171121336b0", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/wK2crd-bgwiTVOJyn2TfQzruU3yPVpRfNhJfFkHS9gI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4389f73f49b308fb5522b7bf328dd9ec54bff1f", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/wK2crd-bgwiTVOJyn2TfQzruU3yPVpRfNhJfFkHS9gI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=264bbb86bb83dfdf4ec23d0c250982da2ffd77d3", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/wK2crd-bgwiTVOJyn2TfQzruU3yPVpRfNhJfFkHS9gI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2cc73ca604b6705cdbaca8bd0e812d3ddc55f3af", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/wK2crd-bgwiTVOJyn2TfQzruU3yPVpRfNhJfFkHS9gI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2855e770901c0620db6b2b3421f8f34a852ae36e", "width": 960, "height": 501}, {"url": "https://external-preview.redd.it/wK2crd-bgwiTVOJyn2TfQzruU3yPVpRfNhJfFkHS9gI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dbb165fe39207f969efaca5923eda64ba91f213b", "width": 1080, "height": 563}], "variants": {}, "id": "H6xwU-hA4_-jwlzfwnpWx1y-gbvvBRO7paRxyFbIDps"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17wkzcu", "is_robot_indexable": true, "report_reasons": null, "author": "tamargal91", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wkzcu/handson_workshop_for_faster_query_performance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wkzcu/handson_workshop_for_faster_query_performance/", "subreddit_subscribers": 139965, "created_utc": 1700135282.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nPower bi \n\nMaking APIs\n\nWeb scraping\n\nBot development", "author_fullname": "t2_hxue1umo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "would these skills benefit me as a data engineer ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w64wu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700088011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Power bi &lt;/p&gt;\n\n&lt;p&gt;Making APIs&lt;/p&gt;\n\n&lt;p&gt;Web scraping&lt;/p&gt;\n\n&lt;p&gt;Bot development&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w64wu", "is_robot_indexable": true, "report_reasons": null, "author": "Single-Sound-1865", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w64wu/would_these_skills_benefit_me_as_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w64wu/would_these_skills_benefit_me_as_a_data_engineer/", "subreddit_subscribers": 139965, "created_utc": 1700088011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys.\n\nCurrently the stack of the company I work at is comprised of a mixture of Cloudera (on-prem and cloud) and Azure Databricks (and the rest of the Azure ecosystem).\n\nRight now, permissions management is kind of a mess. Cloudera Hadoop permissions are managed by Ranger which syncs LDAP groups and then we have policies attached to each group with permissions to use specific databases within Hadoop/HDFS. Our Azure ecossystem permissions model uses IAM Roles (now Entra ID, I guess) to enforce each LDAP group permission's to each ADLS storage account and container.\n\nThere are a lot of other things in the mix, but that's the gist of it. \n\nEnforcing permissions is achieved by a mixture of manual, automatic and some terraform (between Azure and CDP) actions.\n\nHow to make sense of all this? How would you tackle trying to unify the permissions model across all these components?\n\nRight now what comes to mind is using Databricks Unity Catalog and try to create a script to sync the permissions to Ranger.", "author_fullname": "t2_pev7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrated permissions management across different providers of on prem and cloud data infrastructures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vyqzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700069345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys.&lt;/p&gt;\n\n&lt;p&gt;Currently the stack of the company I work at is comprised of a mixture of Cloudera (on-prem and cloud) and Azure Databricks (and the rest of the Azure ecosystem).&lt;/p&gt;\n\n&lt;p&gt;Right now, permissions management is kind of a mess. Cloudera Hadoop permissions are managed by Ranger which syncs LDAP groups and then we have policies attached to each group with permissions to use specific databases within Hadoop/HDFS. Our Azure ecossystem permissions model uses IAM Roles (now Entra ID, I guess) to enforce each LDAP group permission&amp;#39;s to each ADLS storage account and container.&lt;/p&gt;\n\n&lt;p&gt;There are a lot of other things in the mix, but that&amp;#39;s the gist of it. &lt;/p&gt;\n\n&lt;p&gt;Enforcing permissions is achieved by a mixture of manual, automatic and some terraform (between Azure and CDP) actions.&lt;/p&gt;\n\n&lt;p&gt;How to make sense of all this? How would you tackle trying to unify the permissions model across all these components?&lt;/p&gt;\n\n&lt;p&gt;Right now what comes to mind is using Databricks Unity Catalog and try to create a script to sync the permissions to Ranger.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vyqzm", "is_robot_indexable": true, "report_reasons": null, "author": "mommylovesme2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vyqzm/integrated_permissions_management_across/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vyqzm/integrated_permissions_management_across/", "subreddit_subscribers": 139965, "created_utc": 1700069345.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS", "author_fullname": "t2_ancj6nr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_17vwc5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "author_name": "The iT Tech Solutions", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/9rzIhRb2qKk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheiTTechSolutions"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17vwc5q", "height": 200}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/fjtk7XwbiXbVLM9hCNd3gxKDCkXWO7aQEZETfYj8Wj4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700062825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/9rzIhRb2qKk", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?auto=webp&amp;s=8ec9545a9f8b1c557ee9af6002b086b698a9e3c7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d48648c7370d7259d4beec81cf28d52300b0afd", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47748d9418c4591d8b33469fab5a501a97708ba4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d8c859dad79ffa253ab9640e50f591be0e3de7e", "width": 320, "height": 240}], "variants": {}, "id": "-ubKmFhXuBQ2j857IyozL8XfyJT9uZB5ca8iMjrzCJU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vwc5q", "is_robot_indexable": true, "report_reasons": null, "author": "Recent_Ocelot_724", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vwc5q/aws_step_function_variables_resultpath_inputpath/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/9rzIhRb2qKk", "subreddit_subscribers": 139965, "created_utc": 1700062825.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "author_name": "The iT Tech Solutions", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/9rzIhRb2qKk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheiTTechSolutions"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_8d5mczd0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What people are missing about timeGPT", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "name": "t3_17wmkgs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9FjJl2ak4pYJadOUjxBAj9uF5rworHzbvxGrVdAPV_k.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700140422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "thdpth.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.thdpth.com/p/what-people-are-missing-about-timegpt", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/J412SbanU7lSkfuGruK2zA13SvQ9DCGkCXPd3Bg7XDU.jpg?auto=webp&amp;s=8c73ca3ef48367f25b1639580e4175103090eaff", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/J412SbanU7lSkfuGruK2zA13SvQ9DCGkCXPd3Bg7XDU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f12b20d7422a40bd8b08f2f9e4ea7fd12e2ea0f", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/J412SbanU7lSkfuGruK2zA13SvQ9DCGkCXPd3Bg7XDU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=56a069429316979e834f0248bddaa74544e6228c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/J412SbanU7lSkfuGruK2zA13SvQ9DCGkCXPd3Bg7XDU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac792c8a4b23399ec3cc95e71738a41b47964c88", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/J412SbanU7lSkfuGruK2zA13SvQ9DCGkCXPd3Bg7XDU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f5373c7e4e98c25f54be118096e38d897d11343b", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/J412SbanU7lSkfuGruK2zA13SvQ9DCGkCXPd3Bg7XDU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e200645466a1deb2da1168879ed2f6491f8495d", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/J412SbanU7lSkfuGruK2zA13SvQ9DCGkCXPd3Bg7XDU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=53ecdf0ad3b0418542e8a8d287195b1c040d6b49", "width": 1080, "height": 540}], "variants": {}, "id": "JWv5tumU07lDeElNRcK2ItdqLz1KUjCA3SsJBfIwhBo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17wmkgs", "is_robot_indexable": true, "report_reasons": null, "author": "sbalnojan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wmkgs/what_people_are_missing_about_timegpt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.thdpth.com/p/what-people-are-missing-about-timegpt", "subreddit_subscribers": 139965, "created_utc": 1700140422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen discussion and videos were they say it's really hard to contribute to the framework just because they accept only from existing commiters. I want to contribute in order to learn, advance my career and give back to the community and I don't want to dig in learning the internal codebase and later have no chance.", "author_fullname": "t2_76srr1mpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it really hard to contribute to Apache Spark [codebase]?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vv8sl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700059797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen discussion and videos were they say it&amp;#39;s really hard to contribute to the framework just because they accept only from existing commiters. I want to contribute in order to learn, advance my career and give back to the community and I don&amp;#39;t want to dig in learning the internal codebase and later have no chance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17vv8sl", "is_robot_indexable": true, "report_reasons": null, "author": "pr6g_head", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17vv8sl/is_it_really_hard_to_contribute_to_apache_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vv8sl/is_it_really_hard_to_contribute_to_apache_spark/", "subreddit_subscribers": 139965, "created_utc": 1700059797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data engineer/analyst, only been doing it for a little over a year now. I was having a conversation with a senior data scientist who believes that much of data engineering such as ETL processes will be done by AI in the future and job roles will move towards AI engineers. Their reaosning was you will be able to type in what you want and AI will ne able to process the raw unfiltered data.Thoughts?", "author_fullname": "t2_6246prx7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL managed solely by AI?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w1d8e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700076100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data engineer/analyst, only been doing it for a little over a year now. I was having a conversation with a senior data scientist who believes that much of data engineering such as ETL processes will be done by AI in the future and job roles will move towards AI engineers. Their reaosning was you will be able to type in what you want and AI will ne able to process the raw unfiltered data.Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w1d8e", "is_robot_indexable": true, "report_reasons": null, "author": "Nospmis_27", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w1d8e/etl_managed_solely_by_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w1d8e/etl_managed_solely_by_ai/", "subreddit_subscribers": 139965, "created_utc": 1700076100.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}