{"kind": "Listing", "data": {"after": "t3_17vt2w7", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering,\n\nFor anyone that says this is my fault for specializing in Microsoft stack - you're absolutely, 100% correct. I blame only myself. \n\nThe incessant cycle of \"progress\". I'm reaching my wit's end with how we're handling tech debt. It seems like every other year, there's a new 'bright new day' in the Microsoft analytics stack, and it's driving me nuts.\n\nFirst off, let's address the myth of avoiding tech debt. Spoiler alert: it's a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year's innovation is this year's digital paperweight.\n\nIt's a merry-go-round of mediocrity So, what do we do? We slap a new 'notebook' GUI over Spark clusters and pat ourselves on the back for 'innovation.' It's a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever's been rebranded this week, with awards handed out for sales volume, not product quality. \n\nWe've all heard the mantras: \"ADF is the way,\" \"Databricks is the way,\" \"Synapse is the way,\" \"Fabric is the way.\" It's just a parade of platforms, each hailed as the messiah of data engineering, but they're not, they're very naughty boys, only to be replaced by the next shiny thing in a year or two.\n\nI (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and 'platnum's to it.", "author_fullname": "t2_17e8xe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft data products - merry-go-round of mediocrity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vxmth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 169, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 169, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700066411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;For anyone that says this is my fault for specializing in Microsoft stack - you&amp;#39;re absolutely, 100% correct. I blame only myself. &lt;/p&gt;\n\n&lt;p&gt;The incessant cycle of &amp;quot;progress&amp;quot;. I&amp;#39;m reaching my wit&amp;#39;s end with how we&amp;#39;re handling tech debt. It seems like every other year, there&amp;#39;s a new &amp;#39;bright new day&amp;#39; in the Microsoft analytics stack, and it&amp;#39;s driving me nuts.&lt;/p&gt;\n\n&lt;p&gt;First off, let&amp;#39;s address the myth of avoiding tech debt. Spoiler alert: it&amp;#39;s a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year&amp;#39;s innovation is this year&amp;#39;s digital paperweight.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a merry-go-round of mediocrity So, what do we do? We slap a new &amp;#39;notebook&amp;#39; GUI over Spark clusters and pat ourselves on the back for &amp;#39;innovation.&amp;#39; It&amp;#39;s a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever&amp;#39;s been rebranded this week, with awards handed out for sales volume, not product quality. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve all heard the mantras: &amp;quot;ADF is the way,&amp;quot; &amp;quot;Databricks is the way,&amp;quot; &amp;quot;Synapse is the way,&amp;quot; &amp;quot;Fabric is the way.&amp;quot; It&amp;#39;s just a parade of platforms, each hailed as the messiah of data engineering, but they&amp;#39;re not, they&amp;#39;re very naughty boys, only to be replaced by the next shiny thing in a year or two.&lt;/p&gt;\n\n&lt;p&gt;I (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and &amp;#39;platnum&amp;#39;s to it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vxmth", "is_robot_indexable": true, "report_reasons": null, "author": "biowl", "discussion_type": null, "num_comments": 91, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vxmth/microsoft_data_products_merrygoround_of_mediocrity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vxmth/microsoft_data_products_merrygoround_of_mediocrity/", "subreddit_subscribers": 139943, "created_utc": 1700066411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! \ud83d\udc4b\n\nAs a junior data engineer still finding my footing in this vast field, I've been pondering over the trajectory of our tools and platforms. I'm eager to hear from the more seasoned data engineers, leads, and others about your take on this.\n\nThe heart of my concern is whether these platforms might be turning into a sort of never-ending learning treadmill and we data professionals end up 'trapped' inside the ecosystems of Azure, DataBricks, etc. On one hand, it's fantastic to have constant innovations, but on the other, it feels overwhelming at times. As we delve deeper into ecosystems like DataBricks and Azure, are we risking losing sight of the broader landscape? Is there a danger of becoming too specialised in these environments to the detriment of our versatility? Doesn't it give DataBricks and Azure more control over your decision-making and data?\n\nBeing relatively new to this field, I'm completely open to the idea that my concerns might be misplaced, and I'm really keen on understanding different perspectives. I'm all for specialisation, but I also value adaptability and a diverse skill set.\n\nI'm looking forward to hearing from the veterans and experts in this community. Do you think these concerns are valid, or is this simply the nature of our evolving field? How do you balance the need to stay current with specific platforms while maintaining a broad skill set?\n\nThanks for taking the time to share your wisdom and insights! Really looking forward to this discussion!\n\nEdit: sorry, had a duplicate paragraph in there somehow...", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Junior's perspective on centralised tools like DataBricks and Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w0ml8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700085247.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700074166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;As a junior data engineer still finding my footing in this vast field, I&amp;#39;ve been pondering over the trajectory of our tools and platforms. I&amp;#39;m eager to hear from the more seasoned data engineers, leads, and others about your take on this.&lt;/p&gt;\n\n&lt;p&gt;The heart of my concern is whether these platforms might be turning into a sort of never-ending learning treadmill and we data professionals end up &amp;#39;trapped&amp;#39; inside the ecosystems of Azure, DataBricks, etc. On one hand, it&amp;#39;s fantastic to have constant innovations, but on the other, it feels overwhelming at times. As we delve deeper into ecosystems like DataBricks and Azure, are we risking losing sight of the broader landscape? Is there a danger of becoming too specialised in these environments to the detriment of our versatility? Doesn&amp;#39;t it give DataBricks and Azure more control over your decision-making and data?&lt;/p&gt;\n\n&lt;p&gt;Being relatively new to this field, I&amp;#39;m completely open to the idea that my concerns might be misplaced, and I&amp;#39;m really keen on understanding different perspectives. I&amp;#39;m all for specialisation, but I also value adaptability and a diverse skill set.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking forward to hearing from the veterans and experts in this community. Do you think these concerns are valid, or is this simply the nature of our evolving field? How do you balance the need to stay current with specific platforms while maintaining a broad skill set?&lt;/p&gt;\n\n&lt;p&gt;Thanks for taking the time to share your wisdom and insights! Really looking forward to this discussion!&lt;/p&gt;\n\n&lt;p&gt;Edit: sorry, had a duplicate paragraph in there somehow...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w0ml8", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17w0ml8/a_juniors_perspective_on_centralised_tools_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w0ml8/a_juniors_perspective_on_centralised_tools_like/", "subreddit_subscribers": 139943, "created_utc": 1700074166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I built a tool to make it faster/easier to write python scripts that will clean up Excel files. It's mostly targeted towards people who are less technical, or people like me who can never remember the best practice keyword arguments for pd.read\\_csv() lol. \n\nI called it [Computron](https://app.squack.io/?utm_content=dataengineering&amp;utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=v0p3_uifix).   \n\nHere's how it works: \n\n* Upload any messy csv, xlsx, xls, or xlsm file\n* Type out commands for how you want to clean it up\n* Computron builds and executes Python code to follow the command using GPT-4\n* Once you're done, the code can compiled into a stand-alone automation and reused for other files\n* API support for the hosted automations is coming soon \n\nThe thing is I don't want this to be another bullshit AI tool. I'm posting this on a few data-related subreddits, so you guys can [try it](https://app.squack.io/?utm_content=dataengineering&amp;utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=v0p3_uifix) and be brutally honest about how to make it better.   \n\nAs a token of my appreciation for helping, anybody who makes an account at this early stage will have access to all of the paid features forever. I'm also happy to answer any questions, or give anybody a more in depth tutorial.", "author_fullname": "t2_898csv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Data Roomba\" for cleaning up data before onboarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vuaxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700057010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built a tool to make it faster/easier to write python scripts that will clean up Excel files. It&amp;#39;s mostly targeted towards people who are less technical, or people like me who can never remember the best practice keyword arguments for pd.read_csv() lol. &lt;/p&gt;\n\n&lt;p&gt;I called it &lt;a href=\"https://app.squack.io/?utm_content=dataengineering&amp;amp;utm_medium=social&amp;amp;utm_source=reddit&amp;amp;utm_campaign=v0p3_uifix\"&gt;Computron&lt;/a&gt;.   &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upload any messy csv, xlsx, xls, or xlsm file&lt;/li&gt;\n&lt;li&gt;Type out commands for how you want to clean it up&lt;/li&gt;\n&lt;li&gt;Computron builds and executes Python code to follow the command using GPT-4&lt;/li&gt;\n&lt;li&gt;Once you&amp;#39;re done, the code can compiled into a stand-alone automation and reused for other files&lt;/li&gt;\n&lt;li&gt;API support for the hosted automations is coming soon &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The thing is I don&amp;#39;t want this to be another bullshit AI tool. I&amp;#39;m posting this on a few data-related subreddits, so you guys can &lt;a href=\"https://app.squack.io/?utm_content=dataengineering&amp;amp;utm_medium=social&amp;amp;utm_source=reddit&amp;amp;utm_campaign=v0p3_uifix\"&gt;try it&lt;/a&gt; and be brutally honest about how to make it better.   &lt;/p&gt;\n\n&lt;p&gt;As a token of my appreciation for helping, anybody who makes an account at this early stage will have access to all of the paid features forever. I&amp;#39;m also happy to answer any questions, or give anybody a more in depth tutorial.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vuaxr", "is_robot_indexable": true, "report_reasons": null, "author": "evilredpanda", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vuaxr/data_roomba_for_cleaning_up_data_before_onboarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vuaxr/data_roomba_for_cleaning_up_data_before_onboarding/", "subreddit_subscribers": 139943, "created_utc": 1700057010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, so we are having some discussions internally about whether or not our data should be immutable and I wanted some sanity checking on how I am thinking about this. Just to give some general comments on how our data is set up currently the primary data that is used is stored in SQL and is used to serve customers with contracts etc on our website. We also have an analytics team that run a service which takes this relational data and processes it into a flat structure into Big Query to make it more easily digestible for analytics tools like Tableau. \n\nAll of the reasons we want to have immutable data make sense to me (being able to see the state of a contract at any given date, having logs of changes to make audits etc). But it also feels like we can accomplish this with an Audit Table and not changing the underlying data structure. To use \"Contract\" Table as an example I'm seeing two different implementations.\n\n1) We have a Contract table that is used by the customer facing services, who would only care about the current state of their contracts which can be updated, and create a separate Contract_Audit table which creates a new row with the current state of a contract whenever a contract gets updated (which can be automatically managed by our DB libraries)\n\n2) We have a Contract Table and whenever a contract is created or a change is made to it we store a new row for that contract. \n\nIf for the core business functionality we only care about the current state of the data, it seems like it would add needless complexity and data when interacting with these tables. While the needs for things like Audits and Analytics to have the full history of changes is served equally well by the Audit tables.\n\nWould love to hear some inputs or if there is anything I'm missing here, thanks!", "author_fullname": "t2_af3ee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Immutable data versus Audit Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vsexn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700050612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, so we are having some discussions internally about whether or not our data should be immutable and I wanted some sanity checking on how I am thinking about this. Just to give some general comments on how our data is set up currently the primary data that is used is stored in SQL and is used to serve customers with contracts etc on our website. We also have an analytics team that run a service which takes this relational data and processes it into a flat structure into Big Query to make it more easily digestible for analytics tools like Tableau. &lt;/p&gt;\n\n&lt;p&gt;All of the reasons we want to have immutable data make sense to me (being able to see the state of a contract at any given date, having logs of changes to make audits etc). But it also feels like we can accomplish this with an Audit Table and not changing the underlying data structure. To use &amp;quot;Contract&amp;quot; Table as an example I&amp;#39;m seeing two different implementations.&lt;/p&gt;\n\n&lt;p&gt;1) We have a Contract table that is used by the customer facing services, who would only care about the current state of their contracts which can be updated, and create a separate Contract_Audit table which creates a new row with the current state of a contract whenever a contract gets updated (which can be automatically managed by our DB libraries)&lt;/p&gt;\n\n&lt;p&gt;2) We have a Contract Table and whenever a contract is created or a change is made to it we store a new row for that contract. &lt;/p&gt;\n\n&lt;p&gt;If for the core business functionality we only care about the current state of the data, it seems like it would add needless complexity and data when interacting with these tables. While the needs for things like Audits and Analytics to have the full history of changes is served equally well by the Audit tables.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear some inputs or if there is anything I&amp;#39;m missing here, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vsexn", "is_robot_indexable": true, "report_reasons": null, "author": "relderpaway", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vsexn/immutable_data_versus_audit_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vsexn/immutable_data_versus_audit_tables/", "subreddit_subscribers": 139943, "created_utc": 1700050612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at an SME and am building out our DBT infra. Currently a full DBT run on 60 models takes about 12 minutes without any incremental models. Interested to hear just how big these projects get, and potentially some ways to optimise as it grows in size.", "author_fullname": "t2_2tntx2vs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long are your DBT Runs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wdvh8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700108467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at an SME and am building out our DBT infra. Currently a full DBT run on 60 models takes about 12 minutes without any incremental models. Interested to hear just how big these projects get, and potentially some ways to optimise as it grows in size.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17wdvh8", "is_robot_indexable": true, "report_reasons": null, "author": "casematta", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wdvh8/how_long_are_your_dbt_runs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wdvh8/how_long_are_your_dbt_runs/", "subreddit_subscribers": 139943, "created_utc": 1700108467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm very new to engineering but am working in the Microsoft stack...\n\nWhat I am trying to solve - \n\nDaily at 9 am, I need to load a pipe-delimited file from an SFTP server with about 300k lines into a SQL database and then execute a series of SPs on the data. \n\nOnce finished processing, I need to export a response file and drop it to the same SFTP server.\n\nWhat would be the best Azure tools to explore to perform the following activities?\n\nThanks.", "author_fullname": "t2_lqwsjkf1m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Azure tools should I explore to ingest a pipe-delimited file from an SFTP server, bulk load it into a SQL database, and then run a few stored procedures on the data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wb46u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700100622.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m very new to engineering but am working in the Microsoft stack...&lt;/p&gt;\n\n&lt;p&gt;What I am trying to solve - &lt;/p&gt;\n\n&lt;p&gt;Daily at 9 am, I need to load a pipe-delimited file from an SFTP server with about 300k lines into a SQL database and then execute a series of SPs on the data. &lt;/p&gt;\n\n&lt;p&gt;Once finished processing, I need to export a response file and drop it to the same SFTP server.&lt;/p&gt;\n\n&lt;p&gt;What would be the best Azure tools to explore to perform the following activities?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17wb46u", "is_robot_indexable": true, "report_reasons": null, "author": "busythread", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wb46u/what_azure_tools_should_i_explore_to_ingest_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wb46u/what_azure_tools_should_i_explore_to_ingest_a/", "subreddit_subscribers": 139943, "created_utc": 1700100622.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I see a lot of recommendations to never join two fact tables, but I'm not seeing anything for the situation I'm running into where two facts are \"semi-related\" -- call them applications and sales.\n\n1. We have a fact\\_applications and a fact\\_sales.\n2. *Some* applications lead to a sale (customer gets approved and immediately makes a purchase)\n3. *Some* sales have an application (customer with existing count didn't need to re-apply)\n4. So the question arises -- \"Which application (if any) led to this sale?\" or \"Which sale (if any) resulted from this application?\"\n\nTo me, the natural way to reporting on this is to left join one table to the other (by calculating which sale, if any, resulted from an app, and vice versa) -- so we can answer questions like \"X% of approved applications for product A led to a sale, with an average of $$$, vs. Y% for product B with an average of $$$\"\n\nBut, I'm having a hard time finding best practices on dealing with this scenario. These seem clearly not related enough to consolidate into a single fact table, and people have mixed opinions about \"bridge tables\" or whatever we would call them. Does anyone have recommendations?", "author_fullname": "t2_ikd9g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Joining semi-related fact tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w07kw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700073077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see a lot of recommendations to never join two fact tables, but I&amp;#39;m not seeing anything for the situation I&amp;#39;m running into where two facts are &amp;quot;semi-related&amp;quot; -- call them applications and sales.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We have a fact_applications and a fact_sales.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Some&lt;/em&gt; applications lead to a sale (customer gets approved and immediately makes a purchase)&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Some&lt;/em&gt; sales have an application (customer with existing count didn&amp;#39;t need to re-apply)&lt;/li&gt;\n&lt;li&gt;So the question arises -- &amp;quot;Which application (if any) led to this sale?&amp;quot; or &amp;quot;Which sale (if any) resulted from this application?&amp;quot;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To me, the natural way to reporting on this is to left join one table to the other (by calculating which sale, if any, resulted from an app, and vice versa) -- so we can answer questions like &amp;quot;X% of approved applications for product A led to a sale, with an average of $$$, vs. Y% for product B with an average of $$$&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But, I&amp;#39;m having a hard time finding best practices on dealing with this scenario. These seem clearly not related enough to consolidate into a single fact table, and people have mixed opinions about &amp;quot;bridge tables&amp;quot; or whatever we would call them. Does anyone have recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w07kw", "is_robot_indexable": true, "report_reasons": null, "author": "dlb8685", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w07kw/joining_semirelated_fact_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w07kw/joining_semirelated_fact_tables/", "subreddit_subscribers": 139943, "created_utc": 1700073077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2cbhndmb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the semantics of streaming SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 93, "top_awarded_type": null, "hide_score": false, "name": "t3_17w767w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rUps2sFUDPc3RV9N5E2lcSezRTL-oRqmKWOjY_9CYVE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700090431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "arroyo.dev", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.arroyo.dev/blog/what-is-streaming-sql", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?auto=webp&amp;s=a10a0e40248bd3a517dae78743e4b7d21bce37ea", "width": 2688, "height": 1792}, "resolutions": [{"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c028cf653aa21af66b426c5e0828a73b6adc593f", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3217260728733976a48f0ecb5d63189c21501195", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=30ffef5304b9a5611cd13348b158734444837790", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=66af6aac96a7f10b82b00b73572d504da7520d02", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=af926a872160d74a2148fd0cc062e9d6bed27552", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/j85bv-hFVJ-XTIEpCXOEYRCrCjsslSHQXv-EsZgGHsI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5fdae08319f5b54aed967865a159d7999865c7f", "width": 1080, "height": 720}], "variants": {}, "id": "3MUnRrRKbrvJfv2xkXe_FMbMBDhSS76KEXLCfLUK47M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17w767w", "is_robot_indexable": true, "report_reasons": null, "author": "mwylde_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w767w/what_are_the_semantics_of_streaming_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.arroyo.dev/blog/what-is-streaming-sql", "subreddit_subscribers": 139943, "created_utc": 1700090431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey team! Hope you're doing fine. Quick question about segment and snowflake integration. I've landed in a company where they use multiple segment projects and data from those projects is loaded hourly to our snowflake datawarehouse. We then use dbt to transform and load this data into the warehouse.   \n\n\nI've noticed segment loads three standard tables (pages, tracks &amp; identifies) for each project, and then for each tracked event generates a new table with the data that already exists in tracks for each event plus the tracked custom data sent in the event payload. \u00bfIs there a way to avoid segment sending all the context fields in the event custom table? I think it's not good to have this data duplicated in both the custom and tracks table, I'm I wrong?\n\nSecondly, I would like to automate the dbt transform step and avoid doing it myself every time a new event is needed for analysis in the DW. Did anybody do that before? What would be the best approach?. Also, I would like to run the whole segment sync as a whole and avoid generating airflow dags for each synchronization. That would save me a lot of time as well.\n\nThanks for reading!", "author_fullname": "t2_36w05yox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Segment &amp; Snowflake Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vvweu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700061621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey team! Hope you&amp;#39;re doing fine. Quick question about segment and snowflake integration. I&amp;#39;ve landed in a company where they use multiple segment projects and data from those projects is loaded hourly to our snowflake datawarehouse. We then use dbt to transform and load this data into the warehouse.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed segment loads three standard tables (pages, tracks &amp;amp; identifies) for each project, and then for each tracked event generates a new table with the data that already exists in tracks for each event plus the tracked custom data sent in the event payload. \u00bfIs there a way to avoid segment sending all the context fields in the event custom table? I think it&amp;#39;s not good to have this data duplicated in both the custom and tracks table, I&amp;#39;m I wrong?&lt;/p&gt;\n\n&lt;p&gt;Secondly, I would like to automate the dbt transform step and avoid doing it myself every time a new event is needed for analysis in the DW. Did anybody do that before? What would be the best approach?. Also, I would like to run the whole segment sync as a whole and avoid generating airflow dags for each synchronization. That would save me a lot of time as well.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vvweu", "is_robot_indexable": true, "report_reasons": null, "author": "asnopm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vvweu/segment_snowflake_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vvweu/segment_snowflake_integration/", "subreddit_subscribers": 139943, "created_utc": 1700061621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, \nI am working in a start up and we are looking for tools which would help collect data into place from which it could be queried and visualised. \nHowever, when it comes data sources- we have explored a couple of tools such as Grafana and Datadog \n\nOur main sources of data can range from google sheets to salesforce. \n\nI am looking for suggestions which can help with the same. \nThank you!", "author_fullname": "t2_6qhb8q3z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data lake- tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wi9kk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700124577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, \nI am working in a start up and we are looking for tools which would help collect data into place from which it could be queried and visualised. \nHowever, when it comes data sources- we have explored a couple of tools such as Grafana and Datadog &lt;/p&gt;\n\n&lt;p&gt;Our main sources of data can range from google sheets to salesforce. &lt;/p&gt;\n\n&lt;p&gt;I am looking for suggestions which can help with the same. \nThank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17wi9kk", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-79605", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wi9kk/data_lake_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wi9kk/data_lake_tools/", "subreddit_subscribers": 139943, "created_utc": 1700124577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nWelcome back to the third episode of our Google Cloud Storage series. Today, I am delving deep into an advanced and vital aspect of storage management on GCP: Retention Policies. Ensure the safety and integrity of your data like never before!\n\n\ud83d\udccc What's Unpacked in this Video:\n\n\u2705 Retention Policy Basics: A brief refresher and deep dive into what retention policies are and why they're crucial.\n\n\u2705 Adding Retention Policies: Learn the step-by-step process of defining and applying retention policies to your data.\n\n\u2705 Locked vs. Unlocked Retention Policies: Understand the differences, benefits, and use cases of both locked and unlocked policies.\n\n\u2705 The Temporary Hold Feature: Grasp the nuances of the temporary hold, its applications, and how it interacts with your set policies.\n\n\u2705 Interactive Demo: A comprehensive hands-on demonstration, making it easier to understand and implement the discussed concepts.\n\n\ud83d\udccc [https://youtu.be/vqlg1gqW9ts](https://youtu.be/vqlg1gqW9ts)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c", "author_fullname": "t2_lbvus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\ude80 \ud83d\udd25 Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "media_metadata": {"pzw2tn8r3o0c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 60, "x": 108, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2d4ed14974c7e7cadd8807c474a68da9bdb36a0"}, {"y": 121, "x": 216, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b8bf4fb40cfb4c2c8a25839c1bbc5dc01710958a"}, {"y": 180, "x": 320, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0d6675f08fd0631eef63ec806e1258f1f149a76"}, {"y": 360, "x": 640, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=428b6debd32ab0ce99bae8734a85840ae02eddcd"}, {"y": 540, "x": 960, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7eec6f0eafe2909341d927c4e88165d3eec256f"}, {"y": 607, "x": 1080, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6982827dc89892c0c5d73eedde8bfa4b0999c92a"}], "s": {"y": 1080, "x": 1920, "u": "https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c"}, "id": "pzw2tn8r3o0c1"}}, "name": "t3_17wholn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqlg1gqW9ts?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqlg1gqW9ts?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!\"&gt;&lt;/iframe&gt;", "author_name": "Technical Potpourri", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/vqlg1gqW9ts/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TechnicalPotpourri"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqlg1gqW9ts?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17wholn", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Gcer_uuQphoyKkLywRH9T2pRsIfo5hSMsOqyDitteNY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1700122077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome back to the third episode of our Google Cloud Storage series. Today, I am delving deep into an advanced and vital aspect of storage management on GCP: Retention Policies. Ensure the safety and integrity of your data like never before!&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udccc What&amp;#39;s Unpacked in this Video:&lt;/p&gt;\n\n&lt;p&gt;\u2705 Retention Policy Basics: A brief refresher and deep dive into what retention policies are and why they&amp;#39;re crucial.&lt;/p&gt;\n\n&lt;p&gt;\u2705 Adding Retention Policies: Learn the step-by-step process of defining and applying retention policies to your data.&lt;/p&gt;\n\n&lt;p&gt;\u2705 Locked vs. Unlocked Retention Policies: Understand the differences, benefits, and use cases of both locked and unlocked policies.&lt;/p&gt;\n\n&lt;p&gt;\u2705 The Temporary Hold Feature: Grasp the nuances of the temporary hold, its applications, and how it interacts with your set policies.&lt;/p&gt;\n\n&lt;p&gt;\u2705 Interactive Demo: A comprehensive hands-on demonstration, making it easier to understand and implement the discussed concepts.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udccc &lt;a href=\"https://youtu.be/vqlg1gqW9ts\"&gt;https://youtu.be/vqlg1gqW9ts&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c\"&gt;https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/P_Q1RKy0PZJAvDtHfw5-uomHNk8jLrBZSjX57ILA8c4.jpg?auto=webp&amp;s=0652f395b21761011792c98e7816f735c6d457ee", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/P_Q1RKy0PZJAvDtHfw5-uomHNk8jLrBZSjX57ILA8c4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2315685ef4198dc5b2db596fa80fcda3552c30da", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/P_Q1RKy0PZJAvDtHfw5-uomHNk8jLrBZSjX57ILA8c4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a87bf5d5c4be1fe30904b18677f7a93a3770b90", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/P_Q1RKy0PZJAvDtHfw5-uomHNk8jLrBZSjX57ILA8c4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=417bc55067166501f6b7541105d43da187e14984", "width": 320, "height": 240}], "variants": {}, "id": "wBnYNUoDuJjylFRz-RE-mZHu0PX9jAEYqjoauSFqYGg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17wholn", "is_robot_indexable": true, "report_reasons": null, "author": "suddeb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wholn/mastering_google_cloud_storage_part_3_advanced/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wholn/mastering_google_cloud_storage_part_3_advanced/", "subreddit_subscribers": 139943, "created_utc": 1700122077.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/vqlg1gqW9ts?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!\"&gt;&lt;/iframe&gt;", "author_name": "Technical Potpourri", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/vqlg1gqW9ts/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TechnicalPotpourri"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone,\n\nThis is the first time I'm designing a data model for an analytics project. I would love to hear some suggestions and guidance. We are using Snowflake + dbt. \n\nThere are collectors which query source systems using API's and do fresh dumps everyday (There could potentially be no change in records other than arrival timestamps). A simple use case right now is we collect list of instances running across all AWS accounts and check for some tag compliance. From receiving the dumps, how do should I plan it? What does dim and fact look like for this? My biggest struggle has been figuring out how to use the timestamp.   \n\n\n&amp;#x200B;", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Modelling Suggestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wguez", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700118723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone,&lt;/p&gt;\n\n&lt;p&gt;This is the first time I&amp;#39;m designing a data model for an analytics project. I would love to hear some suggestions and guidance. We are using Snowflake + dbt. &lt;/p&gt;\n\n&lt;p&gt;There are collectors which query source systems using API&amp;#39;s and do fresh dumps everyday (There could potentially be no change in records other than arrival timestamps). A simple use case right now is we collect list of instances running across all AWS accounts and check for some tag compliance. From receiving the dumps, how do should I plan it? What does dim and fact look like for this? My biggest struggle has been figuring out how to use the timestamp.   &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17wguez", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wguez/data_modelling_suggestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wguez/data_modelling_suggestion/", "subreddit_subscribers": 139943, "created_utc": 1700118723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nPower bi \n\nMaking APIs\n\nWeb scraping\n\nBot development", "author_fullname": "t2_hxue1umo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "would these skills benefit me as a data engineer ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w64wu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700088011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Power bi &lt;/p&gt;\n\n&lt;p&gt;Making APIs&lt;/p&gt;\n\n&lt;p&gt;Web scraping&lt;/p&gt;\n\n&lt;p&gt;Bot development&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w64wu", "is_robot_indexable": true, "report_reasons": null, "author": "Single-Sound-1865", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w64wu/would_these_skills_benefit_me_as_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w64wu/would_these_skills_benefit_me_as_a_data_engineer/", "subreddit_subscribers": 139943, "created_utc": 1700088011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you want audit trails of DB records, is it better to use server logs (e.g. DataDog) or build your own audit\\_logs table? Why would you do one or the other? \n\nIn addition, do business users need access to audit logs? I'm curious if you'd want to build an audit table so that your business teams can query it, if needed. ", "author_fullname": "t2_cqdfu7nf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you audit the history of a database record? Why do you audit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w27za", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700078398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you want audit trails of DB records, is it better to use server logs (e.g. DataDog) or build your own audit_logs table? Why would you do one or the other? &lt;/p&gt;\n\n&lt;p&gt;In addition, do business users need access to audit logs? I&amp;#39;m curious if you&amp;#39;d want to build an audit table so that your business teams can query it, if needed. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w27za", "is_robot_indexable": true, "report_reasons": null, "author": "Different-General700", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w27za/how_do_you_audit_the_history_of_a_database_record/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w27za/how_do_you_audit_the_history_of_a_database_record/", "subreddit_subscribers": 139943, "created_utc": 1700078398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey all,\n\nIn my company we have many jobs running on Powercenter and I was thinking what would be the easiest way to migrate the logics from Powercenter into SQL statements or BTEQs.\n\nAny inputs and experiences of migirating away from Powercenter are highly appreciated!", "author_fullname": "t2_8edxjw39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to replace Informatica Powercenter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w1htc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700076448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey all,&lt;/p&gt;\n\n&lt;p&gt;In my company we have many jobs running on Powercenter and I was thinking what would be the easiest way to migrate the logics from Powercenter into SQL statements or BTEQs.&lt;/p&gt;\n\n&lt;p&gt;Any inputs and experiences of migirating away from Powercenter are highly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w1htc", "is_robot_indexable": true, "report_reasons": null, "author": "Net_Net_", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w1htc/how_to_replace_informatica_powercenter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w1htc/how_to_replace_informatica_powercenter/", "subreddit_subscribers": 139943, "created_utc": 1700076448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering what other orgs look like for DE teams. We currently have to handle all of our QA as a DE team while our app dev team has a dedicated team of a handful of resources. ", "author_fullname": "t2_81ywblydd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you have a dedicated QA team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vxubk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700066992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering what other orgs look like for DE teams. We currently have to handle all of our QA as a DE team while our app dev team has a dedicated team of a handful of resources. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vxubk", "is_robot_indexable": true, "report_reasons": null, "author": "DataDoyle", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vxubk/do_you_have_a_dedicated_qa_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vxubk/do_you_have_a_dedicated_qa_team/", "subreddit_subscribers": 139943, "created_utc": 1700066992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a computational biologist (8 YoE) in big pharma. I have been home brewing databases for my department among other things. Recently, during a re-org upheaval I was told I could be a good fit for a Sr Data Engineer role at my company, a good fit in the sense that I bring domain expertise and they can give me on-hands training in data engineering. The HM specifically said \u201cI can teach you Data Engineering, I can\u2019t teach you Biology\u201d.\nI am genuinely considering this offer as I have been contemplating a switch to Data Science/Engineering for the past year. I was wondering if anyone in the group has made a similar lateral switch, and what your experience was. I am curious how math heavy such a role would be, I have been told they use BigQuery, DataBricks, Onyx and elements of GCP and Kubernetes, but I\u2019ll be honest I have no experience in any of these. Also, generally how stable is a career in data engineering? Is domain expertise really that important in your opinion?", "author_fullname": "t2_s5s0xh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about a lateral switch into data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vtv0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700055645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a computational biologist (8 YoE) in big pharma. I have been home brewing databases for my department among other things. Recently, during a re-org upheaval I was told I could be a good fit for a Sr Data Engineer role at my company, a good fit in the sense that I bring domain expertise and they can give me on-hands training in data engineering. The HM specifically said \u201cI can teach you Data Engineering, I can\u2019t teach you Biology\u201d.\nI am genuinely considering this offer as I have been contemplating a switch to Data Science/Engineering for the past year. I was wondering if anyone in the group has made a similar lateral switch, and what your experience was. I am curious how math heavy such a role would be, I have been told they use BigQuery, DataBricks, Onyx and elements of GCP and Kubernetes, but I\u2019ll be honest I have no experience in any of these. Also, generally how stable is a career in data engineering? Is domain expertise really that important in your opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17vtv0o", "is_robot_indexable": true, "report_reasons": null, "author": "EuphoricArtichoke", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vtv0o/question_about_a_lateral_switch_into_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vtv0o/question_about_a_lateral_switch_into_data/", "subreddit_subscribers": 139943, "created_utc": 1700055645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning to a Data Product Ecosystem: Leveraging the Evolutionary Architecture 4D Architectures, Data-Driven Routing, Feature Toggles, and more!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17vt8mn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/6vDlkW5g9Ov2fi4qiqucF_O3EGlGVFi-VIH-dXwFbK4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700053598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/transitioning-to-a-data-product-ecosystem", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?auto=webp&amp;s=3082285d046bb96e056e3beb6bab7cc485567de3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=644cca4795796eb921dd4625ff9c54b794d4b0eb", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ea9e084b2aafd7bd8605a6427295eea54585448", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae64c7f1f9a6e6d90348bb507812c8dcba804134", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9b2005f4e3b121435e3c44ed1cf042df746c0fd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d546543f7d963a212d8f96dc95c922942a33bce4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=943e28e4ddb97fe923c7bfb089bcbaa4234bb8d4", "width": 1080, "height": 540}], "variants": {}, "id": "JUVxsJydfJfdVJZ75jZtmnS48hsaGgbV73xyphmtuyA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vt8mn", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vt8mn/transitioning_to_a_data_product_ecosystem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/transitioning-to-a-data-product-ecosystem", "subreddit_subscribers": 139943, "created_utc": 1700053598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have already passed SAA 2 weeks ago. I have 3 years of experience as a DE. Thanks", "author_fullname": "t2_p9gvk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I take AWS Data Analytics or new DE associate cert?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vt23e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700053004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have already passed SAA 2 weeks ago. I have 3 years of experience as a DE. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17vt23e", "is_robot_indexable": true, "report_reasons": null, "author": "marcelorojas56", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vt23e/should_i_take_aws_data_analytics_or_new_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vt23e/should_i_take_aws_data_analytics_or_new_de/", "subreddit_subscribers": 139943, "created_utc": 1700053004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking at Datahub, Amundsen and Atlan.\nEach looks great and Im looking to get some reviews from personal experience. \nIf you have experience with more than one tool - even better! \nMy end goal here is to let new employees a great head start and lower dependancy on power users and other sources. \nMainly an holistic view of the data stream", "author_fullname": "t2_9wdo6orq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data catalog tool - reviews needed!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17wj6dc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700128430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at Datahub, Amundsen and Atlan.\nEach looks great and Im looking to get some reviews from personal experience. \nIf you have experience with more than one tool - even better! \nMy end goal here is to let new employees a great head start and lower dependancy on power users and other sources. \nMainly an holistic view of the data stream&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17wj6dc", "is_robot_indexable": true, "report_reasons": null, "author": "Old-Abalone703", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wj6dc/data_catalog_tool_reviews_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wj6dc/data_catalog_tool_reviews_needed/", "subreddit_subscribers": 139943, "created_utc": 1700128430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Everyone!\n\n&amp;#x200B;\n\nI work for a non-profit and we recently built a data management system to support our monitoring and evaluation efforts.\n\n&amp;#x200B;\n\nWe have been using Airtable as our data storage solution as its simplicity is very attractive. However, it starts to struggle when datasets get too large, and more so when there are multiple linkages built in. We are now looking for an alternative data storage solution.\n\n&amp;#x200B;\n\nRequirements for the solution:\n\n\\- Fairly simple to use\n\n\\- Relatively low code\n\n\\- Cost effective\n\n\\- Relational database\n\n\\- Cloud based and secure\n\nWe have looked into Google SQL - any thoughts on this?\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_t1mvr7oy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data storage solutions that are low code, secure, cloud based", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17wipt9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700126477.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I work for a non-profit and we recently built a data management system to support our monitoring and evaluation efforts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We have been using Airtable as our data storage solution as its simplicity is very attractive. However, it starts to struggle when datasets get too large, and more so when there are multiple linkages built in. We are now looking for an alternative data storage solution.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Requirements for the solution:&lt;/p&gt;\n\n&lt;p&gt;- Fairly simple to use&lt;/p&gt;\n\n&lt;p&gt;- Relatively low code&lt;/p&gt;\n\n&lt;p&gt;- Cost effective&lt;/p&gt;\n\n&lt;p&gt;- Relational database&lt;/p&gt;\n\n&lt;p&gt;- Cloud based and secure&lt;/p&gt;\n\n&lt;p&gt;We have looked into Google SQL - any thoughts on this?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17wipt9", "is_robot_indexable": true, "report_reasons": null, "author": "yaksurf", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17wipt9/data_storage_solutions_that_are_low_code_secure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17wipt9/data_storage_solutions_that_are_low_code_secure/", "subreddit_subscribers": 139943, "created_utc": 1700126477.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nWelcome back to the third episode of our Google Cloud Storage series. Today, I am delving deep into an advanced and vital aspect of storage management on GCP: Retention Policies. Ensure the safety and integrity of your data like never before!\n\n\ud83d\udccc What's Unpacked in this Video:\n\n\u2705 Retention Policy Basics: A brief refresher and deep dive into what retention policies are and why they're crucial.\n\n\u2705 Adding Retention Policies: Learn the step-by-step process of defining and applying retention policies to your data.\n\n\u2705 Locked vs. Unlocked Retention Policies: Understand the differences, benefits, and use cases of both locked and unlocked policies.\n\n\u2705 The Temporary Hold Feature: Grasp the nuances of the temporary hold, its applications, and how it interacts with your set policies.\n\n\u2705 Interactive Demo: A comprehensive hands-on demonstration, making it easier to understand and implement the discussed concepts.\n\n\ud83d\udccc [https://youtu.be/vqlg1gqW9ts](https://youtu.be/vqlg1gqW9ts)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c", "author_fullname": "t2_lbvus", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\ude80 \ud83d\udd25 Mastering Google Cloud Storage Part 3 - Advanced Concepts - Diving Deep into Retention Policies!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17whojj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700122072.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Welcome back to the third episode of our Google Cloud Storage series. Today, I am delving deep into an advanced and vital aspect of storage management on GCP: Retention Policies. Ensure the safety and integrity of your data like never before!&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udccc What&amp;#39;s Unpacked in this Video:&lt;/p&gt;\n\n&lt;p&gt;\u2705 Retention Policy Basics: A brief refresher and deep dive into what retention policies are and why they&amp;#39;re crucial.&lt;/p&gt;\n\n&lt;p&gt;\u2705 Adding Retention Policies: Learn the step-by-step process of defining and applying retention policies to your data.&lt;/p&gt;\n\n&lt;p&gt;\u2705 Locked vs. Unlocked Retention Policies: Understand the differences, benefits, and use cases of both locked and unlocked policies.&lt;/p&gt;\n\n&lt;p&gt;\u2705 The Temporary Hold Feature: Grasp the nuances of the temporary hold, its applications, and how it interacts with your set policies.&lt;/p&gt;\n\n&lt;p&gt;\u2705 Interactive Demo: A comprehensive hands-on demonstration, making it easier to understand and implement the discussed concepts.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udccc &lt;a href=\"https://youtu.be/vqlg1gqW9ts\"&gt;https://youtu.be/vqlg1gqW9ts&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c\"&gt;https://preview.redd.it/pzw2tn8r3o0c1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=0a0fbba1fe850b525c2dbb74c58da7437d62f67c&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17whojj", "is_robot_indexable": true, "report_reasons": null, "author": "suddeb", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17whojj/mastering_google_cloud_storage_part_3_advanced/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17whojj/mastering_google_cloud_storage_part_3_advanced/", "subreddit_subscribers": 139943, "created_utc": 1700122072.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys.\n\nCurrently the stack of the company I work at is comprised of a mixture of Cloudera (on-prem and cloud) and Azure Databricks (and the rest of the Azure ecosystem).\n\nRight now, permissions management is kind of a mess. Cloudera Hadoop permissions are managed by Ranger which syncs LDAP groups and then we have policies attached to each group with permissions to use specific databases within Hadoop/HDFS. Our Azure ecossystem permissions model uses IAM Roles (now Entra ID, I guess) to enforce each LDAP group permission's to each ADLS storage account and container.\n\nThere are a lot of other things in the mix, but that's the gist of it. \n\nEnforcing permissions is achieved by a mixture of manual, automatic and some terraform (between Azure and CDP) actions.\n\nHow to make sense of all this? How would you tackle trying to unify the permissions model across all these components?\n\nRight now what comes to mind is using Databricks Unity Catalog and try to create a script to sync the permissions to Ranger.", "author_fullname": "t2_pev7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrated permissions management across different providers of on prem and cloud data infrastructures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vyqzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700069345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys.&lt;/p&gt;\n\n&lt;p&gt;Currently the stack of the company I work at is comprised of a mixture of Cloudera (on-prem and cloud) and Azure Databricks (and the rest of the Azure ecosystem).&lt;/p&gt;\n\n&lt;p&gt;Right now, permissions management is kind of a mess. Cloudera Hadoop permissions are managed by Ranger which syncs LDAP groups and then we have policies attached to each group with permissions to use specific databases within Hadoop/HDFS. Our Azure ecossystem permissions model uses IAM Roles (now Entra ID, I guess) to enforce each LDAP group permission&amp;#39;s to each ADLS storage account and container.&lt;/p&gt;\n\n&lt;p&gt;There are a lot of other things in the mix, but that&amp;#39;s the gist of it. &lt;/p&gt;\n\n&lt;p&gt;Enforcing permissions is achieved by a mixture of manual, automatic and some terraform (between Azure and CDP) actions.&lt;/p&gt;\n\n&lt;p&gt;How to make sense of all this? How would you tackle trying to unify the permissions model across all these components?&lt;/p&gt;\n\n&lt;p&gt;Right now what comes to mind is using Databricks Unity Catalog and try to create a script to sync the permissions to Ranger.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vyqzm", "is_robot_indexable": true, "report_reasons": null, "author": "mommylovesme2", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vyqzm/integrated_permissions_management_across/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vyqzm/integrated_permissions_management_across/", "subreddit_subscribers": 139943, "created_utc": 1700069345.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS", "author_fullname": "t2_ancj6nr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_17vwc5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "author_name": "The iT Tech Solutions", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/9rzIhRb2qKk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheiTTechSolutions"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17vwc5q", "height": 200}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/fjtk7XwbiXbVLM9hCNd3gxKDCkXWO7aQEZETfYj8Wj4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700062825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/9rzIhRb2qKk", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?auto=webp&amp;s=8ec9545a9f8b1c557ee9af6002b086b698a9e3c7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d48648c7370d7259d4beec81cf28d52300b0afd", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47748d9418c4591d8b33469fab5a501a97708ba4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d8c859dad79ffa253ab9640e50f591be0e3de7e", "width": 320, "height": 240}], "variants": {}, "id": "-ubKmFhXuBQ2j857IyozL8XfyJT9uZB5ca8iMjrzCJU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vwc5q", "is_robot_indexable": true, "report_reasons": null, "author": "Recent_Ocelot_724", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vwc5q/aws_step_function_variables_resultpath_inputpath/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/9rzIhRb2qKk", "subreddit_subscribers": 139943, "created_utc": 1700062825.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "author_name": "The iT Tech Solutions", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/9rzIhRb2qKk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheiTTechSolutions"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am beginning with a new blog series about Microsoft Azure. As the first article you get your hands on the Azure SQL Database service. Where you will create one and import csv data to create a table. Happy for some feedback!\n\n[https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405](https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405)", "author_fullname": "t2_3di0zmcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Effortless Azure: Mastering SQL Database Creation and Data Import", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vt2w7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700053085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am beginning with a new blog series about Microsoft Azure. As the first article you get your hands on the Azure SQL Database service. Where you will create one and import csv data to create a table. Happy for some feedback!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405\"&gt;https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?auto=webp&amp;s=e3518f63ea54c0bace227cb57575f54f713d6723", "width": 728, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e68244bd449a8dc4b1188bd6ba8aabeb9f3d048", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=22a4ab3540f79686ec32b5e4c51c9bf4424430f6", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e133141465c286fc35b94ada6863c8d04d2cbd5", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc30fdc12505f2560f38d0c6282bc0f083ea3deb", "width": 640, "height": 334}], "variants": {}, "id": "_gJpBnOR4HASqLUzy_v5MZHn85O6SH4CEnQM2i9rsXM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vt2w7", "is_robot_indexable": true, "report_reasons": null, "author": "EdgarHuber", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vt2w7/effortless_azure_mastering_sql_database_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vt2w7/effortless_azure_mastering_sql_database_creation/", "subreddit_subscribers": 139943, "created_utc": 1700053085.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}