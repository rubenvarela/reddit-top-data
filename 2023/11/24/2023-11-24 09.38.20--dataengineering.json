{"kind": "Listing", "data": {"after": null, "dist": 14, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a rant.\nI've talked with manager and He told me that at end of january I've to do PL-300.\n\nI work since two years as Data Consultant(mainly DE/ DA / PBI /PMO/ dwh)for one of big 4 consultancy, 6 role in 1.\n\nMy feedback are extremely good, deadlines are meet, numbers do match and since I started pipelines only crash few times during year thanks to my fixes, working on 5-6 project as same time,\nProposed new innovations, projects that has been accepted and made the company earn.\n\nI've done PL-300 twice and did not pass it both for lack of time in studying and because I was forced to do it because my manager was like \"try it, if you pass, it's less pain\".\nI'm more strong in DE and we agreed to go for DE certification but they want PL-300.\n\nIn my team I'm the only one doing PowerBi and DE / SA / PM while I build like 5-6 dashboard for the same client and sometimes I do finish late but is hard for me to find time to study as\n\nI'm always fighting for good data quality, messy data model and all dirty hell of data made by multiple people working on customer.\n\nClient doesn't require it but my company does as it has partnerships.\n\nI told my manager that I hate certifications due to memory and the thought of passing hours after work and during holidays studying it's a no for me\nAs they're important for me as I De-stress.\nAlso I've added \"if I take the certification, you take it too. It is not fair and you'll understand what it will mean to study for it, why since you're tech manager and you work with me, I'm the only one forced to take certs but you are not?\"\n\nI've met a lot of people with senior experience working in big companies that hates certifications.\nI do like studying but not to sacrifice my holidays.\n\n1. Should I start a plan B?\n2. Have you been in same shoes?", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rant about certifications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181zzgt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700741938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a rant.\nI&amp;#39;ve talked with manager and He told me that at end of january I&amp;#39;ve to do PL-300.&lt;/p&gt;\n\n&lt;p&gt;I work since two years as Data Consultant(mainly DE/ DA / PBI /PMO/ dwh)for one of big 4 consultancy, 6 role in 1.&lt;/p&gt;\n\n&lt;p&gt;My feedback are extremely good, deadlines are meet, numbers do match and since I started pipelines only crash few times during year thanks to my fixes, working on 5-6 project as same time,\nProposed new innovations, projects that has been accepted and made the company earn.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done PL-300 twice and did not pass it both for lack of time in studying and because I was forced to do it because my manager was like &amp;quot;try it, if you pass, it&amp;#39;s less pain&amp;quot;.\nI&amp;#39;m more strong in DE and we agreed to go for DE certification but they want PL-300.&lt;/p&gt;\n\n&lt;p&gt;In my team I&amp;#39;m the only one doing PowerBi and DE / SA / PM while I build like 5-6 dashboard for the same client and sometimes I do finish late but is hard for me to find time to study as&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m always fighting for good data quality, messy data model and all dirty hell of data made by multiple people working on customer.&lt;/p&gt;\n\n&lt;p&gt;Client doesn&amp;#39;t require it but my company does as it has partnerships.&lt;/p&gt;\n\n&lt;p&gt;I told my manager that I hate certifications due to memory and the thought of passing hours after work and during holidays studying it&amp;#39;s a no for me\nAs they&amp;#39;re important for me as I De-stress.\nAlso I&amp;#39;ve added &amp;quot;if I take the certification, you take it too. It is not fair and you&amp;#39;ll understand what it will mean to study for it, why since you&amp;#39;re tech manager and you work with me, I&amp;#39;m the only one forced to take certs but you are not?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve met a lot of people with senior experience working in big companies that hates certifications.\nI do like studying but not to sacrifice my holidays.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Should I start a plan B?&lt;/li&gt;\n&lt;li&gt;Have you been in same shoes?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181zzgt", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181zzgt/rant_about_certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181zzgt/rant_about_certifications/", "subreddit_subscribers": 141398, "created_utc": 1700741938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My workplace makes queries to data in Snowflake, from Databricks, but it seems like Snowflake is working hard to develop their own ETL, ML pipelines using NVIDIA Rapids/GPU computing even.\n\nI have been signing up for online seminars on Snowflake data science/data engineering because I have a problem with Databricks.\n\nI want to use OpenZiti on Databricks so only client machines with a Ziti identity can connect to the Databricks.  The Databricks itself needs an OpenZiti identity so it can connect to an Ziti Edge Router which in turn would connect to Ziti Edge Router on Azure that connects to my web app on Azure.  From here I want to use Azure private link to connect to OpenAI services (natural language -&gt; SQL query) and to my Snowflake on Azure.\n\nSo the Snowflake on Azure can be totally cut off from the internet as well as my web app on Azure.  But Databricks does not have this capability, ie. close of all incoming ports, only outbound to Ziti edge router and have a Ziti identity.\n\n&amp;#x200B;", "author_fullname": "t2_7bmygxmk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Databricks becoming redundant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182fuyq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700788080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My workplace makes queries to data in Snowflake, from Databricks, but it seems like Snowflake is working hard to develop their own ETL, ML pipelines using NVIDIA Rapids/GPU computing even.&lt;/p&gt;\n\n&lt;p&gt;I have been signing up for online seminars on Snowflake data science/data engineering because I have a problem with Databricks.&lt;/p&gt;\n\n&lt;p&gt;I want to use OpenZiti on Databricks so only client machines with a Ziti identity can connect to the Databricks.  The Databricks itself needs an OpenZiti identity so it can connect to an Ziti Edge Router which in turn would connect to Ziti Edge Router on Azure that connects to my web app on Azure.  From here I want to use Azure private link to connect to OpenAI services (natural language -&amp;gt; SQL query) and to my Snowflake on Azure.&lt;/p&gt;\n\n&lt;p&gt;So the Snowflake on Azure can be totally cut off from the internet as well as my web app on Azure.  But Databricks does not have this capability, ie. close of all incoming ports, only outbound to Ziti edge router and have a Ziti identity.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "182fuyq", "is_robot_indexable": true, "report_reasons": null, "author": "webNoob13", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182fuyq/is_databricks_becoming_redundant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182fuyq/is_databricks_becoming_redundant/", "subreddit_subscribers": 141398, "created_utc": 1700788080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For example \n\nOne input is \n\n    {\"id\": \"abc\", \"users\": [{\"user_id\": 1}]}\n\nAnd another is \n\n    {\"id\": \"def\", \"users\": [{\"user_id\": 1}, {\"user_id\": 2}]}\n\nI'd like the flattened table to be something like (not necessarily this)\n\n| id  | user_1_id | user_2_id |\n|-----|-----------|-----------|\n| abc | 1         | null      |\n| def | 1         | 2         |\n|     |           |           |\n\n\nBut you don't know the max number of users an item/row might have. And it's unwieldy to have user_n_id for n columns.\n\nI know in big query there are nested arrays that could help here but I'm looking for a more tech-agnostic method of dealing with this. Something from the data modeling world I imagine.\n\nI imagine it means separating the latter into 2 rows to yield something like below\n\n| id  | user_id |\n|-----|---------|\n| abc | 1       |\n| def | 1       |\n| def | 2       |\n\nAppreciate any pointers", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle json -&gt; tabular format when an array field has a variable number of objects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1828nvo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700766956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example &lt;/p&gt;\n\n&lt;p&gt;One input is &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{&amp;quot;id&amp;quot;: &amp;quot;abc&amp;quot;, &amp;quot;users&amp;quot;: [{&amp;quot;user_id&amp;quot;: 1}]}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And another is &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{&amp;quot;id&amp;quot;: &amp;quot;def&amp;quot;, &amp;quot;users&amp;quot;: [{&amp;quot;user_id&amp;quot;: 1}, {&amp;quot;user_id&amp;quot;: 2}]}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;d like the flattened table to be something like (not necessarily this)&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;id&lt;/th&gt;\n&lt;th&gt;user_1_id&lt;/th&gt;\n&lt;th&gt;user_2_id&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;abc&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;null&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;def&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;But you don&amp;#39;t know the max number of users an item/row might have. And it&amp;#39;s unwieldy to have user_n_id for n columns.&lt;/p&gt;\n\n&lt;p&gt;I know in big query there are nested arrays that could help here but I&amp;#39;m looking for a more tech-agnostic method of dealing with this. Something from the data modeling world I imagine.&lt;/p&gt;\n\n&lt;p&gt;I imagine it means separating the latter into 2 rows to yield something like below&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;id&lt;/th&gt;\n&lt;th&gt;user_id&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;abc&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;def&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;def&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Appreciate any pointers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1828nvo", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1828nvo/how_to_handle_json_tabular_format_when_an_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1828nvo/how_to_handle_json_tabular_format_when_an_array/", "subreddit_subscribers": 141398, "created_utc": 1700766956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. I work as a Business Analyst in a startup that is about to be funded. I always thought about what should be an ideal data stack for us, since we don't have many resources, such as a DW, and most of the reports/dashboards consume data directly from the source using Power Query. \n\nBut I had a talk with the CEO today and he told me to plan what we need to truly build our data environment. \n\nIn my mind, the first thing we should do is to develop a cloud DW, consuming data from our multiple sources (mostly APIs and relational DBs). I have talked a bit with a couple of AIs to expand my views, but I wanted to hear from more experienced professionals. \n\n\\- What tools do I need to build it from scratch?\n\n\\- What processes need to be done?\n\n\\- What costs should I be aware of?\n\n\\- What knowledge base do you recommend?\n\n\\- What **else** should I be aware of?\n\nFrom an outside view, I can imagine it being built within the Microsoft stack, using Data Factory and Data Warehouse.\n\nThanks in advance!", "author_fullname": "t2_8ss95xyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business Analyst got a DE (?) Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182a674", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700771396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I work as a Business Analyst in a startup that is about to be funded. I always thought about what should be an ideal data stack for us, since we don&amp;#39;t have many resources, such as a DW, and most of the reports/dashboards consume data directly from the source using Power Query. &lt;/p&gt;\n\n&lt;p&gt;But I had a talk with the CEO today and he told me to plan what we need to truly build our data environment. &lt;/p&gt;\n\n&lt;p&gt;In my mind, the first thing we should do is to develop a cloud DW, consuming data from our multiple sources (mostly APIs and relational DBs). I have talked a bit with a couple of AIs to expand my views, but I wanted to hear from more experienced professionals. &lt;/p&gt;\n\n&lt;p&gt;- What tools do I need to build it from scratch?&lt;/p&gt;\n\n&lt;p&gt;- What processes need to be done?&lt;/p&gt;\n\n&lt;p&gt;- What costs should I be aware of?&lt;/p&gt;\n\n&lt;p&gt;- What knowledge base do you recommend?&lt;/p&gt;\n\n&lt;p&gt;- What &lt;strong&gt;else&lt;/strong&gt; should I be aware of?&lt;/p&gt;\n\n&lt;p&gt;From an outside view, I can imagine it being built within the Microsoft stack, using Data Factory and Data Warehouse.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182a674", "is_robot_indexable": true, "report_reasons": null, "author": "Practical_Gap_3354", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182a674/business_analyst_got_a_de_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182a674/business_analyst_got_a_de_project/", "subreddit_subscribers": 141398, "created_utc": 1700771396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Dear Data Engineers,**\n\nI wanted to update you on an ongoing project involving the shifting of our ETL load from AWS RDS (Postgres ) to Redshift.\n\nKey points to note :\n\n\\- This would be a 30 minutes scheduled job \\[Not One Time Migration \\]\n\n\\- Change Data Capture is applicable\n\n\\- Raw data in actually on AWS RDS Postgres (Not aurora)\n\n\\- Fact/Dim Table should be in Redshift \n\n&amp;#x200B;\n\nAs of now, our ETL process is facilitated by a Python container running as a AWS Fargate job. My plan is to leverage the existing project and make minimal changes, particularly in the realm of Python libraries, to enable seamless data ingestion into Redshift.\n\n&amp;#x200B;\n\nI have a few questions that I would appreciate your insights on:\n\n* Is the proposed approach of modifying the existing project a sound one?\n* Are there any recommended Python libraries for efficiently ingesting data into Redshift?\n* If PySpark is considered, are there alternative ways to run PySpark projects aside from Glue or EMR? (Currently, budget constraints limit our options in this regard.)\n\nI value your thoughts and any additional input you may have on this matter. If you have any relevant resource materials, kindly share them with me.\n\n&amp;#x200B;\n\nThank you for your time and expertise.", "author_fullname": "t2_7nvr4m4i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Discussion on ETL infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182mjd0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700814674.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700810568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Dear Data Engineers,&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I wanted to update you on an ongoing project involving the shifting of our ETL load from AWS RDS (Postgres ) to Redshift.&lt;/p&gt;\n\n&lt;p&gt;Key points to note :&lt;/p&gt;\n\n&lt;p&gt;- This would be a 30 minutes scheduled job [Not One Time Migration ]&lt;/p&gt;\n\n&lt;p&gt;- Change Data Capture is applicable&lt;/p&gt;\n\n&lt;p&gt;- Raw data in actually on AWS RDS Postgres (Not aurora)&lt;/p&gt;\n\n&lt;p&gt;- Fact/Dim Table should be in Redshift &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As of now, our ETL process is facilitated by a Python container running as a AWS Fargate job. My plan is to leverage the existing project and make minimal changes, particularly in the realm of Python libraries, to enable seamless data ingestion into Redshift.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have a few questions that I would appreciate your insights on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is the proposed approach of modifying the existing project a sound one?&lt;/li&gt;\n&lt;li&gt;Are there any recommended Python libraries for efficiently ingesting data into Redshift?&lt;/li&gt;\n&lt;li&gt;If PySpark is considered, are there alternative ways to run PySpark projects aside from Glue or EMR? (Currently, budget constraints limit our options in this regard.)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I value your thoughts and any additional input you may have on this matter. If you have any relevant resource materials, kindly share them with me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time and expertise.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182mjd0", "is_robot_indexable": true, "report_reasons": null, "author": "Flimsy-Mirror974", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182mjd0/discussion_on_etl_infrastructure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182mjd0/discussion_on_etl_infrastructure/", "subreddit_subscribers": 141398, "created_utc": 1700810568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a question on how you would approach this data model design. \n\nMy data model records Sales Revenue, with the special case that a Sales Transaction can generate Sales Revenue for multiple offices.\n\nAn example of a Sales Transactions worth 10k can generate 3k revenue for Office 1 and 7k revenue for Office 2.\n\nCurrently, my revenue fact table stores the generated revenue like this:\noffice_key, date_key, transaction_key, revenue_amount\n\nI have a reporting requirement where I want to drill through from revenue on a certain day to the transactions that attributed to revenue on that day. \n\nDoes it make sense to link from my revenue fact table to the transaction fact table by transaction_key?", "author_fullname": "t2_75hil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data modelling question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181xfa5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700731517.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a question on how you would approach this data model design. &lt;/p&gt;\n\n&lt;p&gt;My data model records Sales Revenue, with the special case that a Sales Transaction can generate Sales Revenue for multiple offices.&lt;/p&gt;\n\n&lt;p&gt;An example of a Sales Transactions worth 10k can generate 3k revenue for Office 1 and 7k revenue for Office 2.&lt;/p&gt;\n\n&lt;p&gt;Currently, my revenue fact table stores the generated revenue like this:\noffice_key, date_key, transaction_key, revenue_amount&lt;/p&gt;\n\n&lt;p&gt;I have a reporting requirement where I want to drill through from revenue on a certain day to the transactions that attributed to revenue on that day. &lt;/p&gt;\n\n&lt;p&gt;Does it make sense to link from my revenue fact table to the transaction fact table by transaction_key?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181xfa5", "is_robot_indexable": true, "report_reasons": null, "author": "dezwarteridder", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181xfa5/data_modelling_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181xfa5/data_modelling_question/", "subreddit_subscribers": 141398, "created_utc": 1700731517.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you are trying to match your customers into a master customer id, what fields are you matching on? Our ordering system uses email but you could use different emails to place orders. Any recommendations?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Matching Customer Records", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182k9w0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700802559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are trying to match your customers into a master customer id, what fields are you matching on? Our ordering system uses email but you could use different emails to place orders. Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182k9w0", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182k9w0/matching_customer_records/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182k9w0/matching_customer_records/", "subreddit_subscribers": 141398, "created_utc": 1700802559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Did anyone here managed it to create a table which has partition projection enabled.\n\nI can manage it via Athena, but i want to create it via Trino. Thats why my catalog is glue and the data on s3.\n\nCouldnt find examples in the docu for that", "author_fullname": "t2_kabee6pi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trino table partition projection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1827e97", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700763415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did anyone here managed it to create a table which has partition projection enabled.&lt;/p&gt;\n\n&lt;p&gt;I can manage it via Athena, but i want to create it via Trino. Thats why my catalog is glue and the data on s3.&lt;/p&gt;\n\n&lt;p&gt;Couldnt find examples in the docu for that&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1827e97", "is_robot_indexable": true, "report_reasons": null, "author": "WeaknessNecessary657", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1827e97/trino_table_partition_projection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1827e97/trino_table_partition_projection/", "subreddit_subscribers": 141398, "created_utc": 1700763415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi I\u2019m very new to working in Azure and with ADFs. I\u2019m working on a project where I need to transform XML files to a CSV output in a specific format. I\u2019ve managed to implement a data flow to get the transformation working as intended, with a blob container for the source and sink respectively. It works as intended when I have one file in the source blob, but when I try with multiple it starts to act a bit weird and tries pulling data from each file into one csv file I think? Ideally I want it so that a specific csv file is generated for each xml file in the source system. Even better if I could get the ADF to trigger with a new blob created and only process the most recently uploaded files. I\u2019ve tried figuring this out but I\u2019m pretty stuck with this processing source items separately thing, is this a for each I should be using?", "author_fullname": "t2_rrfc2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ADF data flow, scaling up for multiple file inputs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181yrb3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700737144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I\u2019m very new to working in Azure and with ADFs. I\u2019m working on a project where I need to transform XML files to a CSV output in a specific format. I\u2019ve managed to implement a data flow to get the transformation working as intended, with a blob container for the source and sink respectively. It works as intended when I have one file in the source blob, but when I try with multiple it starts to act a bit weird and tries pulling data from each file into one csv file I think? Ideally I want it so that a specific csv file is generated for each xml file in the source system. Even better if I could get the ADF to trigger with a new blob created and only process the most recently uploaded files. I\u2019ve tried figuring this out but I\u2019m pretty stuck with this processing source items separately thing, is this a for each I should be using?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181yrb3", "is_robot_indexable": true, "report_reasons": null, "author": "EvilDoctorShadex", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181yrb3/adf_data_flow_scaling_up_for_multiple_file_inputs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181yrb3/adf_data_flow_scaling_up_for_multiple_file_inputs/", "subreddit_subscribers": 141398, "created_utc": 1700737144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_crthfc7kd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WebGL Visualizer for dbt DAGs with hundreds or thousands of models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182mb4v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1700809693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "large-dbt-dag-visualizer.whiai.repl.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://large-dbt-dag-visualizer.whiai.repl.co/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "182mb4v", "is_robot_indexable": true, "report_reasons": null, "author": "devschema", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/182mb4v/webgl_visualizer_for_dbt_dags_with_hundreds_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://large-dbt-dag-visualizer.whiai.repl.co/", "subreddit_subscribers": 141398, "created_utc": 1700809693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone here work with Oracle WMS data? I started on a new project and it seems the team I'm in doesn't know how to extract data from Oracle WMS. I found their Rest APIs but these are slow. (124 rows per request... sigh).\n\nIs there a cloud database that Oracle WMS clients have access to to extract data quickly etc?", "author_fullname": "t2_a42ncwcn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle Warehouse Management System (WMS) Cloud, how to access the cloud database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182kyth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700804897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone here work with Oracle WMS data? I started on a new project and it seems the team I&amp;#39;m in doesn&amp;#39;t know how to extract data from Oracle WMS. I found their Rest APIs but these are slow. (124 rows per request... sigh).&lt;/p&gt;\n\n&lt;p&gt;Is there a cloud database that Oracle WMS clients have access to to extract data quickly etc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "182kyth", "is_robot_indexable": true, "report_reasons": null, "author": "MassiveDefender", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182kyth/oracle_warehouse_management_system_wms_cloud_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182kyth/oracle_warehouse_management_system_wms_cloud_how/", "subreddit_subscribers": 141398, "created_utc": 1700804897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone have experience orchestrating AKS containers via Airflow? At the moment we are using a kubectl cli command to alert the agent pool to run in a bash operator in airflow, but doing so doesn\u2019t actually wait for the task to finish or provide any indication of if the task succeeded or failed.", "author_fullname": "t2_9d5p6jq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure K8s Service via Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18217zc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700746173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have experience orchestrating AKS containers via Airflow? At the moment we are using a kubectl cli command to alert the agent pool to run in a bash operator in airflow, but doing so doesn\u2019t actually wait for the task to finish or provide any indication of if the task succeeded or failed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18217zc", "is_robot_indexable": true, "report_reasons": null, "author": "avclipavclip", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18217zc/azure_k8s_service_via_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18217zc/azure_k8s_service_via_airflow/", "subreddit_subscribers": 141398, "created_utc": 1700746173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Is adopting this approach advisable? I granted the Cloud Function Service Agent role to the Cloud Function Service Agent Service Account in another project, enabling Cloud Functions to trigger upon the arrival of new files. However, due to the absence of official documentation or troubleshooting specifics for this use case, I remain uncertain. Your assistance would be appreciated. ", "author_fullname": "t2_4cullil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud function storage trigger on bucket of other project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181xqek", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700732819.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is adopting this approach advisable? I granted the Cloud Function Service Agent role to the Cloud Function Service Agent Service Account in another project, enabling Cloud Functions to trigger upon the arrival of new files. However, due to the absence of official documentation or troubleshooting specifics for this use case, I remain uncertain. Your assistance would be appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181xqek", "is_robot_indexable": true, "report_reasons": null, "author": "tmanipra", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181xqek/cloud_function_storage_trigger_on_bucket_of_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181xqek/cloud_function_storage_trigger_on_bucket_of_other/", "subreddit_subscribers": 141398, "created_utc": 1700732819.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_h557nj7lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "44 Best Resources to learn Data Engineering (YouTube, Books, Courses, &amp; Tutorials)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1823qih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CqnD0jXp1zy-FvPpGjVU-Jw9G9sUiC_Xd-9NZ2V_A3c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700753655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mltut.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.mltut.com/best-resources-to-learn-data-engineering/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?auto=webp&amp;s=a500e2d0a55645b45e0cebf7674818a8b1a001ec", "width": 2240, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=846b2e4d36e11dd8b46ba3ae70e61fe3cb2d6be6", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=93e59e083e2cd40196a9c0e4ea2783db8f4a95a3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a47b29eae87c633967e98ac37f5cd8052261793", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d441865f1baacfeef03dee52e49abfad9bb30a7e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3fc69de998a7ca3ca810b9446754b56529c75f73", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efcb52b6d7702e76f91f823e3c4894de7dda24e9", "width": 1080, "height": 607}], "variants": {}, "id": "Dfiy7g_nIo24a7MPAubj8Jy84Wz8r8RZoQE-MJidPWc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1823qih", "is_robot_indexable": true, "report_reasons": null, "author": "Aqsa81", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1823qih/44_best_resources_to_learn_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.mltut.com/best-resources-to-learn-data-engineering/", "subreddit_subscribers": 141398, "created_utc": 1700753655.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}