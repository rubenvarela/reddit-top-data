{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My workplace makes queries to data in Snowflake, from Databricks, but it seems like Snowflake is working hard to develop their own ETL, ML pipelines using NVIDIA Rapids/GPU computing even.\n\nI have been signing up for online seminars on Snowflake data science/data engineering because I have a problem with Databricks.\n\nI want to use OpenZiti on Databricks so only client machines with a Ziti identity can connect to the Databricks.  The Databricks itself needs an OpenZiti identity so it can connect to an Ziti Edge Router which in turn would connect to Ziti Edge Router on Azure that connects to my web app on Azure.  From here I want to use Azure private link to connect to OpenAI services (natural language -&gt; SQL query) and to my Snowflake on Azure.\n\nSo the Snowflake on Azure can be totally cut off from the internet as well as my web app on Azure.  But Databricks does not have this capability, ie. close of all incoming ports, only outbound to Ziti edge router and have a Ziti identity.\n\n&amp;#x200B;", "author_fullname": "t2_7bmygxmk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Databricks becoming redundant?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182fuyq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700788080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My workplace makes queries to data in Snowflake, from Databricks, but it seems like Snowflake is working hard to develop their own ETL, ML pipelines using NVIDIA Rapids/GPU computing even.&lt;/p&gt;\n\n&lt;p&gt;I have been signing up for online seminars on Snowflake data science/data engineering because I have a problem with Databricks.&lt;/p&gt;\n\n&lt;p&gt;I want to use OpenZiti on Databricks so only client machines with a Ziti identity can connect to the Databricks.  The Databricks itself needs an OpenZiti identity so it can connect to an Ziti Edge Router which in turn would connect to Ziti Edge Router on Azure that connects to my web app on Azure.  From here I want to use Azure private link to connect to OpenAI services (natural language -&amp;gt; SQL query) and to my Snowflake on Azure.&lt;/p&gt;\n\n&lt;p&gt;So the Snowflake on Azure can be totally cut off from the internet as well as my web app on Azure.  But Databricks does not have this capability, ie. close of all incoming ports, only outbound to Ziti edge router and have a Ziti identity.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "182fuyq", "is_robot_indexable": true, "report_reasons": null, "author": "webNoob13", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182fuyq/is_databricks_becoming_redundant/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182fuyq/is_databricks_becoming_redundant/", "subreddit_subscribers": 141423, "created_utc": 1700788080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For example \n\nOne input is \n\n    {\"id\": \"abc\", \"users\": [{\"user_id\": 1}]}\n\nAnd another is \n\n    {\"id\": \"def\", \"users\": [{\"user_id\": 1}, {\"user_id\": 2}]}\n\nI'd like the flattened table to be something like (not necessarily this)\n\n| id  | user_1_id | user_2_id |\n|-----|-----------|-----------|\n| abc | 1         | null      |\n| def | 1         | 2         |\n|     |           |           |\n\n\nBut you don't know the max number of users an item/row might have. And it's unwieldy to have user_n_id for n columns.\n\nI know in big query there are nested arrays that could help here but I'm looking for a more tech-agnostic method of dealing with this. Something from the data modeling world I imagine.\n\nI imagine it means separating the latter into 2 rows to yield something like below\n\n| id  | user_id |\n|-----|---------|\n| abc | 1       |\n| def | 1       |\n| def | 2       |\n\nAppreciate any pointers", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to handle json -&gt; tabular format when an array field has a variable number of objects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1828nvo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700766956.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For example &lt;/p&gt;\n\n&lt;p&gt;One input is &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{&amp;quot;id&amp;quot;: &amp;quot;abc&amp;quot;, &amp;quot;users&amp;quot;: [{&amp;quot;user_id&amp;quot;: 1}]}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And another is &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{&amp;quot;id&amp;quot;: &amp;quot;def&amp;quot;, &amp;quot;users&amp;quot;: [{&amp;quot;user_id&amp;quot;: 1}, {&amp;quot;user_id&amp;quot;: 2}]}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I&amp;#39;d like the flattened table to be something like (not necessarily this)&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;id&lt;/th&gt;\n&lt;th&gt;user_1_id&lt;/th&gt;\n&lt;th&gt;user_2_id&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;abc&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;null&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;def&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;But you don&amp;#39;t know the max number of users an item/row might have. And it&amp;#39;s unwieldy to have user_n_id for n columns.&lt;/p&gt;\n\n&lt;p&gt;I know in big query there are nested arrays that could help here but I&amp;#39;m looking for a more tech-agnostic method of dealing with this. Something from the data modeling world I imagine.&lt;/p&gt;\n\n&lt;p&gt;I imagine it means separating the latter into 2 rows to yield something like below&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;id&lt;/th&gt;\n&lt;th&gt;user_id&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;abc&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;def&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;def&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Appreciate any pointers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1828nvo", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1828nvo/how_to_handle_json_tabular_format_when_an_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1828nvo/how_to_handle_json_tabular_format_when_an_array/", "subreddit_subscribers": 141423, "created_utc": 1700766956.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone. I work as a Business Analyst in a startup that is about to be funded. I always thought about what should be an ideal data stack for us, since we don't have many resources, such as a DW, and most of the reports/dashboards consume data directly from the source using Power Query. \n\nBut I had a talk with the CEO today and he told me to plan what we need to truly build our data environment. \n\nIn my mind, the first thing we should do is to develop a cloud DW, consuming data from our multiple sources (mostly APIs and relational DBs). I have talked a bit with a couple of AIs to expand my views, but I wanted to hear from more experienced professionals. \n\n\\- What tools do I need to build it from scratch?\n\n\\- What processes need to be done?\n\n\\- What costs should I be aware of?\n\n\\- What knowledge base do you recommend?\n\n\\- What **else** should I be aware of?\n\nFrom an outside view, I can imagine it being built within the Microsoft stack, using Data Factory and Data Warehouse.\n\nThanks in advance!", "author_fullname": "t2_8ss95xyj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business Analyst got a DE (?) Project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182a674", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700771396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I work as a Business Analyst in a startup that is about to be funded. I always thought about what should be an ideal data stack for us, since we don&amp;#39;t have many resources, such as a DW, and most of the reports/dashboards consume data directly from the source using Power Query. &lt;/p&gt;\n\n&lt;p&gt;But I had a talk with the CEO today and he told me to plan what we need to truly build our data environment. &lt;/p&gt;\n\n&lt;p&gt;In my mind, the first thing we should do is to develop a cloud DW, consuming data from our multiple sources (mostly APIs and relational DBs). I have talked a bit with a couple of AIs to expand my views, but I wanted to hear from more experienced professionals. &lt;/p&gt;\n\n&lt;p&gt;- What tools do I need to build it from scratch?&lt;/p&gt;\n\n&lt;p&gt;- What processes need to be done?&lt;/p&gt;\n\n&lt;p&gt;- What costs should I be aware of?&lt;/p&gt;\n\n&lt;p&gt;- What knowledge base do you recommend?&lt;/p&gt;\n\n&lt;p&gt;- What &lt;strong&gt;else&lt;/strong&gt; should I be aware of?&lt;/p&gt;\n\n&lt;p&gt;From an outside view, I can imagine it being built within the Microsoft stack, using Data Factory and Data Warehouse.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182a674", "is_robot_indexable": true, "report_reasons": null, "author": "Practical_Gap_3354", "discussion_type": null, "num_comments": 10, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182a674/business_analyst_got_a_de_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182a674/business_analyst_got_a_de_project/", "subreddit_subscribers": 141423, "created_utc": 1700771396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "**Dear Data Engineers,**\n\nI wanted to update you on an ongoing project involving the shifting of our ETL load from AWS RDS (Postgres ) to Redshift.\n\nKey points to note :\n\n\\- This would be a 30 minutes scheduled job \\[Not One Time Migration \\]\n\n\\- Change Data Capture is applicable\n\n\\- Raw data in actually on AWS RDS Postgres (Not aurora)\n\n\\- Fact/Dim Table should be in Redshift \n\n&amp;#x200B;\n\nAs of now, our ETL process is facilitated by a Python container running as a AWS Fargate job. My plan is to leverage the existing project and make minimal changes, particularly in the realm of Python libraries, to enable seamless data ingestion into Redshift.\n\n&amp;#x200B;\n\nI have a few questions that I would appreciate your insights on:\n\n* Is the proposed approach of modifying the existing project a sound one?\n* Are there any recommended Python libraries for efficiently ingesting data into Redshift?\n* If PySpark is considered, are there alternative ways to run PySpark projects aside from Glue or EMR? (Currently, budget constraints limit our options in this regard.)\n\nI value your thoughts and any additional input you may have on this matter. If you have any relevant resource materials, kindly share them with me.\n\n&amp;#x200B;\n\nThank you for your time and expertise.", "author_fullname": "t2_7nvr4m4i4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Discussion on ETL infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182mjd0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700814674.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700810568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Dear Data Engineers,&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I wanted to update you on an ongoing project involving the shifting of our ETL load from AWS RDS (Postgres ) to Redshift.&lt;/p&gt;\n\n&lt;p&gt;Key points to note :&lt;/p&gt;\n\n&lt;p&gt;- This would be a 30 minutes scheduled job [Not One Time Migration ]&lt;/p&gt;\n\n&lt;p&gt;- Change Data Capture is applicable&lt;/p&gt;\n\n&lt;p&gt;- Raw data in actually on AWS RDS Postgres (Not aurora)&lt;/p&gt;\n\n&lt;p&gt;- Fact/Dim Table should be in Redshift &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;As of now, our ETL process is facilitated by a Python container running as a AWS Fargate job. My plan is to leverage the existing project and make minimal changes, particularly in the realm of Python libraries, to enable seamless data ingestion into Redshift.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I have a few questions that I would appreciate your insights on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is the proposed approach of modifying the existing project a sound one?&lt;/li&gt;\n&lt;li&gt;Are there any recommended Python libraries for efficiently ingesting data into Redshift?&lt;/li&gt;\n&lt;li&gt;If PySpark is considered, are there alternative ways to run PySpark projects aside from Glue or EMR? (Currently, budget constraints limit our options in this regard.)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I value your thoughts and any additional input you may have on this matter. If you have any relevant resource materials, kindly share them with me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time and expertise.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182mjd0", "is_robot_indexable": true, "report_reasons": null, "author": "Flimsy-Mirror974", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182mjd0/discussion_on_etl_infrastructure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182mjd0/discussion_on_etl_infrastructure/", "subreddit_subscribers": 141423, "created_utc": 1700810568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_crthfc7kd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WebGL Visualizer for dbt DAGs with hundreds or thousands of models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182mb4v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1700809693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "large-dbt-dag-visualizer.whiai.repl.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://large-dbt-dag-visualizer.whiai.repl.co/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "182mb4v", "is_robot_indexable": true, "report_reasons": null, "author": "devschema", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/182mb4v/webgl_visualizer_for_dbt_dags_with_hundreds_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://large-dbt-dag-visualizer.whiai.repl.co/", "subreddit_subscribers": 141423, "created_utc": 1700809693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you are starting a new position as data engineering manager, what would be your thought process or actions to be successful in the role?", "author_fullname": "t2_3x0urbjs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineering manager", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182pl4i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700823305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are starting a new position as data engineering manager, what would be your thought process or actions to be successful in the role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182pl4i", "is_robot_indexable": true, "report_reasons": null, "author": "educationruinedme1", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182pl4i/data_engineering_manager/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182pl4i/data_engineering_manager/", "subreddit_subscribers": 141423, "created_utc": 1700823305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you are trying to match your customers into a master customer id, what fields are you matching on? Our ordering system uses email but you could use different emails to place orders. Any recommendations?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Matching Customer Records", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182k9w0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700802559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are trying to match your customers into a master customer id, what fields are you matching on? Our ordering system uses email but you could use different emails to place orders. Any recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182k9w0", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182k9w0/matching_customer_records/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182k9w0/matching_customer_records/", "subreddit_subscribers": 141423, "created_utc": 1700802559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Did anyone here managed it to create a table which has partition projection enabled.\n\nI can manage it via Athena, but i want to create it via Trino. Thats why my catalog is glue and the data on s3.\n\nCouldnt find examples in the docu for that", "author_fullname": "t2_kabee6pi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trino table partition projection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1827e97", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700763415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Did anyone here managed it to create a table which has partition projection enabled.&lt;/p&gt;\n\n&lt;p&gt;I can manage it via Athena, but i want to create it via Trino. Thats why my catalog is glue and the data on s3.&lt;/p&gt;\n\n&lt;p&gt;Couldnt find examples in the docu for that&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1827e97", "is_robot_indexable": true, "report_reasons": null, "author": "WeaknessNecessary657", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1827e97/trino_table_partition_projection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1827e97/trino_table_partition_projection/", "subreddit_subscribers": 141423, "created_utc": 1700763415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for advice in how to move our internal network data analysis dashboard to web-based so that it can be accessed anywhere. I'm primarily asking here to get your suggestions on where to house the data. Here's the context:\n\n* **End Application**: A web-based DA dashboard for shipping tracking. \n* **Consumers**: Less than 100 external customers, 10 internal employees\n* **Data Freshness**: Data being updated every 4 hours is acceptable\n* **Data Security**: This isn't health care or gov't data so extra-ordinary steps aren't required here.\n* **Data**: Basically two datasets - The shipping data (200 million rows (50-100 GB) a year) and the tracking data (1 billion rows (50-100 GB) a year)\n* **Context**: Small company, greenfield, wide latitude for 'learning as we build' mentality, lax timeframe\n* **Current setup**: Streamlit served on internal network on top of local SQL Servers\n\nI'm not married to the idea of moving local data up to a platform but to my neophyte ears that makes sense for ease of access. \n\nWould BigQuery make sense given the small-ish amount of data?\n\nI'm clearly learning as I go here lol but any pointers would be helpful.", "author_fullname": "t2_g4dmf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Platforms for Web-Based DA Dashboard", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_182qj5h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700827083.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for advice in how to move our internal network data analysis dashboard to web-based so that it can be accessed anywhere. I&amp;#39;m primarily asking here to get your suggestions on where to house the data. Here&amp;#39;s the context:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;End Application&lt;/strong&gt;: A web-based DA dashboard for shipping tracking. &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Consumers&lt;/strong&gt;: Less than 100 external customers, 10 internal employees&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Freshness&lt;/strong&gt;: Data being updated every 4 hours is acceptable&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data Security&lt;/strong&gt;: This isn&amp;#39;t health care or gov&amp;#39;t data so extra-ordinary steps aren&amp;#39;t required here.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: Basically two datasets - The shipping data (200 million rows (50-100 GB) a year) and the tracking data (1 billion rows (50-100 GB) a year)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt;: Small company, greenfield, wide latitude for &amp;#39;learning as we build&amp;#39; mentality, lax timeframe&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Current setup&lt;/strong&gt;: Streamlit served on internal network on top of local SQL Servers&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m not married to the idea of moving local data up to a platform but to my neophyte ears that makes sense for ease of access. &lt;/p&gt;\n\n&lt;p&gt;Would BigQuery make sense given the small-ish amount of data?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m clearly learning as I go here lol but any pointers would be helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "182qj5h", "is_robot_indexable": true, "report_reasons": null, "author": "ace_reporter", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182qj5h/data_platforms_for_webbased_da_dashboard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182qj5h/data_platforms_for_webbased_da_dashboard/", "subreddit_subscribers": 141423, "created_utc": 1700827083.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I was asked to help another team with their code, and some genius put the jsons into a folder structure where both month and minute were abbreviated using the letter m.\n\nIs there a way to load the date that automatically renames one of the two columns?\n\nEdit: We are using Pyspark load. I am not allowed to rename the folders.\n\nEdit2: Sample path: \"/y=2023/m=07/d=04/h=01/m=07/sample.json\"", "author_fullname": "t2_6cgfmz1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I prevent the following error: \"[COLUMN_ALREADY_EXISTS] The column `m` already exists. Consider to choose another name or rename the existing column.\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182od2y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700819039.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700818016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was asked to help another team with their code, and some genius put the jsons into a folder structure where both month and minute were abbreviated using the letter m.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to load the date that automatically renames one of the two columns?&lt;/p&gt;\n\n&lt;p&gt;Edit: We are using Pyspark load. I am not allowed to rename the folders.&lt;/p&gt;\n\n&lt;p&gt;Edit2: Sample path: &amp;quot;/y=2023/m=07/d=04/h=01/m=07/sample.json&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "182od2y", "is_robot_indexable": true, "report_reasons": null, "author": "BewitchedHare", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182od2y/how_can_i_prevent_the_following_error_column/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182od2y/how_can_i_prevent_the_following_error_column/", "subreddit_subscribers": 141423, "created_utc": 1700818016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know the best/safest option to copy a managed  table as a backup would be to use DEEP CLONE. But if it\u2019s not a big table is there any issues with just copying the source folder and renaming it as a quick and dirty solution? So long as your not changing anything within it the timestamps and versioning should remain as at the time of copy?\n\nAm I missing something massive here?", "author_fullname": "t2_scnmi5ys", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backing up Dbx Managed Table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182oayp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700817745.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know the best/safest option to copy a managed  table as a backup would be to use DEEP CLONE. But if it\u2019s not a big table is there any issues with just copying the source folder and renaming it as a quick and dirty solution? So long as your not changing anything within it the timestamps and versioning should remain as at the time of copy?&lt;/p&gt;\n\n&lt;p&gt;Am I missing something massive here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "182oayp", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Aardvark258", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182oayp/backing_up_dbx_managed_table/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182oayp/backing_up_dbx_managed_table/", "subreddit_subscribers": 141423, "created_utc": 1700817745.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone here work with Oracle WMS data? I started on a new project and it seems the team I'm in doesn't know how to extract data from Oracle WMS. I found their Rest APIs but these are slow. (124 rows per request... sigh).\n\nIs there a cloud database that Oracle WMS clients have access to to extract data quickly etc?", "author_fullname": "t2_a42ncwcn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle Warehouse Management System (WMS) Cloud, how to access the cloud database?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_182kyth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700804897.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone here work with Oracle WMS data? I started on a new project and it seems the team I&amp;#39;m in doesn&amp;#39;t know how to extract data from Oracle WMS. I found their Rest APIs but these are slow. (124 rows per request... sigh).&lt;/p&gt;\n\n&lt;p&gt;Is there a cloud database that Oracle WMS clients have access to to extract data quickly etc?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "182kyth", "is_robot_indexable": true, "report_reasons": null, "author": "MassiveDefender", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182kyth/oracle_warehouse_management_system_wms_cloud_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182kyth/oracle_warehouse_management_system_wms_cloud_how/", "subreddit_subscribers": 141423, "created_utc": 1700804897.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone have experience orchestrating AKS containers via Airflow? At the moment we are using a kubectl cli command to alert the agent pool to run in a bash operator in airflow, but doing so doesn\u2019t actually wait for the task to finish or provide any indication of if the task succeeded or failed.", "author_fullname": "t2_9d5p6jq3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Azure K8s Service via Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18217zc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700746173.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone have experience orchestrating AKS containers via Airflow? At the moment we are using a kubectl cli command to alert the agent pool to run in a bash operator in airflow, but doing so doesn\u2019t actually wait for the task to finish or provide any indication of if the task succeeded or failed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "18217zc", "is_robot_indexable": true, "report_reasons": null, "author": "avclipavclip", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18217zc/azure_k8s_service_via_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18217zc/azure_k8s_service_via_airflow/", "subreddit_subscribers": 141423, "created_utc": 1700746173.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_h557nj7lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "44 Best Resources to learn Data Engineering (YouTube, Books, Courses, &amp; Tutorials)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1823qih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CqnD0jXp1zy-FvPpGjVU-Jw9G9sUiC_Xd-9NZ2V_A3c.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700753655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mltut.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.mltut.com/best-resources-to-learn-data-engineering/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?auto=webp&amp;s=a500e2d0a55645b45e0cebf7674818a8b1a001ec", "width": 2240, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=846b2e4d36e11dd8b46ba3ae70e61fe3cb2d6be6", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=93e59e083e2cd40196a9c0e4ea2783db8f4a95a3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a47b29eae87c633967e98ac37f5cd8052261793", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d441865f1baacfeef03dee52e49abfad9bb30a7e", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3fc69de998a7ca3ca810b9446754b56529c75f73", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/1NkFMNXwXvVR1OEIDUFJu02P5zdR_ceifYkBd6ehlEc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=efcb52b6d7702e76f91f823e3c4894de7dda24e9", "width": 1080, "height": 607}], "variants": {}, "id": "Dfiy7g_nIo24a7MPAubj8Jy84Wz8r8RZoQE-MJidPWc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1823qih", "is_robot_indexable": true, "report_reasons": null, "author": "Aqsa81", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1823qih/44_best_resources_to_learn_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.mltut.com/best-resources-to-learn-data-engineering/", "subreddit_subscribers": 141423, "created_utc": 1700753655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "#  Overview of SQL\n\nSQL, pivotal in relational database management, serves as the backbone for efficiently managing, querying, and manipulating data across diverse systems like MySQL, SQL Server. Its uniform syntax ensures adaptability, making it a vital skill in various tech sectors. Evolving beyond basic data handling, SQL now embraces procedural elements, enhancing its capabilities in handling complex datasets and analytics. This evolution positions SQL as a key player not just in database management but also in data analysis, warehousing, and business intelligence.  \n \n\n## Understanding Python\n\nPython stands out as a high-level, interpreted programming language, celebrated for its exceptional readability and straightforward syntax, which lowers the barrier to entry for programming beginners. Unlike many other languages, Python\u2019s design philosophy emphasizes code readability and allows developers to express concepts in fewer lines of code than possible in languages like C++ or Java.\n\nPython\u2019s versatility is one of its strongest assets. It finds its applications sprawling across various domains \u2013 from web development, where frameworks like [Django](https://www.djangoproject.com/) and Flask facilitate rapid website design, to the realms of data science and machine learning, where it has become the lingua franca. This broad applicability is largely due to Python\u2019s extensive library ecosystem. Libraries such as Pandas and [NumPy](https://numpy.org/doc/stable/user/absolute_beginners.html) revolutionize data manipulation and analysis, enabling complex operations to be executed with ease. Similarly, libraries like TensorFlow and Scikit-Learn have made Python a pivotal tool in machine learning and AI development.\n\n## Choosing What to Learn First\n\nFrom an expert perspective, the decision on whether to start learning SQL or Python first in data engineering hinges on your career trajectory and the nature of the data work you aim to engage in. If your path is leaning more towards roles that revolve around database administration, business intelligence, or data warehousing, [SQL](https://dataengineeracademy.com/courses/sql-data-engineer-interview/) should be your starting point. Its simplicity and focus on structured query language provide a foundational understanding of how databases operate. This knowledge is crucial for efficiently managing and manipulating data within relational databases, a skill in high demand in numerous tech sectors.\n\nHowever, if your aspirations include delving into the realms of data science, machine learning, or comprehensive data analytics, Python is the recommended starting point. The language\u2019s versatility and the breadth of its applications make it a powerhouse in data engineering. With libraries like Pandas for data manipulation and Scikit-Learn for machine learning, Python equips you with tools to handle a wide array of data tasks, from processing large datasets to developing complex algorithms.\n\n&amp;#x200B;", "author_fullname": "t2_hiszlyka4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL vs Python. Which should I learn?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_182qumx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.13, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700828203.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Overview of SQL&lt;/h1&gt;\n\n&lt;p&gt;SQL, pivotal in relational database management, serves as the backbone for efficiently managing, querying, and manipulating data across diverse systems like MySQL, SQL Server. Its uniform syntax ensures adaptability, making it a vital skill in various tech sectors. Evolving beyond basic data handling, SQL now embraces procedural elements, enhancing its capabilities in handling complex datasets and analytics. This evolution positions SQL as a key player not just in database management but also in data analysis, warehousing, and business intelligence.  &lt;/p&gt;\n\n&lt;h2&gt;Understanding Python&lt;/h2&gt;\n\n&lt;p&gt;Python stands out as a high-level, interpreted programming language, celebrated for its exceptional readability and straightforward syntax, which lowers the barrier to entry for programming beginners. Unlike many other languages, Python\u2019s design philosophy emphasizes code readability and allows developers to express concepts in fewer lines of code than possible in languages like C++ or Java.&lt;/p&gt;\n\n&lt;p&gt;Python\u2019s versatility is one of its strongest assets. It finds its applications sprawling across various domains \u2013 from web development, where frameworks like &lt;a href=\"https://www.djangoproject.com/\"&gt;Django&lt;/a&gt; and Flask facilitate rapid website design, to the realms of data science and machine learning, where it has become the lingua franca. This broad applicability is largely due to Python\u2019s extensive library ecosystem. Libraries such as Pandas and &lt;a href=\"https://numpy.org/doc/stable/user/absolute_beginners.html\"&gt;NumPy&lt;/a&gt; revolutionize data manipulation and analysis, enabling complex operations to be executed with ease. Similarly, libraries like TensorFlow and Scikit-Learn have made Python a pivotal tool in machine learning and AI development.&lt;/p&gt;\n\n&lt;h2&gt;Choosing What to Learn First&lt;/h2&gt;\n\n&lt;p&gt;From an expert perspective, the decision on whether to start learning SQL or Python first in data engineering hinges on your career trajectory and the nature of the data work you aim to engage in. If your path is leaning more towards roles that revolve around database administration, business intelligence, or data warehousing, &lt;a href=\"https://dataengineeracademy.com/courses/sql-data-engineer-interview/\"&gt;SQL&lt;/a&gt; should be your starting point. Its simplicity and focus on structured query language provide a foundational understanding of how databases operate. This knowledge is crucial for efficiently managing and manipulating data within relational databases, a skill in high demand in numerous tech sectors.&lt;/p&gt;\n\n&lt;p&gt;However, if your aspirations include delving into the realms of data science, machine learning, or comprehensive data analytics, Python is the recommended starting point. The language\u2019s versatility and the breadth of its applications make it a powerhouse in data engineering. With libraries like Pandas for data manipulation and Scikit-Learn for machine learning, Python equips you with tools to handle a wide array of data tasks, from processing large datasets to developing complex algorithms.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VfsFzz4G1jt_2qdBtHSdBJjML-hpwJBolfSTCOOmgww.jpg?auto=webp&amp;s=6b0215b62a09bc99f3774a38489f5949ac2cce26", "width": 1200, "height": 546}, "resolutions": [{"url": "https://external-preview.redd.it/VfsFzz4G1jt_2qdBtHSdBJjML-hpwJBolfSTCOOmgww.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d9cac63cc9fe3c1f665a6f1036995dc4da36ceae", "width": 108, "height": 49}, {"url": "https://external-preview.redd.it/VfsFzz4G1jt_2qdBtHSdBJjML-hpwJBolfSTCOOmgww.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2f7bf78f893eaab29f80396b46588170df62067", "width": 216, "height": 98}, {"url": "https://external-preview.redd.it/VfsFzz4G1jt_2qdBtHSdBJjML-hpwJBolfSTCOOmgww.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec0cb11f2120085d034791250cbd87f501c170b1", "width": 320, "height": 145}, {"url": "https://external-preview.redd.it/VfsFzz4G1jt_2qdBtHSdBJjML-hpwJBolfSTCOOmgww.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=29f2c4e2964a96b330941ac0d0645d6792df10b5", "width": 640, "height": 291}, {"url": "https://external-preview.redd.it/VfsFzz4G1jt_2qdBtHSdBJjML-hpwJBolfSTCOOmgww.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=eef4a115da3cf25f7e893b1aa7ef2dce08cd6dc6", "width": 960, "height": 436}, {"url": "https://external-preview.redd.it/VfsFzz4G1jt_2qdBtHSdBJjML-hpwJBolfSTCOOmgww.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bdbf5a7f760758bf67f7171b8d43e7600ef96454", "width": 1080, "height": 491}], "variants": {}, "id": "8PVp3zrJQPyQMjrglD-gB2lXB2co_wiLCO2TLKUiqF8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "182qumx", "is_robot_indexable": true, "report_reasons": null, "author": "chris_garzon", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/182qumx/sql_vs_python_which_should_i_learn/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/182qumx/sql_vs_python_which_should_i_learn/", "subreddit_subscribers": 141423, "created_utc": 1700828203.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}