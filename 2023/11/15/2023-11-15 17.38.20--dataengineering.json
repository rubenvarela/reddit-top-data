{"kind": "Listing", "data": {"after": "t3_17vbfnf", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work at a well known tech company, with about 5 years of experience.\n\n\n\n\nSince the competition to get into other unicorn and top tech companies have been too much, I've been applying to mid-level and senior data engineering roles at startups to add some variety to my engineering experience.  I've honestly been surprised that I've never been able to get an offer.\n\n\n\n\nAre a lot of startups and smaller tech/non-tech companies now only considering very experienced senior/staff engineers with way more years of experience than me?  The tech screenings have been going pretty well, as I do a lot of Leetcode and system design review.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are most companies that are hiring only considering extremely qualified candidates?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v81vl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 67, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 67, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699984810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work at a well known tech company, with about 5 years of experience.&lt;/p&gt;\n\n&lt;p&gt;Since the competition to get into other unicorn and top tech companies have been too much, I&amp;#39;ve been applying to mid-level and senior data engineering roles at startups to add some variety to my engineering experience.  I&amp;#39;ve honestly been surprised that I&amp;#39;ve never been able to get an offer.&lt;/p&gt;\n\n&lt;p&gt;Are a lot of startups and smaller tech/non-tech companies now only considering very experienced senior/staff engineers with way more years of experience than me?  The tech screenings have been going pretty well, as I do a lot of Leetcode and system design review.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17v81vl", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17v81vl/are_most_companies_that_are_hiring_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v81vl/are_most_companies_that_are_hiring_only/", "subreddit_subscribers": 139818, "created_utc": 1699984810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've heard recently some developer friends talking about what questions to be prepared to answer in-depth, stuff like OOP, data structures, etc.\n\nI understand that in our area it's different, because services changes depending on the cloud provider that you're using (if you're using cloud). Services like Glue Jobs abstract the under-the-hood configuration of Spark, but you still need to know how to use Spark. Not to mention that DE might have different tasks from company to company.\n\nSo, what do you guys think that are questions that set aside good DEs to bad DEs?\n\n&amp;#x200B;", "author_fullname": "t2_jsmqklq8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the fundamental questions in a data engineering interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ve0jc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700000462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve heard recently some developer friends talking about what questions to be prepared to answer in-depth, stuff like OOP, data structures, etc.&lt;/p&gt;\n\n&lt;p&gt;I understand that in our area it&amp;#39;s different, because services changes depending on the cloud provider that you&amp;#39;re using (if you&amp;#39;re using cloud). Services like Glue Jobs abstract the under-the-hood configuration of Spark, but you still need to know how to use Spark. Not to mention that DE might have different tasks from company to company.&lt;/p&gt;\n\n&lt;p&gt;So, what do you guys think that are questions that set aside good DEs to bad DEs?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ve0jc", "is_robot_indexable": true, "report_reasons": null, "author": "Rude_effect_74", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ve0jc/what_are_the_fundamental_questions_in_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ve0jc/what_are_the_fundamental_questions_in_a_data/", "subreddit_subscribers": 139818, "created_utc": 1700000462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I built a tool to make it faster/easier to write python scripts that will clean up Excel files. It's mostly targeted towards people who are less technical, or people like me who can never remember the best practice keyword arguments for pd.read\\_csv() lol. \n\nI called it [Computron](https://app.squack.io/?utm_content=dataengineering&amp;utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=v0p3_uifix).   \n\nHere's how it works: \n\n* Upload any messy csv, xlsx, xls, or xlsm file\n* Type out commands for how you want to clean it up\n* Computron builds and executes Python code to follow the command using GPT-4\n* Once you're done, the code can compiled into a stand-alone automation and reused for other files\n* API support for the hosted automations is coming soon \n\nThe thing is I don't want this to be another bullshit AI tool. I'm posting this on a few data-related subreddits, so you guys can [try it](https://app.squack.io/?utm_content=dataengineering&amp;utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=v0p3_uifix) and be brutally honest about how to make it better.   \n\nAs a token of my appreciation for helping, anybody who makes an account at this early stage will have access to all of the paid features forever. I'm also happy to answer any questions, or give anybody a more in depth tutorial.", "author_fullname": "t2_898csv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Data Roomba\" for cleaning up data before onboarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vuaxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700057010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built a tool to make it faster/easier to write python scripts that will clean up Excel files. It&amp;#39;s mostly targeted towards people who are less technical, or people like me who can never remember the best practice keyword arguments for pd.read_csv() lol. &lt;/p&gt;\n\n&lt;p&gt;I called it &lt;a href=\"https://app.squack.io/?utm_content=dataengineering&amp;amp;utm_medium=social&amp;amp;utm_source=reddit&amp;amp;utm_campaign=v0p3_uifix\"&gt;Computron&lt;/a&gt;.   &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upload any messy csv, xlsx, xls, or xlsm file&lt;/li&gt;\n&lt;li&gt;Type out commands for how you want to clean it up&lt;/li&gt;\n&lt;li&gt;Computron builds and executes Python code to follow the command using GPT-4&lt;/li&gt;\n&lt;li&gt;Once you&amp;#39;re done, the code can compiled into a stand-alone automation and reused for other files&lt;/li&gt;\n&lt;li&gt;API support for the hosted automations is coming soon &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The thing is I don&amp;#39;t want this to be another bullshit AI tool. I&amp;#39;m posting this on a few data-related subreddits, so you guys can &lt;a href=\"https://app.squack.io/?utm_content=dataengineering&amp;amp;utm_medium=social&amp;amp;utm_source=reddit&amp;amp;utm_campaign=v0p3_uifix\"&gt;try it&lt;/a&gt; and be brutally honest about how to make it better.   &lt;/p&gt;\n\n&lt;p&gt;As a token of my appreciation for helping, anybody who makes an account at this early stage will have access to all of the paid features forever. I&amp;#39;m also happy to answer any questions, or give anybody a more in depth tutorial.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vuaxr", "is_robot_indexable": true, "report_reasons": null, "author": "evilredpanda", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vuaxr/data_roomba_for_cleaning_up_data_before_onboarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vuaxr/data_roomba_for_cleaning_up_data_before_onboarding/", "subreddit_subscribers": 139818, "created_utc": 1700057010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A few hours? \n\nA few days? \n\nA sprint? More than 1 sprint? \n\n&amp;#x200B;\n\nWhat's a reasonable expectation? We often have an issue where business users expect a data warehouse to be a thing where you can just send data easily with a few clicks and then the users can query it. \n\nHowever we know it has to be ingested, tested, tables created, scheduled, upserted/merged, etc. But maybe our process is actually bad and slower than it needs to be. What is a reasonable expectation? ", "author_fullname": "t2_b3n9i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long should it take to integrate a new data source into the data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vapzq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699991785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few hours? &lt;/p&gt;\n\n&lt;p&gt;A few days? &lt;/p&gt;\n\n&lt;p&gt;A sprint? More than 1 sprint? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s a reasonable expectation? We often have an issue where business users expect a data warehouse to be a thing where you can just send data easily with a few clicks and then the users can query it. &lt;/p&gt;\n\n&lt;p&gt;However we know it has to be ingested, tested, tables created, scheduled, upserted/merged, etc. But maybe our process is actually bad and slower than it needs to be. What is a reasonable expectation? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vapzq", "is_robot_indexable": true, "report_reasons": null, "author": "third_dude", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vapzq/how_long_should_it_take_to_integrate_a_new_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vapzq/how_long_should_it_take_to_integrate_a_new_data/", "subreddit_subscribers": 139818, "created_utc": 1699991785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering,\n\nFor anyone that says this is my fault for specializing in Microsoft stack - you're absolutely, 100% correct. I blame only myself. \n\nThe incessant cycle of \"progress\". I'm reaching my wit's end with how we're handling tech debt. It seems like every other year, there's a new 'bright new day' in the Microsoft analytics stack, and it's driving me nuts.\n\nFirst off, let's address the myth of avoiding tech debt. Spoiler alert: it's a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year's innovation is this year's digital paperweight.\n\nIt's a merry-go-round of mediocrity So, what do we do? We slap a new 'notebook' GUI over Spark clusters and pat ourselves on the back for 'innovation.' It's a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever's been rebranded this week, with awards handed out for sales volume, not product quality. \n\nWe've all heard the mantras: \"ADF is the way,\" \"Databricks is the way,\" \"Synapse is the way,\" \"Fabric is the way.\" It's just a parade of platforms, each hailed as the messiah of data engineering, but they're not, they're very naughty boys, only to be replaced by the next shiny thing in a year or two.\n\nI (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and 'platnum's to it.", "author_fullname": "t2_17e8xe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft data products - merry-go-round of mediocrity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17vxmth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700066411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;For anyone that says this is my fault for specializing in Microsoft stack - you&amp;#39;re absolutely, 100% correct. I blame only myself. &lt;/p&gt;\n\n&lt;p&gt;The incessant cycle of &amp;quot;progress&amp;quot;. I&amp;#39;m reaching my wit&amp;#39;s end with how we&amp;#39;re handling tech debt. It seems like every other year, there&amp;#39;s a new &amp;#39;bright new day&amp;#39; in the Microsoft analytics stack, and it&amp;#39;s driving me nuts.&lt;/p&gt;\n\n&lt;p&gt;First off, let&amp;#39;s address the myth of avoiding tech debt. Spoiler alert: it&amp;#39;s a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year&amp;#39;s innovation is this year&amp;#39;s digital paperweight.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a merry-go-round of mediocrity So, what do we do? We slap a new &amp;#39;notebook&amp;#39; GUI over Spark clusters and pat ourselves on the back for &amp;#39;innovation.&amp;#39; It&amp;#39;s a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever&amp;#39;s been rebranded this week, with awards handed out for sales volume, not product quality. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve all heard the mantras: &amp;quot;ADF is the way,&amp;quot; &amp;quot;Databricks is the way,&amp;quot; &amp;quot;Synapse is the way,&amp;quot; &amp;quot;Fabric is the way.&amp;quot; It&amp;#39;s just a parade of platforms, each hailed as the messiah of data engineering, but they&amp;#39;re not, they&amp;#39;re very naughty boys, only to be replaced by the next shiny thing in a year or two.&lt;/p&gt;\n\n&lt;p&gt;I (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and &amp;#39;platnum&amp;#39;s to it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vxmth", "is_robot_indexable": true, "report_reasons": null, "author": "biowl", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vxmth/microsoft_data_products_merrygoround_of_mediocrity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vxmth/microsoft_data_products_merrygoround_of_mediocrity/", "subreddit_subscribers": 139818, "created_utc": 1700066411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, so we are having some discussions internally about whether or not our data should be immutable and I wanted some sanity checking on how I am thinking about this. Just to give some general comments on how our data is set up currently the primary data that is used is stored in SQL and is used to serve customers with contracts etc on our website. We also have an analytics team that run a service which takes this relational data and processes it into a flat structure into Big Query to make it more easily digestible for analytics tools like Tableau. \n\nAll of the reasons we want to have immutable data make sense to me (being able to see the state of a contract at any given date, having logs of changes to make audits etc). But it also feels like we can accomplish this with an Audit Table and not changing the underlying data structure. To use \"Contract\" Table as an example I'm seeing two different implementations.\n\n1) We have a Contract table that is used by the customer facing services, who would only care about the current state of their contracts which can be updated, and create a separate Contract_Audit table which creates a new row with the current state of a contract whenever a contract gets updated (which can be automatically managed by our DB libraries)\n\n2) We have a Contract Table and whenever a contract is created or a change is made to it we store a new row for that contract. \n\nIf for the core business functionality we only care about the current state of the data, it seems like it would add needless complexity and data when interacting with these tables. While the needs for things like Audits and Analytics to have the full history of changes is served equally well by the Audit tables.\n\nWould love to hear some inputs or if there is anything I'm missing here, thanks!", "author_fullname": "t2_af3ee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Immutable data versus Audit Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vsexn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700050612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, so we are having some discussions internally about whether or not our data should be immutable and I wanted some sanity checking on how I am thinking about this. Just to give some general comments on how our data is set up currently the primary data that is used is stored in SQL and is used to serve customers with contracts etc on our website. We also have an analytics team that run a service which takes this relational data and processes it into a flat structure into Big Query to make it more easily digestible for analytics tools like Tableau. &lt;/p&gt;\n\n&lt;p&gt;All of the reasons we want to have immutable data make sense to me (being able to see the state of a contract at any given date, having logs of changes to make audits etc). But it also feels like we can accomplish this with an Audit Table and not changing the underlying data structure. To use &amp;quot;Contract&amp;quot; Table as an example I&amp;#39;m seeing two different implementations.&lt;/p&gt;\n\n&lt;p&gt;1) We have a Contract table that is used by the customer facing services, who would only care about the current state of their contracts which can be updated, and create a separate Contract_Audit table which creates a new row with the current state of a contract whenever a contract gets updated (which can be automatically managed by our DB libraries)&lt;/p&gt;\n\n&lt;p&gt;2) We have a Contract Table and whenever a contract is created or a change is made to it we store a new row for that contract. &lt;/p&gt;\n\n&lt;p&gt;If for the core business functionality we only care about the current state of the data, it seems like it would add needless complexity and data when interacting with these tables. While the needs for things like Audits and Analytics to have the full history of changes is served equally well by the Audit tables.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear some inputs or if there is anything I&amp;#39;m missing here, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vsexn", "is_robot_indexable": true, "report_reasons": null, "author": "relderpaway", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vsexn/immutable_data_versus_audit_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vsexn/immutable_data_versus_audit_tables/", "subreddit_subscribers": 139818, "created_utc": 1700050612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have experience with building star schema data marts and small - single use case focused - data warehouses but I am wondering about an enterprise (spanning all verticals within the company) data warehouse design best practices.\n\nSome questions that come to mind:\n\n1. As a rule of thumb.  for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called \"stage\" tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on a larger scale too or if maybe you just let historical information sit in file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.\n2. Do you **track history** for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called \"stage\" tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on larger scale too or if maybe you just let historical information sitting in a file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.\n3. How do you deal with **sensitive data** (PII, etc), do you separate them into special tables? Do you tokenize potentially sensitive natural keys? Do you even let PII and such enter the warehouse or store it somewhere else (some kind of vault)?\n4. Do you try to build one \"**source of truth**\" set of tables, that models your whole business and that everybody else then builds on top of (e.g. departmental data marts), or do you just bring in the data, capture the history, and cleanse it and let the departmental experts figure out themselves how to combine 10 different versions of customer data (from different systems) into one customer dataset? If you do try to build a \"source of truth\" is the resulting model typically a Star/Galaxy schema, or data vault, or some kind of hybrid?\n5. Do you have a **centralized** team owning the data warehouses and building out all the tables/models or do you split it between different departmental experts (kinda what data mesh suggests)?\n\nI have an opinion on these things based on my smaller-scale experience but wonder what worked best for folks who actually designed/maintained enterprise-scale dwhs before. If it makes any difference, assume the use of modern cloud data stack technologies.", "author_fullname": "t2_gx2hs6l34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Ideal\" data model for an enterprise data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vbmjo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699994398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have experience with building star schema data marts and small - single use case focused - data warehouses but I am wondering about an enterprise (spanning all verticals within the company) data warehouse design best practices.&lt;/p&gt;\n\n&lt;p&gt;Some questions that come to mind:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;As a rule of thumb.  for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called &amp;quot;stage&amp;quot; tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on a larger scale too or if maybe you just let historical information sit in file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.&lt;/li&gt;\n&lt;li&gt;Do you &lt;strong&gt;track history&lt;/strong&gt; for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called &amp;quot;stage&amp;quot; tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on larger scale too or if maybe you just let historical information sitting in a file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.&lt;/li&gt;\n&lt;li&gt;How do you deal with &lt;strong&gt;sensitive data&lt;/strong&gt; (PII, etc), do you separate them into special tables? Do you tokenize potentially sensitive natural keys? Do you even let PII and such enter the warehouse or store it somewhere else (some kind of vault)?&lt;/li&gt;\n&lt;li&gt;Do you try to build one &amp;quot;&lt;strong&gt;source of truth&lt;/strong&gt;&amp;quot; set of tables, that models your whole business and that everybody else then builds on top of (e.g. departmental data marts), or do you just bring in the data, capture the history, and cleanse it and let the departmental experts figure out themselves how to combine 10 different versions of customer data (from different systems) into one customer dataset? If you do try to build a &amp;quot;source of truth&amp;quot; is the resulting model typically a Star/Galaxy schema, or data vault, or some kind of hybrid?&lt;/li&gt;\n&lt;li&gt;Do you have a &lt;strong&gt;centralized&lt;/strong&gt; team owning the data warehouses and building out all the tables/models or do you split it between different departmental experts (kinda what data mesh suggests)?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have an opinion on these things based on my smaller-scale experience but wonder what worked best for folks who actually designed/maintained enterprise-scale dwhs before. If it makes any difference, assume the use of modern cloud data stack technologies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vbmjo", "is_robot_indexable": true, "report_reasons": null, "author": "InsightInk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vbmjo/ideal_data_model_for_an_enterprise_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vbmjo/ideal_data_model_for_an_enterprise_data_warehouse/", "subreddit_subscribers": 139818, "created_utc": 1699994398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I inherited a legacy ETL pipeline with a \"multi-stage\" architecture: each stage is a Django command script (a Python script run using python manage.py). For example, the first script (command) downloads and saves a file, also creating an entry in the SQL database. Another command then waits for this entry, updates its status (a column in the table), and performs data cleaning based on the file. Subsequent script commands take over when the status attribute is updated as completed for the previous stage, each performing additional data cleaning operations.\n\nThus, the architecture is segmented into different stages, each being a separate Python script running in a while loop, scanning the SQL table for documents with the requisite status attribute. The data processing, among others, involves sending files to different microservices (REST APIs) or executing simple indexing into other data sources (like other SQL tables or Elasticsearch).\n\nIn production, each script is containerized, with every stage deployed as a separate service in Kubernetes, featuring multiple replicated pods for each stage.\n\nAlthough this setup is quite beneficial as it allows increasing the number of replicas to manage congestion in a particular service, it feels overly complex and possibly overengineered. I am concerned about the appropriateness of using Django commands in an infinite loop and am exploring ways to simplify the ETL pipeline from its current state. Kubernetes deployments also make debugging each stage challenging, particularly when issues arise. With 8-9 different stages, each as a separate deployment, troubleshooting becomes time-consuming and complicated.\n\nI am seeking advice on what would be a good alternative to above described architecture, if instead of using django commands or kubernetes multi deployments there are better tools for ETL pipelines. Any recommendations what changes are worth making?", "author_fullname": "t2_ig1d8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving, simplified an ETL pipeline architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vo8b9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700032634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I inherited a legacy ETL pipeline with a &amp;quot;multi-stage&amp;quot; architecture: each stage is a Django command script (a Python script run using python manage.py). For example, the first script (command) downloads and saves a file, also creating an entry in the SQL database. Another command then waits for this entry, updates its status (a column in the table), and performs data cleaning based on the file. Subsequent script commands take over when the status attribute is updated as completed for the previous stage, each performing additional data cleaning operations.&lt;/p&gt;\n\n&lt;p&gt;Thus, the architecture is segmented into different stages, each being a separate Python script running in a while loop, scanning the SQL table for documents with the requisite status attribute. The data processing, among others, involves sending files to different microservices (REST APIs) or executing simple indexing into other data sources (like other SQL tables or Elasticsearch).&lt;/p&gt;\n\n&lt;p&gt;In production, each script is containerized, with every stage deployed as a separate service in Kubernetes, featuring multiple replicated pods for each stage.&lt;/p&gt;\n\n&lt;p&gt;Although this setup is quite beneficial as it allows increasing the number of replicas to manage congestion in a particular service, it feels overly complex and possibly overengineered. I am concerned about the appropriateness of using Django commands in an infinite loop and am exploring ways to simplify the ETL pipeline from its current state. Kubernetes deployments also make debugging each stage challenging, particularly when issues arise. With 8-9 different stages, each as a separate deployment, troubleshooting becomes time-consuming and complicated.&lt;/p&gt;\n\n&lt;p&gt;I am seeking advice on what would be a good alternative to above described architecture, if instead of using django commands or kubernetes multi deployments there are better tools for ETL pipelines. Any recommendations what changes are worth making?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vo8b9", "is_robot_indexable": true, "report_reasons": null, "author": "investopim", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vo8b9/improving_simplified_an_etl_pipeline_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vo8b9/improving_simplified_an_etl_pipeline_architecture/", "subreddit_subscribers": 139818, "created_utc": 1700032634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My colleague u/gunnarmorling and I have launched a monthly collection of read-worthy posts, events, announcements, and papers in the **data and stream processing space**.\n\n\ud83d\udcf0 The first edition is online now: [https://www.decodable.co/blog/checkpoint-chronicle-november-2023](https://www.decodable.co/blog/checkpoint-chronicle-november-2023)\n\nWould love to get any feedback (flamesuit at the ready ;) )", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checkpoint Chronicle - a monthly collection of interesting stuff in the data and stream processing space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v82or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699984858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My colleague &lt;a href=\"/u/gunnarmorling\"&gt;u/gunnarmorling&lt;/a&gt; and I have launched a monthly collection of read-worthy posts, events, announcements, and papers in the &lt;strong&gt;data and stream processing space&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcf0 The first edition is online now: &lt;a href=\"https://www.decodable.co/blog/checkpoint-chronicle-november-2023\"&gt;https://www.decodable.co/blog/checkpoint-chronicle-november-2023&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to get any feedback (flamesuit at the ready ;) )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?auto=webp&amp;s=a5142a62838c4edbf1c8b142600c82d4078d4b8f", "width": 640, "height": 402}, "resolutions": [{"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e83c3976ff24e6f20b66f959a4f87a8ccd221c9", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eeac073f1252f578b40279dffec10df8b889ae68", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84f7d88cecec068b380bfd91ff2a78bbf7f3624c", "width": 320, "height": 201}, {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3633d887d2af2d791833a95162b8cf03423b1c05", "width": 640, "height": 402}], "variants": {}, "id": "ozZsXqouUgAEnICZ6yJXC3C6GWE3PNs5b5Br2ZNTpuw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v82or", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v82or/checkpoint_chronicle_a_monthly_collection_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v82or/checkpoint_chronicle_a_monthly_collection_of/", "subreddit_subscribers": 139818, "created_utc": 1699984858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you have a go-to SQL formatting style that's kinda like PEP8 for Python?\n\nStyle guide recommendations?", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Formatting Tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vlv1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700055712.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700023410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have a go-to SQL formatting style that&amp;#39;s kinda like PEP8 for Python?&lt;/p&gt;\n\n&lt;p&gt;Style guide recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vlv1s", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vlv1s/sql_formatting_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vlv1s/sql_formatting_tips/", "subreddit_subscribers": 139818, "created_utc": 1700023410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. I'm migrating SAS code to Databricks, and one thing that I need to reproduce is summary statistics, especially frequency distributions . In particular it's \"proc freq\" and univariate functions in SAS, for example. \n\nI calculated the frequency distribution manually, but it would be helpful if there was a function to give you that and more. I'm searching but not seeing much.\n\nIs there a particular Pyspark library I should be looking at? Thanks. ", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Pyspark have more detailed summary statistics beyond .describe and .summary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vi2u5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700011573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I&amp;#39;m migrating SAS code to Databricks, and one thing that I need to reproduce is summary statistics, especially frequency distributions . In particular it&amp;#39;s &amp;quot;proc freq&amp;quot; and univariate functions in SAS, for example. &lt;/p&gt;\n\n&lt;p&gt;I calculated the frequency distribution manually, but it would be helpful if there was a function to give you that and more. I&amp;#39;m searching but not seeing much.&lt;/p&gt;\n\n&lt;p&gt;Is there a particular Pyspark library I should be looking at? Thanks. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vi2u5", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vi2u5/does_pyspark_have_more_detailed_summary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vi2u5/does_pyspark_have_more_detailed_summary/", "subreddit_subscribers": 139818, "created_utc": 1700011573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning to a Data Product Ecosystem: Leveraging the Evolutionary Architecture 4D Architectures, Data-Driven Routing, Feature Toggles, and more!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17vt8mn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/6vDlkW5g9Ov2fi4qiqucF_O3EGlGVFi-VIH-dXwFbK4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700053598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/transitioning-to-a-data-product-ecosystem", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?auto=webp&amp;s=3082285d046bb96e056e3beb6bab7cc485567de3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=644cca4795796eb921dd4625ff9c54b794d4b0eb", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ea9e084b2aafd7bd8605a6427295eea54585448", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae64c7f1f9a6e6d90348bb507812c8dcba804134", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9b2005f4e3b121435e3c44ed1cf042df746c0fd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d546543f7d963a212d8f96dc95c922942a33bce4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=943e28e4ddb97fe923c7bfb089bcbaa4234bb8d4", "width": 1080, "height": 540}], "variants": {}, "id": "JUVxsJydfJfdVJZ75jZtmnS48hsaGgbV73xyphmtuyA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vt8mn", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vt8mn/transitioning_to_a_data_product_ecosystem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/transitioning-to-a-data-product-ecosystem", "subreddit_subscribers": 139818, "created_utc": 1700053598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am beginning with a new blog series about Microsoft Azure. As the first article you get your hands on the Azure SQL Database service. Where you will create one and import csv data to create a table. Happy for some feedback!\n\n[https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405](https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405)", "author_fullname": "t2_3di0zmcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Effortless Azure: Mastering SQL Database Creation and Data Import", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vt2w7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700053085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am beginning with a new blog series about Microsoft Azure. As the first article you get your hands on the Azure SQL Database service. Where you will create one and import csv data to create a table. Happy for some feedback!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405\"&gt;https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?auto=webp&amp;s=e3518f63ea54c0bace227cb57575f54f713d6723", "width": 728, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e68244bd449a8dc4b1188bd6ba8aabeb9f3d048", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=22a4ab3540f79686ec32b5e4c51c9bf4424430f6", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e133141465c286fc35b94ada6863c8d04d2cbd5", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc30fdc12505f2560f38d0c6282bc0f083ea3deb", "width": 640, "height": 334}], "variants": {}, "id": "_gJpBnOR4HASqLUzy_v5MZHn85O6SH4CEnQM2i9rsXM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vt2w7", "is_robot_indexable": true, "report_reasons": null, "author": "EdgarHuber", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vt2w7/effortless_azure_mastering_sql_database_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vt2w7/effortless_azure_mastering_sql_database_creation/", "subreddit_subscribers": 139818, "created_utc": 1700053085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "we are currently developing an open-source tool for automated feature engineering on relational data and time series.\n\nhttps://github.com/getml/getml-community\n\nIt is similar to featuretools or tsfresh, but over 100x faster than these tools and considerably more memory-efficient. As far as we know, it is the fastest open-source tool for this purpose in the world.\n\nThis is possible because we are using a customized database engine written in C++. But we have a Python interface, of course.\n\nAny kind of feedback, particularly constructive criticism, is very welcome.", "author_fullname": "t2_jb6i198h7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "getml: The fastest open-Source tool for automated feature engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vp0cl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700036067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;we are currently developing an open-source tool for automated feature engineering on relational data and time series.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/getml/getml-community\"&gt;https://github.com/getml/getml-community&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It is similar to featuretools or tsfresh, but over 100x faster than these tools and considerably more memory-efficient. As far as we know, it is the fastest open-source tool for this purpose in the world.&lt;/p&gt;\n\n&lt;p&gt;This is possible because we are using a customized database engine written in C++. But we have a Python interface, of course.&lt;/p&gt;\n\n&lt;p&gt;Any kind of feedback, particularly constructive criticism, is very welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?auto=webp&amp;s=eb2045b295522111b93ce18ba65695fb9ee9fa1c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=485764bed081e0d849c206179843b8d5527797d1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f263cbe1aabc5ecd312d14c613a74cfff8dd7c30", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1944111d10e89d167d1af93a86786103eec17ae6", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=276765321edee973a0d7df6b933d35c43d9cd0c1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3fd4c9612a6f78142907b5cad0043ad0d5bfe069", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fd4bff5b2bdf63ed65fb335da4a9a6796bb877f", "width": 1080, "height": 540}], "variants": {}, "id": "zCk3-zYBuePTRrdQOPtxaaeRuLOJGQKxQD5N-zvb_sM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17vp0cl", "is_robot_indexable": true, "report_reasons": null, "author": "liuzicheng1987", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vp0cl/getml_the_fastest_opensource_tool_for_automated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vp0cl/getml_the_fastest_opensource_tool_for_automated/", "subreddit_subscribers": 139818, "created_utc": 1700036067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does Databricks CE have real-time collaborative editing like, say, Google Docs?", "author_fullname": "t2_nvxordug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Community Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vod8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700033520.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700033206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does Databricks CE have real-time collaborative editing like, say, Google Docs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vod8c", "is_robot_indexable": true, "report_reasons": null, "author": "obsculosa", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vod8c/databricks_community_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vod8c/databricks_community_edition/", "subreddit_subscribers": 139818, "created_utc": 1700033206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I recently joined a small healthcare as the first data analyst and soon after we purchased powerBI. I cirrently work on excel to clean data and powerbi to import and analyze, prepare dashboards/reports. While this is very simple to what I want to do as a data analyst (especially that we are expanding our services currently). While this also means more data and more complexity.\n\nCurrently I store data in a drive that is hosted by our IT department. \n\nI know basic SQL and want to keep that knowledge growing by using it at my work. I talked to my manager and he said if you think sql will help us any better than the regular drive where we store data, then we can get sql. \n\nWhats the best SQL thing (not a technical person here) that I can use in my company that will help me maintain a database, use sql commands frequently and what would be the benefits of using a sql database vs regular drive where I store data manually. Also is ms sql a good option or azure sql database? \n\nJust so you understand the nature of my work. We have 50+ employees submitting 50+ records everyday on google sheet. I take that data and paste in in excel, save the file with the employees name and date/year and store it in the drive. Every week, Im suppose to send a report saying how many records are submitting by each employee and whats the average, how many offices the reports were sent to and stuff like that. \n\nId appreciate a deep insight on what to use, how to convince manager and how to get sql in my organization", "author_fullname": "t2_83gq3oxts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS SQL / AZURE SQL data base?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v7i4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699983376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently joined a small healthcare as the first data analyst and soon after we purchased powerBI. I cirrently work on excel to clean data and powerbi to import and analyze, prepare dashboards/reports. While this is very simple to what I want to do as a data analyst (especially that we are expanding our services currently). While this also means more data and more complexity.&lt;/p&gt;\n\n&lt;p&gt;Currently I store data in a drive that is hosted by our IT department. &lt;/p&gt;\n\n&lt;p&gt;I know basic SQL and want to keep that knowledge growing by using it at my work. I talked to my manager and he said if you think sql will help us any better than the regular drive where we store data, then we can get sql. &lt;/p&gt;\n\n&lt;p&gt;Whats the best SQL thing (not a technical person here) that I can use in my company that will help me maintain a database, use sql commands frequently and what would be the benefits of using a sql database vs regular drive where I store data manually. Also is ms sql a good option or azure sql database? &lt;/p&gt;\n\n&lt;p&gt;Just so you understand the nature of my work. We have 50+ employees submitting 50+ records everyday on google sheet. I take that data and paste in in excel, save the file with the employees name and date/year and store it in the drive. Every week, Im suppose to send a report saying how many records are submitting by each employee and whats the average, how many offices the reports were sent to and stuff like that. &lt;/p&gt;\n\n&lt;p&gt;Id appreciate a deep insight on what to use, how to convince manager and how to get sql in my organization&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17v7i4n", "is_robot_indexable": true, "report_reasons": null, "author": "FigTraditional1201", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v7i4n/ms_sql_azure_sql_data_base/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v7i4n/ms_sql_azure_sql_data_base/", "subreddit_subscribers": 139818, "created_utc": 1699983376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering what other orgs look like for DE teams. We currently have to handle all of our QA as a DE team while our app dev team has a dedicated team of a handful of resources. ", "author_fullname": "t2_81ywblydd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you have a dedicated QA team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17vxubk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700066992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering what other orgs look like for DE teams. We currently have to handle all of our QA as a DE team while our app dev team has a dedicated team of a handful of resources. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vxubk", "is_robot_indexable": true, "report_reasons": null, "author": "DataDoyle", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vxubk/do_you_have_a_dedicated_qa_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vxubk/do_you_have_a_dedicated_qa_team/", "subreddit_subscribers": 139818, "created_utc": 1700066992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ancj6nr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Create multiple Lambda functions using single docker image", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_17vwdnx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ti8TlQhDeXEfBimGs77DP_EhS43FUOYsxkkHnZLAmmM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700062935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/9TsFcShFuSw", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WALxJhPlAzzcprNHLhlwjaTV9Xm9BWLK5mR-T40JCJs.jpg?auto=webp&amp;s=09081783114059b844df4c2bba23c26837f00ebc", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/WALxJhPlAzzcprNHLhlwjaTV9Xm9BWLK5mR-T40JCJs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0b52241720a5d63d4bba880043bf2f4cf6cefe3c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/WALxJhPlAzzcprNHLhlwjaTV9Xm9BWLK5mR-T40JCJs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=202ac3ba92bc057bd0588dff04fbeca3750abdb7", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/WALxJhPlAzzcprNHLhlwjaTV9Xm9BWLK5mR-T40JCJs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8a2822171b3e14caa0f638a0bce9ae4f7dc181f", "width": 320, "height": 240}], "variants": {}, "id": "x_Rwuff3Kjb9HAnmEmzzSootwBWxGmcikFSh1J7gLy8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vwdnx", "is_robot_indexable": true, "report_reasons": null, "author": "Recent_Ocelot_724", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vwdnx/create_multiple_lambda_functions_using_single/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/9TsFcShFuSw", "subreddit_subscribers": 139818, "created_utc": 1700062935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS", "author_fullname": "t2_ancj6nr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_17vwc5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "author_name": "The iT Tech Solutions", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/9rzIhRb2qKk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheiTTechSolutions"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17vwc5q", "height": 200}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/fjtk7XwbiXbVLM9hCNd3gxKDCkXWO7aQEZETfYj8Wj4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700062825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/9rzIhRb2qKk", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?auto=webp&amp;s=8ec9545a9f8b1c557ee9af6002b086b698a9e3c7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d48648c7370d7259d4beec81cf28d52300b0afd", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47748d9418c4591d8b33469fab5a501a97708ba4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d8c859dad79ffa253ab9640e50f591be0e3de7e", "width": 320, "height": 240}], "variants": {}, "id": "-ubKmFhXuBQ2j857IyozL8XfyJT9uZB5ca8iMjrzCJU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vwc5q", "is_robot_indexable": true, "report_reasons": null, "author": "Recent_Ocelot_724", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vwc5q/aws_step_function_variables_resultpath_inputpath/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/9rzIhRb2qKk", "subreddit_subscribers": 139818, "created_utc": 1700062825.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "author_name": "The iT Tech Solutions", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/9rzIhRb2qKk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheiTTechSolutions"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey team! Hope you're doing fine. Quick question about segment and snowflake integration. I've landed in a company where they use multiple segment projects and data from those projects is loaded hourly to our snowflake datawarehouse. We then use dbt to transform and load this data into the warehouse.   \n\n\nI've noticed segment loads three standard tables (pages, tracks &amp; identifies) for each project, and then for each tracked event generates a new table with the data that already exists in tracks for each event plus the tracked custom data sent in the event payload. \u00bfIs there a way to avoid segment sending all the context fields in the event custom table? I think it's not good to have this data duplicated in both the custom and tracks table, I'm I wrong?\n\nSecondly, I would like to automate the dbt transform step and avoid doing it myself every time a new event is needed for analysis in the DW. Did anybody do that before? What would be the best approach?. Also, I would like to run the whole segment sync as a whole and avoid generating airflow dags for each synchronization. That would save me a lot of time as well.\n\nThanks for reading!", "author_fullname": "t2_36w05yox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Segment &amp; Snowflake Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vvweu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700061621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey team! Hope you&amp;#39;re doing fine. Quick question about segment and snowflake integration. I&amp;#39;ve landed in a company where they use multiple segment projects and data from those projects is loaded hourly to our snowflake datawarehouse. We then use dbt to transform and load this data into the warehouse.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed segment loads three standard tables (pages, tracks &amp;amp; identifies) for each project, and then for each tracked event generates a new table with the data that already exists in tracks for each event plus the tracked custom data sent in the event payload. \u00bfIs there a way to avoid segment sending all the context fields in the event custom table? I think it&amp;#39;s not good to have this data duplicated in both the custom and tracks table, I&amp;#39;m I wrong?&lt;/p&gt;\n\n&lt;p&gt;Secondly, I would like to automate the dbt transform step and avoid doing it myself every time a new event is needed for analysis in the DW. Did anybody do that before? What would be the best approach?. Also, I would like to run the whole segment sync as a whole and avoid generating airflow dags for each synchronization. That would save me a lot of time as well.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vvweu", "is_robot_indexable": true, "report_reasons": null, "author": "asnopm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vvweu/segment_snowflake_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vvweu/segment_snowflake_integration/", "subreddit_subscribers": 139818, "created_utc": 1700061621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen discussion and videos were they say it's really hard to contribute to the framework just because they accept only from existing commiters. I want to contribute in order to learn, advance my career and give back to the community and I don't want to dig in learning the internal codebase and later have no chance.", "author_fullname": "t2_76srr1mpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it really hard to contribute to Apache Spark [codebase]?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vv8sl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700059797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen discussion and videos were they say it&amp;#39;s really hard to contribute to the framework just because they accept only from existing commiters. I want to contribute in order to learn, advance my career and give back to the community and I don&amp;#39;t want to dig in learning the internal codebase and later have no chance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17vv8sl", "is_robot_indexable": true, "report_reasons": null, "author": "pr6g_head", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17vv8sl/is_it_really_hard_to_contribute_to_apache_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vv8sl/is_it_really_hard_to_contribute_to_apache_spark/", "subreddit_subscribers": 139818, "created_utc": 1700059797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a computational biologist (8 YoE) in big pharma. I have been home brewing databases for my department among other things. Recently, during a re-org upheaval I was told I could be a good fit for a Sr Data Engineer role at my company, a good fit in the sense that I bring domain expertise and they can give me on-hands training in data engineering. The HM specifically said \u201cI can teach you Data Engineering, I can\u2019t teach you Biology\u201d.\nI am genuinely considering this offer as I have been contemplating a switch to Data Science/Engineering for the past year. I was wondering if anyone in the group has made a similar lateral switch, and what your experience was. I am curious how math heavy such a role would be, I have been told they use BigQuery, DataBricks, Onyx and elements of GCP and Kubernetes, but I\u2019ll be honest I have no experience in any of these. Also, generally how stable is a career in data engineering? Is domain expertise really that important in your opinion?", "author_fullname": "t2_s5s0xh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about a lateral switch into data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vtv0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700055645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a computational biologist (8 YoE) in big pharma. I have been home brewing databases for my department among other things. Recently, during a re-org upheaval I was told I could be a good fit for a Sr Data Engineer role at my company, a good fit in the sense that I bring domain expertise and they can give me on-hands training in data engineering. The HM specifically said \u201cI can teach you Data Engineering, I can\u2019t teach you Biology\u201d.\nI am genuinely considering this offer as I have been contemplating a switch to Data Science/Engineering for the past year. I was wondering if anyone in the group has made a similar lateral switch, and what your experience was. I am curious how math heavy such a role would be, I have been told they use BigQuery, DataBricks, Onyx and elements of GCP and Kubernetes, but I\u2019ll be honest I have no experience in any of these. Also, generally how stable is a career in data engineering? Is domain expertise really that important in your opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17vtv0o", "is_robot_indexable": true, "report_reasons": null, "author": "EuphoricArtichoke", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vtv0o/question_about_a_lateral_switch_into_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vtv0o/question_about_a_lateral_switch_into_data/", "subreddit_subscribers": 139818, "created_utc": 1700055645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have already passed SAA 2 weeks ago. I have 3 years of experience as a DE. Thanks", "author_fullname": "t2_p9gvk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I take AWS Data Analytics or new DE associate cert?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vt23e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700053004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have already passed SAA 2 weeks ago. I have 3 years of experience as a DE. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17vt23e", "is_robot_indexable": true, "report_reasons": null, "author": "marcelorojas56", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vt23e/should_i_take_aws_data_analytics_or_new_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vt23e/should_i_take_aws_data_analytics_or_new_de/", "subreddit_subscribers": 139818, "created_utc": 1700053004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there\n\nI am trying to build a small pipeline to get idea of how things work.\nThe pipeline: Fetch data from API -\u203a load it to the local DB -\u203a track/visualise on looker Studio\n\nThe Toolkit: Python, Mysql DB, GCP. and for the orchestration: airflow or task scheduler\n\nCurrently, Im at the data ingesting part (setting up Mysql DB), and I just want share my process and hope you can suggest impovements to the process,\n\nLets say I need to load data of all the locations supported by source platform (OpenAQ), I'm doing requests.get() the data, transform it to fit my local schema, mostly extract what is necessary and load it using Mysql.connector,\nso like wise for each DB entity (Table), I'm making a new python file to do the loading.\n\nI find the process very not simple and I couldn\u2019t find any ideas specific for this situation or as narrow as this, so Im hoping you guys can shed some light for me.\n\nYour comments/suggestions/critique will help me improve my understanding immensely.\n\nThanks", "author_fullname": "t2_jsaf7cene", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I improve Extract and load?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vf7ih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700003588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I am trying to build a small pipeline to get idea of how things work.\nThe pipeline: Fetch data from API -\u203a load it to the local DB -\u203a track/visualise on looker Studio&lt;/p&gt;\n\n&lt;p&gt;The Toolkit: Python, Mysql DB, GCP. and for the orchestration: airflow or task scheduler&lt;/p&gt;\n\n&lt;p&gt;Currently, Im at the data ingesting part (setting up Mysql DB), and I just want share my process and hope you can suggest impovements to the process,&lt;/p&gt;\n\n&lt;p&gt;Lets say I need to load data of all the locations supported by source platform (OpenAQ), I&amp;#39;m doing requests.get() the data, transform it to fit my local schema, mostly extract what is necessary and load it using Mysql.connector,\nso like wise for each DB entity (Table), I&amp;#39;m making a new python file to do the loading.&lt;/p&gt;\n\n&lt;p&gt;I find the process very not simple and I couldn\u2019t find any ideas specific for this situation or as narrow as this, so Im hoping you guys can shed some light for me.&lt;/p&gt;\n\n&lt;p&gt;Your comments/suggestions/critique will help me improve my understanding immensely.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vf7ih", "is_robot_indexable": true, "report_reasons": null, "author": "trickytoughtruth", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vf7ih/how_can_i_improve_extract_and_load/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vf7ih/how_can_i_improve_extract_and_load/", "subreddit_subscribers": 139818, "created_utc": 1700003588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm currently working in somewhat of a data analytics role but am looking to transition into data engineering. In my current position, I have an idea to create my own project would be helpful to talk about in a portfolio.\n\nThe current process is simply to pull data from an integrated tool that our clients' use and export it into Excel to analyze and create a Power BI dashboard. The integrated tool that is used also has a reporting api that I have access to that can also provide more data than the default integrated tool. The api has a over a dozen endpoints that can be accessed.\n\nIt's not a huge amount of data that can be pulled at once (probably very low thousand rows and then a few hundred per day/week) as the tool has a data retention policy of a few months. So I was thinking of pulling the most data that I can within the retention range into some sort of database, then appending new data to it as time goes on.\n\nI started using Python and Pandas to create some dataframes with some of the most used endpoints and merge them together. But I don't think that would be the most efficient way.\n\nHow do you think I should approach idea to turn it into a nice DE project? What tools would be most efficient and beneficial to use?\n\nCheers", "author_fullname": "t2_6h4gboa8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions of creating a new data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vbfnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699993887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working in somewhat of a data analytics role but am looking to transition into data engineering. In my current position, I have an idea to create my own project would be helpful to talk about in a portfolio.&lt;/p&gt;\n\n&lt;p&gt;The current process is simply to pull data from an integrated tool that our clients&amp;#39; use and export it into Excel to analyze and create a Power BI dashboard. The integrated tool that is used also has a reporting api that I have access to that can also provide more data than the default integrated tool. The api has a over a dozen endpoints that can be accessed.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not a huge amount of data that can be pulled at once (probably very low thousand rows and then a few hundred per day/week) as the tool has a data retention policy of a few months. So I was thinking of pulling the most data that I can within the retention range into some sort of database, then appending new data to it as time goes on.&lt;/p&gt;\n\n&lt;p&gt;I started using Python and Pandas to create some dataframes with some of the most used endpoints and merge them together. But I don&amp;#39;t think that would be the most efficient way.&lt;/p&gt;\n\n&lt;p&gt;How do you think I should approach idea to turn it into a nice DE project? What tools would be most efficient and beneficial to use?&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vbfnf", "is_robot_indexable": true, "report_reasons": null, "author": "BBlluurrrryy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vbfnf/suggestions_of_creating_a_new_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vbfnf/suggestions_of_creating_a_new_data_pipeline/", "subreddit_subscribers": 139818, "created_utc": 1699993887.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}