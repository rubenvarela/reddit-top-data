{"kind": "Listing", "data": {"after": "t3_17v75ce", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work at a well known tech company, with about 5 years of experience.\n\n\n\n\nSince the competition to get into other unicorn and top tech companies have been too much, I've been applying to mid-level and senior data engineering roles at startups to add some variety to my engineering experience.  I've honestly been surprised that I've never been able to get an offer.\n\n\n\n\nAre a lot of startups and smaller tech/non-tech companies now only considering very experienced senior/staff engineers with way more years of experience than me?  The tech screenings have been going pretty well, as I do a lot of Leetcode and system design review.", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are most companies that are hiring only considering extremely qualified candidates?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v81vl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 57, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 57, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699984810.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work at a well known tech company, with about 5 years of experience.&lt;/p&gt;\n\n&lt;p&gt;Since the competition to get into other unicorn and top tech companies have been too much, I&amp;#39;ve been applying to mid-level and senior data engineering roles at startups to add some variety to my engineering experience.  I&amp;#39;ve honestly been surprised that I&amp;#39;ve never been able to get an offer.&lt;/p&gt;\n\n&lt;p&gt;Are a lot of startups and smaller tech/non-tech companies now only considering very experienced senior/staff engineers with way more years of experience than me?  The tech screenings have been going pretty well, as I do a lot of Leetcode and system design review.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17v81vl", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17v81vl/are_most_companies_that_are_hiring_only/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v81vl/are_most_companies_that_are_hiring_only/", "subreddit_subscribers": 139761, "created_utc": 1699984810.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m after finishing the snowflake fundamentals course and want to pursue as many certifications as possible, any advice would be great, thank you.", "author_fullname": "t2_6j3548qy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best snowflake certifications/hands on essentials to have as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17uzdy2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699957617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m after finishing the snowflake fundamentals course and want to pursue as many certifications as possible, any advice would be great, thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17uzdy2", "is_robot_indexable": true, "report_reasons": null, "author": "Kokadoodles", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uzdy2/what_are_the_best_snowflake_certificationshands/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17uzdy2/what_are_the_best_snowflake_certificationshands/", "subreddit_subscribers": 139761, "created_utc": 1699957617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've heard recently some developer friends talking about what questions to be prepared to answer in-depth, stuff like OOP, data structures, etc.\n\nI understand that in our area it's different, because services changes depending on the cloud provider that you're using (if you're using cloud). Services like Glue Jobs abstract the under-the-hood configuration of Spark, but you still need to know how to use Spark. Not to mention that DE might have different tasks from company to company.\n\nSo, what do you guys think that are questions that set aside good DEs to bad DEs?\n\n&amp;#x200B;", "author_fullname": "t2_jsmqklq8w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the fundamental questions in a data engineering interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17ve0jc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700000462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve heard recently some developer friends talking about what questions to be prepared to answer in-depth, stuff like OOP, data structures, etc.&lt;/p&gt;\n\n&lt;p&gt;I understand that in our area it&amp;#39;s different, because services changes depending on the cloud provider that you&amp;#39;re using (if you&amp;#39;re using cloud). Services like Glue Jobs abstract the under-the-hood configuration of Spark, but you still need to know how to use Spark. Not to mention that DE might have different tasks from company to company.&lt;/p&gt;\n\n&lt;p&gt;So, what do you guys think that are questions that set aside good DEs to bad DEs?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17ve0jc", "is_robot_indexable": true, "report_reasons": null, "author": "Rude_effect_74", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17ve0jc/what_are_the_fundamental_questions_in_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17ve0jc/what_are_the_fundamental_questions_in_a_data/", "subreddit_subscribers": 139761, "created_utc": 1700000462.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My colleague u/gunnarmorling and I have launched a monthly collection of read-worthy posts, events, announcements, and papers in the **data and stream processing space**.\n\n\ud83d\udcf0 The first edition is online now: [https://www.decodable.co/blog/checkpoint-chronicle-november-2023](https://www.decodable.co/blog/checkpoint-chronicle-november-2023)\n\nWould love to get any feedback (flamesuit at the ready ;) )", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Checkpoint Chronicle - a monthly collection of interesting stuff in the data and stream processing space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v82or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699984858.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My colleague &lt;a href=\"/u/gunnarmorling\"&gt;u/gunnarmorling&lt;/a&gt; and I have launched a monthly collection of read-worthy posts, events, announcements, and papers in the &lt;strong&gt;data and stream processing space&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;\ud83d\udcf0 The first edition is online now: &lt;a href=\"https://www.decodable.co/blog/checkpoint-chronicle-november-2023\"&gt;https://www.decodable.co/blog/checkpoint-chronicle-november-2023&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love to get any feedback (flamesuit at the ready ;) )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?auto=webp&amp;s=a5142a62838c4edbf1c8b142600c82d4078d4b8f", "width": 640, "height": 402}, "resolutions": [{"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1e83c3976ff24e6f20b66f959a4f87a8ccd221c9", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eeac073f1252f578b40279dffec10df8b889ae68", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=84f7d88cecec068b380bfd91ff2a78bbf7f3624c", "width": 320, "height": 201}, {"url": "https://external-preview.redd.it/WbVFb1EZykw4iRsMcVoxirrVjIrGIGpzq4OyuCQgsWI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3633d887d2af2d791833a95162b8cf03423b1c05", "width": 640, "height": 402}], "variants": {}, "id": "ozZsXqouUgAEnICZ6yJXC3C6GWE3PNs5b5Br2ZNTpuw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v82or", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v82or/checkpoint_chronicle_a_monthly_collection_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v82or/checkpoint_chronicle_a_monthly_collection_of/", "subreddit_subscribers": 139761, "created_utc": 1699984858.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I have experience with building star schema data marts and small - single use case focused - data warehouses but I am wondering about an enterprise (spanning all verticals within the company) data warehouse design best practices.\n\nSome questions that come to mind:\n\n1. As a rule of thumb.  for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called \"stage\" tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on a larger scale too or if maybe you just let historical information sit in file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.\n2. Do you **track history** for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called \"stage\" tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on larger scale too or if maybe you just let historical information sitting in a file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.\n3. How do you deal with **sensitive data** (PII, etc), do you separate them into special tables? Do you tokenize potentially sensitive natural keys? Do you even let PII and such enter the warehouse or store it somewhere else (some kind of vault)?\n4. Do you try to build one \"**source of truth**\" set of tables, that models your whole business and that everybody else then builds on top of (e.g. departmental data marts), or do you just bring in the data, capture the history, and cleanse it and let the departmental experts figure out themselves how to combine 10 different versions of customer data (from different systems) into one customer dataset? If you do try to build a \"source of truth\" is the resulting model typically a Star/Galaxy schema, or data vault, or some kind of hybrid?\n5. Do you have a **centralized** team owning the data warehouses and building out all the tables/models or do you split it between different departmental experts (kinda what data mesh suggests)?\n\nI have an opinion on these things based on my smaller-scale experience but wonder what worked best for folks who actually designed/maintained enterprise-scale dwhs before. If it makes any difference, assume the use of modern cloud data stack technologies.", "author_fullname": "t2_gx2hs6l34", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Ideal\" data model for an enterprise data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vbmjo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699994398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I have experience with building star schema data marts and small - single use case focused - data warehouses but I am wondering about an enterprise (spanning all verticals within the company) data warehouse design best practices.&lt;/p&gt;\n\n&lt;p&gt;Some questions that come to mind:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;As a rule of thumb.  for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called &amp;quot;stage&amp;quot; tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on a larger scale too or if maybe you just let historical information sit in file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.&lt;/li&gt;\n&lt;li&gt;Do you &lt;strong&gt;track history&lt;/strong&gt; for all tables by default or only where you already know it will be needed? I have used valid from/to fields on what we called &amp;quot;stage&amp;quot; tables - persistent tables that contained all data we have received from the source, before any transformations. Then we only pulled historical data from them into data marts that actually needed it. Not sure if that\u2019s how it\u2019s typically done on larger scale too or if maybe you just let historical information sitting in a file storage (raw exports from systems) and pull it into DWH only if it becomes necessary.&lt;/li&gt;\n&lt;li&gt;How do you deal with &lt;strong&gt;sensitive data&lt;/strong&gt; (PII, etc), do you separate them into special tables? Do you tokenize potentially sensitive natural keys? Do you even let PII and such enter the warehouse or store it somewhere else (some kind of vault)?&lt;/li&gt;\n&lt;li&gt;Do you try to build one &amp;quot;&lt;strong&gt;source of truth&lt;/strong&gt;&amp;quot; set of tables, that models your whole business and that everybody else then builds on top of (e.g. departmental data marts), or do you just bring in the data, capture the history, and cleanse it and let the departmental experts figure out themselves how to combine 10 different versions of customer data (from different systems) into one customer dataset? If you do try to build a &amp;quot;source of truth&amp;quot; is the resulting model typically a Star/Galaxy schema, or data vault, or some kind of hybrid?&lt;/li&gt;\n&lt;li&gt;Do you have a &lt;strong&gt;centralized&lt;/strong&gt; team owning the data warehouses and building out all the tables/models or do you split it between different departmental experts (kinda what data mesh suggests)?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I have an opinion on these things based on my smaller-scale experience but wonder what worked best for folks who actually designed/maintained enterprise-scale dwhs before. If it makes any difference, assume the use of modern cloud data stack technologies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vbmjo", "is_robot_indexable": true, "report_reasons": null, "author": "InsightInk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vbmjo/ideal_data_model_for_an_enterprise_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vbmjo/ideal_data_model_for_an_enterprise_data_warehouse/", "subreddit_subscribers": 139761, "created_utc": 1699994398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "A few hours? \n\nA few days? \n\nA sprint? More than 1 sprint? \n\n&amp;#x200B;\n\nWhat's a reasonable expectation? We often have an issue where business users expect a data warehouse to be a thing where you can just send data easily with a few clicks and then the users can query it. \n\nHowever we know it has to be ingested, tested, tables created, scheduled, upserted/merged, etc. But maybe our process is actually bad and slower than it needs to be. What is a reasonable expectation? ", "author_fullname": "t2_b3n9i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How long should it take to integrate a new data source into the data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vapzq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699991785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few hours? &lt;/p&gt;\n\n&lt;p&gt;A few days? &lt;/p&gt;\n\n&lt;p&gt;A sprint? More than 1 sprint? &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s a reasonable expectation? We often have an issue where business users expect a data warehouse to be a thing where you can just send data easily with a few clicks and then the users can query it. &lt;/p&gt;\n\n&lt;p&gt;However we know it has to be ingested, tested, tables created, scheduled, upserted/merged, etc. But maybe our process is actually bad and slower than it needs to be. What is a reasonable expectation? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vapzq", "is_robot_indexable": true, "report_reasons": null, "author": "third_dude", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vapzq/how_long_should_it_take_to_integrate_a_new_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vapzq/how_long_should_it_take_to_integrate_a_new_data/", "subreddit_subscribers": 139761, "created_utc": 1699991785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm using Meltano for data integration. I need to put data into an Iceberg data lake. Unfortunately, meltano doesn't have a target for Iceberg. Is there anyone who can point me in the right direction?", "author_fullname": "t2_1ajx7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is anyone experienced with both Meltano and Iceberg?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v6jdr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1699985801.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699980834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m using Meltano for data integration. I need to put data into an Iceberg data lake. Unfortunately, meltano doesn&amp;#39;t have a target for Iceberg. Is there anyone who can point me in the right direction?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v6jdr", "is_robot_indexable": true, "report_reasons": null, "author": "TaeefNajib", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v6jdr/is_anyone_experienced_with_both_meltano_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v6jdr/is_anyone_experienced_with_both_meltano_and/", "subreddit_subscribers": 139761, "created_utc": 1699980834.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey fellow SQL monkeys, some of you may know that my team has been drowning in adhoc SQL requests, most of them are simple and should be automated. The time spent on context switching and dealing with the stakeholders could be better spent on modelling, cleaning up our dbt project, and improving our pipeline orchestration.\n\nSome background on me: I've been building and scaling data teams at startups the last 5 years. The startup I'm at now has about 300 employees with a 3 person data team including me. We use \"self-serve\" tools like Looker and hold periodic training sessions, but there's always data requests that can't be self-served and it blows up our backlog. Our team is stretched thin and we decided to deploy a LLM to automate these requests. It's been working very well, we're seeing most requests answered by the LLM, with only a few that we have to review. \n\nWe've been posting in a few data communities in Reddit over the past month and it seems like this isn't just an issue at my org, that's why we're hoping to get your feedback to make something awesome and give time back to your teams", "author_fullname": "t2_l386p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "looking for users to pilot tool to automate adhoc SQL requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v5yak", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699979260.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey fellow SQL monkeys, some of you may know that my team has been drowning in adhoc SQL requests, most of them are simple and should be automated. The time spent on context switching and dealing with the stakeholders could be better spent on modelling, cleaning up our dbt project, and improving our pipeline orchestration.&lt;/p&gt;\n\n&lt;p&gt;Some background on me: I&amp;#39;ve been building and scaling data teams at startups the last 5 years. The startup I&amp;#39;m at now has about 300 employees with a 3 person data team including me. We use &amp;quot;self-serve&amp;quot; tools like Looker and hold periodic training sessions, but there&amp;#39;s always data requests that can&amp;#39;t be self-served and it blows up our backlog. Our team is stretched thin and we decided to deploy a LLM to automate these requests. It&amp;#39;s been working very well, we&amp;#39;re seeing most requests answered by the LLM, with only a few that we have to review. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve been posting in a few data communities in Reddit over the past month and it seems like this isn&amp;#39;t just an issue at my org, that&amp;#39;s why we&amp;#39;re hoping to get your feedback to make something awesome and give time back to your teams&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v5yak", "is_robot_indexable": true, "report_reasons": null, "author": "ruckrawjers", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v5yak/looking_for_users_to_pilot_tool_to_automate_adhoc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v5yak/looking_for_users_to_pilot_tool_to_automate_adhoc/", "subreddit_subscribers": 139761, "created_utc": 1699979260.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I inherited a legacy ETL pipeline with a \"multi-stage\" architecture: each stage is a Django command script (a Python script run using python manage.py). For example, the first script (command) downloads and saves a file, also creating an entry in the SQL database. Another command then waits for this entry, updates its status (a column in the table), and performs data cleaning based on the file. Subsequent script commands take over when the status attribute is updated as completed for the previous stage, each performing additional data cleaning operations.\n\nThus, the architecture is segmented into different stages, each being a separate Python script running in a while loop, scanning the SQL table for documents with the requisite status attribute. The data processing, among others, involves sending files to different microservices (REST APIs) or executing simple indexing into other data sources (like other SQL tables or Elasticsearch).\n\nIn production, each script is containerized, with every stage deployed as a separate service in Kubernetes, featuring multiple replicated pods for each stage.\n\nAlthough this setup is quite beneficial as it allows increasing the number of replicas to manage congestion in a particular service, it feels overly complex and possibly overengineered. I am concerned about the appropriateness of using Django commands in an infinite loop and am exploring ways to simplify the ETL pipeline from its current state. Kubernetes deployments also make debugging each stage challenging, particularly when issues arise. With 8-9 different stages, each as a separate deployment, troubleshooting becomes time-consuming and complicated.\n\nI am seeking advice on what would be a good alternative to above described architecture, if instead of using django commands or kubernetes multi deployments there are better tools for ETL pipelines. Any recommendations what changes are worth making?", "author_fullname": "t2_ig1d8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving, simplified an ETL pipeline architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17vo8b9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700032634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I inherited a legacy ETL pipeline with a &amp;quot;multi-stage&amp;quot; architecture: each stage is a Django command script (a Python script run using python manage.py). For example, the first script (command) downloads and saves a file, also creating an entry in the SQL database. Another command then waits for this entry, updates its status (a column in the table), and performs data cleaning based on the file. Subsequent script commands take over when the status attribute is updated as completed for the previous stage, each performing additional data cleaning operations.&lt;/p&gt;\n\n&lt;p&gt;Thus, the architecture is segmented into different stages, each being a separate Python script running in a while loop, scanning the SQL table for documents with the requisite status attribute. The data processing, among others, involves sending files to different microservices (REST APIs) or executing simple indexing into other data sources (like other SQL tables or Elasticsearch).&lt;/p&gt;\n\n&lt;p&gt;In production, each script is containerized, with every stage deployed as a separate service in Kubernetes, featuring multiple replicated pods for each stage.&lt;/p&gt;\n\n&lt;p&gt;Although this setup is quite beneficial as it allows increasing the number of replicas to manage congestion in a particular service, it feels overly complex and possibly overengineered. I am concerned about the appropriateness of using Django commands in an infinite loop and am exploring ways to simplify the ETL pipeline from its current state. Kubernetes deployments also make debugging each stage challenging, particularly when issues arise. With 8-9 different stages, each as a separate deployment, troubleshooting becomes time-consuming and complicated.&lt;/p&gt;\n\n&lt;p&gt;I am seeking advice on what would be a good alternative to above described architecture, if instead of using django commands or kubernetes multi deployments there are better tools for ETL pipelines. Any recommendations what changes are worth making?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vo8b9", "is_robot_indexable": true, "report_reasons": null, "author": "investopim", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vo8b9/improving_simplified_an_etl_pipeline_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vo8b9/improving_simplified_an_etl_pipeline_architecture/", "subreddit_subscribers": 139761, "created_utc": 1700032634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. I'm migrating SAS code to Databricks, and one thing that I need to reproduce is summary statistics, especially frequency distributions . In particular it's \"proc freq\" and univariate functions in SAS, for example. \n\nI calculated the frequency distribution manually, but it would be helpful if there was a function to give you that and more. I'm searching but not seeing much.\n\nIs there a particular Pyspark library I should be looking at? Thanks. ", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Pyspark have more detailed summary statistics beyond .describe and .summary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vi2u5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700011573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I&amp;#39;m migrating SAS code to Databricks, and one thing that I need to reproduce is summary statistics, especially frequency distributions . In particular it&amp;#39;s &amp;quot;proc freq&amp;quot; and univariate functions in SAS, for example. &lt;/p&gt;\n\n&lt;p&gt;I calculated the frequency distribution manually, but it would be helpful if there was a function to give you that and more. I&amp;#39;m searching but not seeing much.&lt;/p&gt;\n\n&lt;p&gt;Is there a particular Pyspark library I should be looking at? Thanks. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vi2u5", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vi2u5/does_pyspark_have_more_detailed_summary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vi2u5/does_pyspark_have_more_detailed_summary/", "subreddit_subscribers": 139761, "created_utc": 1700011573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I recently joined a small healthcare as the first data analyst and soon after we purchased powerBI. I cirrently work on excel to clean data and powerbi to import and analyze, prepare dashboards/reports. While this is very simple to what I want to do as a data analyst (especially that we are expanding our services currently). While this also means more data and more complexity.\n\nCurrently I store data in a drive that is hosted by our IT department. \n\nI know basic SQL and want to keep that knowledge growing by using it at my work. I talked to my manager and he said if you think sql will help us any better than the regular drive where we store data, then we can get sql. \n\nWhats the best SQL thing (not a technical person here) that I can use in my company that will help me maintain a database, use sql commands frequently and what would be the benefits of using a sql database vs regular drive where I store data manually. Also is ms sql a good option or azure sql database? \n\nJust so you understand the nature of my work. We have 50+ employees submitting 50+ records everyday on google sheet. I take that data and paste in in excel, save the file with the employees name and date/year and store it in the drive. Every week, Im suppose to send a report saying how many records are submitting by each employee and whats the average, how many offices the reports were sent to and stuff like that. \n\nId appreciate a deep insight on what to use, how to convince manager and how to get sql in my organization", "author_fullname": "t2_83gq3oxts", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MS SQL / AZURE SQL data base?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v7i4n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699983376.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently joined a small healthcare as the first data analyst and soon after we purchased powerBI. I cirrently work on excel to clean data and powerbi to import and analyze, prepare dashboards/reports. While this is very simple to what I want to do as a data analyst (especially that we are expanding our services currently). While this also means more data and more complexity.&lt;/p&gt;\n\n&lt;p&gt;Currently I store data in a drive that is hosted by our IT department. &lt;/p&gt;\n\n&lt;p&gt;I know basic SQL and want to keep that knowledge growing by using it at my work. I talked to my manager and he said if you think sql will help us any better than the regular drive where we store data, then we can get sql. &lt;/p&gt;\n\n&lt;p&gt;Whats the best SQL thing (not a technical person here) that I can use in my company that will help me maintain a database, use sql commands frequently and what would be the benefits of using a sql database vs regular drive where I store data manually. Also is ms sql a good option or azure sql database? &lt;/p&gt;\n\n&lt;p&gt;Just so you understand the nature of my work. We have 50+ employees submitting 50+ records everyday on google sheet. I take that data and paste in in excel, save the file with the employees name and date/year and store it in the drive. Every week, Im suppose to send a report saying how many records are submitting by each employee and whats the average, how many offices the reports were sent to and stuff like that. &lt;/p&gt;\n\n&lt;p&gt;Id appreciate a deep insight on what to use, how to convince manager and how to get sql in my organization&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17v7i4n", "is_robot_indexable": true, "report_reasons": null, "author": "FigTraditional1201", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v7i4n/ms_sql_azure_sql_data_base/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v7i4n/ms_sql_azure_sql_data_base/", "subreddit_subscribers": 139761, "created_utc": 1699983376.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to prepare for your databricks BDR interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17voos1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700034658.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17voos1", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17voos1/how_to_prepare_for_your_databricks_bdr_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17voos1/how_to_prepare_for_your_databricks_bdr_interview/", "subreddit_subscribers": 139761, "created_utc": 1700034658.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does Databricks CE have real-time collaborative editing like, say, Google Docs?", "author_fullname": "t2_nvxordug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Community Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17vod8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700033520.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700033206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does Databricks CE have real-time collaborative editing like, say, Google Docs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vod8c", "is_robot_indexable": true, "report_reasons": null, "author": "obsculosa", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vod8c/databricks_community_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vod8c/databricks_community_edition/", "subreddit_subscribers": 139761, "created_utc": 1700033206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5ggm5svc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataCamp Black Friday Sale", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 77, "top_awarded_type": null, "hide_score": false, "name": "t3_17vmdyq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/T8i8CFUa7nptxBQq7xJYLh36Ug2RzEP8H89WdBqL9tk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700025297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "google.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.google.com/amp/s/onlinecoursesgalore.com/datacamp-black-friday-sale/amp/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ppmy4qvcSCSVlQgBiYhbXioRIMfepttWdKBLzRPtaV0.jpg?auto=webp&amp;s=75f6f761ea64c9400576f70b2ad299b3aef73ce5", "width": 800, "height": 445}, "resolutions": [{"url": "https://external-preview.redd.it/ppmy4qvcSCSVlQgBiYhbXioRIMfepttWdKBLzRPtaV0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f63c2de49fa78f19e877f232c10543b1be5f72a5", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ppmy4qvcSCSVlQgBiYhbXioRIMfepttWdKBLzRPtaV0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c9aeac477eb602d2534eebebbffc52e76b167195", "width": 216, "height": 120}, {"url": "https://external-preview.redd.it/ppmy4qvcSCSVlQgBiYhbXioRIMfepttWdKBLzRPtaV0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08b98c3463a7abe4c621ccf06cab2cc58cd8234a", "width": 320, "height": 178}, {"url": "https://external-preview.redd.it/ppmy4qvcSCSVlQgBiYhbXioRIMfepttWdKBLzRPtaV0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2681d47f93000ff07c192d05fd5c9cd8fbe4873", "width": 640, "height": 356}], "variants": {}, "id": "CT4bgrJlc2B-Eu0LBa6UNhPMnqnVZMp1hJ9BfFVHLGs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vmdyq", "is_robot_indexable": true, "report_reasons": null, "author": "awsconsultant", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vmdyq/datacamp_black_friday_sale/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.google.com/amp/s/onlinecoursesgalore.com/datacamp-black-friday-sale/amp/", "subreddit_subscribers": 139761, "created_utc": 1700025297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you have a go-to SQL formatting style that's kinda like PEP8 for Python?", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Formatting Tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vlv1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700023410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have a go-to SQL formatting style that&amp;#39;s kinda like PEP8 for Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vlv1s", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vlv1s/sql_formatting_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vlv1s/sql_formatting_tips/", "subreddit_subscribers": 139761, "created_utc": 1700023410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer, and I will be joining a finance and investment company a month from now. Given that I have the free time, I would like to dedicate it in learning the said domain. Particularly focusing on property debt, mid-market corporate debt and direct property investment. My goal is to gain foundational knowledge in this domain so I won\u2019t struggle understanding too many jargons. These topics are very alien to me and would appreciate any help from you guys.\n\nAnyone here came from a Finance Background which can help cherry-pick the following courses?\n\n[The Complete Financial Analyst Training and Investing Course](https://www.udemy.com/share/101WaS3@DFYP-ZaN24CF27gDDd1SLpl6Rs8hQ_U6ftm8ByVNG9QcSoT1LAWoNX_k8jLMZasv/)\n\n[The Complete Financial Analyst Course](https://www.udemy.com/share/101WmC3@-0eh1EU-715TfKuvgnhV_RfN5pPT3DIiWCslhw18lmKjvMqKG4GsHWLjUpzCuZ86/)\n\n[The Complete Investment Banking Course](https://www.udemy.com/share/101Wt03@wvADSnUd7alNVSc7POyk5xI6n5w_4UugI7Rkz5XJZntX3vhJlna1lFG48rc2wF30/)", "author_fullname": "t2_85ty6e1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finance and Investment Data Domain", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vfzwd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700005704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer, and I will be joining a finance and investment company a month from now. Given that I have the free time, I would like to dedicate it in learning the said domain. Particularly focusing on property debt, mid-market corporate debt and direct property investment. My goal is to gain foundational knowledge in this domain so I won\u2019t struggle understanding too many jargons. These topics are very alien to me and would appreciate any help from you guys.&lt;/p&gt;\n\n&lt;p&gt;Anyone here came from a Finance Background which can help cherry-pick the following courses?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/share/101WaS3@DFYP-ZaN24CF27gDDd1SLpl6Rs8hQ_U6ftm8ByVNG9QcSoT1LAWoNX_k8jLMZasv/\"&gt;The Complete Financial Analyst Training and Investing Course&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/share/101WmC3@-0eh1EU-715TfKuvgnhV_RfN5pPT3DIiWCslhw18lmKjvMqKG4GsHWLjUpzCuZ86/\"&gt;The Complete Financial Analyst Course&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/share/101Wt03@wvADSnUd7alNVSc7POyk5xI6n5w_4UugI7Rkz5XJZntX3vhJlna1lFG48rc2wF30/\"&gt;The Complete Investment Banking Course&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vfzwd", "is_robot_indexable": true, "report_reasons": null, "author": "BestBlackberry1314", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vfzwd/finance_and_investment_data_domain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vfzwd/finance_and_investment_data_domain/", "subreddit_subscribers": 139761, "created_utc": 1700005704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there\n\nI am trying to build a small pipeline to get idea of how things work.\nThe pipeline: Fetch data from API -\u203a load it to the local DB -\u203a track/visualise on looker Studio\n\nThe Toolkit: Python, Mysql DB, GCP. and for the orchestration: airflow or task scheduler\n\nCurrently, Im at the data ingesting part (setting up Mysql DB), and I just want share my process and hope you can suggest impovements to the process,\n\nLets say I need to load data of all the locations supported by source platform (OpenAQ), I'm doing requests.get() the data, transform it to fit my local schema, mostly extract what is necessary and load it using Mysql.connector,\nso like wise for each DB entity (Table), I'm making a new python file to do the loading.\n\nI find the process very not simple and I couldn\u2019t find any ideas specific for this situation or as narrow as this, so Im hoping you guys can shed some light for me.\n\nYour comments/suggestions/critique will help me improve my understanding immensely.\n\nThanks", "author_fullname": "t2_jsaf7cene", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I improve Extract and load?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vf7ih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700003588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I am trying to build a small pipeline to get idea of how things work.\nThe pipeline: Fetch data from API -\u203a load it to the local DB -\u203a track/visualise on looker Studio&lt;/p&gt;\n\n&lt;p&gt;The Toolkit: Python, Mysql DB, GCP. and for the orchestration: airflow or task scheduler&lt;/p&gt;\n\n&lt;p&gt;Currently, Im at the data ingesting part (setting up Mysql DB), and I just want share my process and hope you can suggest impovements to the process,&lt;/p&gt;\n\n&lt;p&gt;Lets say I need to load data of all the locations supported by source platform (OpenAQ), I&amp;#39;m doing requests.get() the data, transform it to fit my local schema, mostly extract what is necessary and load it using Mysql.connector,\nso like wise for each DB entity (Table), I&amp;#39;m making a new python file to do the loading.&lt;/p&gt;\n\n&lt;p&gt;I find the process very not simple and I couldn\u2019t find any ideas specific for this situation or as narrow as this, so Im hoping you guys can shed some light for me.&lt;/p&gt;\n\n&lt;p&gt;Your comments/suggestions/critique will help me improve my understanding immensely.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vf7ih", "is_robot_indexable": true, "report_reasons": null, "author": "trickytoughtruth", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vf7ih/how_can_i_improve_extract_and_load/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vf7ih/how_can_i_improve_extract_and_load/", "subreddit_subscribers": 139761, "created_utc": 1700003588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All,\n\nI'm currently working in somewhat of a data analytics role but am looking to transition into data engineering. In my current position, I have an idea to create my own project would be helpful to talk about in a portfolio.\n\nThe current process is simply to pull data from an integrated tool that our clients' use and export it into Excel to analyze and create a Power BI dashboard. The integrated tool that is used also has a reporting api that I have access to that can also provide more data than the default integrated tool. The api has a over a dozen endpoints that can be accessed.\n\nIt's not a huge amount of data that can be pulled at once (probably very low thousand rows and then a few hundred per day/week) as the tool has a data retention policy of a few months. So I was thinking of pulling the most data that I can within the retention range into some sort of database, then appending new data to it as time goes on.\n\nI started using Python and Pandas to create some dataframes with some of the most used endpoints and merge them together. But I don't think that would be the most efficient way.\n\nHow do you think I should approach idea to turn it into a nice DE project? What tools would be most efficient and beneficial to use?\n\nCheers", "author_fullname": "t2_6h4gboa8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions of creating a new data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vbfnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699993887.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working in somewhat of a data analytics role but am looking to transition into data engineering. In my current position, I have an idea to create my own project would be helpful to talk about in a portfolio.&lt;/p&gt;\n\n&lt;p&gt;The current process is simply to pull data from an integrated tool that our clients&amp;#39; use and export it into Excel to analyze and create a Power BI dashboard. The integrated tool that is used also has a reporting api that I have access to that can also provide more data than the default integrated tool. The api has a over a dozen endpoints that can be accessed.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not a huge amount of data that can be pulled at once (probably very low thousand rows and then a few hundred per day/week) as the tool has a data retention policy of a few months. So I was thinking of pulling the most data that I can within the retention range into some sort of database, then appending new data to it as time goes on.&lt;/p&gt;\n\n&lt;p&gt;I started using Python and Pandas to create some dataframes with some of the most used endpoints and merge them together. But I don&amp;#39;t think that would be the most efficient way.&lt;/p&gt;\n\n&lt;p&gt;How do you think I should approach idea to turn it into a nice DE project? What tools would be most efficient and beneficial to use?&lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vbfnf", "is_robot_indexable": true, "report_reasons": null, "author": "BBlluurrrryy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vbfnf/suggestions_of_creating_a_new_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vbfnf/suggestions_of_creating_a_new_data_pipeline/", "subreddit_subscribers": 139761, "created_utc": 1699993887.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I need to ensure that the company where I work is capable of handling requests for personal data deletion (due to GDPR). I am looking for a solution that can centralize data from different data sources (MSSQS, MySql, etc.) so that I can identify where sensitive information is stored and anonymize it. Which solution can I use? I've researched and found Atlan, but I'm unsure if it's the perfect fit for my needs. ", "author_fullname": "t2_dat51pdz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice on data discovery solutions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v49dz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699974609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to ensure that the company where I work is capable of handling requests for personal data deletion (due to GDPR). I am looking for a solution that can centralize data from different data sources (MSSQS, MySql, etc.) so that I can identify where sensitive information is stored and anonymize it. Which solution can I use? I&amp;#39;ve researched and found Atlan, but I&amp;#39;m unsure if it&amp;#39;s the perfect fit for my needs. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v49dz", "is_robot_indexable": true, "report_reasons": null, "author": "cyberporing", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v49dz/need_advice_on_data_discovery_solutions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v49dz/need_advice_on_data_discovery_solutions/", "subreddit_subscribers": 139761, "created_utc": 1699974609.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The title pretty much says it all. I\u2019m using Databricks Lakehouse as my warehouse in case that changes anything. We\u2019re using incremental materializations for our fact table data into Silver, but the architects want table materialization for dimension data as a whole, and fact data from Silver to Gold.", "author_fullname": "t2_xbe9keo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I materialize my models as tables in dbt but not actually drop my tables every time the model is run?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v1i0h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699966091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title pretty much says it all. I\u2019m using Databricks Lakehouse as my warehouse in case that changes anything. We\u2019re using incremental materializations for our fact table data into Silver, but the architects want table materialization for dimension data as a whole, and fact data from Silver to Gold.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v1i0h", "is_robot_indexable": true, "report_reasons": null, "author": "therealtonyryantime", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v1i0h/how_can_i_materialize_my_models_as_tables_in_dbt/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v1i0h/how_can_i_materialize_my_models_as_tables_in_dbt/", "subreddit_subscribers": 139761, "created_utc": 1699966091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_c5qi5iukw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Personal Odyssey: \u201cTransforming Data Analytics with AI\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17uz7yw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.56, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/2RbtNib7Tb1qf0rIeTxN2L6bMN_AI_cswEpRxweyMsk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699956835.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/emrcv4gega0c1.png", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/emrcv4gega0c1.png?auto=webp&amp;s=e641c6ce3bd8e8ca7a487d664c1c422cf3ed4af3", "width": 4184, "height": 2108}, "resolutions": [{"url": "https://preview.redd.it/emrcv4gega0c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e11c5e27db9ff611210473a5001e0d19408b2597", "width": 108, "height": 54}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ca8970619b3224854c0103fc4beda842e793eb2", "width": 216, "height": 108}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fdac6852afcd5eebfda79fcc184b4a41d7ba47b", "width": 320, "height": 161}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f1e9ce5b0c691de0a595dbd3c0b6391ad79c6182", "width": 640, "height": 322}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08a4e3d1988c1dcff300989f8c94eeec2c8a6ff9", "width": 960, "height": 483}, {"url": "https://preview.redd.it/emrcv4gega0c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9a7c5798c217403019734db5be074abfe116cecd", "width": 1080, "height": 544}], "variants": {}, "id": "9qbS3SOEIwlA1QHAaXh0VYAmEzHVCKByJTy0_rB_mAw"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17uz7yw", "is_robot_indexable": true, "report_reasons": null, "author": "legoaitech", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17uz7yw/a_personal_odyssey_transforming_data_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/emrcv4gega0c1.png", "subreddit_subscribers": 139761, "created_utc": 1699956835.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI\u2019m a very new data engineer at a company who\u2019s wanting me to figure out if they should use DBT or not. Currently using GCP BigQuery but am trying to see if the built in data governance tools in Dataplex are sufficient or if DBT will be useful. \n\nWhat are the main differences in using Attribute Store, Profile, and Data Quality in Dataplex vs starting to use DBT? Mostly everything is coded in python.\n\n\nThanks", "author_fullname": "t2_l2ggjmpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I suggest DBT to my company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v6kpk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699980935.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m a very new data engineer at a company who\u2019s wanting me to figure out if they should use DBT or not. Currently using GCP BigQuery but am trying to see if the built in data governance tools in Dataplex are sufficient or if DBT will be useful. &lt;/p&gt;\n\n&lt;p&gt;What are the main differences in using Attribute Store, Profile, and Data Quality in Dataplex vs starting to use DBT? Mostly everything is coded in python.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v6kpk", "is_robot_indexable": true, "report_reasons": null, "author": "Prestigious-Egg-2582", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v6kpk/should_i_suggest_dbt_to_my_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v6kpk/should_i_suggest_dbt_to_my_company/", "subreddit_subscribers": 139761, "created_utc": 1699980935.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Guys,\n\nRecently I have written a blog on Spark Optimizations in Medium.., would highly appreciate your reviews..\n\nlink: [https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1](https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1)\n\nThank you...", "author_fullname": "t2_b9lhjoqu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark Optimizations &amp; Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v19u6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1699965242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys,&lt;/p&gt;\n\n&lt;p&gt;Recently I have written a blog on Spark Optimizations in Medium.., would highly appreciate your reviews..&lt;/p&gt;\n\n&lt;p&gt;link: &lt;a href=\"https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1\"&gt;https://medium.com/@rakeshreddy1618/spark-job-optimizations-databricks-87d8095937e1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?auto=webp&amp;s=e19c54812ba96b901b55cc127fa6ba6b84465164", "width": 1200, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc787dc826cc9ba7b2a0415ff7039dc8edfab4ba", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66c58e683324bef9e85c739f0ccd24d321cb6121", "width": 216, "height": 116}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=af8ee5a165e80af11111b8c168a15b4356fcd864", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d7c652ea446c6cd7e3230f515d29026f1e893322", "width": 640, "height": 346}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5a0cf82e63521d7f38b7ad13fa093a7a1f3de1fd", "width": 960, "height": 520}, {"url": "https://external-preview.redd.it/hk6_LXkzHHivr3ecXic6DLMIwaQSAugQP56FjPWLBYo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f858035747215b6705bfbaf47797f9e986b68795", "width": 1080, "height": 585}], "variants": {}, "id": "RTzjCNOob_aJ6Ogb_CAni_XWChMnzTF_FIm-ifa23Zk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v19u6", "is_robot_indexable": true, "report_reasons": null, "author": "Electrical_Duty8486", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v19u6/spark_optimizations_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v19u6/spark_optimizations_databricks/", "subreddit_subscribers": 139761, "created_utc": 1699965242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m using fivetran as a connector but i could find Odoo to Bigquery \nAny idea how to replicate my data into my bigquery data warehouse?", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Odoo &lt;&gt; Bigquery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17v10wf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1699964313.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m using fivetran as a connector but i could find Odoo to Bigquery \nAny idea how to replicate my data into my bigquery data warehouse?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17v10wf", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v10wf/odoo_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17v10wf/odoo_bigquery/", "subreddit_subscribers": 139761, "created_utc": 1699964313.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_6f2li", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cost-predictable logging at scale with ClickHouse, Grafana &amp; WarpStream", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_17v75ce", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/z_i73tkAespAEloVaohpZrJLTCPzT9EmEKh-iQhSKTs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1699982427.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "clickhouse.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://clickhouse.com/blog/cost-predictable-logging-with-clickhouse-vs-datadog-elastic-stack", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?auto=webp&amp;s=0f62866c7add760051b827fc27a17325fd41c075", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=62312bed324273672c1a6e3b3aefb564a0965ffc", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=168b62593e8eb085ca5f1177be1e76f4e6cfe311", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c86ba87a4ea4caf45a4a4220a5398ac6019c256f", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eba908273861bcab4188d5f92fb246b3f917fd9a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=093b0fd61ea2f95e938cec95cc961c6a2c382665", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/WP8O8K4pSjMJsKMph5tQgrIAOTtIo5UR7hDLtBDu89o.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d22096c7b3883535e3492680d612b2b7281d8357", "width": 1080, "height": 567}], "variants": {}, "id": "rexK6oTJje2ZVAcaX2x42RXc0eqDADY0FOwezzCXrRg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17v75ce", "is_robot_indexable": true, "report_reasons": null, "author": "kadermo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17v75ce/costpredictable_logging_at_scale_with_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://clickhouse.com/blog/cost-predictable-logging-with-clickhouse-vs-datadog-elastic-stack", "subreddit_subscribers": 139761, "created_utc": 1699982427.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}