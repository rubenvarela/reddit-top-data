{"kind": "Listing", "data": {"after": "t3_17vfzwd", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering,\n\nFor anyone that says this is my fault for specializing in Microsoft stack - you're absolutely, 100% correct. I blame only myself. \n\nThe incessant cycle of \"progress\". I'm reaching my wit's end with how we're handling tech debt. It seems like every other year, there's a new 'bright new day' in the Microsoft analytics stack, and it's driving me nuts.\n\nFirst off, let's address the myth of avoiding tech debt. Spoiler alert: it's a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year's innovation is this year's digital paperweight.\n\nIt's a merry-go-round of mediocrity So, what do we do? We slap a new 'notebook' GUI over Spark clusters and pat ourselves on the back for 'innovation.' It's a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever's been rebranded this week, with awards handed out for sales volume, not product quality. \n\nWe've all heard the mantras: \"ADF is the way,\" \"Databricks is the way,\" \"Synapse is the way,\" \"Fabric is the way.\" It's just a parade of platforms, each hailed as the messiah of data engineering, but they're not, they're very naughty boys, only to be replaced by the next shiny thing in a year or two.\n\nI (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and 'platnum's to it.", "author_fullname": "t2_17e8xe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Microsoft data products - merry-go-round of mediocrity", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vxmth", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 94, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 94, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700066411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;For anyone that says this is my fault for specializing in Microsoft stack - you&amp;#39;re absolutely, 100% correct. I blame only myself. &lt;/p&gt;\n\n&lt;p&gt;The incessant cycle of &amp;quot;progress&amp;quot;. I&amp;#39;m reaching my wit&amp;#39;s end with how we&amp;#39;re handling tech debt. It seems like every other year, there&amp;#39;s a new &amp;#39;bright new day&amp;#39; in the Microsoft analytics stack, and it&amp;#39;s driving me nuts.&lt;/p&gt;\n\n&lt;p&gt;First off, let&amp;#39;s address the myth of avoiding tech debt. Spoiler alert: it&amp;#39;s a fairy tale. Every couple of years, MS flips the script, and suddenly, what was cutting-edge is now old news. The execs, bless their hearts, eat up all the marketing spiel and suddenly, last year&amp;#39;s innovation is this year&amp;#39;s digital paperweight.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a merry-go-round of mediocrity So, what do we do? We slap a new &amp;#39;notebook&amp;#39; GUI over Spark clusters and pat ourselves on the back for &amp;#39;innovation.&amp;#39; It&amp;#39;s a cycle as predictable as it is frustrating. Microsoft partners? Under constant pressure to sell whatever&amp;#39;s been rebranded this week, with awards handed out for sales volume, not product quality. &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve all heard the mantras: &amp;quot;ADF is the way,&amp;quot; &amp;quot;Databricks is the way,&amp;quot; &amp;quot;Synapse is the way,&amp;quot; &amp;quot;Fabric is the way.&amp;quot; It&amp;#39;s just a parade of platforms, each hailed as the messiah of data engineering, but they&amp;#39;re not, they&amp;#39;re very naughty boys, only to be replaced by the next shiny thing in a year or two.&lt;/p&gt;\n\n&lt;p&gt;I (and anyone working with Azure/MS tech) need to get some self-respect and leave the execs, wordcels and &amp;#39;platnum&amp;#39;s to it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vxmth", "is_robot_indexable": true, "report_reasons": null, "author": "biowl", "discussion_type": null, "num_comments": 53, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vxmth/microsoft_data_products_merrygoround_of_mediocrity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vxmth/microsoft_data_products_merrygoround_of_mediocrity/", "subreddit_subscribers": 139857, "created_utc": 1700066411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! \ud83d\udc4b\n\nAs a junior data engineer still finding my footing in this vast field, I've been pondering over the trajectory of our tools and platforms. I'm eager to hear from the more seasoned data engineers, leads, and others about your take on this.\n\nThe heart of my concern is whether these platforms might be turning into a sort of never-ending learning treadmill and we data professionals end up 'trapped' inside the ecosystems of Azure, DataBricks, etc. On one hand, it's fantastic to have constant innovations, but on the other, it feels overwhelming at times. As we delve deeper into ecosystems like DataBricks and Azure, are we risking losing sight of the broader landscape? Is there a danger of becoming too specialised in these environments to the detriment of our versatility? Doesn't it give DataBricks and Azure more control over your decision-making and data?\n\nBeing relatively new to this field, I'm completely open to the idea that my concerns might be misplaced, and I'm really keen on understanding different perspectives. I'm all for specialisation, but I also value adaptability and a diverse skill set.\n\nI'm looking forward to hearing from the veterans and experts in this community. Do you think these concerns are valid, or is this simply the nature of our evolving field? How do you balance the need to stay current with specific platforms while maintaining a broad skill set?\n\nThanks for taking the time to share your wisdom and insights! Really looking forward to this discussion!\n\nEdit: sorry, had a duplicate paragraph in there somehow...", "author_fullname": "t2_ukaxc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Junior's perspective on centralised tools like DataBricks and Azure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w0ml8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "a96f3daa-e787-11ed-bb3c-927138abd1d2", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700085247.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700074166.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! \ud83d\udc4b&lt;/p&gt;\n\n&lt;p&gt;As a junior data engineer still finding my footing in this vast field, I&amp;#39;ve been pondering over the trajectory of our tools and platforms. I&amp;#39;m eager to hear from the more seasoned data engineers, leads, and others about your take on this.&lt;/p&gt;\n\n&lt;p&gt;The heart of my concern is whether these platforms might be turning into a sort of never-ending learning treadmill and we data professionals end up &amp;#39;trapped&amp;#39; inside the ecosystems of Azure, DataBricks, etc. On one hand, it&amp;#39;s fantastic to have constant innovations, but on the other, it feels overwhelming at times. As we delve deeper into ecosystems like DataBricks and Azure, are we risking losing sight of the broader landscape? Is there a danger of becoming too specialised in these environments to the detriment of our versatility? Doesn&amp;#39;t it give DataBricks and Azure more control over your decision-making and data?&lt;/p&gt;\n\n&lt;p&gt;Being relatively new to this field, I&amp;#39;m completely open to the idea that my concerns might be misplaced, and I&amp;#39;m really keen on understanding different perspectives. I&amp;#39;m all for specialisation, but I also value adaptability and a diverse skill set.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking forward to hearing from the veterans and experts in this community. Do you think these concerns are valid, or is this simply the nature of our evolving field? How do you balance the need to stay current with specific platforms while maintaining a broad skill set?&lt;/p&gt;\n\n&lt;p&gt;Thanks for taking the time to share your wisdom and insights! Really looking forward to this discussion!&lt;/p&gt;\n\n&lt;p&gt;Edit: sorry, had a duplicate paragraph in there somehow...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Junior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w0ml8", "is_robot_indexable": true, "report_reasons": null, "author": "dildan101", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17w0ml8/a_juniors_perspective_on_centralised_tools_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w0ml8/a_juniors_perspective_on_centralised_tools_like/", "subreddit_subscribers": 139857, "created_utc": 1700074166.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I built a tool to make it faster/easier to write python scripts that will clean up Excel files. It's mostly targeted towards people who are less technical, or people like me who can never remember the best practice keyword arguments for pd.read\\_csv() lol. \n\nI called it [Computron](https://app.squack.io/?utm_content=dataengineering&amp;utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=v0p3_uifix).   \n\nHere's how it works: \n\n* Upload any messy csv, xlsx, xls, or xlsm file\n* Type out commands for how you want to clean it up\n* Computron builds and executes Python code to follow the command using GPT-4\n* Once you're done, the code can compiled into a stand-alone automation and reused for other files\n* API support for the hosted automations is coming soon \n\nThe thing is I don't want this to be another bullshit AI tool. I'm posting this on a few data-related subreddits, so you guys can [try it](https://app.squack.io/?utm_content=dataengineering&amp;utm_medium=social&amp;utm_source=reddit&amp;utm_campaign=v0p3_uifix) and be brutally honest about how to make it better.   \n\nAs a token of my appreciation for helping, anybody who makes an account at this early stage will have access to all of the paid features forever. I'm also happy to answer any questions, or give anybody a more in depth tutorial.", "author_fullname": "t2_898csv1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Data Roomba\" for cleaning up data before onboarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vuaxr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700057010.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I built a tool to make it faster/easier to write python scripts that will clean up Excel files. It&amp;#39;s mostly targeted towards people who are less technical, or people like me who can never remember the best practice keyword arguments for pd.read_csv() lol. &lt;/p&gt;\n\n&lt;p&gt;I called it &lt;a href=\"https://app.squack.io/?utm_content=dataengineering&amp;amp;utm_medium=social&amp;amp;utm_source=reddit&amp;amp;utm_campaign=v0p3_uifix\"&gt;Computron&lt;/a&gt;.   &lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s how it works: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upload any messy csv, xlsx, xls, or xlsm file&lt;/li&gt;\n&lt;li&gt;Type out commands for how you want to clean it up&lt;/li&gt;\n&lt;li&gt;Computron builds and executes Python code to follow the command using GPT-4&lt;/li&gt;\n&lt;li&gt;Once you&amp;#39;re done, the code can compiled into a stand-alone automation and reused for other files&lt;/li&gt;\n&lt;li&gt;API support for the hosted automations is coming soon &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The thing is I don&amp;#39;t want this to be another bullshit AI tool. I&amp;#39;m posting this on a few data-related subreddits, so you guys can &lt;a href=\"https://app.squack.io/?utm_content=dataengineering&amp;amp;utm_medium=social&amp;amp;utm_source=reddit&amp;amp;utm_campaign=v0p3_uifix\"&gt;try it&lt;/a&gt; and be brutally honest about how to make it better.   &lt;/p&gt;\n\n&lt;p&gt;As a token of my appreciation for helping, anybody who makes an account at this early stage will have access to all of the paid features forever. I&amp;#39;m also happy to answer any questions, or give anybody a more in depth tutorial.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vuaxr", "is_robot_indexable": true, "report_reasons": null, "author": "evilredpanda", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vuaxr/data_roomba_for_cleaning_up_data_before_onboarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vuaxr/data_roomba_for_cleaning_up_data_before_onboarding/", "subreddit_subscribers": 139857, "created_utc": 1700057010.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, so we are having some discussions internally about whether or not our data should be immutable and I wanted some sanity checking on how I am thinking about this. Just to give some general comments on how our data is set up currently the primary data that is used is stored in SQL and is used to serve customers with contracts etc on our website. We also have an analytics team that run a service which takes this relational data and processes it into a flat structure into Big Query to make it more easily digestible for analytics tools like Tableau. \n\nAll of the reasons we want to have immutable data make sense to me (being able to see the state of a contract at any given date, having logs of changes to make audits etc). But it also feels like we can accomplish this with an Audit Table and not changing the underlying data structure. To use \"Contract\" Table as an example I'm seeing two different implementations.\n\n1) We have a Contract table that is used by the customer facing services, who would only care about the current state of their contracts which can be updated, and create a separate Contract_Audit table which creates a new row with the current state of a contract whenever a contract gets updated (which can be automatically managed by our DB libraries)\n\n2) We have a Contract Table and whenever a contract is created or a change is made to it we store a new row for that contract. \n\nIf for the core business functionality we only care about the current state of the data, it seems like it would add needless complexity and data when interacting with these tables. While the needs for things like Audits and Analytics to have the full history of changes is served equally well by the Audit tables.\n\nWould love to hear some inputs or if there is anything I'm missing here, thanks!", "author_fullname": "t2_af3ee", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Immutable data versus Audit Tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vsexn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700050612.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, so we are having some discussions internally about whether or not our data should be immutable and I wanted some sanity checking on how I am thinking about this. Just to give some general comments on how our data is set up currently the primary data that is used is stored in SQL and is used to serve customers with contracts etc on our website. We also have an analytics team that run a service which takes this relational data and processes it into a flat structure into Big Query to make it more easily digestible for analytics tools like Tableau. &lt;/p&gt;\n\n&lt;p&gt;All of the reasons we want to have immutable data make sense to me (being able to see the state of a contract at any given date, having logs of changes to make audits etc). But it also feels like we can accomplish this with an Audit Table and not changing the underlying data structure. To use &amp;quot;Contract&amp;quot; Table as an example I&amp;#39;m seeing two different implementations.&lt;/p&gt;\n\n&lt;p&gt;1) We have a Contract table that is used by the customer facing services, who would only care about the current state of their contracts which can be updated, and create a separate Contract_Audit table which creates a new row with the current state of a contract whenever a contract gets updated (which can be automatically managed by our DB libraries)&lt;/p&gt;\n\n&lt;p&gt;2) We have a Contract Table and whenever a contract is created or a change is made to it we store a new row for that contract. &lt;/p&gt;\n\n&lt;p&gt;If for the core business functionality we only care about the current state of the data, it seems like it would add needless complexity and data when interacting with these tables. While the needs for things like Audits and Analytics to have the full history of changes is served equally well by the Audit tables.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear some inputs or if there is anything I&amp;#39;m missing here, thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vsexn", "is_robot_indexable": true, "report_reasons": null, "author": "relderpaway", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vsexn/immutable_data_versus_audit_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vsexn/immutable_data_versus_audit_tables/", "subreddit_subscribers": 139857, "created_utc": 1700050612.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " I inherited a legacy ETL pipeline with a \"multi-stage\" architecture: each stage is a Django command script (a Python script run using python manage.py). For example, the first script (command) downloads and saves a file, also creating an entry in the SQL database. Another command then waits for this entry, updates its status (a column in the table), and performs data cleaning based on the file. Subsequent script commands take over when the status attribute is updated as completed for the previous stage, each performing additional data cleaning operations.\n\nThus, the architecture is segmented into different stages, each being a separate Python script running in a while loop, scanning the SQL table for documents with the requisite status attribute. The data processing, among others, involves sending files to different microservices (REST APIs) or executing simple indexing into other data sources (like other SQL tables or Elasticsearch).\n\nIn production, each script is containerized, with every stage deployed as a separate service in Kubernetes, featuring multiple replicated pods for each stage.\n\nAlthough this setup is quite beneficial as it allows increasing the number of replicas to manage congestion in a particular service, it feels overly complex and possibly overengineered. I am concerned about the appropriateness of using Django commands in an infinite loop and am exploring ways to simplify the ETL pipeline from its current state. Kubernetes deployments also make debugging each stage challenging, particularly when issues arise. With 8-9 different stages, each as a separate deployment, troubleshooting becomes time-consuming and complicated.\n\nI am seeking advice on what would be a good alternative to above described architecture, if instead of using django commands or kubernetes multi deployments there are better tools for ETL pipelines. Any recommendations what changes are worth making?", "author_fullname": "t2_ig1d8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improving, simplified an ETL pipeline architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vo8b9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700032634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I inherited a legacy ETL pipeline with a &amp;quot;multi-stage&amp;quot; architecture: each stage is a Django command script (a Python script run using python manage.py). For example, the first script (command) downloads and saves a file, also creating an entry in the SQL database. Another command then waits for this entry, updates its status (a column in the table), and performs data cleaning based on the file. Subsequent script commands take over when the status attribute is updated as completed for the previous stage, each performing additional data cleaning operations.&lt;/p&gt;\n\n&lt;p&gt;Thus, the architecture is segmented into different stages, each being a separate Python script running in a while loop, scanning the SQL table for documents with the requisite status attribute. The data processing, among others, involves sending files to different microservices (REST APIs) or executing simple indexing into other data sources (like other SQL tables or Elasticsearch).&lt;/p&gt;\n\n&lt;p&gt;In production, each script is containerized, with every stage deployed as a separate service in Kubernetes, featuring multiple replicated pods for each stage.&lt;/p&gt;\n\n&lt;p&gt;Although this setup is quite beneficial as it allows increasing the number of replicas to manage congestion in a particular service, it feels overly complex and possibly overengineered. I am concerned about the appropriateness of using Django commands in an infinite loop and am exploring ways to simplify the ETL pipeline from its current state. Kubernetes deployments also make debugging each stage challenging, particularly when issues arise. With 8-9 different stages, each as a separate deployment, troubleshooting becomes time-consuming and complicated.&lt;/p&gt;\n\n&lt;p&gt;I am seeking advice on what would be a good alternative to above described architecture, if instead of using django commands or kubernetes multi deployments there are better tools for ETL pipelines. Any recommendations what changes are worth making?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vo8b9", "is_robot_indexable": true, "report_reasons": null, "author": "investopim", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vo8b9/improving_simplified_an_etl_pipeline_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vo8b9/improving_simplified_an_etl_pipeline_architecture/", "subreddit_subscribers": 139857, "created_utc": 1700032634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you have a go-to SQL formatting style that's kinda like PEP8 for Python?\n\nStyle guide recommendations?", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Formatting Tips?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vlv1s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700055712.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700023410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you have a go-to SQL formatting style that&amp;#39;s kinda like PEP8 for Python?&lt;/p&gt;\n\n&lt;p&gt;Style guide recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vlv1s", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vlv1s/sql_formatting_tips/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vlv1s/sql_formatting_tips/", "subreddit_subscribers": 139857, "created_utc": 1700023410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I see a lot of recommendations to never join two fact tables, but I'm not seeing anything for the situation I'm running into where two facts are \"semi-related\" -- call them applications and sales.\n\n1. We have a fact\\_applications and a fact\\_sales.\n2. *Some* applications lead to a sale (customer gets approved and immediately makes a purchase)\n3. *Some* sales have an application (customer with existing count didn't need to re-apply)\n4. So the question arises -- \"Which application (if any) led to this sale?\" or \"Which sale (if any) resulted from this application?\"\n\nTo me, the natural way to reporting on this is to left join one table to the other (by calculating which sale, if any, resulted from an app, and vice versa) -- so we can answer questions like \"X% of approved applications for product A led to a sale, with an average of $$$, vs. Y% for product B with an average of $$$\"\n\nBut, I'm having a hard time finding best practices on dealing with this scenario. These seem clearly not related enough to consolidate into a single fact table, and people have mixed opinions about \"bridge tables\" or whatever we would call them. Does anyone have recommendations?", "author_fullname": "t2_ikd9g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Joining semi-related fact tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w07kw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700073077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see a lot of recommendations to never join two fact tables, but I&amp;#39;m not seeing anything for the situation I&amp;#39;m running into where two facts are &amp;quot;semi-related&amp;quot; -- call them applications and sales.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We have a fact_applications and a fact_sales.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Some&lt;/em&gt; applications lead to a sale (customer gets approved and immediately makes a purchase)&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Some&lt;/em&gt; sales have an application (customer with existing count didn&amp;#39;t need to re-apply)&lt;/li&gt;\n&lt;li&gt;So the question arises -- &amp;quot;Which application (if any) led to this sale?&amp;quot; or &amp;quot;Which sale (if any) resulted from this application?&amp;quot;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;To me, the natural way to reporting on this is to left join one table to the other (by calculating which sale, if any, resulted from an app, and vice versa) -- so we can answer questions like &amp;quot;X% of approved applications for product A led to a sale, with an average of $$$, vs. Y% for product B with an average of $$$&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;But, I&amp;#39;m having a hard time finding best practices on dealing with this scenario. These seem clearly not related enough to consolidate into a single fact table, and people have mixed opinions about &amp;quot;bridge tables&amp;quot; or whatever we would call them. Does anyone have recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w07kw", "is_robot_indexable": true, "report_reasons": null, "author": "dlb8685", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w07kw/joining_semirelated_fact_tables/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w07kw/joining_semirelated_fact_tables/", "subreddit_subscribers": 139857, "created_utc": 1700073077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "we are currently developing an open-source tool for automated feature engineering on relational data and time series.\n\nhttps://github.com/getml/getml-community\n\nIt is similar to featuretools or tsfresh, but over 100x faster than these tools and considerably more memory-efficient. As far as we know, it is the fastest open-source tool for this purpose in the world.\n\nThis is possible because we are using a customized database engine written in C++. But we have a Python interface, of course.\n\nAny kind of feedback, particularly constructive criticism, is very welcome.", "author_fullname": "t2_jb6i198h7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "getml: The fastest open-Source tool for automated feature engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vp0cl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700036067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;we are currently developing an open-source tool for automated feature engineering on relational data and time series.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/getml/getml-community\"&gt;https://github.com/getml/getml-community&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It is similar to featuretools or tsfresh, but over 100x faster than these tools and considerably more memory-efficient. As far as we know, it is the fastest open-source tool for this purpose in the world.&lt;/p&gt;\n\n&lt;p&gt;This is possible because we are using a customized database engine written in C++. But we have a Python interface, of course.&lt;/p&gt;\n\n&lt;p&gt;Any kind of feedback, particularly constructive criticism, is very welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?auto=webp&amp;s=eb2045b295522111b93ce18ba65695fb9ee9fa1c", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=485764bed081e0d849c206179843b8d5527797d1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f263cbe1aabc5ecd312d14c613a74cfff8dd7c30", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1944111d10e89d167d1af93a86786103eec17ae6", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=276765321edee973a0d7df6b933d35c43d9cd0c1", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3fd4c9612a6f78142907b5cad0043ad0d5bfe069", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Vnu92fAGTLhQkxCi5SSitxup4Sn7ZrDarkAifXCV6mM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7fd4bff5b2bdf63ed65fb335da4a9a6796bb877f", "width": 1080, "height": 540}], "variants": {}, "id": "zCk3-zYBuePTRrdQOPtxaaeRuLOJGQKxQD5N-zvb_sM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17vp0cl", "is_robot_indexable": true, "report_reasons": null, "author": "liuzicheng1987", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vp0cl/getml_the_fastest_opensource_tool_for_automated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vp0cl/getml_the_fastest_opensource_tool_for_automated/", "subreddit_subscribers": 139857, "created_utc": 1700036067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does Databricks CE have real-time collaborative editing like, say, Google Docs?", "author_fullname": "t2_nvxordug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Community Edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vod8c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700033520.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700033206.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does Databricks CE have real-time collaborative editing like, say, Google Docs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vod8c", "is_robot_indexable": true, "report_reasons": null, "author": "obsculosa", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vod8c/databricks_community_edition/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vod8c/databricks_community_edition/", "subreddit_subscribers": 139857, "created_utc": 1700033206.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi. I'm migrating SAS code to Databricks, and one thing that I need to reproduce is summary statistics, especially frequency distributions . In particular it's \"proc freq\" and univariate functions in SAS, for example. \n\nI calculated the frequency distribution manually, but it would be helpful if there was a function to give you that and more. I'm searching but not seeing much.\n\nIs there a particular Pyspark library I should be looking at? Thanks. ", "author_fullname": "t2_2o0q5m4h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does Pyspark have more detailed summary statistics beyond .describe and .summary?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vi2u5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700011573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I&amp;#39;m migrating SAS code to Databricks, and one thing that I need to reproduce is summary statistics, especially frequency distributions . In particular it&amp;#39;s &amp;quot;proc freq&amp;quot; and univariate functions in SAS, for example. &lt;/p&gt;\n\n&lt;p&gt;I calculated the frequency distribution manually, but it would be helpful if there was a function to give you that and more. I&amp;#39;m searching but not seeing much.&lt;/p&gt;\n\n&lt;p&gt;Is there a particular Pyspark library I should be looking at? Thanks. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vi2u5", "is_robot_indexable": true, "report_reasons": null, "author": "rotterdamn8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vi2u5/does_pyspark_have_more_detailed_summary/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vi2u5/does_pyspark_have_more_detailed_summary/", "subreddit_subscribers": 139857, "created_utc": 1700011573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey team! Hope you're doing fine. Quick question about segment and snowflake integration. I've landed in a company where they use multiple segment projects and data from those projects is loaded hourly to our snowflake datawarehouse. We then use dbt to transform and load this data into the warehouse.   \n\n\nI've noticed segment loads three standard tables (pages, tracks &amp; identifies) for each project, and then for each tracked event generates a new table with the data that already exists in tracks for each event plus the tracked custom data sent in the event payload. \u00bfIs there a way to avoid segment sending all the context fields in the event custom table? I think it's not good to have this data duplicated in both the custom and tracks table, I'm I wrong?\n\nSecondly, I would like to automate the dbt transform step and avoid doing it myself every time a new event is needed for analysis in the DW. Did anybody do that before? What would be the best approach?. Also, I would like to run the whole segment sync as a whole and avoid generating airflow dags for each synchronization. That would save me a lot of time as well.\n\nThanks for reading!", "author_fullname": "t2_36w05yox", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Segment &amp; Snowflake Integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vvweu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700061621.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey team! Hope you&amp;#39;re doing fine. Quick question about segment and snowflake integration. I&amp;#39;ve landed in a company where they use multiple segment projects and data from those projects is loaded hourly to our snowflake datawarehouse. We then use dbt to transform and load this data into the warehouse.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed segment loads three standard tables (pages, tracks &amp;amp; identifies) for each project, and then for each tracked event generates a new table with the data that already exists in tracks for each event plus the tracked custom data sent in the event payload. \u00bfIs there a way to avoid segment sending all the context fields in the event custom table? I think it&amp;#39;s not good to have this data duplicated in both the custom and tracks table, I&amp;#39;m I wrong?&lt;/p&gt;\n\n&lt;p&gt;Secondly, I would like to automate the dbt transform step and avoid doing it myself every time a new event is needed for analysis in the DW. Did anybody do that before? What would be the best approach?. Also, I would like to run the whole segment sync as a whole and avoid generating airflow dags for each synchronization. That would save me a lot of time as well.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vvweu", "is_robot_indexable": true, "report_reasons": null, "author": "asnopm", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vvweu/segment_snowflake_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vvweu/segment_snowflake_integration/", "subreddit_subscribers": 139857, "created_utc": 1700061621.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a computational biologist (8 YoE) in big pharma. I have been home brewing databases for my department among other things. Recently, during a re-org upheaval I was told I could be a good fit for a Sr Data Engineer role at my company, a good fit in the sense that I bring domain expertise and they can give me on-hands training in data engineering. The HM specifically said \u201cI can teach you Data Engineering, I can\u2019t teach you Biology\u201d.\nI am genuinely considering this offer as I have been contemplating a switch to Data Science/Engineering for the past year. I was wondering if anyone in the group has made a similar lateral switch, and what your experience was. I am curious how math heavy such a role would be, I have been told they use BigQuery, DataBricks, Onyx and elements of GCP and Kubernetes, but I\u2019ll be honest I have no experience in any of these. Also, generally how stable is a career in data engineering? Is domain expertise really that important in your opinion?", "author_fullname": "t2_s5s0xh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about a lateral switch into data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vtv0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700055645.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a computational biologist (8 YoE) in big pharma. I have been home brewing databases for my department among other things. Recently, during a re-org upheaval I was told I could be a good fit for a Sr Data Engineer role at my company, a good fit in the sense that I bring domain expertise and they can give me on-hands training in data engineering. The HM specifically said \u201cI can teach you Data Engineering, I can\u2019t teach you Biology\u201d.\nI am genuinely considering this offer as I have been contemplating a switch to Data Science/Engineering for the past year. I was wondering if anyone in the group has made a similar lateral switch, and what your experience was. I am curious how math heavy such a role would be, I have been told they use BigQuery, DataBricks, Onyx and elements of GCP and Kubernetes, but I\u2019ll be honest I have no experience in any of these. Also, generally how stable is a career in data engineering? Is domain expertise really that important in your opinion?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17vtv0o", "is_robot_indexable": true, "report_reasons": null, "author": "EuphoricArtichoke", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vtv0o/question_about_a_lateral_switch_into_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vtv0o/question_about_a_lateral_switch_into_data/", "subreddit_subscribers": 139857, "created_utc": 1700055645.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_baajg5kk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning to a Data Product Ecosystem: Leveraging the Evolutionary Architecture 4D Architectures, Data-Driven Routing, Feature Toggles, and more!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_17vt8mn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/6vDlkW5g9Ov2fi4qiqucF_O3EGlGVFi-VIH-dXwFbK4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700053598.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "moderndata101.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://moderndata101.substack.com/p/transitioning-to-a-data-product-ecosystem", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?auto=webp&amp;s=3082285d046bb96e056e3beb6bab7cc485567de3", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=644cca4795796eb921dd4625ff9c54b794d4b0eb", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ea9e084b2aafd7bd8605a6427295eea54585448", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae64c7f1f9a6e6d90348bb507812c8dcba804134", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9b2005f4e3b121435e3c44ed1cf042df746c0fd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d546543f7d963a212d8f96dc95c922942a33bce4", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/OVCEwViJiEaaof0ceiYOqBgOpDYhuSQ7jApkV66E8fs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=943e28e4ddb97fe923c7bfb089bcbaa4234bb8d4", "width": 1080, "height": 540}], "variants": {}, "id": "JUVxsJydfJfdVJZ75jZtmnS48hsaGgbV73xyphmtuyA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vt8mn", "is_robot_indexable": true, "report_reasons": null, "author": "growth_man", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vt8mn/transitioning_to_a_data_product_ecosystem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://moderndata101.substack.com/p/transitioning-to-a-data-product-ecosystem", "subreddit_subscribers": 139857, "created_utc": 1700053598.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am beginning with a new blog series about Microsoft Azure. As the first article you get your hands on the Azure SQL Database service. Where you will create one and import csv data to create a table. Happy for some feedback!\n\n[https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405](https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405)", "author_fullname": "t2_3di0zmcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Effortless Azure: Mastering SQL Database Creation and Data Import", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vt2w7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700053085.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am beginning with a new blog series about Microsoft Azure. As the first article you get your hands on the Azure SQL Database service. Where you will create one and import csv data to create a table. Happy for some feedback!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405\"&gt;https://erwinschleier.medium.com/effortless-azure-mastering-sql-database-creation-and-data-import-8ef924af5405&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?auto=webp&amp;s=e3518f63ea54c0bace227cb57575f54f713d6723", "width": 728, "height": 380}, "resolutions": [{"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e68244bd449a8dc4b1188bd6ba8aabeb9f3d048", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=22a4ab3540f79686ec32b5e4c51c9bf4424430f6", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7e133141465c286fc35b94ada6863c8d04d2cbd5", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/MoxgIMsC6GyqDyFlAMjgQuYD8hXb6wXt0Xlu269sGXk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc30fdc12505f2560f38d0c6282bc0f083ea3deb", "width": 640, "height": 334}], "variants": {}, "id": "_gJpBnOR4HASqLUzy_v5MZHn85O6SH4CEnQM2i9rsXM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "17vt2w7", "is_robot_indexable": true, "report_reasons": null, "author": "EdgarHuber", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vt2w7/effortless_azure_mastering_sql_database_creation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vt2w7/effortless_azure_mastering_sql_database_creation/", "subreddit_subscribers": 139857, "created_utc": 1700053085.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have already passed SAA 2 weeks ago. I have 3 years of experience as a DE. Thanks", "author_fullname": "t2_p9gvk8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I take AWS Data Analytics or new DE associate cert?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vt23e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700053004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have already passed SAA 2 weeks ago. I have 3 years of experience as a DE. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17vt23e", "is_robot_indexable": true, "report_reasons": null, "author": "marcelorojas56", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vt23e/should_i_take_aws_data_analytics_or_new_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vt23e/should_i_take_aws_data_analytics_or_new_de/", "subreddit_subscribers": 139857, "created_utc": 1700053004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nPower bi \n\nMaking APIs\n\nWeb scraping\n\nBot development", "author_fullname": "t2_hxue1umo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "would these skills benefit me as a data engineer ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17w64wu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700088011.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Power bi &lt;/p&gt;\n\n&lt;p&gt;Making APIs&lt;/p&gt;\n\n&lt;p&gt;Web scraping&lt;/p&gt;\n\n&lt;p&gt;Bot development&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w64wu", "is_robot_indexable": true, "report_reasons": null, "author": "Single-Sound-1865", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w64wu/would_these_skills_benefit_me_as_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w64wu/would_these_skills_benefit_me_as_a_data_engineer/", "subreddit_subscribers": 139857, "created_utc": 1700088011.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you want audit trails of DB records, is it better to use server logs (e.g. DataDog) or build your own audit\\_logs table? Why would you do one or the other? \n\nIn addition, do business users need access to audit logs? I'm curious if you'd want to build an audit table so that your business teams can query it, if needed. ", "author_fullname": "t2_cqdfu7nf4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you audit the history of a database record? Why do you audit?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w27za", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700078398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you want audit trails of DB records, is it better to use server logs (e.g. DataDog) or build your own audit_logs table? Why would you do one or the other? &lt;/p&gt;\n\n&lt;p&gt;In addition, do business users need access to audit logs? I&amp;#39;m curious if you&amp;#39;d want to build an audit table so that your business teams can query it, if needed. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w27za", "is_robot_indexable": true, "report_reasons": null, "author": "Different-General700", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w27za/how_do_you_audit_the_history_of_a_database_record/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w27za/how_do_you_audit_the_history_of_a_database_record/", "subreddit_subscribers": 139857, "created_utc": 1700078398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey all,\n\nIn my company we have many jobs running on Powercenter and I was thinking what would be the easiest way to migrate the logics from Powercenter into SQL statements or BTEQs.\n\nAny inputs and experiences of migirating away from Powercenter are highly appreciated!", "author_fullname": "t2_8edxjw39", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to replace Informatica Powercenter?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17w1htc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700076448.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey all,&lt;/p&gt;\n\n&lt;p&gt;In my company we have many jobs running on Powercenter and I was thinking what would be the easiest way to migrate the logics from Powercenter into SQL statements or BTEQs.&lt;/p&gt;\n\n&lt;p&gt;Any inputs and experiences of migirating away from Powercenter are highly appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17w1htc", "is_robot_indexable": true, "report_reasons": null, "author": "Net_Net_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17w1htc/how_to_replace_informatica_powercenter/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17w1htc/how_to_replace_informatica_powercenter/", "subreddit_subscribers": 139857, "created_utc": 1700076448.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys.\n\nCurrently the stack of the company I work at is comprised of a mixture of Cloudera (on-prem and cloud) and Azure Databricks (and the rest of the Azure ecosystem).\n\nRight now, permissions management is kind of a mess. Cloudera Hadoop permissions are managed by Ranger which syncs LDAP groups and then we have policies attached to each group with permissions to use specific databases within Hadoop/HDFS. Our Azure ecossystem permissions model uses IAM Roles (now Entra ID, I guess) to enforce each LDAP group permission's to each ADLS storage account and container.\n\nThere are a lot of other things in the mix, but that's the gist of it. \n\nEnforcing permissions is achieved by a mixture of manual, automatic and some terraform (between Azure and CDP) actions.\n\nHow to make sense of all this? How would you tackle trying to unify the permissions model across all these components?\n\nRight now what comes to mind is using Databricks Unity Catalog and try to create a script to sync the permissions to Ranger.", "author_fullname": "t2_pev7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrated permissions management across different providers of on prem and cloud data infrastructures", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vyqzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700069345.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys.&lt;/p&gt;\n\n&lt;p&gt;Currently the stack of the company I work at is comprised of a mixture of Cloudera (on-prem and cloud) and Azure Databricks (and the rest of the Azure ecosystem).&lt;/p&gt;\n\n&lt;p&gt;Right now, permissions management is kind of a mess. Cloudera Hadoop permissions are managed by Ranger which syncs LDAP groups and then we have policies attached to each group with permissions to use specific databases within Hadoop/HDFS. Our Azure ecossystem permissions model uses IAM Roles (now Entra ID, I guess) to enforce each LDAP group permission&amp;#39;s to each ADLS storage account and container.&lt;/p&gt;\n\n&lt;p&gt;There are a lot of other things in the mix, but that&amp;#39;s the gist of it. &lt;/p&gt;\n\n&lt;p&gt;Enforcing permissions is achieved by a mixture of manual, automatic and some terraform (between Azure and CDP) actions.&lt;/p&gt;\n\n&lt;p&gt;How to make sense of all this? How would you tackle trying to unify the permissions model across all these components?&lt;/p&gt;\n\n&lt;p&gt;Right now what comes to mind is using Databricks Unity Catalog and try to create a script to sync the permissions to Ranger.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vyqzm", "is_robot_indexable": true, "report_reasons": null, "author": "mommylovesme2", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vyqzm/integrated_permissions_management_across/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vyqzm/integrated_permissions_management_across/", "subreddit_subscribers": 139857, "created_utc": 1700069345.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Wondering what other orgs look like for DE teams. We currently have to handle all of our QA as a DE team while our app dev team has a dedicated team of a handful of resources. ", "author_fullname": "t2_81ywblydd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you have a dedicated QA team?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vxubk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700066992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Wondering what other orgs look like for DE teams. We currently have to handle all of our QA as a DE team while our app dev team has a dedicated team of a handful of resources. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17vxubk", "is_robot_indexable": true, "report_reasons": null, "author": "DataDoyle", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vxubk/do_you_have_a_dedicated_qa_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vxubk/do_you_have_a_dedicated_qa_team/", "subreddit_subscribers": 139857, "created_utc": 1700066992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "AWS", "author_fullname": "t2_ancj6nr6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_17vwc5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "author_name": "The iT Tech Solutions", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/9rzIhRb2qKk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheiTTechSolutions"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/17vwc5q", "height": 200}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/fjtk7XwbiXbVLM9hCNd3gxKDCkXWO7aQEZETfYj8Wj4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700062825.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AWS&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/9rzIhRb2qKk", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?auto=webp&amp;s=8ec9545a9f8b1c557ee9af6002b086b698a9e3c7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d48648c7370d7259d4beec81cf28d52300b0afd", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=47748d9418c4591d8b33469fab5a501a97708ba4", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/3MNcd1v2n-Kp-dCWTDN1f8V2L8HvEb2DTFaDXlGLccY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d8c859dad79ffa253ab9640e50f591be0e3de7e", "width": 320, "height": 240}], "variants": {}, "id": "-ubKmFhXuBQ2j857IyozL8XfyJT9uZB5ca8iMjrzCJU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vwc5q", "is_robot_indexable": true, "report_reasons": null, "author": "Recent_Ocelot_724", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vwc5q/aws_step_function_variables_resultpath_inputpath/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/9rzIhRb2qKk", "subreddit_subscribers": 139857, "created_utc": 1700062825.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "AWS Step Function Variables | ResultPath, InputPath, OutputPath", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/9rzIhRb2qKk?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"AWS Step Function Variables | ResultPath, InputPath, OutputPath\"&gt;&lt;/iframe&gt;", "author_name": "The iT Tech Solutions", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/9rzIhRb2qKk/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@TheiTTechSolutions"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've seen discussion and videos were they say it's really hard to contribute to the framework just because they accept only from existing commiters. I want to contribute in order to learn, advance my career and give back to the community and I don't want to dig in learning the internal codebase and later have no chance.", "author_fullname": "t2_76srr1mpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it really hard to contribute to Apache Spark [codebase]?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vv8sl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700059797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen discussion and videos were they say it&amp;#39;s really hard to contribute to the framework just because they accept only from existing commiters. I want to contribute in order to learn, advance my career and give back to the community and I don&amp;#39;t want to dig in learning the internal codebase and later have no chance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17vv8sl", "is_robot_indexable": true, "report_reasons": null, "author": "pr6g_head", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/17vv8sl/is_it_really_hard_to_contribute_to_apache_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vv8sl/is_it_really_hard_to_contribute_to_apache_spark/", "subreddit_subscribers": 139857, "created_utc": 1700059797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there\n\nI am trying to build a small pipeline to get idea of how things work.\nThe pipeline: Fetch data from API -\u203a load it to the local DB -\u203a track/visualise on looker Studio\n\nThe Toolkit: Python, Mysql DB, GCP. and for the orchestration: airflow or task scheduler\n\nCurrently, Im at the data ingesting part (setting up Mysql DB), and I just want share my process and hope you can suggest impovements to the process,\n\nLets say I need to load data of all the locations supported by source platform (OpenAQ), I'm doing requests.get() the data, transform it to fit my local schema, mostly extract what is necessary and load it using Mysql.connector,\nso like wise for each DB entity (Table), I'm making a new python file to do the loading.\n\nI find the process very not simple and I couldn\u2019t find any ideas specific for this situation or as narrow as this, so Im hoping you guys can shed some light for me.\n\nYour comments/suggestions/critique will help me improve my understanding immensely.\n\nThanks", "author_fullname": "t2_jsaf7cene", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I improve Extract and load?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vf7ih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700003588.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I am trying to build a small pipeline to get idea of how things work.\nThe pipeline: Fetch data from API -\u203a load it to the local DB -\u203a track/visualise on looker Studio&lt;/p&gt;\n\n&lt;p&gt;The Toolkit: Python, Mysql DB, GCP. and for the orchestration: airflow or task scheduler&lt;/p&gt;\n\n&lt;p&gt;Currently, Im at the data ingesting part (setting up Mysql DB), and I just want share my process and hope you can suggest impovements to the process,&lt;/p&gt;\n\n&lt;p&gt;Lets say I need to load data of all the locations supported by source platform (OpenAQ), I&amp;#39;m doing requests.get() the data, transform it to fit my local schema, mostly extract what is necessary and load it using Mysql.connector,\nso like wise for each DB entity (Table), I&amp;#39;m making a new python file to do the loading.&lt;/p&gt;\n\n&lt;p&gt;I find the process very not simple and I couldn\u2019t find any ideas specific for this situation or as narrow as this, so Im hoping you guys can shed some light for me.&lt;/p&gt;\n\n&lt;p&gt;Your comments/suggestions/critique will help me improve my understanding immensely.&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vf7ih", "is_robot_indexable": true, "report_reasons": null, "author": "trickytoughtruth", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vf7ih/how_can_i_improve_extract_and_load/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vf7ih/how_can_i_improve_extract_and_load/", "subreddit_subscribers": 139857, "created_utc": 1700003588.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_n0ywlxbk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real-time intent detection + alerting via LLMs using Pathway LLM App and Streamlit, both open-source Pythonic frameworks. Can be used in LLMOps for monitoring, PII detection, and in BizOps to notify teams of critical changes by others in their documents. Link: https://github.com/pathwaycom/llm-app", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_17vrysc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"reddit_video": {"bitrate_kbps": 800, "fallback_url": "https://v.redd.it/36fqeu2b2i0c1/DASH_360.mp4?source=fallback", "has_audio": false, "height": 360, "width": 640, "scrubber_media_url": "https://v.redd.it/36fqeu2b2i0c1/DASH_96.mp4", "dash_url": "https://v.redd.it/36fqeu2b2i0c1/DASHPlaylist.mpd?a=1702683496%2CYTI0MWUyNWVhYjAyZWU1ODk2NTA3ODJkZDhiNWVlY2JlZjQzOGIzNjRjZGFkZTMzNzFiYTkxZmFhOGZjNDVkZQ%3D%3D&amp;v=1&amp;f=sd", "duration": 30, "hls_url": "https://v.redd.it/36fqeu2b2i0c1/HLSPlaylist.m3u8?a=1702683496%2CMWJhNjUxYmYzYzJiNjMyNWUxZDA5ZWRjMjY1ZDFiMTllZWQxZGU0NzU4Njc2ZWUxODI0NmNkYTZmN2ZlZTQxZA%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://external-preview.redd.it/ZWZjZGI1N2MyaTBjMRc0bNAEw3fQUPOXv7BBxV4cnFVPQNTlvvbJfufw1i3u.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=168568b6a2391281e61ad184f46c8f95fcaa4e50", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "hosted:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700048943.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "v.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://v.redd.it/36fqeu2b2i0c1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/ZWZjZGI1N2MyaTBjMRc0bNAEw3fQUPOXv7BBxV4cnFVPQNTlvvbJfufw1i3u.png?format=pjpg&amp;auto=webp&amp;s=05dbca532f58058de54c0e902ee8d9ed65821841", "width": 800, "height": 450}, "resolutions": [{"url": "https://external-preview.redd.it/ZWZjZGI1N2MyaTBjMRc0bNAEw3fQUPOXv7BBxV4cnFVPQNTlvvbJfufw1i3u.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=52f5699214f17024b75d1acf4c4e63dbb1d23809", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/ZWZjZGI1N2MyaTBjMRc0bNAEw3fQUPOXv7BBxV4cnFVPQNTlvvbJfufw1i3u.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c571e7ff0c404d7da7366383046967022b9fa28e", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/ZWZjZGI1N2MyaTBjMRc0bNAEw3fQUPOXv7BBxV4cnFVPQNTlvvbJfufw1i3u.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=06c1ce6420f8da1921b231874ec13a19759f65b8", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/ZWZjZGI1N2MyaTBjMRc0bNAEw3fQUPOXv7BBxV4cnFVPQNTlvvbJfufw1i3u.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=fdcdb267fe9bcafbf4ddfadecc7186a6ab5ba71c", "width": 640, "height": 360}], "variants": {}, "id": "ZWZjZGI1N2MyaTBjMRc0bNAEw3fQUPOXv7BBxV4cnFVPQNTlvvbJfufw1i3u"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "17vrysc", "is_robot_indexable": true, "report_reasons": null, "author": "muditjps", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vrysc/realtime_intent_detection_alerting_via_llms_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://v.redd.it/36fqeu2b2i0c1", "subreddit_subscribers": 139857, "created_utc": 1700048943.0, "num_crossposts": 0, "media": {"reddit_video": {"bitrate_kbps": 800, "fallback_url": "https://v.redd.it/36fqeu2b2i0c1/DASH_360.mp4?source=fallback", "has_audio": false, "height": 360, "width": 640, "scrubber_media_url": "https://v.redd.it/36fqeu2b2i0c1/DASH_96.mp4", "dash_url": "https://v.redd.it/36fqeu2b2i0c1/DASHPlaylist.mpd?a=1702683496%2CYTI0MWUyNWVhYjAyZWU1ODk2NTA3ODJkZDhiNWVlY2JlZjQzOGIzNjRjZGFkZTMzNzFiYTkxZmFhOGZjNDVkZQ%3D%3D&amp;v=1&amp;f=sd", "duration": 30, "hls_url": "https://v.redd.it/36fqeu2b2i0c1/HLSPlaylist.m3u8?a=1702683496%2CMWJhNjUxYmYzYzJiNjMyNWUxZDA5ZWRjMjY1ZDFiMTllZWQxZGU0NzU4Njc2ZWUxODI0NmNkYTZmN2ZlZTQxZA%3D%3D&amp;v=1&amp;f=sd", "is_gif": true, "transcoding_status": "completed"}}, "is_video": true}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a data engineer, and I will be joining a finance and investment company a month from now. Given that I have the free time, I would like to dedicate it in learning the said domain. Particularly focusing on property debt, mid-market corporate debt and direct property investment. My goal is to gain foundational knowledge in this domain so I won\u2019t struggle understanding too many jargons. These topics are very alien to me and would appreciate any help from you guys.\n\nAnyone here came from a Finance Background which can help cherry-pick the following courses?\n\n[The Complete Financial Analyst Training and Investing Course](https://www.udemy.com/share/101WaS3@DFYP-ZaN24CF27gDDd1SLpl6Rs8hQ_U6ftm8ByVNG9QcSoT1LAWoNX_k8jLMZasv/)\n\n[The Complete Financial Analyst Course](https://www.udemy.com/share/101WmC3@-0eh1EU-715TfKuvgnhV_RfN5pPT3DIiWCslhw18lmKjvMqKG4GsHWLjUpzCuZ86/)\n\n[The Complete Investment Banking Course](https://www.udemy.com/share/101Wt03@wvADSnUd7alNVSc7POyk5xI6n5w_4UugI7Rkz5XJZntX3vhJlna1lFG48rc2wF30/)", "author_fullname": "t2_85ty6e1i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Finance and Investment Data Domain", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17vfzwd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700005704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a data engineer, and I will be joining a finance and investment company a month from now. Given that I have the free time, I would like to dedicate it in learning the said domain. Particularly focusing on property debt, mid-market corporate debt and direct property investment. My goal is to gain foundational knowledge in this domain so I won\u2019t struggle understanding too many jargons. These topics are very alien to me and would appreciate any help from you guys.&lt;/p&gt;\n\n&lt;p&gt;Anyone here came from a Finance Background which can help cherry-pick the following courses?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/share/101WaS3@DFYP-ZaN24CF27gDDd1SLpl6Rs8hQ_U6ftm8ByVNG9QcSoT1LAWoNX_k8jLMZasv/\"&gt;The Complete Financial Analyst Training and Investing Course&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/share/101WmC3@-0eh1EU-715TfKuvgnhV_RfN5pPT3DIiWCslhw18lmKjvMqKG4GsHWLjUpzCuZ86/\"&gt;The Complete Financial Analyst Course&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/share/101Wt03@wvADSnUd7alNVSc7POyk5xI6n5w_4UugI7Rkz5XJZntX3vhJlna1lFG48rc2wF30/\"&gt;The Complete Investment Banking Course&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17vfzwd", "is_robot_indexable": true, "report_reasons": null, "author": "BestBlackberry1314", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17vfzwd/finance_and_investment_data_domain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17vfzwd/finance_and_investment_data_domain/", "subreddit_subscribers": 139857, "created_utc": 1700005704.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}