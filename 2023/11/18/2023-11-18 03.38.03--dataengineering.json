{"kind": "Listing", "data": {"after": "t3_17xeyjk", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,  \nWe have Oracle database as our RDBS and we are facing issues on performance on some complex queries where number of rows are very high. Management wants us to implement big data and do all reporting/visualisation from big data and only transactional data in oracle rdbms. There is no big data expert in our team for now. I am planning to use Hadoop file structure and push the data from Oracle to HDFS using sqoop and then use Impala for querying. On top of we planning to use Qlik Sense ( we already have license for this )which already has impala connector. Do you have any other suggestions or opinion on this. Is there any case study/ documentation of this type of data orchestration", "author_fullname": "t2_cug6y6a0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDBS to Big data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xbk3s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700214336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;br/&gt;\nWe have Oracle database as our RDBS and we are facing issues on performance on some complex queries where number of rows are very high. Management wants us to implement big data and do all reporting/visualisation from big data and only transactional data in oracle rdbms. There is no big data expert in our team for now. I am planning to use Hadoop file structure and push the data from Oracle to HDFS using sqoop and then use Impala for querying. On top of we planning to use Qlik Sense ( we already have license for this )which already has impala connector. Do you have any other suggestions or opinion on this. Is there any case study/ documentation of this type of data orchestration&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17xbk3s", "is_robot_indexable": true, "report_reasons": null, "author": "jagdishgg", "discussion_type": null, "num_comments": 54, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xbk3s/rdbs_to_big_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xbk3s/rdbs_to_big_data/", "subreddit_subscribers": 140226, "created_utc": 1700214336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I came from a company where the data engineer I was working across told me that automation was \u201cnot possible.\u201d \n\nI am of the opinion that automation, at least partial automation, is a possibility in most data warehousing and ETL processes.\n\nCould someone tell me when automation isn\u2019t possible?", "author_fullname": "t2_6mus1in0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When is Automation \u201cNot Possible?\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xuptt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700270307.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came from a company where the data engineer I was working across told me that automation was \u201cnot possible.\u201d &lt;/p&gt;\n\n&lt;p&gt;I am of the opinion that automation, at least partial automation, is a possibility in most data warehousing and ETL processes.&lt;/p&gt;\n\n&lt;p&gt;Could someone tell me when automation isn\u2019t possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17xuptt", "is_robot_indexable": true, "report_reasons": null, "author": "scarlet_poppies", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xuptt/when_is_automation_not_possible/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xuptt/when_is_automation_not_possible/", "subreddit_subscribers": 140226, "created_utc": 1700270307.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, anyone know some resources where i can find large amount of data (1-10 tb) for personal projects?\n\nMy idea is to make a pipeline that can ingest transform and load the data in a cloud environment (GCP free trial) i think that if i find the data source then i can implement a \"data lake\" in gcs and transform with a spark job in data proc, model this data to answer \"bussines problems\" and put into a datawatehouse (bq)\n\nIn my actual job i already make some elt Jobs with dbt and orchestator tools but is like 20 GB/ day and i want to do a \"real Big data problem\"\n\nAny suggestions for my idea? Or any tips for personal projects?\n\nThanks you", "author_fullname": "t2_bvpnqeta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Places to find large amount of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17x5jun", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700190677.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, anyone know some resources where i can find large amount of data (1-10 tb) for personal projects?&lt;/p&gt;\n\n&lt;p&gt;My idea is to make a pipeline that can ingest transform and load the data in a cloud environment (GCP free trial) i think that if i find the data source then i can implement a &amp;quot;data lake&amp;quot; in gcs and transform with a spark job in data proc, model this data to answer &amp;quot;bussines problems&amp;quot; and put into a datawatehouse (bq)&lt;/p&gt;\n\n&lt;p&gt;In my actual job i already make some elt Jobs with dbt and orchestator tools but is like 20 GB/ day and i want to do a &amp;quot;real Big data problem&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Any suggestions for my idea? Or any tips for personal projects?&lt;/p&gt;\n\n&lt;p&gt;Thanks you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17x5jun", "is_robot_indexable": true, "report_reasons": null, "author": "aaaasd12", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17x5jun/places_to_find_large_amount_of_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17x5jun/places_to_find_large_amount_of_data/", "subreddit_subscribers": 140226, "created_utc": 1700190677.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, apologies for the very nooby question, but I am brand new to DE and starting work on my first real-life project. \n\nI understand the theory etc behind STAR designs vs OBT but am stuck on how the former is actually put into practice. \n\nIn my example, I receive a CSV file monthly with hotel stays for the previous month. I run a Python script to ingest it and load it to a staging table, and then have a view running off it with some transformations, etc and I'm looking to potentially change this up and use a STAR design. For simplicity I'll just say the file contains the hotel name, country, check-in and checkout date, and the name of the guest; and I'd want to create a hotel dim table.\n\nAm I correct in assuming that the process would look something like the below:\n\n1) Create a \"hotel\" dim table with all the unique records currently in the staging table\n\n2) In future, every month do a \"SELECT DISTINCT\" on it, and compare it with all the rows in the new CSV, insert any that aren't there\n\n3) Somehow do a \"lookup\" against the hotel dim table and insert the recordID of the corresponding hotel\n\n4) Load the rest of the data to the staging table \n\nIt seems like potentially a lot of \"SELECT DISTINCTS\" depending on how many dim tables one has, or is there a more elegant way to do this?\n\nAgain sorry for the silly question. If anyone has any resources/articles even on how this is typically implemented that would also be useful.\n\n&amp;#x200B;", "author_fullname": "t2_az62upwd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "STAR design in practise", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xd5r7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700233875.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700221055.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, apologies for the very nooby question, but I am brand new to DE and starting work on my first real-life project. &lt;/p&gt;\n\n&lt;p&gt;I understand the theory etc behind STAR designs vs OBT but am stuck on how the former is actually put into practice. &lt;/p&gt;\n\n&lt;p&gt;In my example, I receive a CSV file monthly with hotel stays for the previous month. I run a Python script to ingest it and load it to a staging table, and then have a view running off it with some transformations, etc and I&amp;#39;m looking to potentially change this up and use a STAR design. For simplicity I&amp;#39;ll just say the file contains the hotel name, country, check-in and checkout date, and the name of the guest; and I&amp;#39;d want to create a hotel dim table.&lt;/p&gt;\n\n&lt;p&gt;Am I correct in assuming that the process would look something like the below:&lt;/p&gt;\n\n&lt;p&gt;1) Create a &amp;quot;hotel&amp;quot; dim table with all the unique records currently in the staging table&lt;/p&gt;\n\n&lt;p&gt;2) In future, every month do a &amp;quot;SELECT DISTINCT&amp;quot; on it, and compare it with all the rows in the new CSV, insert any that aren&amp;#39;t there&lt;/p&gt;\n\n&lt;p&gt;3) Somehow do a &amp;quot;lookup&amp;quot; against the hotel dim table and insert the recordID of the corresponding hotel&lt;/p&gt;\n\n&lt;p&gt;4) Load the rest of the data to the staging table &lt;/p&gt;\n\n&lt;p&gt;It seems like potentially a lot of &amp;quot;SELECT DISTINCTS&amp;quot; depending on how many dim tables one has, or is there a more elegant way to do this?&lt;/p&gt;\n\n&lt;p&gt;Again sorry for the silly question. If anyone has any resources/articles even on how this is typically implemented that would also be useful.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xd5r7", "is_robot_indexable": true, "report_reasons": null, "author": "hektar9987", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xd5r7/star_design_in_practise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xd5r7/star_design_in_practise/", "subreddit_subscribers": 140226, "created_utc": 1700221055.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am pulling down electricity usage from my utility and they give the data in Epoch time in UTC.  But when I convert to local time how do I deal with the issue where in the autumn there are 25 hourly timestamps for the one day - so this year there will be two entries for 2023-11-05-01-00  (at least here in EDT/EST zone) or in the spring when there are only 23 hourly timestamps - there will be a missing hour?\n\nThis isn't anything mission critical so I guess I can just throw away the data, but I was wondering how this is dealt with in data engineering?", "author_fullname": "t2_2c03y5x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you deal with hourly data when DST (summer time) starts and (especially) ends?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17x730s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700195792.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am pulling down electricity usage from my utility and they give the data in Epoch time in UTC.  But when I convert to local time how do I deal with the issue where in the autumn there are 25 hourly timestamps for the one day - so this year there will be two entries for 2023-11-05-01-00  (at least here in EDT/EST zone) or in the spring when there are only 23 hourly timestamps - there will be a missing hour?&lt;/p&gt;\n\n&lt;p&gt;This isn&amp;#39;t anything mission critical so I guess I can just throw away the data, but I was wondering how this is dealt with in data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17x730s", "is_robot_indexable": true, "report_reasons": null, "author": "shoresy99", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17x730s/how_do_you_deal_with_hourly_data_when_dst_summer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17x730s/how_do_you_deal_with_hourly_data_when_dst_summer/", "subreddit_subscribers": 140226, "created_utc": 1700195792.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently did a course and that is making me hate spark, the instructor really didn't taught spark in a beginner friendly manner and everyhting felt very highlevel, no code line was explained in depth and how are things working, I thought it would get better as I will learn more but got burned out by that.\n\nCan anyone suggest a course that helped them learn pyspark. Also my tech stack is mainly aws-glue-pyspark and snowflake.\n\nSo if you have got any reccommedation for python,sql,snowflake too as a data engineer resource then please share it too.", "author_fullname": "t2_3xcrjr5n2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Please share the course or resource which helped you learned pyspark as a beginner to a professional and how did you practiced spark afterwards outside job", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xhqs4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700235405.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently did a course and that is making me hate spark, the instructor really didn&amp;#39;t taught spark in a beginner friendly manner and everyhting felt very highlevel, no code line was explained in depth and how are things working, I thought it would get better as I will learn more but got burned out by that.&lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest a course that helped them learn pyspark. Also my tech stack is mainly aws-glue-pyspark and snowflake.&lt;/p&gt;\n\n&lt;p&gt;So if you have got any reccommedation for python,sql,snowflake too as a data engineer resource then please share it too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xhqs4", "is_robot_indexable": true, "report_reasons": null, "author": "ImpressionOwn137", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xhqs4/please_share_the_course_or_resource_which_helped/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xhqs4/please_share_the_course_or_resource_which_helped/", "subreddit_subscribers": 140226, "created_utc": 1700235405.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello there,\n\nI've got a 20Go csv file I need to work with.\n\nInitially I tried to load with DuckDB / DBeaver but it took almost 4 hours* to convert to an SQL table.\nAnd even the most basic queries are way too slow (several minutes)\n\nDo you have any advice on how to tame this file ?\nOpen to SQL or Python, not good enough with Java, Spark or Polar.\n\nThanks!\n\n*My computer is an i9 12th gen with 32G DDR5. Not an old timer (yet)", "author_fullname": "t2_bqy2s6be", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "20go csv file to SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xrrv5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700262127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got a 20Go csv file I need to work with.&lt;/p&gt;\n\n&lt;p&gt;Initially I tried to load with DuckDB / DBeaver but it took almost 4 hours* to convert to an SQL table.\nAnd even the most basic queries are way too slow (several minutes)&lt;/p&gt;\n\n&lt;p&gt;Do you have any advice on how to tame this file ?\nOpen to SQL or Python, not good enough with Java, Spark or Polar.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n\n&lt;p&gt;*My computer is an i9 12th gen with 32G DDR5. Not an old timer (yet)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xrrv5", "is_robot_indexable": true, "report_reasons": null, "author": "reddit_ski", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xrrv5/20go_csv_file_to_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xrrv5/20go_csv_file_to_sql/", "subreddit_subscribers": 140226, "created_utc": 1700262127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to use Snowflake with pay-as-you-go pricing for the business critical edition? Is anyone doing this?", "author_fullname": "t2_i7s0h23q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake pay-as-you-go pricing for the business critical edition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xlkql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700245736.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to use Snowflake with pay-as-you-go pricing for the business critical edition? Is anyone doing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xlkql", "is_robot_indexable": true, "report_reasons": null, "author": "LA_throwaway_one", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xlkql/snowflake_payasyougo_pricing_for_the_business/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xlkql/snowflake_payasyougo_pricing_for_the_business/", "subreddit_subscribers": 140226, "created_utc": 1700245736.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'll try to keep this short.  I work at a Fortune 100 company with a \"senior engineer consultant\" title, although that doesn't really describe what I do.  I'm a business employee (not IT) and my day-to-day is:\n\n* Run validations to ensure IT data replication worked (Python &amp; SQL) 100+ tables, ~30M daily records.  Hadoop Hive, Oracle, Teradata.\n* Pull tables from disparate data sources and compare to find differences (MS Power Query)\n* Tell IT what changes they need to make to the tables \n* Write adhoc SQL reports\n\nI spend most of my day in Toad writing SQL.  I make about $130k.  I want to move to an even more tech role (DE) but am concerned I lack qualifications.  My resume looks more like a business user and not a tech employee.  I hate Tableau and creating visualizations - I want to be more back end.  Writing new SQL is the best part of my job.\n\n* 10 years strong SQL (CTE, multi joins, window/analytic functions)\n* 5 years moderate Python, Java, AppsScript\n* 5 years Tableau, Cognos\n* 5 years Talend (ETL)\n\nAm I qualified to apply for a mid or senior data engineer position? I can tolerate a modest pay cut but going back to something like $70k would hurt.  What do I need to highlight on my resume to get noticed outside of SQL and Python? A coworker suggested I should be focusing on dbt. Certifications worth obtaining?", "author_fullname": "t2_1e1kvt8o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I qualified to jump to a mid/senior data engineer position from a business data analyst role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xj3we", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700239638.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700239146.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll try to keep this short.  I work at a Fortune 100 company with a &amp;quot;senior engineer consultant&amp;quot; title, although that doesn&amp;#39;t really describe what I do.  I&amp;#39;m a business employee (not IT) and my day-to-day is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Run validations to ensure IT data replication worked (Python &amp;amp; SQL) 100+ tables, ~30M daily records.  Hadoop Hive, Oracle, Teradata.&lt;/li&gt;\n&lt;li&gt;Pull tables from disparate data sources and compare to find differences (MS Power Query)&lt;/li&gt;\n&lt;li&gt;Tell IT what changes they need to make to the tables &lt;/li&gt;\n&lt;li&gt;Write adhoc SQL reports&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I spend most of my day in Toad writing SQL.  I make about $130k.  I want to move to an even more tech role (DE) but am concerned I lack qualifications.  My resume looks more like a business user and not a tech employee.  I hate Tableau and creating visualizations - I want to be more back end.  Writing new SQL is the best part of my job.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;10 years strong SQL (CTE, multi joins, window/analytic functions)&lt;/li&gt;\n&lt;li&gt;5 years moderate Python, Java, AppsScript&lt;/li&gt;\n&lt;li&gt;5 years Tableau, Cognos&lt;/li&gt;\n&lt;li&gt;5 years Talend (ETL)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Am I qualified to apply for a mid or senior data engineer position? I can tolerate a modest pay cut but going back to something like $70k would hurt.  What do I need to highlight on my resume to get noticed outside of SQL and Python? A coworker suggested I should be focusing on dbt. Certifications worth obtaining?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17xj3we", "is_robot_indexable": true, "report_reasons": null, "author": "IT-Banker", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xj3we/am_i_qualified_to_jump_to_a_midsenior_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xj3we/am_i_qualified_to_jump_to_a_midsenior_data/", "subreddit_subscribers": 140226, "created_utc": 1700239146.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello fellow data engineers,\n\nI am currently working on a project where I\u2019m ingesting data from two different sources, each with its own API. The data sources have 20 and 10 tables, respectively. I am facing a few challenges and would appreciate your insights on the following aspects:\n\n\t1.\tAutomating Table Creation in PostgreSQL:\n\t\u2022\tMy approach so far has been to use Python scripts to generate SQL queries for table creation, based on the keys from JSON responses provided by the APIs. These tables are then manually created in an initially empty PostgreSQL database. Is this a standard approach, or are there more efficient methods to automate this process?\n\t2.\tData Import Methodology:\n\t\u2022\tAfter creating the tables, I am converting the JSON response dictionaries into CSV files and then using PostgreSQL\u2019s COPY FROM command to import the data into the corresponding tables. Is this a recommended practice, or are there better alternatives for importing JSON data directly into PostgreSQL?\n\t3.\tHandling Schema Changes:\n\t\u2022\tA concern I have for future operations is how to efficiently manage potential schema changes, such as alterations in column names or the order of keys in JSON responses, which could impact the CSV file structure and consequently the database import process. What strategies or best practices can be employed to handle such dynamic schema changes? \n\n      4. Big data vs small data \n           What changes in the process if for example the data is 1GB vs 10GB vs 100GB per source? Because right now I am making multiple calls to 1 API to not overload the memory. I am thus appending the data onto the csv file in batches. Or is there another more favourable way to load the data?\n\nAny advice or experiences you could share on these topics would be greatly appreciated, as I\u2019m aiming to optimize the process and ensure long-term maintainability.\n\nThank you in advance for your insights!", "author_fullname": "t2_ebebn3prf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice on data ingestion into postgresql", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_17xv74t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700271684.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello fellow data engineers,&lt;/p&gt;\n\n&lt;p&gt;I am currently working on a project where I\u2019m ingesting data from two different sources, each with its own API. The data sources have 20 and 10 tables, respectively. I am facing a few challenges and would appreciate your insights on the following aspects:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;1.  Automating Table Creation in PostgreSQL:\n\u2022 My approach so far has been to use Python scripts to generate SQL queries for table creation, based on the keys from JSON responses provided by the APIs. These tables are then manually created in an initially empty PostgreSQL database. Is this a standard approach, or are there more efficient methods to automate this process?\n2.  Data Import Methodology:\n\u2022 After creating the tables, I am converting the JSON response dictionaries into CSV files and then using PostgreSQL\u2019s COPY FROM command to import the data into the corresponding tables. Is this a recommended practice, or are there better alternatives for importing JSON data directly into PostgreSQL?\n3.  Handling Schema Changes:\n\u2022 A concern I have for future operations is how to efficiently manage potential schema changes, such as alterations in column names or the order of keys in JSON responses, which could impact the CSV file structure and consequently the database import process. What strategies or best practices can be employed to handle such dynamic schema changes? \n\n  4. Big data vs small data \n       What changes in the process if for example the data is 1GB vs 10GB vs 100GB per source? Because right now I am making multiple calls to 1 API to not overload the memory. I am thus appending the data onto the csv file in batches. Or is there another more favourable way to load the data?\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Any advice or experiences you could share on these topics would be greatly appreciated, as I\u2019m aiming to optimize the process and ensure long-term maintainability.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xv74t", "is_robot_indexable": true, "report_reasons": null, "author": "mtn331", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xv74t/seeking_advice_on_data_ingestion_into_postgresql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xv74t/seeking_advice_on_data_ingestion_into_postgresql/", "subreddit_subscribers": 140226, "created_utc": 1700271684.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi reddit,I'm trying to implement a lakehous with hive 4 and iceberg and i think i'm almost there but...When I create a table like:\n\n    CREATE TABLE ice (id INT, name STRING) STORED BY ICEBERG;\n\nAnd then insert some data\n\n    INSERT INTO ice VALUES (1, 'one');\n\nAnd then select\n\n    SELECT * FROM ice;\n\nI don't see the inserted record.  \n\n\nEDIT: I do see the parquet files on storage, and see the folder and file of the metadata, as if the table was created as location\\_based\\_table, But documentation says that if no specific icber.catalog is set, it will use the defaul hive catalog.\n\nWhat am I missing?I suspect that is something related to the metastore, but can't find what...Thanks in advance, cheers.", "author_fullname": "t2_aytgj0xr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Created Hive tables STORED BY ICEBER; SELECT * returns empty", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xtb5i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700266559.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700266293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi reddit,I&amp;#39;m trying to implement a lakehous with hive 4 and iceberg and i think i&amp;#39;m almost there but...When I create a table like:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE ice (id INT, name STRING) STORED BY ICEBERG;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And then insert some data&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;INSERT INTO ice VALUES (1, &amp;#39;one&amp;#39;);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And then select&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;SELECT * FROM ice;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I don&amp;#39;t see the inserted record.  &lt;/p&gt;\n\n&lt;p&gt;EDIT: I do see the parquet files on storage, and see the folder and file of the metadata, as if the table was created as location_based_table, But documentation says that if no specific icber.catalog is set, it will use the defaul hive catalog.&lt;/p&gt;\n\n&lt;p&gt;What am I missing?I suspect that is something related to the metastore, but can&amp;#39;t find what...Thanks in advance, cheers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xtb5i", "is_robot_indexable": true, "report_reasons": null, "author": "Brilliant-Adward", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xtb5i/created_hive_tables_stored_by_iceber_select/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xtb5i/created_hive_tables_stored_by_iceber_select/", "subreddit_subscribers": 140226, "created_utc": 1700266293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background: I'm part of the SEM Team at a large Fortune company, where I work as a marketing analyst. My primary tools are MS Excel and Power BI. While SQL isn't a part of my daily tasks, I do possess some familiarity with writing SQL queries.\n\nOur team is transitioning to Bigquery in 2024 with the help of the IT team. I want to use this opportunity to make a move to Data Engineering. being in analytics helped me solidify the business side of things. I came from marketing anyway, so my expertise wasn't just in running numbers and building models and tools - I came in with a strategic education and strategic experience.\n\nI strongly believe gaining the D.E. experience while still being heavily involved in Analytics gives me the edge and makes me indispensable for my team.\n\nMy questions: Given my background and future requirements, what should I consider in a BigQuery course before paying for it? I just don't want to learn the querying/SQL side of big query rather I want to be actively involved in the data engineering side of Big Query so I can liaison between my team and IT.\n\nI looked into Coursera for BigQuery courses, there are 100s of courses. I don't know what I should pick.\n\nPlease drop your thoughts/suggestions.", "author_fullname": "t2_p2qjz042", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Transitioning to 'Analytics Engineering' by blending my existing Analytics and acquiring Data Engineering skills", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xl0r8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700244249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background: I&amp;#39;m part of the SEM Team at a large Fortune company, where I work as a marketing analyst. My primary tools are MS Excel and Power BI. While SQL isn&amp;#39;t a part of my daily tasks, I do possess some familiarity with writing SQL queries.&lt;/p&gt;\n\n&lt;p&gt;Our team is transitioning to Bigquery in 2024 with the help of the IT team. I want to use this opportunity to make a move to Data Engineering. being in analytics helped me solidify the business side of things. I came from marketing anyway, so my expertise wasn&amp;#39;t just in running numbers and building models and tools - I came in with a strategic education and strategic experience.&lt;/p&gt;\n\n&lt;p&gt;I strongly believe gaining the D.E. experience while still being heavily involved in Analytics gives me the edge and makes me indispensable for my team.&lt;/p&gt;\n\n&lt;p&gt;My questions: Given my background and future requirements, what should I consider in a BigQuery course before paying for it? I just don&amp;#39;t want to learn the querying/SQL side of big query rather I want to be actively involved in the data engineering side of Big Query so I can liaison between my team and IT.&lt;/p&gt;\n\n&lt;p&gt;I looked into Coursera for BigQuery courses, there are 100s of courses. I don&amp;#39;t know what I should pick.&lt;/p&gt;\n\n&lt;p&gt;Please drop your thoughts/suggestions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17xl0r8", "is_robot_indexable": true, "report_reasons": null, "author": "NumbTheFather", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xl0r8/transitioning_to_analytics_engineering_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xl0r8/transitioning_to_analytics_engineering_by/", "subreddit_subscribers": 140226, "created_utc": 1700244249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi y\u2019all! \n\nI\u2019m not a data engineer and I\u2019d like to ask few questions to follow with the team because i want to understand the process and if their plans logical or not. I\u2019ve tried to google it but i couldn\u2019t understand it cause again.. i\u2019m not a data engineer. \n\n1/ in which phase of building a data warehouse you\u2019ll start designing the schemas and do the data modelling? \n\n2/ can you start designing the schemas before you setup your cloud and dw? (some delays here and would like to know if we can start something else instead of waiting) \n\n3/ team decided to go with dbt (data building tool) does this tool have advanced features like building the schemas automatically for example or is there any sort of built ins\u2026 or the only way to create your facts and dimensions is by coding? \n\n\ni\u2019m in a startup and we don\u2019t have much expertise in this field, and I would like to know a bit more. \n\nAlso, please if you know any good YT channels that explains this process from the very beginning, share it with me. \n\nThank you in advance,\nBest,", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Schemas and data modelling", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xi4ru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700236495.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi y\u2019all! &lt;/p&gt;\n\n&lt;p&gt;I\u2019m not a data engineer and I\u2019d like to ask few questions to follow with the team because i want to understand the process and if their plans logical or not. I\u2019ve tried to google it but i couldn\u2019t understand it cause again.. i\u2019m not a data engineer. &lt;/p&gt;\n\n&lt;p&gt;1/ in which phase of building a data warehouse you\u2019ll start designing the schemas and do the data modelling? &lt;/p&gt;\n\n&lt;p&gt;2/ can you start designing the schemas before you setup your cloud and dw? (some delays here and would like to know if we can start something else instead of waiting) &lt;/p&gt;\n\n&lt;p&gt;3/ team decided to go with dbt (data building tool) does this tool have advanced features like building the schemas automatically for example or is there any sort of built ins\u2026 or the only way to create your facts and dimensions is by coding? &lt;/p&gt;\n\n&lt;p&gt;i\u2019m in a startup and we don\u2019t have much expertise in this field, and I would like to know a bit more. &lt;/p&gt;\n\n&lt;p&gt;Also, please if you know any good YT channels that explains this process from the very beginning, share it with me. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance,\nBest,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17xi4ru", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xi4ru/schemas_and_data_modelling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xi4ru/schemas_and_data_modelling/", "subreddit_subscribers": 140226, "created_utc": 1700236495.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hey guys i am having a problem to submit my script code into kafka due to dependencies issues , in the same script i read data and send it to elasticsearch . this is the link to my stackoverflow question : [https://stackoverflow.com/questions/77501116/getting-some-logs-from-kafka-topic-into-spark-and-the-sending-it-to-elasticsearc](https://stackoverflow.com/questions/77501116/getting-some-logs-from-kafka-topic-into-spark-and-the-sending-it-to-elasticsearc)  \n i could really use your help thank you", "author_fullname": "t2_6lzy3mq7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "spark streaming data from kafka topic to elasticsearch", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xh6lu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700233856.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey guys i am having a problem to submit my script code into kafka due to dependencies issues , in the same script i read data and send it to elasticsearch . this is the link to my stackoverflow question : &lt;a href=\"https://stackoverflow.com/questions/77501116/getting-some-logs-from-kafka-topic-into-spark-and-the-sending-it-to-elasticsearc\"&gt;https://stackoverflow.com/questions/77501116/getting-some-logs-from-kafka-topic-into-spark-and-the-sending-it-to-elasticsearc&lt;/a&gt;&lt;br/&gt;\n i could really use your help thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?auto=webp&amp;s=a70d21ce9f01f64670d2200ca9fc3f39b94a7e48", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0aad06750c23b98c9b7595343a8b54a42dc18851", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/yzSfTlKTSYGpEXeFgyDvHlfoLGOFQJqPuH_Y38RBz2U.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b66126834977e269be586d07464046049ed09138", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xh6lu", "is_robot_indexable": true, "report_reasons": null, "author": "ekkoogod", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xh6lu/spark_streaming_data_from_kafka_topic_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xh6lu/spark_streaming_data_from_kafka_topic_to/", "subreddit_subscribers": 140226, "created_utc": 1700233856.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to design my DAGs like so:\n\n    Parent DAG:\n    start &gt;&gt; notify_start &gt;&gt; notify_complete &gt;&gt; end\n\n&amp;#x200B;\n\n    dag1:\n    sense_start &gt;&gt; run_job_1 &gt;&gt; end\n    \n    dag2:\n    sense_start &gt;&gt; run_job_2 &gt;&gt; end\n    \n    dag3: ... etc.\n\n&amp;#x200B;\n\nEach sense\\_start will be a ExternalTaskSensor sensing the notify\\_start task. \n\nI'd like the parent DAG to **wait** for all the downstream DAGs to complete before notify\\_complete executes, without hardcoding knowledge of the downstream DAGs into it.\n\nIs there a way to do this? I want the flexibility to easily add new DAGs (dag\\_4, dag\\_5 etc.) without having to change code upstream that knows about the new additions.\n\nI thought I could add in ExternalTaskMarker to the Parent DAG, but that needs knowledge of the downstream it seems.", "author_fullname": "t2_6a6ra", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an Airflow operator that can wait on multiple ExternalTaskSensor that depend on it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xc7ya", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700217288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to design my DAGs like so:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Parent DAG:\nstart &amp;gt;&amp;gt; notify_start &amp;gt;&amp;gt; notify_complete &amp;gt;&amp;gt; end\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;dag1:\nsense_start &amp;gt;&amp;gt; run_job_1 &amp;gt;&amp;gt; end\n\ndag2:\nsense_start &amp;gt;&amp;gt; run_job_2 &amp;gt;&amp;gt; end\n\ndag3: ... etc.\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Each sense_start will be a ExternalTaskSensor sensing the notify_start task. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like the parent DAG to &lt;strong&gt;wait&lt;/strong&gt; for all the downstream DAGs to complete before notify_complete executes, without hardcoding knowledge of the downstream DAGs into it.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to do this? I want the flexibility to easily add new DAGs (dag_4, dag_5 etc.) without having to change code upstream that knows about the new additions.&lt;/p&gt;\n\n&lt;p&gt;I thought I could add in ExternalTaskMarker to the Parent DAG, but that needs knowledge of the downstream it seems.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xc7ya", "is_robot_indexable": true, "report_reasons": null, "author": "Firepanda", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xc7ya/is_there_an_airflow_operator_that_can_wait_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xc7ya/is_there_an_airflow_operator_that_can_wait_on/", "subreddit_subscribers": 140226, "created_utc": 1700217288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I (45M) have been working on customer dashboards for my most recent job.  Its boring AF, you can only make so many Tableau dashboards before wanting to punch yourself in the face.  I can continue to do this, make decent $ at it, but man, work has to have something more fun than just the $ that I make.  \n\n\nI want to get back into data engineering - but I am not sure which types of jobs I should apply to, I don't have luck getting interviews.  \n\n\nMy background is in RDBMS data warehousing, and building data warehouse applications (series of stored procedures that accomplished a very complicated and data intensive accounting process with slowly changing rules).  Creating logical and physical data models is something I am good at.  I understand, write, and tune SQL quite well, and have many years job experience doing these tasks.  I ran and operated things on Teradata, and can tune their sharded MPP pretty well.  All of these were orchestrated in UC4, I haven't used tools like Airflow, but aren't they the same?  \n\n\nI was a user of Impala, but never engineered anything on Hadoop - only consumed data there.  \n\n\nI've built a Snowflake Data Warehouse on top of AWS S3, and was minimally involved in creating data pipelines to those AWS buckets (I built firehose jobs that partitioned our data).  Parsing JSON events and creating a real-time ETL - I have done this stuff pretty well.  \n\n\nWhere I feel I lack, and interviewers know it:\n\nI have NO on the job experience with Python, Spark, Databricks, Azure.  People look for my experience with SQL Server, and I'm like... Isn't it the same as these others?  I've not configured RedShift before, I've only consumed data there, so no experience standing up files as a datalake via Redshift - I've only ingested files into Snowflake (not even used files as external tables).  \n\n\nI have built fairly complicated for-fun Python projects at home, but otherwise I have NO on the job experience there.  I downloaded a local instance of Spark, and did some stupid stuff with it (weeee!!!!), but I've never seen jobs in production on Spark.  \n\n\nWhat types of jobs should I be applying for?  How do I go about get practical (preferably on the job) experience in the things I am lacking?  I feel my skills are atrophied, no longer relevant to jobs people are hiring for, and am struggling figuring out how to bridge the gap to these newer technologies.", "author_fullname": "t2_ncssul4iz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kind of DE job to apply to", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xt48f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700265772.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I (45M) have been working on customer dashboards for my most recent job.  Its boring AF, you can only make so many Tableau dashboards before wanting to punch yourself in the face.  I can continue to do this, make decent $ at it, but man, work has to have something more fun than just the $ that I make.  &lt;/p&gt;\n\n&lt;p&gt;I want to get back into data engineering - but I am not sure which types of jobs I should apply to, I don&amp;#39;t have luck getting interviews.  &lt;/p&gt;\n\n&lt;p&gt;My background is in RDBMS data warehousing, and building data warehouse applications (series of stored procedures that accomplished a very complicated and data intensive accounting process with slowly changing rules).  Creating logical and physical data models is something I am good at.  I understand, write, and tune SQL quite well, and have many years job experience doing these tasks.  I ran and operated things on Teradata, and can tune their sharded MPP pretty well.  All of these were orchestrated in UC4, I haven&amp;#39;t used tools like Airflow, but aren&amp;#39;t they the same?  &lt;/p&gt;\n\n&lt;p&gt;I was a user of Impala, but never engineered anything on Hadoop - only consumed data there.  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built a Snowflake Data Warehouse on top of AWS S3, and was minimally involved in creating data pipelines to those AWS buckets (I built firehose jobs that partitioned our data).  Parsing JSON events and creating a real-time ETL - I have done this stuff pretty well.  &lt;/p&gt;\n\n&lt;p&gt;Where I feel I lack, and interviewers know it:&lt;/p&gt;\n\n&lt;p&gt;I have NO on the job experience with Python, Spark, Databricks, Azure.  People look for my experience with SQL Server, and I&amp;#39;m like... Isn&amp;#39;t it the same as these others?  I&amp;#39;ve not configured RedShift before, I&amp;#39;ve only consumed data there, so no experience standing up files as a datalake via Redshift - I&amp;#39;ve only ingested files into Snowflake (not even used files as external tables).  &lt;/p&gt;\n\n&lt;p&gt;I have built fairly complicated for-fun Python projects at home, but otherwise I have NO on the job experience there.  I downloaded a local instance of Spark, and did some stupid stuff with it (weeee!!!!), but I&amp;#39;ve never seen jobs in production on Spark.  &lt;/p&gt;\n\n&lt;p&gt;What types of jobs should I be applying for?  How do I go about get practical (preferably on the job) experience in the things I am lacking?  I feel my skills are atrophied, no longer relevant to jobs people are hiring for, and am struggling figuring out how to bridge the gap to these newer technologies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "17xt48f", "is_robot_indexable": true, "report_reasons": null, "author": "IllustriousCorgi9877", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xt48f/kind_of_de_job_to_apply_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xt48f/kind_of_de_job_to_apply_to/", "subreddit_subscribers": 140226, "created_utc": 1700265772.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Everyone,  \nI'm interested in resource allocation for training deep learning models in clusters. Previously I have worked with Open source serverless frameworks such as OpenWhisk for other projects (Not on ML/DL). I'm looking for open source frameworks that manage resource allocation for deep learning models. I came across Ray while doing some searches. My goal is to modify some core parts of the framework and evaluate my ideas. Has anyone tried doing the same thing before? I welcome any suggestions since I haven't previously worked on this topic. \n\nThanks!", "author_fullname": "t2_5worb5ev", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking Advice on Open Source Frameworks for Resource Allocation in Deep Learning Model Training", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xqoo0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700259325.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Everyone,&lt;br/&gt;\nI&amp;#39;m interested in resource allocation for training deep learning models in clusters. Previously I have worked with Open source serverless frameworks such as OpenWhisk for other projects (Not on ML/DL). I&amp;#39;m looking for open source frameworks that manage resource allocation for deep learning models. I came across Ray while doing some searches. My goal is to modify some core parts of the framework and evaluate my ideas. Has anyone tried doing the same thing before? I welcome any suggestions since I haven&amp;#39;t previously worked on this topic. &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xqoo0", "is_robot_indexable": true, "report_reasons": null, "author": "ahmadreza_hadi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xqoo0/seeking_advice_on_open_source_frameworks_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xqoo0/seeking_advice_on_open_source_frameworks_for/", "subreddit_subscribers": 140226, "created_utc": 1700259325.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone here interview with Booz Allen for a data engineer position that's asking for 2+ years of experience?\n\nIf so can you give any feedback or tips for preparation?", "author_fullname": "t2_p5wlf0g4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview coming up", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xpsml", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700256996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone here interview with Booz Allen for a data engineer position that&amp;#39;s asking for 2+ years of experience?&lt;/p&gt;\n\n&lt;p&gt;If so can you give any feedback or tips for preparation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "17xpsml", "is_robot_indexable": true, "report_reasons": null, "author": "El_Cato_Crande", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xpsml/interview_coming_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xpsml/interview_coming_up/", "subreddit_subscribers": 140226, "created_utc": 1700256996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "With a medallion architecture, what would be the best practice to deal with tables that have the same subject?\n\nIf I have for example sales orders or customer information from the same business, but they come from different source systems, and these tables dosent have the same schema.\n\nShould I keep the 3NF in the silver layer and only combine these informations in my final gold table? Or the silver layer would be the place to create these final models by subject with business logic?  \nI come from a DW/BI background, but these layers still confuse me", "author_fullname": "t2_2t4hdsut", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with different sources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xoo22", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700254091.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With a medallion architecture, what would be the best practice to deal with tables that have the same subject?&lt;/p&gt;\n\n&lt;p&gt;If I have for example sales orders or customer information from the same business, but they come from different source systems, and these tables dosent have the same schema.&lt;/p&gt;\n\n&lt;p&gt;Should I keep the 3NF in the silver layer and only combine these informations in my final gold table? Or the silver layer would be the place to create these final models by subject with business logic?&lt;br/&gt;\nI come from a DW/BI background, but these layers still confuse me&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xoo22", "is_robot_indexable": true, "report_reasons": null, "author": "ltofanelli", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xoo22/how_to_deal_with_different_sources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xoo22/how_to_deal_with_different_sources/", "subreddit_subscribers": 140226, "created_utc": 1700254091.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is it possible to use dbt Cloud with Azure DevOps GIT without the enterprise license of dbt Cloud somehow? \n\nIt would be much easier for us to have dbt Cloud and all our Azure resources in Azure DevOps GIT and pipelines. I really don't want to be forced to use Github for dbt Cloud and have two GIT repos. But it's a tiny project and it doesn't justify an expensive dbt Cloud enterprise account.", "author_fullname": "t2_u2p974i5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt Cloud + Azure DevOps GIT without enterprise license?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xo1w2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700252384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to use dbt Cloud with Azure DevOps GIT without the enterprise license of dbt Cloud somehow? &lt;/p&gt;\n\n&lt;p&gt;It would be much easier for us to have dbt Cloud and all our Azure resources in Azure DevOps GIT and pipelines. I really don&amp;#39;t want to be forced to use Github for dbt Cloud and have two GIT repos. But it&amp;#39;s a tiny project and it doesn&amp;#39;t justify an expensive dbt Cloud enterprise account.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xo1w2", "is_robot_indexable": true, "report_reasons": null, "author": "MarcScripts", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xo1w2/dbt_cloud_azure_devops_git_without_enterprise/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xo1w2/dbt_cloud_azure_devops_git_without_enterprise/", "subreddit_subscribers": 140226, "created_utc": 1700252384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe have data coming from rest apis that need to get loaded in Snowflake. We are keeping costs to a minimum but have potentially a lot of requests coming through this so needs to scale.\n\nI was thinking ELT\n\nAPI -&gt; blob using azure functions then to snowflake and use dbt in snowflake.\n\nAny other ideas? I also looked at some previous posts and they said something similar but mentioned \u201cairflow to manage it all\u201d. Where would airflow fit into this?\n\nShould we use a tool instead of azure functions", "author_fullname": "t2_o1vjl2jil", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ELT API to Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xni9x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700251871.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700250914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We have data coming from rest apis that need to get loaded in Snowflake. We are keeping costs to a minimum but have potentially a lot of requests coming through this so needs to scale.&lt;/p&gt;\n\n&lt;p&gt;I was thinking ELT&lt;/p&gt;\n\n&lt;p&gt;API -&amp;gt; blob using azure functions then to snowflake and use dbt in snowflake.&lt;/p&gt;\n\n&lt;p&gt;Any other ideas? I also looked at some previous posts and they said something similar but mentioned \u201cairflow to manage it all\u201d. Where would airflow fit into this?&lt;/p&gt;\n\n&lt;p&gt;Should we use a tool instead of azure functions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17xni9x", "is_robot_indexable": true, "report_reasons": null, "author": "Distinct-Mention4792", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xni9x/elt_api_to_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xni9x/elt_api_to_snowflake/", "subreddit_subscribers": 140226, "created_utc": 1700250914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "&amp;#x200B;\n\nWe have a customer is using azure sql serverices to aggregate data that they feed into powerbi. They are using azure datafactory to intake data from various sources. We have created a site to site vpn between azure and a software  vendor's (ModMed) AWS SQL instance and the VPN is connected. We are unable to connect the data factory to the vendor's AWS SQL instance over the VPN using a linked service. \n\nWe are not confident in how the linked service should be configured or if we need to add a service endpoint (IP address) to the data factory.\n\nOne suggestion we've received is to create a virtual machine and connect it to the VPN tunnel, setup an Integration runtime agent running on the virtual machine, set the Data factory linked service to \u201cconnect via the integration runtime\u201d. \n\nThis is less than ideal because adding another VM adds cost. \n\nCan we configure the data factory to connect directly to the AWS instance over the VPN, or is a VM or workstation running the IR agent the only solution?", "author_fullname": "t2_66hzi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trouble connecting Modmed with Azure. Suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xldc1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700245193.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;We have a customer is using azure sql serverices to aggregate data that they feed into powerbi. They are using azure datafactory to intake data from various sources. We have created a site to site vpn between azure and a software  vendor&amp;#39;s (ModMed) AWS SQL instance and the VPN is connected. We are unable to connect the data factory to the vendor&amp;#39;s AWS SQL instance over the VPN using a linked service. &lt;/p&gt;\n\n&lt;p&gt;We are not confident in how the linked service should be configured or if we need to add a service endpoint (IP address) to the data factory.&lt;/p&gt;\n\n&lt;p&gt;One suggestion we&amp;#39;ve received is to create a virtual machine and connect it to the VPN tunnel, setup an Integration runtime agent running on the virtual machine, set the Data factory linked service to \u201cconnect via the integration runtime\u201d. &lt;/p&gt;\n\n&lt;p&gt;This is less than ideal because adding another VM adds cost. &lt;/p&gt;\n\n&lt;p&gt;Can we configure the data factory to connect directly to the AWS instance over the VPN, or is a VM or workstation running the IR agent the only solution?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xldc1", "is_robot_indexable": true, "report_reasons": null, "author": "traft00", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xldc1/trouble_connecting_modmed_with_azure_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xldc1/trouble_connecting_modmed_with_azure_suggestions/", "subreddit_subscribers": 140226, "created_utc": 1700245193.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to incorporate dbt and was reading on incremental models. In my case, the past data of the source frequently gets changed without notification. How to ensure that my downstream models have the correct data? Is manual backfilling the only option?", "author_fullname": "t2_6fccledv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt incremental models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xhqj2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700235385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to incorporate dbt and was reading on incremental models. In my case, the past data of the source frequently gets changed without notification. How to ensure that my downstream models have the correct data? Is manual backfilling the only option?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "17xhqj2", "is_robot_indexable": true, "report_reasons": null, "author": "deadlypiranha", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xhqj2/dbt_incremental_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xhqj2/dbt_incremental_models/", "subreddit_subscribers": 140226, "created_utc": 1700235385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Reddit Family, I have a request\u2026\n\nWe\u2019ve just launched Lassoo Headless Analytics to the public.\n\n[Lassoo](https://lassoo.io?ref=reddit_de) Headless Analytics collects, organizes, enriches, and reconciles first-party behavioral customer data from your website or SaaS app. Then, it lets you report or activate your data for Customer360, advanced analytics, and AI/ML use cases through direct PostgreSQL access via the tools you already use. \n\nWe\u2019re offering 3-4 companies an extended trial to kick the tires of our technology. The goal is to work closely with you - providing white glove service - to generate a compelling success story by helping you put your customer data to use.\n\nWe\u2019ll help you get the most value from your data by guiding you through analysis and activation, leveraging our experience with brands ranging from Volkswagen to growing e-commerce shops.\n\nIf you\u2019re interested, please DM me.\n\nP.S. It would be great if you could please share this with any colleagues you think could benefit from this offer", "author_fullname": "t2_m53gg9re", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Headless Analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xft3b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700229984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit Family, I have a request\u2026&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve just launched Lassoo Headless Analytics to the public.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lassoo.io?ref=reddit_de\"&gt;Lassoo&lt;/a&gt; Headless Analytics collects, organizes, enriches, and reconciles first-party behavioral customer data from your website or SaaS app. Then, it lets you report or activate your data for Customer360, advanced analytics, and AI/ML use cases through direct PostgreSQL access via the tools you already use. &lt;/p&gt;\n\n&lt;p&gt;We\u2019re offering 3-4 companies an extended trial to kick the tires of our technology. The goal is to work closely with you - providing white glove service - to generate a compelling success story by helping you put your customer data to use.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ll help you get the most value from your data by guiding you through analysis and activation, leveraging our experience with brands ranging from Volkswagen to growing e-commerce shops.&lt;/p&gt;\n\n&lt;p&gt;If you\u2019re interested, please DM me.&lt;/p&gt;\n\n&lt;p&gt;P.S. It would be great if you could please share this with any colleagues you think could benefit from this offer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xft3b", "is_robot_indexable": true, "report_reasons": null, "author": "Euphoric-Let-8960", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xft3b/headless_analytics/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xft3b/headless_analytics/", "subreddit_subscribers": 140226, "created_utc": 1700229984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Team,  \n\n\nI have been tasked with Migrating databases from SQL Server to Azure PostgreSQL. I know how to Migrate a Database from SQL Server to an Azure SQL Database using the Azure Database Migration Service. However, I am not familiar with how I can do it for other Database services like PostgreSQL or MYSQL.  \n\n\nI have search online but no positive solution. I will appreciate if anyone can guild me or point me to a good resource I can use in achieving this.  \n\n\nThanks", "author_fullname": "t2_6fwa4j9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migrate Database from SQL Server On-premise to Azure PostgreSQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_17xeyjk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700227369.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Team,  &lt;/p&gt;\n\n&lt;p&gt;I have been tasked with Migrating databases from SQL Server to Azure PostgreSQL. I know how to Migrate a Database from SQL Server to an Azure SQL Database using the Azure Database Migration Service. However, I am not familiar with how I can do it for other Database services like PostgreSQL or MYSQL.  &lt;/p&gt;\n\n&lt;p&gt;I have search online but no positive solution. I will appreciate if anyone can guild me or point me to a good resource I can use in achieving this.  &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "17xeyjk", "is_robot_indexable": true, "report_reasons": null, "author": "kiddojazz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/17xeyjk/migrate_database_from_sql_server_onpremise_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/17xeyjk/migrate_database_from_sql_server_onpremise_to/", "subreddit_subscribers": 140226, "created_utc": 1700227369.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}