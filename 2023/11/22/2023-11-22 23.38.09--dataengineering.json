{"kind": "Listing", "data": {"after": "t3_18154ki", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thinking of switching to data engineering. I currently make $91k in clinical informatics with amazing WLB. \n\nWondering, what is the WLB like as a data engineer? How does it compare to SWE WLB?", "author_fullname": "t2_7x2alm42", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the work life balance like as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180ysrr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700620924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thinking of switching to data engineering. I currently make $91k in clinical informatics with amazing WLB. &lt;/p&gt;\n\n&lt;p&gt;Wondering, what is the WLB like as a data engineer? How does it compare to SWE WLB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180ysrr", "is_robot_indexable": true, "report_reasons": null, "author": "InstaMastery", "discussion_type": null, "num_comments": 49, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180ysrr/what_is_the_work_life_balance_like_as_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180ysrr/what_is_the_work_life_balance_like_as_a_data/", "subreddit_subscribers": 141181, "created_utc": 1700620924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I lurked in the sub and saw posts talking about transitioning from data engineering to other roles like backend , dataops , data analysis, BI  , cloud engineering and devops \n\n\nI think there are no roles 100% data engineering and the fact i didn't find alot of jobs or interns so it's best to be more rounded? \n\nSo what skills are mostly common between them and data engineering. i know it depends on the company and the role but what you guys think?", "author_fullname": "t2_hxue1umo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what roles are close to data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1817rwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700655796.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700655302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I lurked in the sub and saw posts talking about transitioning from data engineering to other roles like backend , dataops , data analysis, BI  , cloud engineering and devops &lt;/p&gt;\n\n&lt;p&gt;I think there are no roles 100% data engineering and the fact i didn&amp;#39;t find alot of jobs or interns so it&amp;#39;s best to be more rounded? &lt;/p&gt;\n\n&lt;p&gt;So what skills are mostly common between them and data engineering. i know it depends on the company and the role but what you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1817rwr", "is_robot_indexable": true, "report_reasons": null, "author": "Single-Sound-1865", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1817rwr/what_roles_are_close_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1817rwr/what_roles_are_close_to_data_engineering/", "subreddit_subscribers": 141181, "created_utc": 1700655302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Source data is big json objects. We don't use all fields but some of the ones we do use are heavily nested.\n\nMy instinct is to land the json into a raw stage, then extract the pertinent fields to yield flat tables that can feed data marts. \n\nObviously that's a very high lever overview. But wondering if anyone has opinions on that direction vs somehow keeping JSON intact.\n\nThanks for any pointers, links, thoughts.", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dealing with big JSON objects - flatten into tabular or find a way to query JSON efficiently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181axwe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700664967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Source data is big json objects. We don&amp;#39;t use all fields but some of the ones we do use are heavily nested.&lt;/p&gt;\n\n&lt;p&gt;My instinct is to land the json into a raw stage, then extract the pertinent fields to yield flat tables that can feed data marts. &lt;/p&gt;\n\n&lt;p&gt;Obviously that&amp;#39;s a very high lever overview. But wondering if anyone has opinions on that direction vs somehow keeping JSON intact.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any pointers, links, thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181axwe", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181axwe/dealing_with_big_json_objects_flatten_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181axwe/dealing_with_big_json_objects_flatten_into/", "subreddit_subscribers": 141181, "created_utc": 1700664967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I find that most of the material out there is beginner level for data modeling for both relational data modeling and warehousing. Is there a good course that you can recommend that\u2019s at least intermediate or advanced. Thank you! \ud83d\ude4f\ud83c\udffb", "author_fullname": "t2_g7w5zhnv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best data modelling course?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180wmae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700614288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find that most of the material out there is beginner level for data modeling for both relational data modeling and warehousing. Is there a good course that you can recommend that\u2019s at least intermediate or advanced. Thank you! \ud83d\ude4f\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180wmae", "is_robot_indexable": true, "report_reasons": null, "author": "long_spy200", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180wmae/what_is_the_best_data_modelling_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180wmae/what_is_the_best_data_modelling_course/", "subreddit_subscribers": 141181, "created_utc": 1700614288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking for a site with SQL puzzles that are close to real-world situations, like HackerRank but even more practical. Any go-to recommendations?\n\nIf the puzzles would have data modeling questions, that would be awesome!", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Puzzles Recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181a45d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700662562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a site with SQL puzzles that are close to real-world situations, like HackerRank but even more practical. Any go-to recommendations?&lt;/p&gt;\n\n&lt;p&gt;If the puzzles would have data modeling questions, that would be awesome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181a45d", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181a45d/sql_puzzles_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181a45d/sql_puzzles_recommendations/", "subreddit_subscribers": 141181, "created_utc": 1700662562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \nI'll be taking my test next week and I' took Alan Rodrigues's course and saw a lot of videos of YouTube. \nIf anyone has fresh tips for me it would be highly appreciated", "author_fullname": "t2_38po62bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP 203 Nov 23", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180umnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700608751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, \nI&amp;#39;ll be taking my test next week and I&amp;#39; took Alan Rodrigues&amp;#39;s course and saw a lot of videos of YouTube. \nIf anyone has fresh tips for me it would be highly appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "180umnf", "is_robot_indexable": true, "report_reasons": null, "author": "Esteban_Rdz", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180umnf/dp_203_nov_23/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180umnf/dp_203_nov_23/", "subreddit_subscribers": 141181, "created_utc": 1700608751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello,\n\nFor context, we ingest lot of data from IoT devices. We have around 1TB of data in our database (timescaleDB on top of postgresql). \n\nWe would like to limit the retention of our time series to reduce the cost of our managed PSQL. \n\nToday, we store everything in parquet files with hive partitioning for each time series table we have in timescale (&lt;table&gt;/&lt;year&gt;/&lt;month&gt;/part-0.parquet). An airflow job overwrite the parquet file everyday.\n\nTo use data store in parquet files on s3 like storage we use duckdb. \n\nThe main drawback of this is that we cannot easily edit data in the data lake. For instance, sometime we have to apply correction factor on a bunch of measures from some devices on historical data and it\u2019s a nightmare to do this easily on data store in the data lake. \n\nThat\u2019s why we are thinking of using a lake house with Iceberg in mind. \n\nIn the same time we are planing to use Flink or Spark streaming to process sensors data, so I guess it will be an enabler to love to a warehouse. \n\nThe idea will be to use Flink or Spark streaming + Iceberg + Trino on top of it to query / edit data in the lake house. \n\nIn a first step we could use Kafka connect to move data form Kafka to iceberg. \n\nQuestions are:\n\n- does the stack make sense ?\n- is it viable with not so much data ? Or is it too much overhead ? \n\nNotes: we cannot use managed service for the lake house, everything will be hosted on k8s.", "author_fullname": "t2_a26tdl3m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should we use a lake house ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181dnx9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700671681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;For context, we ingest lot of data from IoT devices. We have around 1TB of data in our database (timescaleDB on top of postgresql). &lt;/p&gt;\n\n&lt;p&gt;We would like to limit the retention of our time series to reduce the cost of our managed PSQL. &lt;/p&gt;\n\n&lt;p&gt;Today, we store everything in parquet files with hive partitioning for each time series table we have in timescale (&amp;lt;table&amp;gt;/&amp;lt;year&amp;gt;/&amp;lt;month&amp;gt;/part-0.parquet). An airflow job overwrite the parquet file everyday.&lt;/p&gt;\n\n&lt;p&gt;To use data store in parquet files on s3 like storage we use duckdb. &lt;/p&gt;\n\n&lt;p&gt;The main drawback of this is that we cannot easily edit data in the data lake. For instance, sometime we have to apply correction factor on a bunch of measures from some devices on historical data and it\u2019s a nightmare to do this easily on data store in the data lake. &lt;/p&gt;\n\n&lt;p&gt;That\u2019s why we are thinking of using a lake house with Iceberg in mind. &lt;/p&gt;\n\n&lt;p&gt;In the same time we are planing to use Flink or Spark streaming to process sensors data, so I guess it will be an enabler to love to a warehouse. &lt;/p&gt;\n\n&lt;p&gt;The idea will be to use Flink or Spark streaming + Iceberg + Trino on top of it to query / edit data in the lake house. &lt;/p&gt;\n\n&lt;p&gt;In a first step we could use Kafka connect to move data form Kafka to iceberg. &lt;/p&gt;\n\n&lt;p&gt;Questions are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;does the stack make sense ?&lt;/li&gt;\n&lt;li&gt;is it viable with not so much data ? Or is it too much overhead ? &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Notes: we cannot use managed service for the lake house, everything will be hosted on k8s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181dnx9", "is_robot_indexable": true, "report_reasons": null, "author": "Disastrous-Camp979", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181dnx9/should_we_use_a_lake_house/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181dnx9/should_we_use_a_lake_house/", "subreddit_subscribers": 141181, "created_utc": 1700671681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "TL;DR: Should we add dbt to our tech stack when our existing stack doesn't really require it?\n\nWe're using Google Cloud Storage (parquet) and Snowflake as storage layer and Google Dataproc (managed Spark, via PySpark) for data processing. Dataproc jobs are scheduled via Airflow. \n\nWe recently started using dbt for some transformations and data integration steps within Snowflake (filtering, joining, aggregating).\n\nThe data scientists in our team like it because they can basically work in SQL. A few software developers and data engineers are sceptical, arguing that maintaining an additional data processing framework increases the complexity of the tool chain - especially given that Python + PySpark already do the job. \n\nI'm torn between seeing the case for a simple set of tools and enjoying the simplicity of implementing data integration steps in dbt. How would you decide whether or not to add the complexity of another data processing framework? Have you been in similar situations? ", "author_fullname": "t2_l895f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "More tools, more complexity?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181f78m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700675579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: Should we add dbt to our tech stack when our existing stack doesn&amp;#39;t really require it?&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re using Google Cloud Storage (parquet) and Snowflake as storage layer and Google Dataproc (managed Spark, via PySpark) for data processing. Dataproc jobs are scheduled via Airflow. &lt;/p&gt;\n\n&lt;p&gt;We recently started using dbt for some transformations and data integration steps within Snowflake (filtering, joining, aggregating).&lt;/p&gt;\n\n&lt;p&gt;The data scientists in our team like it because they can basically work in SQL. A few software developers and data engineers are sceptical, arguing that maintaining an additional data processing framework increases the complexity of the tool chain - especially given that Python + PySpark already do the job. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m torn between seeing the case for a simple set of tools and enjoying the simplicity of implementing data integration steps in dbt. How would you decide whether or not to add the complexity of another data processing framework? Have you been in similar situations? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181f78m", "is_robot_indexable": true, "report_reasons": null, "author": "FlatRobots", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181f78m/more_tools_more_complexity/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181f78m/more_tools_more_complexity/", "subreddit_subscribers": 141181, "created_utc": 1700675579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, \n\nI've built a pipeline locally that tasks GA4 data, applies some transformations and tests with DBT, and stores the incrementally transformed tables to a GCP dataset. I put my dbt code in a docker container bc I read that this could help me deploy my dbt project to the cloud faster. But how do you go about automating the DBT code from the docker container? Right now I use taskfile locally to automate the building of the image, container and the serving of the dbt documentation but I fail to understand how I go from there to having this pipeline automated in GCP. \n\n&amp;#x200B;\n\nThank you for your time if you read this. ", "author_fullname": "t2_1jh436du", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you deploy your DBT code in a Docker container to GCP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181admm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700663326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built a pipeline locally that tasks GA4 data, applies some transformations and tests with DBT, and stores the incrementally transformed tables to a GCP dataset. I put my dbt code in a docker container bc I read that this could help me deploy my dbt project to the cloud faster. But how do you go about automating the DBT code from the docker container? Right now I use taskfile locally to automate the building of the image, container and the serving of the dbt documentation but I fail to understand how I go from there to having this pipeline automated in GCP. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time if you read this. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181admm", "is_robot_indexable": true, "report_reasons": null, "author": "Jeannetton", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181admm/how_do_you_deploy_your_dbt_code_in_a_docker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181admm/how_do_you_deploy_your_dbt_code_in_a_docker/", "subreddit_subscribers": 141181, "created_utc": 1700663326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would great expectations/dbt and a viz tool on top to display the metrics ? How do you handle logging and monitoring of chemistry drift of raw files when landing ? Is monitoring / messaging via slack and cloud watch enough ?\n\nJust curious as to what people think works well, ideally open source. That can cover a product and analytic pipelines.", "author_fullname": "t2_13551s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your favorite method tools and metrics to implement data quality checks / monitors for data pipelines that feed product database (Postgres) and data science teams (snowflake / data bricks) (s3 for both)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1810lqk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700626709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would great expectations/dbt and a viz tool on top to display the metrics ? How do you handle logging and monitoring of chemistry drift of raw files when landing ? Is monitoring / messaging via slack and cloud watch enough ?&lt;/p&gt;\n\n&lt;p&gt;Just curious as to what people think works well, ideally open source. That can cover a product and analytic pipelines.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1810lqk", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance2", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1810lqk/what_is_your_favorite_method_tools_and_metrics_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1810lqk/what_is_your_favorite_method_tools_and_metrics_to/", "subreddit_subscribers": 141181, "created_utc": 1700626709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Without going into too much detail, I am a data engineer in an industry with antiquated data collecting methods (so data transformations are complex), and often convoluted business logic (due to nature of the business).\n\nMy team works close to the end of the data stream, so we build data pipelines that are served directly to analytics teams. Because of this, we are expected to develop strong business knowledge, while also working through the complexity of the data itself.\n\nI find myself often struggling to balance these two sides - planning and building infrastructure (understanding \u201cwhat\u201d the data is), but also fielding questions about the intricacies of the data itself from analysts (\u201cwhy\u201d the data is what it is). It feels like my work goes between being a developer and something like an \u201canalytics engineer\u201d in close proximity to the analysis.\n\nMy documentation and note taking feels pretty strong, but at times it is challenging and the context switching can be tough. My manager and colleagues are happy with my work, so that part is good. \n\nCurious to hear from others on this balancing act and how they handled it. I know there is a lot of variance in this type of thing across teams/companies/industries.", "author_fullname": "t2_3jl2j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Balance of \u201cWhat\u201d vs \u201cWhy\u201d in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181gm5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700679137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Without going into too much detail, I am a data engineer in an industry with antiquated data collecting methods (so data transformations are complex), and often convoluted business logic (due to nature of the business).&lt;/p&gt;\n\n&lt;p&gt;My team works close to the end of the data stream, so we build data pipelines that are served directly to analytics teams. Because of this, we are expected to develop strong business knowledge, while also working through the complexity of the data itself.&lt;/p&gt;\n\n&lt;p&gt;I find myself often struggling to balance these two sides - planning and building infrastructure (understanding \u201cwhat\u201d the data is), but also fielding questions about the intricacies of the data itself from analysts (\u201cwhy\u201d the data is what it is). It feels like my work goes between being a developer and something like an \u201canalytics engineer\u201d in close proximity to the analysis.&lt;/p&gt;\n\n&lt;p&gt;My documentation and note taking feels pretty strong, but at times it is challenging and the context switching can be tough. My manager and colleagues are happy with my work, so that part is good. &lt;/p&gt;\n\n&lt;p&gt;Curious to hear from others on this balancing act and how they handled it. I know there is a lot of variance in this type of thing across teams/companies/industries.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181gm5g", "is_robot_indexable": true, "report_reasons": null, "author": "Stupideye", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/181gm5g/balance_of_what_vs_why_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181gm5g/balance_of_what_vs_why_in_data_engineering/", "subreddit_subscribers": 141181, "created_utc": 1700679137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Coming from a background of piping in data into snowflake from apis, I manage ci/cd via dbt / circle CI / terraform / GitHub. Ie  a dev/qa/prod env / schemas that let\u2019s say a new column is added to a dbt model and in turn a table now has a new column and then it gets merged to prod and the prod schema has new Column. \n\n\nWhat\u2019s the best way to do the above and can I leverage any of my knowledge from implementing a process for a dwh rather than a prod db. Also any chance I can kill two birds with one stone ?", "author_fullname": "t2_xo4dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python / AWS / Postgres data pipeline that ingests data from ali and writes to Postgres that is backed for a saas app. How do best manage ci/cd", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1810qpk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700627204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Coming from a background of piping in data into snowflake from apis, I manage ci/cd via dbt / circle CI / terraform / GitHub. Ie  a dev/qa/prod env / schemas that let\u2019s say a new column is added to a dbt model and in turn a table now has a new column and then it gets merged to prod and the prod schema has new Column. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s the best way to do the above and can I leverage any of my knowledge from implementing a process for a dwh rather than a prod db. Also any chance I can kill two birds with one stone ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1810qpk", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1810qpk/python_aws_postgres_data_pipeline_that_ingests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1810qpk/python_aws_postgres_data_pipeline_that_ingests/", "subreddit_subscribers": 141181, "created_utc": 1700627204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What can you expect from Apache Doris as a data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1817mpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1700654775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "doris.apache.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://doris.apache.org/blog/apache-doris-summit-asia-2023-what-can-you-expect-from-apache-doris-as-a-data-warehouse", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1817mpu", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1817mpu/what_can_you_expect_from_apache_doris_as_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://doris.apache.org/blog/apache-doris-summit-asia-2023-what-can-you-expect-from-apache-doris-as-a-data-warehouse", "subreddit_subscribers": 141181, "created_utc": 1700654775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Management seems interested in investing in the West Monroe's data bricks accelerator and I'm curious if anyone has any experience with it or Databricks accelerator's in general.\n\nWe have a small data engineering team and the belief is that using that accelerator will increase our speed to market.  I'm curious if anyone has experience with these types of solutions and how they fare relative to other solutions on the market.\n\n[https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator](https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator)", "author_fullname": "t2_3pgbq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do anyone have experience with West Monroe's Databricks Lakehouse Accelerator Mizu?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180yzxe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700621539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Management seems interested in investing in the West Monroe&amp;#39;s data bricks accelerator and I&amp;#39;m curious if anyone has any experience with it or Databricks accelerator&amp;#39;s in general.&lt;/p&gt;\n\n&lt;p&gt;We have a small data engineering team and the belief is that using that accelerator will increase our speed to market.  I&amp;#39;m curious if anyone has experience with these types of solutions and how they fare relative to other solutions on the market.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator\"&gt;https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?auto=webp&amp;s=f52f3657bb5f3877cbe4a55bd18794778eed9704", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8850f78c52e7797d26c19ad6fda65deb722f263", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=974ce6593363bfbd6c815744281c23d866f10cf2", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=21d02e2d873de235403725ad86e741de9b7fc69e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee266c3f3bf1c2d433acba20b20308e49cffd5ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c151d3b5becb70af0a66fe80b20f59902ce1bc2", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e2e95bfc0db065b7f319e2973b702c60bd9b13f", "width": 1080, "height": 567}], "variants": {}, "id": "2pz3rWcq_ScNWhN7QkxHJWo9cO-lvk6W0rqq9t78ngs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180yzxe", "is_robot_indexable": true, "report_reasons": null, "author": "tehsandvich", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180yzxe/do_anyone_have_experience_with_west_monroes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180yzxe/do_anyone_have_experience_with_west_monroes/", "subreddit_subscribers": 141181, "created_utc": 1700621539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a long weekend coming up with the holidays and wanted to use it to learn a new technology domain that will help me grow my career.\n\nWhich one do you think has more career potential? I am trying to avoid hyped technologies and focus on what companies sought-out for.  Perhaps you can share which skill is more strategic to your company? \n\n[View Poll](https://www.reddit.com/poll/181hts2)", "author_fullname": "t2_5n7kgpm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Long weekend learnings \u2014 stream processing or LLMs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181hts2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700682285.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a long weekend coming up with the holidays and wanted to use it to learn a new technology domain that will help me grow my career.&lt;/p&gt;\n\n&lt;p&gt;Which one do you think has more career potential? I am trying to avoid hyped technologies and focus on what companies sought-out for.  Perhaps you can share which skill is more strategic to your company? &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/181hts2\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "181hts2", "is_robot_indexable": true, "report_reasons": null, "author": "Striking_Solid_5020", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1700941485258, "options": [{"text": "LLM", "id": "26114307"}, {"text": "Stream processing", "id": "26114308"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 18, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181hts2/long_weekend_learnings_stream_processing_or_llms/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/181hts2/long_weekend_learnings_stream_processing_or_llms/", "subreddit_subscribers": 141181, "created_utc": 1700682285.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI just completed a small personal project which is a ELT pipeline that ingests data from API daily at a specific time and gets the air-quality data of the previous day and load it as a parquet file in a GCS bucket partitioned by date. Then it takes the same file and loads it into BigQuery. This is orchestrated by prefect and it occurs right after each other, they have a parent flow and are subflows. It works fine so far but I have a few doubts,\n\n1. What if I want to have different time intervals for API to GCS and GCS to BigQuery. How do I partition the data then. Lets say I hit the API every hour, they should there be a hourly partition where there are separate files for each hour or can I have one file for each day where i append the hourly data?(Does that depend on the size of the file)\n2. Let's assume i change it to hourly partition and sperate the two flows. Data is loaded into BigQuery once a day while data is loaded to GCS once an hour. Now should i just go through the partitions that has the matching date(24 partitions) everytime or is there a way to find out which partitions have already been loaded and which one haven't?\n3. Is decoupling these 2 flows the better practice? How does it work in production pipelines? If yes then how do I ensure all of the previous days data is in the bucket before loading it to BigQuery?", "author_fullname": "t2_w16t5qhn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to orchestrate flows with 2 different time intervals", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181h5j3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700680515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just completed a small personal project which is a ELT pipeline that ingests data from API daily at a specific time and gets the air-quality data of the previous day and load it as a parquet file in a GCS bucket partitioned by date. Then it takes the same file and loads it into BigQuery. This is orchestrated by prefect and it occurs right after each other, they have a parent flow and are subflows. It works fine so far but I have a few doubts,&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What if I want to have different time intervals for API to GCS and GCS to BigQuery. How do I partition the data then. Lets say I hit the API every hour, they should there be a hourly partition where there are separate files for each hour or can I have one file for each day where i append the hourly data?(Does that depend on the size of the file)&lt;/li&gt;\n&lt;li&gt;Let&amp;#39;s assume i change it to hourly partition and sperate the two flows. Data is loaded into BigQuery once a day while data is loaded to GCS once an hour. Now should i just go through the partitions that has the matching date(24 partitions) everytime or is there a way to find out which partitions have already been loaded and which one haven&amp;#39;t?&lt;/li&gt;\n&lt;li&gt;Is decoupling these 2 flows the better practice? How does it work in production pipelines? If yes then how do I ensure all of the previous days data is in the bucket before loading it to BigQuery?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181h5j3", "is_robot_indexable": true, "report_reasons": null, "author": "booberrypie_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/181h5j3/how_to_orchestrate_flows_with_2_different_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181h5j3/how_to_orchestrate_flows_with_2_different_time/", "subreddit_subscribers": 141181, "created_utc": 1700680515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is Kappa\u2019s simplicity the key to your real-time processing needs, or does Lambda\u2019s hybrid versatility suit your batch and streaming scenarios better? Discover the differences, use cases, and benefits that make each framework unique in this guide \n\n[https://memphis.dev/kappa-vs-lambda/](https://memphis.dev/kappa-vs-lambda/)", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kappa vs Lambda guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181bgav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700666312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is Kappa\u2019s simplicity the key to your real-time processing needs, or does Lambda\u2019s hybrid versatility suit your batch and streaming scenarios better? Discover the differences, use cases, and benefits that make each framework unique in this guide &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://memphis.dev/kappa-vs-lambda/\"&gt;https://memphis.dev/kappa-vs-lambda/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?auto=webp&amp;s=41573d9ac660082eaeba6ad842c839de9e381218", "width": 1024, "height": 681}, "resolutions": [{"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=62a231fdacf5ee11c67d8c543212d54c74a32d5b", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4cb76675f01006e97763cb3169495e3de0cb381", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b236b59564bf213abb77081fa03030f894d734aa", "width": 320, "height": 212}, {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=441827d1797e3f4d0dd2a1dcce80cc06cd906b7d", "width": 640, "height": 425}, {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=454279e23a364b257dab8aafc43e7c18d8305eb6", "width": 960, "height": 638}], "variants": {}, "id": "65E_XwJLXDsdcg8e5-svW1D1aVB5bA4Ka1ewYsvf7qY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "181bgav", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181bgav/kappa_vs_lambda_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181bgav/kappa_vs_lambda_guide/", "subreddit_subscribers": 141181, "created_utc": 1700666312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nwe are currently in the process of modernizing the data stack and piloted a couple of monolithic/old ETL/ELT-Solutions like Informtica, Talend etc. and came to the conclusion than none is fitting our needs and only work well for the simplest of tasks and everything remotely complex requires workarounds etc.. It\u00b4s simply not better than the old solution.\n\nWe therefore consindering \"hand roling\" our platform with with an orchestrator and k8s/docker as the execution environment + python as the language of choice, because we have already k8s/helm/git-ops  and python knowledge for developing and deploying numerous python services/webservices for hosting models and other purposes.\n\nMy biggest concern is on how to decide which orchestrator we should use. We have the following requirements.\n\n* **strictly On-prem deployment (regulated environment):**\n   * At best with an helm-chart on k8s (locally hosted OKD Clusters), any other platform would also be ok but k8s is preferred.\n* **Implementation partner to get us started:**\n   * The partner should at best be a german company/a company in DACH Region.\n* **Community**:\n   * Theres a community where one can find infos about working with the orchestrator etc and it\u00b4s not enough to have docs alone.\n* **Paid/OSS**\n   * Both are ok\n\nI personaly tried dagster and airflow and liked dagster more but could finda german implementation partner and the community is also much small.\n\nIt seems like lately a lot of posts/articles/people state that working with airflow is a pain/its a bad piece of software and one should avoid it. \n\nI\u00b4m therefore asking if airflow is still a valid solution to invest in and start the journey with it and if anyone uses dagster in production and knows german partners etc.?\n\nOr maybe I\u00b4m missing a third solution whe should definetly check out and also has german partners?\n\nBest regards\n\n&amp;#x200B;", "author_fullname": "t2_aue40tqc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrator/Scheduler - which one is valid in 2023 to run on an on prem k8s environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1819vgc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700661890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;we are currently in the process of modernizing the data stack and piloted a couple of monolithic/old ETL/ELT-Solutions like Informtica, Talend etc. and came to the conclusion than none is fitting our needs and only work well for the simplest of tasks and everything remotely complex requires workarounds etc.. It\u00b4s simply not better than the old solution.&lt;/p&gt;\n\n&lt;p&gt;We therefore consindering &amp;quot;hand roling&amp;quot; our platform with with an orchestrator and k8s/docker as the execution environment + python as the language of choice, because we have already k8s/helm/git-ops  and python knowledge for developing and deploying numerous python services/webservices for hosting models and other purposes.&lt;/p&gt;\n\n&lt;p&gt;My biggest concern is on how to decide which orchestrator we should use. We have the following requirements.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;strictly On-prem deployment (regulated environment):&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;At best with an helm-chart on k8s (locally hosted OKD Clusters), any other platform would also be ok but k8s is preferred.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Implementation partner to get us started:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The partner should at best be a german company/a company in DACH Region.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Community&lt;/strong&gt;:\n\n&lt;ul&gt;\n&lt;li&gt;Theres a community where one can find infos about working with the orchestrator etc and it\u00b4s not enough to have docs alone.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Paid/OSS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Both are ok&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I personaly tried dagster and airflow and liked dagster more but could finda german implementation partner and the community is also much small.&lt;/p&gt;\n\n&lt;p&gt;It seems like lately a lot of posts/articles/people state that working with airflow is a pain/its a bad piece of software and one should avoid it. &lt;/p&gt;\n\n&lt;p&gt;I\u00b4m therefore asking if airflow is still a valid solution to invest in and start the journey with it and if anyone uses dagster in production and knows german partners etc.?&lt;/p&gt;\n\n&lt;p&gt;Or maybe I\u00b4m missing a third solution whe should definetly check out and also has german partners?&lt;/p&gt;\n\n&lt;p&gt;Best regards&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1819vgc", "is_robot_indexable": true, "report_reasons": null, "author": "Salfiiii", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1819vgc/orchestratorscheduler_which_one_is_valid_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1819vgc/orchestratorscheduler_which_one_is_valid_in_2023/", "subreddit_subscribers": 141181, "created_utc": 1700661890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nMy company has on-premises databases.\n\nI get data from those tables and I have made SSRS reports which uses that data.\n\nNow they want to move that database to cloud (Oracle Cloud).\n\nI want the SSRS reports to get the data from that database on the cloud database.\n\nThe machine I use as a server for SSRS Server is also on-premise.\n\nHow I will connect SSRS reports to cloud database, is it possible to do all this.\n\nI have to present a report about all this in 2 hours.", "author_fullname": "t2_3k9gevl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migration of SSRS &amp; Database from On-Prem to Oracle Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1814cnq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700641250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has on-premises databases.&lt;/p&gt;\n\n&lt;p&gt;I get data from those tables and I have made SSRS reports which uses that data.&lt;/p&gt;\n\n&lt;p&gt;Now they want to move that database to cloud (Oracle Cloud).&lt;/p&gt;\n\n&lt;p&gt;I want the SSRS reports to get the data from that database on the cloud database.&lt;/p&gt;\n\n&lt;p&gt;The machine I use as a server for SSRS Server is also on-premise.&lt;/p&gt;\n\n&lt;p&gt;How I will connect SSRS reports to cloud database, is it possible to do all this.&lt;/p&gt;\n\n&lt;p&gt;I have to present a report about all this in 2 hours.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1814cnq", "is_robot_indexable": true, "report_reasons": null, "author": "Pillstyr", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1814cnq/migration_of_ssrs_database_from_onprem_to_oracle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1814cnq/migration_of_ssrs_database_from_onprem_to_oracle/", "subreddit_subscribers": 141181, "created_utc": 1700641250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a Junior DA. My work usually consists of:\n* AB testing, forecasting\n* delivering insights to management\n* creating small data viz apps using a Python framework\n* writing data transformation logic\n* writing data orchestration logic\n\nI would like to dig deeper in many areas but I am unsure what to focus on. My work is about 70% software engineering and 30% analysis so far.\n\nI have a BS in CS. I would be happy to hear what skills are really necessary as a DA in your opinion (not for my current job, but for me on the long term).", "author_fullname": "t2_qekiscf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal development goals as a DA", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181i1uq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700682875.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a Junior DA. My work usually consists of:\n* AB testing, forecasting\n* delivering insights to management\n* creating small data viz apps using a Python framework\n* writing data transformation logic\n* writing data orchestration logic&lt;/p&gt;\n\n&lt;p&gt;I would like to dig deeper in many areas but I am unsure what to focus on. My work is about 70% software engineering and 30% analysis so far.&lt;/p&gt;\n\n&lt;p&gt;I have a BS in CS. I would be happy to hear what skills are really necessary as a DA in your opinion (not for my current job, but for me on the long term).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "181i1uq", "is_robot_indexable": true, "report_reasons": null, "author": "almightygodszoke", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181i1uq/personal_development_goals_as_a_da/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181i1uq/personal_development_goals_as_a_da/", "subreddit_subscribers": 141181, "created_utc": 1700682875.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey yall, I'm setting up a hybrid ML pipeline for my company and have a question about how to implement a triggered function locally. We're using GCP as a platform and ideally, we would have a bigquery table update publish a message into pubsub that triggers a cloud function that does inference on one of their Vertex AI custom-model endpoints. But for some reason, that endpoint does NOT play well with the xgboost datatypes, no matter what we tried. So my next approach was to run the models on a local Ray serve cluster and run the inference from there. Everything went well re:inference. In order to implement that, I would have to have a local subscription to the same topic and have that trigger a script locally that does the inference. What would the equivalent trigger be in a local set up like this (mesage --&gt; function)? Do I have to set it up in like an Airflow situation? Am I thinking about this correctly? Any input would be appreciated!", "author_fullname": "t2_h2af93ho", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Assistance on triggering local function", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181ejsr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700673962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey yall, I&amp;#39;m setting up a hybrid ML pipeline for my company and have a question about how to implement a triggered function locally. We&amp;#39;re using GCP as a platform and ideally, we would have a bigquery table update publish a message into pubsub that triggers a cloud function that does inference on one of their Vertex AI custom-model endpoints. But for some reason, that endpoint does NOT play well with the xgboost datatypes, no matter what we tried. So my next approach was to run the models on a local Ray serve cluster and run the inference from there. Everything went well re:inference. In order to implement that, I would have to have a local subscription to the same topic and have that trigger a script locally that does the inference. What would the equivalent trigger be in a local set up like this (mesage --&amp;gt; function)? Do I have to set it up in like an Airflow situation? Am I thinking about this correctly? Any input would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181ejsr", "is_robot_indexable": true, "report_reasons": null, "author": "Plusdebeurre", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181ejsr/assistance_on_triggering_local_function/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181ejsr/assistance_on_triggering_local_function/", "subreddit_subscribers": 141181, "created_utc": 1700673962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a government client. As of now, we are not authorized to link the BitBucket account to the Databricks workspace. We\u2019ve been trying to create a script to programmatically branch and push changes. It\u2019s getting pretty ugly and having a difficult time tracking changes. Would love to see some code samples if anyone has done similar before. Thanks", "author_fullname": "t2_feymqzcjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used git on Databricks without their native integration?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181dn6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700671625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a government client. As of now, we are not authorized to link the BitBucket account to the Databricks workspace. We\u2019ve been trying to create a script to programmatically branch and push changes. It\u2019s getting pretty ugly and having a difficult time tracking changes. Would love to see some code samples if anyone has done similar before. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181dn6o", "is_robot_indexable": true, "report_reasons": null, "author": "TheConSpooky", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181dn6o/has_anyone_used_git_on_databricks_without_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181dn6o/has_anyone_used_git_on_databricks_without_their/", "subreddit_subscribers": 141181, "created_utc": 1700671625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, does anyone have experience with dbt and synapse spark pools? I know dbt have an adapter for synapse sql pools and an adapter for databricks. Dbt has also a spark adapter and Im wondering if its possible to use that with synapse? Now that Microsoft introducing fabric, they are taking away a lot of adapters, now its not possible to connect to synapse spark pools from VScode without a fabric workspace for example \ud83e\udee0", "author_fullname": "t2_4c41t3vo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt and synapse spark pools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1819gix", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700660729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, does anyone have experience with dbt and synapse spark pools? I know dbt have an adapter for synapse sql pools and an adapter for databricks. Dbt has also a spark adapter and Im wondering if its possible to use that with synapse? Now that Microsoft introducing fabric, they are taking away a lot of adapters, now its not possible to connect to synapse spark pools from VScode without a fabric workspace for example \ud83e\udee0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1819gix", "is_robot_indexable": true, "report_reasons": null, "author": "fugas1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1819gix/dbt_and_synapse_spark_pools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1819gix/dbt_and_synapse_spark_pools/", "subreddit_subscribers": 141181, "created_utc": 1700660729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This must be something devs run into a lot, but I can only find this list on [datahub](https://datahub.io/core/un-locode) or [this list on Kaggle](https://www.kaggle.com/datasets/programmerrdai/unlocode-code-list?select=subdivision-codes.csv), but I'm not sure if they're up to date or maintained.\n\nWhat's the best way of finding and implementing ISO or UN lists into your projects?", "author_fullname": "t2_cstta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where would you find a full list of UN/LOCODE countries and cities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18191po", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700659471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This must be something devs run into a lot, but I can only find this list on &lt;a href=\"https://datahub.io/core/un-locode\"&gt;datahub&lt;/a&gt; or &lt;a href=\"https://www.kaggle.com/datasets/programmerrdai/unlocode-code-list?select=subdivision-codes.csv\"&gt;this list on Kaggle&lt;/a&gt;, but I&amp;#39;m not sure if they&amp;#39;re up to date or maintained.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way of finding and implementing ISO or UN lists into your projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?auto=webp&amp;s=cece6e86958b39e85d5fbb63c8608ce3e3fe53dd", "width": 1414, "height": 1418}, "resolutions": [{"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4600462a3d997639ddd1ddf74b7c482874d15241", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d19ba67edd405544066bd443d7d46cd70eab498c", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51ea8b83757621409c3470b045562586e2f34aa8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d6b1c3be342aff1fa0e97f624699f56789df7c46", "width": 640, "height": 641}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0e1eb7acddfe28f090b21ef460255b08110a653", "width": 960, "height": 962}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfd6fae80fcc789dc65e9218d37eb7299937cfc5", "width": 1080, "height": 1083}], "variants": {}, "id": "3DephRZhy7zpapaXeulxTbJvfqGyCVgvyc3obhC9CUo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18191po", "is_robot_indexable": true, "report_reasons": null, "author": "Snugglosaurus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18191po/where_would_you_find_a_full_list_of_unlocode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18191po/where_would_you_find_a_full_list_of_unlocode/", "subreddit_subscribers": 141181, "created_utc": 1700659471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry in advance for long post. I tried to just put a lot of detail but before you read just know I am overall just asking for some resources on setting up Amundsen in 2023, and just a sanity check on my initial research. I've been working on this for 2 days.  \n\n\nHello. I am currently in the process of evaluating Amundsen but I am somewhat lost. I've tried following the Quickstart and the examples. I tried opting in for Atlas for the push capabilities but that didn't seem to work so I just decided to attempt to run everything as close to the docs as possible. The deploy with docker compose worked with default settings, but since that no step going forward worked.\n\nI wanted to test the databuilder which seems like the defacto api for putting information into the metadata engine, so I spun up a postgres database with docker and loaded a table into the public schema. I then ran the example sample\\_postgres\\_extractor which did not work because the version of SqlAlchemy after installing the requirements file did not use the same execution method in the databuilder. As much as I wanted to follow the steps as closely as possible I attempted to downpatch SqlAlchemy. This worked after downpatching a few other requirements that were incompatible with the older version of SqlAlchemy but I simply got an elasticsearch.exceptions.ConnectionError. I tried to test running the example dags which also didn't work. \"apache-airflow-providers-elasticsearch 5.1.0 requires elasticsearch&lt;9,&gt;8, but you have elasticsearch 7.17.9 which is incompatible.\" It seems that the airflow elasticsearch provider uses a higher version of the elasticsearch package that amundsen does so the installation fails. For demo purposes I just downpatched the airflow provider so no more conflicts. I ran the dag and it seems to work, though I still get warnings in my logs that I have incompatible versions of elasticsearch. For the local demo airflow this isn't an issue but for our production environment we are using this provider and cannot remove it or downpatch that package.  \n\n\nWith that experience, I feel like I must be missing some resources? I've heard around, even on this subreddit, that many are using Amundsen and very happy with it. I have to ask if the documentation here [https://www.amundsen.io/amundsen/installation/](https://www.amundsen.io/amundsen/installation/) is what people actually recommend to evaluate? Are there better resources for installing/testing the extractors? Are there any good tutorials on writing a custom extractor or using the extractors that exist that aren't just code examples to copy? I would like to know more about how the specific apis in databuilder work because I generally want a good idea of how I would interact or modify one of the examples to fit our needs. I read through the Dashboard Ingestion guidance ([https://www.amundsen.io/amundsen/databuilder/docs/dashboard\\_ingestion\\_guide/](https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/)) and this basically just said use the databuilder and check the examples for how to do that. Considering the examples aren't working I am unsure of how to proceed.\n\nIs there a more up to date guide on the databuilder or a better way to install it? Is it contained in a pip package or do I actually have to build from source with setup tools? Is there a good and recent tutorial outside of the docs that shows the steps today to get up and running? Are there good deployment guides from this year?\n\nAs much as I want to give it a fair shot, I don't know how to really evaluate at this point because I can't really give my team a good estimate on how much work it would be to set this up. I am also not super confident looking at the state of databuilder that it has been well maintained. From what I can see there are quite a few package versions it requires that are deprecated, or nearing end of life.  I must be missing something right?", "author_fullname": "t2_ptgbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amundsen resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18154ki", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700644635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry in advance for long post. I tried to just put a lot of detail but before you read just know I am overall just asking for some resources on setting up Amundsen in 2023, and just a sanity check on my initial research. I&amp;#39;ve been working on this for 2 days.  &lt;/p&gt;\n\n&lt;p&gt;Hello. I am currently in the process of evaluating Amundsen but I am somewhat lost. I&amp;#39;ve tried following the Quickstart and the examples. I tried opting in for Atlas for the push capabilities but that didn&amp;#39;t seem to work so I just decided to attempt to run everything as close to the docs as possible. The deploy with docker compose worked with default settings, but since that no step going forward worked.&lt;/p&gt;\n\n&lt;p&gt;I wanted to test the databuilder which seems like the defacto api for putting information into the metadata engine, so I spun up a postgres database with docker and loaded a table into the public schema. I then ran the example sample_postgres_extractor which did not work because the version of SqlAlchemy after installing the requirements file did not use the same execution method in the databuilder. As much as I wanted to follow the steps as closely as possible I attempted to downpatch SqlAlchemy. This worked after downpatching a few other requirements that were incompatible with the older version of SqlAlchemy but I simply got an elasticsearch.exceptions.ConnectionError. I tried to test running the example dags which also didn&amp;#39;t work. &amp;quot;apache-airflow-providers-elasticsearch 5.1.0 requires elasticsearch&amp;lt;9,&amp;gt;8, but you have elasticsearch 7.17.9 which is incompatible.&amp;quot; It seems that the airflow elasticsearch provider uses a higher version of the elasticsearch package that amundsen does so the installation fails. For demo purposes I just downpatched the airflow provider so no more conflicts. I ran the dag and it seems to work, though I still get warnings in my logs that I have incompatible versions of elasticsearch. For the local demo airflow this isn&amp;#39;t an issue but for our production environment we are using this provider and cannot remove it or downpatch that package.  &lt;/p&gt;\n\n&lt;p&gt;With that experience, I feel like I must be missing some resources? I&amp;#39;ve heard around, even on this subreddit, that many are using Amundsen and very happy with it. I have to ask if the documentation here &lt;a href=\"https://www.amundsen.io/amundsen/installation/\"&gt;https://www.amundsen.io/amundsen/installation/&lt;/a&gt; is what people actually recommend to evaluate? Are there better resources for installing/testing the extractors? Are there any good tutorials on writing a custom extractor or using the extractors that exist that aren&amp;#39;t just code examples to copy? I would like to know more about how the specific apis in databuilder work because I generally want a good idea of how I would interact or modify one of the examples to fit our needs. I read through the Dashboard Ingestion guidance (&lt;a href=\"https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/\"&gt;https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/&lt;/a&gt;) and this basically just said use the databuilder and check the examples for how to do that. Considering the examples aren&amp;#39;t working I am unsure of how to proceed.&lt;/p&gt;\n\n&lt;p&gt;Is there a more up to date guide on the databuilder or a better way to install it? Is it contained in a pip package or do I actually have to build from source with setup tools? Is there a good and recent tutorial outside of the docs that shows the steps today to get up and running? Are there good deployment guides from this year?&lt;/p&gt;\n\n&lt;p&gt;As much as I want to give it a fair shot, I don&amp;#39;t know how to really evaluate at this point because I can&amp;#39;t really give my team a good estimate on how much work it would be to set this up. I am also not super confident looking at the state of databuilder that it has been well maintained. From what I can see there are quite a few package versions it requires that are deprecated, or nearing end of life.  I must be missing something right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18154ki", "is_robot_indexable": true, "report_reasons": null, "author": "miscbits", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18154ki/amundsen_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18154ki/amundsen_resources/", "subreddit_subscribers": 141181, "created_utc": 1700644635.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}