{"kind": "Listing", "data": {"after": "t3_180mbx3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I'm currently learning about orchestrating data pipeline with Airflow. With all the free resources I have seen, it seems that a majority of the demos are showing developers to save the python code in the `dags/` folder and then call PythonOperator to run the task. Note that the python code would not only include the Dags and tasks code, but they also have the actual ETL codes in there. To be honest, I am not usually comfortable in jamming codes for different purpose in one .py file because I prefer separating codes for ETL from codes that defines a DAG and tasks in other places.\n\nSo, some questions I have, maybe a stupid question, are:\n\nIn real practice, are we really saving the codes in the `dags` folder?  \n\nSay that I have a function that does simple data cleansing with pandas. in real practice, should I just define a function in a .py and then also define the dags and tasks in the same python file? Would the `dags` folder be flooded? Is there any ways to just save the dags and tasks codes in the dags folder, but import the actual ETL codes from other .py files saved in another folder?\n\nI feel the question may be leaning towards a 'no', but what I'm looking to learn is the \\`why\\` and \\`how is that being done in real practice\\`.\n\n&amp;#x200B;\n\nThanks!!!", "author_fullname": "t2_8vauvmym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is Airflow really used in real practice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180lef1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 80, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 80, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700585257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;m currently learning about orchestrating data pipeline with Airflow. With all the free resources I have seen, it seems that a majority of the demos are showing developers to save the python code in the &lt;code&gt;dags/&lt;/code&gt; folder and then call PythonOperator to run the task. Note that the python code would not only include the Dags and tasks code, but they also have the actual ETL codes in there. To be honest, I am not usually comfortable in jamming codes for different purpose in one .py file because I prefer separating codes for ETL from codes that defines a DAG and tasks in other places.&lt;/p&gt;\n\n&lt;p&gt;So, some questions I have, maybe a stupid question, are:&lt;/p&gt;\n\n&lt;p&gt;In real practice, are we really saving the codes in the &lt;code&gt;dags&lt;/code&gt; folder?  &lt;/p&gt;\n\n&lt;p&gt;Say that I have a function that does simple data cleansing with pandas. in real practice, should I just define a function in a .py and then also define the dags and tasks in the same python file? Would the &lt;code&gt;dags&lt;/code&gt; folder be flooded? Is there any ways to just save the dags and tasks codes in the dags folder, but import the actual ETL codes from other .py files saved in another folder?&lt;/p&gt;\n\n&lt;p&gt;I feel the question may be leaning towards a &amp;#39;no&amp;#39;, but what I&amp;#39;m looking to learn is the `why` and `how is that being done in real practice`.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180lef1", "is_robot_indexable": true, "report_reasons": null, "author": "sevkw", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180lef1/how_is_airflow_really_used_in_real_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180lef1/how_is_airflow_really_used_in_real_practice/", "subreddit_subscribers": 141077, "created_utc": 1700585257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thinking of switching to data engineering. I currently make $91k in clinical informatics with amazing WLB. \n\nWondering, what is the WLB like as a data engineer? How does it compare to SWE WLB?", "author_fullname": "t2_7x2alm42", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the work life balance like as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180ysrr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700620924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thinking of switching to data engineering. I currently make $91k in clinical informatics with amazing WLB. &lt;/p&gt;\n\n&lt;p&gt;Wondering, what is the WLB like as a data engineer? How does it compare to SWE WLB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180ysrr", "is_robot_indexable": true, "report_reasons": null, "author": "InstaMastery", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180ysrr/what_is_the_work_life_balance_like_as_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180ysrr/what_is_the_work_life_balance_like_as_a_data/", "subreddit_subscribers": 141077, "created_utc": 1700620924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I find that most of the material out there is beginner level for data modeling for both relational data modeling and warehousing. Is there a good course that you can recommend that\u2019s at least intermediate or advanced. Thank you! \ud83d\ude4f\ud83c\udffb", "author_fullname": "t2_g7w5zhnv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best data modelling course?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180wmae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700614288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find that most of the material out there is beginner level for data modeling for both relational data modeling and warehousing. Is there a good course that you can recommend that\u2019s at least intermediate or advanced. Thank you! \ud83d\ude4f\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180wmae", "is_robot_indexable": true, "report_reasons": null, "author": "long_spy200", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180wmae/what_is_the_best_data_modelling_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180wmae/what_is_the_best_data_modelling_course/", "subreddit_subscribers": 141077, "created_utc": 1700614288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently when we merge into our protected `development` or `production` branches, our CI/CD pipeline gets triggered and it builds one or more docker containers and pushes them to a repo where our run tasks pick them up. At each task invocation dbt starts by parsing the entire project that is contained within that particular image. \n\nA few weeks ago I passed some non-existent tags to our task runner, which is all in all 8 different dbt operations, and it took about 7 minutes to complete. All of that time is spent spinning up the task, parsing, finding no matches for these tags, and shutting down. \n\nI would like to incorporate parsing into our CI/CD pipelines so that parsing happens only when it's required, which is when new code is merged in, and otherwise successive operations on unchanged codebases just reference the same saved manifest to save some time during runs. Even if is just a couple of minutes.  \n\nBut I haven't come up with a clean way to do it. I could include `dbt parse` as a `RUN` command in each project's Dockerfile. But then I need to get the database credentials to the docker build, as passing a large number of arguments seems clunky. Additionally, I then need to include the database credentials as CI/CD variables in my pipeline which will make key rotation difficult. \n\nHas anyone found a clean way to do this?", "author_fullname": "t2_6nf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone had success incorporating dbt's parse step into your CI/CD pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180ml58", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700588268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently when we merge into our protected &lt;code&gt;development&lt;/code&gt; or &lt;code&gt;production&lt;/code&gt; branches, our CI/CD pipeline gets triggered and it builds one or more docker containers and pushes them to a repo where our run tasks pick them up. At each task invocation dbt starts by parsing the entire project that is contained within that particular image. &lt;/p&gt;\n\n&lt;p&gt;A few weeks ago I passed some non-existent tags to our task runner, which is all in all 8 different dbt operations, and it took about 7 minutes to complete. All of that time is spent spinning up the task, parsing, finding no matches for these tags, and shutting down. &lt;/p&gt;\n\n&lt;p&gt;I would like to incorporate parsing into our CI/CD pipelines so that parsing happens only when it&amp;#39;s required, which is when new code is merged in, and otherwise successive operations on unchanged codebases just reference the same saved manifest to save some time during runs. Even if is just a couple of minutes.  &lt;/p&gt;\n\n&lt;p&gt;But I haven&amp;#39;t come up with a clean way to do it. I could include &lt;code&gt;dbt parse&lt;/code&gt; as a &lt;code&gt;RUN&lt;/code&gt; command in each project&amp;#39;s Dockerfile. But then I need to get the database credentials to the docker build, as passing a large number of arguments seems clunky. Additionally, I then need to include the database credentials as CI/CD variables in my pipeline which will make key rotation difficult. &lt;/p&gt;\n\n&lt;p&gt;Has anyone found a clean way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180ml58", "is_robot_indexable": true, "report_reasons": null, "author": "radil", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180ml58/has_anyone_had_success_incorporating_dbts_parse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180ml58/has_anyone_had_success_incorporating_dbts_parse/", "subreddit_subscribers": 141077, "created_utc": 1700588268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 5 years of json application log files stored in 60 second folder partitions with one json log file per event in S3. So a single 60 second partition could have 100\u2019s of json files. This is obviously unusable for analytics. \n\nI need to consolidate the log data and repartition into nested year/month/day partitions with one json log file per day\n\nAny thoughts on the best way to do this within AWS or outside of AWS, that\u2019s not I/O bound? ", "author_fullname": "t2_r168sf7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to repartition json data in s3 bucket", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180q34j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700597454.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700597255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 5 years of json application log files stored in 60 second folder partitions with one json log file per event in S3. So a single 60 second partition could have 100\u2019s of json files. This is obviously unusable for analytics. &lt;/p&gt;\n\n&lt;p&gt;I need to consolidate the log data and repartition into nested year/month/day partitions with one json log file per day&lt;/p&gt;\n\n&lt;p&gt;Any thoughts on the best way to do this within AWS or outside of AWS, that\u2019s not I/O bound? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180q34j", "is_robot_indexable": true, "report_reasons": null, "author": "johncena9519", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180q34j/best_way_to_repartition_json_data_in_s3_bucket/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180q34j/best_way_to_repartition_json_data_in_s3_bucket/", "subreddit_subscribers": 141077, "created_utc": 1700597255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Coming from a background of piping in data into snowflake from apis, I manage ci/cd via dbt / circle CI / terraform / GitHub. Ie  a dev/qa/prod env / schemas that let\u2019s say a new column is added to a dbt model and in turn a table now has a new column and then it gets merged to prod and the prod schema has new Column. \n\n\nWhat\u2019s the best way to do the above and can I leverage any of my knowledge from implementing a process for a dwh rather than a prod db. Also any chance I can kill two birds with one stone ?", "author_fullname": "t2_xo4dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python / AWS / Postgres data pipeline that ingests data from ali and writes to Postgres that is backed for a saas app. How do best manage ci/cd", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1810qpk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700627204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Coming from a background of piping in data into snowflake from apis, I manage ci/cd via dbt / circle CI / terraform / GitHub. Ie  a dev/qa/prod env / schemas that let\u2019s say a new column is added to a dbt model and in turn a table now has a new column and then it gets merged to prod and the prod schema has new Column. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s the best way to do the above and can I leverage any of my knowledge from implementing a process for a dwh rather than a prod db. Also any chance I can kill two birds with one stone ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1810qpk", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1810qpk/python_aws_postgres_data_pipeline_that_ingests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1810qpk/python_aws_postgres_data_pipeline_that_ingests/", "subreddit_subscribers": 141077, "created_utc": 1700627204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bxjjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Change Data Capture Breaks Encapsulation\". Does it, though?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "name": "t3_180m4ib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lb_31UM3cfi5BJLJ7V8rI9LuJZDwEVlI8UMRcWYEUxM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700587093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "decodable.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.decodable.co/blog/change-data-capture-breaks-encapsulation-does-it-though", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?auto=webp&amp;s=76d0fb3735486ab22191f7d87d4f31f534c0b3ba", "width": 1898, "height": 1233}, "resolutions": [{"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c620453ffb0a56ab41a2188e1956d8a52f4c078", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ca2423f02ac40f19b1b79f2731eb418ff11ec83", "width": 216, "height": 140}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09eaca410b99c679520de72384272bfa3a935393", "width": 320, "height": 207}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=298d58ab70c8baa397e0e3f20b55b4c8d35ae7c5", "width": 640, "height": 415}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7542d65a374213316e86de0f856f5f789cec9f69", "width": 960, "height": 623}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=22b0a0db31c57353d318821141190a39ae8c5b96", "width": 1080, "height": 701}], "variants": {}, "id": "fZR5RCGRbhiP6OBRB8mmdV7xnByAN1Vs7fs2gFHquvU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "180m4ib", "is_robot_indexable": true, "report_reasons": null, "author": "gunnarmorling", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180m4ib/change_data_capture_breaks_encapsulation_does_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.decodable.co/blog/change-data-capture-breaks-encapsulation-does-it-though", "subreddit_subscribers": 141077, "created_utc": 1700587093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \nI'll be taking my test next week and I' took Alan Rodrigues's course and saw a lot of videos of YouTube. \nIf anyone has fresh tips for me it would be highly appreciated", "author_fullname": "t2_38po62bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP 203 Nov 23", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180umnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700608751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, \nI&amp;#39;ll be taking my test next week and I&amp;#39; took Alan Rodrigues&amp;#39;s course and saw a lot of videos of YouTube. \nIf anyone has fresh tips for me it would be highly appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "180umnf", "is_robot_indexable": true, "report_reasons": null, "author": "Esteban_Rdz", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180umnf/dp_203_nov_23/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180umnf/dp_203_nov_23/", "subreddit_subscribers": 141077, "created_utc": 1700608751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would great expectations/dbt and a viz tool on top to display the metrics ? How do you handle logging and monitoring of chemistry drift of raw files when landing ? Is monitoring / messaging via slack and cloud watch enough ?\n\nJust curious as to what people think works well, ideally open source. That can cover a product and analytic pipelines.", "author_fullname": "t2_13551s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your favorite method tools and metrics to implement data quality checks / monitors for data pipelines that feed product database (Postgres) and data science teams (snowflake / data bricks) (s3 for both)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1810lqk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700626709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would great expectations/dbt and a viz tool on top to display the metrics ? How do you handle logging and monitoring of chemistry drift of raw files when landing ? Is monitoring / messaging via slack and cloud watch enough ?&lt;/p&gt;\n\n&lt;p&gt;Just curious as to what people think works well, ideally open source. That can cover a product and analytic pipelines.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1810lqk", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance2", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1810lqk/what_is_your_favorite_method_tools_and_metrics_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1810lqk/what_is_your_favorite_method_tools_and_metrics_to/", "subreddit_subscribers": 141077, "created_utc": 1700626709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Management seems interested in investing in the West Monroe's data bricks accelerator and I'm curious if anyone has any experience with it or Databricks accelerator's in general.\n\nWe have a small data engineering team and the belief is that using that accelerator will increase our speed to market.  I'm curious if anyone has experience with these types of solutions and how they fare relative to other solutions on the market.\n\n[https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator](https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator)", "author_fullname": "t2_3pgbq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do anyone have experience with West Monroe's Databricks Lakehouse Accelerator Mizu?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180yzxe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700621539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Management seems interested in investing in the West Monroe&amp;#39;s data bricks accelerator and I&amp;#39;m curious if anyone has any experience with it or Databricks accelerator&amp;#39;s in general.&lt;/p&gt;\n\n&lt;p&gt;We have a small data engineering team and the belief is that using that accelerator will increase our speed to market.  I&amp;#39;m curious if anyone has experience with these types of solutions and how they fare relative to other solutions on the market.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator\"&gt;https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?auto=webp&amp;s=f52f3657bb5f3877cbe4a55bd18794778eed9704", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8850f78c52e7797d26c19ad6fda65deb722f263", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=974ce6593363bfbd6c815744281c23d866f10cf2", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=21d02e2d873de235403725ad86e741de9b7fc69e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee266c3f3bf1c2d433acba20b20308e49cffd5ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c151d3b5becb70af0a66fe80b20f59902ce1bc2", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e2e95bfc0db065b7f319e2973b702c60bd9b13f", "width": 1080, "height": 567}], "variants": {}, "id": "2pz3rWcq_ScNWhN7QkxHJWo9cO-lvk6W0rqq9t78ngs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180yzxe", "is_robot_indexable": true, "report_reasons": null, "author": "tehsandvich", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180yzxe/do_anyone_have_experience_with_west_monroes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180yzxe/do_anyone_have_experience_with_west_monroes/", "subreddit_subscribers": 141077, "created_utc": 1700621539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Long story short I\u2019m an eLearning Developer wanting to turn into a Data Engineer. I have 1+ years of SQL experience. I\u2019m finishing up a Data Engineering course on Datacamp.\nI\u2019m looking for interview prep questions so that I can crack this asap. Thanks in advance!", "author_fullname": "t2_yoget", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep for DE roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180gajt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700571170.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Long story short I\u2019m an eLearning Developer wanting to turn into a Data Engineer. I have 1+ years of SQL experience. I\u2019m finishing up a Data Engineering course on Datacamp.\nI\u2019m looking for interview prep questions so that I can crack this asap. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "180gajt", "is_robot_indexable": true, "report_reasons": null, "author": "Drrazor", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180gajt/interview_prep_for_de_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180gajt/interview_prep_for_de_roles/", "subreddit_subscribers": 141077, "created_utc": 1700571170.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I work in data for an organization with about 100 people in a two man data team. I and my college have the same job title of business intelligence specialist. We are both industrial engineers who specialized in data and data analysis and have the same responsibilities. Those are :\n\n1. Deploy and maintain data pipelines from various systems to the data warehouse\n2. Design, build and maintain data warehouse\n3. Design, build and maintain Power BI datasets and reports\n4. Perform statistical analysis on data and calculate KPI's\n5. Find ideas for creating value with mathematical/machine learning models, build and execute them\n6. Various other tasks related to data and tech such as automation of work flows, building power apps for data entry and other purposes and more\n\nFrom any resources about job titles and data teams I find it seems we fill every position in the data team structure as all purpose data specialists. The broad meaning of business intelligence specialist is one who analyses data and provides solutions for business operations, which, in our case, catches our role of statistical analysis and Power BI building in my opinion. The rest of our responsibilities, who account for at least 60% of our work, are not defined in the job title, and the pay is not reflective of our work either but that's another story... So my question is, what job title would be descriptive of all the different activities a data team performs?\n\nMany thanks in advance.", "author_fullname": "t2_6j5e93w9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Comprehensive job title", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180enru", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700565354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I work in data for an organization with about 100 people in a two man data team. I and my college have the same job title of business intelligence specialist. We are both industrial engineers who specialized in data and data analysis and have the same responsibilities. Those are :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Deploy and maintain data pipelines from various systems to the data warehouse&lt;/li&gt;\n&lt;li&gt;Design, build and maintain data warehouse&lt;/li&gt;\n&lt;li&gt;Design, build and maintain Power BI datasets and reports&lt;/li&gt;\n&lt;li&gt;Perform statistical analysis on data and calculate KPI&amp;#39;s&lt;/li&gt;\n&lt;li&gt;Find ideas for creating value with mathematical/machine learning models, build and execute them&lt;/li&gt;\n&lt;li&gt;Various other tasks related to data and tech such as automation of work flows, building power apps for data entry and other purposes and more&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;From any resources about job titles and data teams I find it seems we fill every position in the data team structure as all purpose data specialists. The broad meaning of business intelligence specialist is one who analyses data and provides solutions for business operations, which, in our case, catches our role of statistical analysis and Power BI building in my opinion. The rest of our responsibilities, who account for at least 60% of our work, are not defined in the job title, and the pay is not reflective of our work either but that&amp;#39;s another story... So my question is, what job title would be descriptive of all the different activities a data team performs?&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "180enru", "is_robot_indexable": true, "report_reasons": null, "author": "arachnarus96", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180enru/comprehensive_job_title/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180enru/comprehensive_job_title/", "subreddit_subscribers": 141077, "created_utc": 1700565354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry in advance for long post. I tried to just put a lot of detail but before you read just know I am overall just asking for some resources on setting up Amundsen in 2023, and just a sanity check on my initial research. I've been working on this for 2 days.  \n\n\nHello. I am currently in the process of evaluating Amundsen but I am somewhat lost. I've tried following the Quickstart and the examples. I tried opting in for Atlas for the push capabilities but that didn't seem to work so I just decided to attempt to run everything as close to the docs as possible. The deploy with docker compose worked with default settings, but since that no step going forward worked.\n\nI wanted to test the databuilder which seems like the defacto api for putting information into the metadata engine, so I spun up a postgres database with docker and loaded a table into the public schema. I then ran the example sample\\_postgres\\_extractor which did not work because the version of SqlAlchemy after installing the requirements file did not use the same execution method in the databuilder. As much as I wanted to follow the steps as closely as possible I attempted to downpatch SqlAlchemy. This worked after downpatching a few other requirements that were incompatible with the older version of SqlAlchemy but I simply got an elasticsearch.exceptions.ConnectionError. I tried to test running the example dags which also didn't work. \"apache-airflow-providers-elasticsearch 5.1.0 requires elasticsearch&lt;9,&gt;8, but you have elasticsearch 7.17.9 which is incompatible.\" It seems that the airflow elasticsearch provider uses a higher version of the elasticsearch package that amundsen does so the installation fails. For demo purposes I just downpatched the airflow provider so no more conflicts. I ran the dag and it seems to work, though I still get warnings in my logs that I have incompatible versions of elasticsearch. For the local demo airflow this isn't an issue but for our production environment we are using this provider and cannot remove it or downpatch that package.  \n\n\nWith that experience, I feel like I must be missing some resources? I've heard around, even on this subreddit, that many are using Amundsen and very happy with it. I have to ask if the documentation here [https://www.amundsen.io/amundsen/installation/](https://www.amundsen.io/amundsen/installation/) is what people actually recommend to evaluate? Are there better resources for installing/testing the extractors? Are there any good tutorials on writing a custom extractor or using the extractors that exist that aren't just code examples to copy? I would like to know more about how the specific apis in databuilder work because I generally want a good idea of how I would interact or modify one of the examples to fit our needs. I read through the Dashboard Ingestion guidance ([https://www.amundsen.io/amundsen/databuilder/docs/dashboard\\_ingestion\\_guide/](https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/)) and this basically just said use the databuilder and check the examples for how to do that. Considering the examples aren't working I am unsure of how to proceed.\n\nIs there a more up to date guide on the databuilder or a better way to install it? Is it contained in a pip package or do I actually have to build from source with setup tools? Is there a good and recent tutorial outside of the docs that shows the steps today to get up and running? Are there good deployment guides from this year?\n\nAs much as I want to give it a fair shot, I don't know how to really evaluate at this point because I can't really give my team a good estimate on how much work it would be to set this up. I am also not super confident looking at the state of databuilder that it has been well maintained. From what I can see there are quite a few package versions it requires that are deprecated, or nearing end of life.  I must be missing something right?", "author_fullname": "t2_ptgbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amundsen resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_18154ki", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700644635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry in advance for long post. I tried to just put a lot of detail but before you read just know I am overall just asking for some resources on setting up Amundsen in 2023, and just a sanity check on my initial research. I&amp;#39;ve been working on this for 2 days.  &lt;/p&gt;\n\n&lt;p&gt;Hello. I am currently in the process of evaluating Amundsen but I am somewhat lost. I&amp;#39;ve tried following the Quickstart and the examples. I tried opting in for Atlas for the push capabilities but that didn&amp;#39;t seem to work so I just decided to attempt to run everything as close to the docs as possible. The deploy with docker compose worked with default settings, but since that no step going forward worked.&lt;/p&gt;\n\n&lt;p&gt;I wanted to test the databuilder which seems like the defacto api for putting information into the metadata engine, so I spun up a postgres database with docker and loaded a table into the public schema. I then ran the example sample_postgres_extractor which did not work because the version of SqlAlchemy after installing the requirements file did not use the same execution method in the databuilder. As much as I wanted to follow the steps as closely as possible I attempted to downpatch SqlAlchemy. This worked after downpatching a few other requirements that were incompatible with the older version of SqlAlchemy but I simply got an elasticsearch.exceptions.ConnectionError. I tried to test running the example dags which also didn&amp;#39;t work. &amp;quot;apache-airflow-providers-elasticsearch 5.1.0 requires elasticsearch&amp;lt;9,&amp;gt;8, but you have elasticsearch 7.17.9 which is incompatible.&amp;quot; It seems that the airflow elasticsearch provider uses a higher version of the elasticsearch package that amundsen does so the installation fails. For demo purposes I just downpatched the airflow provider so no more conflicts. I ran the dag and it seems to work, though I still get warnings in my logs that I have incompatible versions of elasticsearch. For the local demo airflow this isn&amp;#39;t an issue but for our production environment we are using this provider and cannot remove it or downpatch that package.  &lt;/p&gt;\n\n&lt;p&gt;With that experience, I feel like I must be missing some resources? I&amp;#39;ve heard around, even on this subreddit, that many are using Amundsen and very happy with it. I have to ask if the documentation here &lt;a href=\"https://www.amundsen.io/amundsen/installation/\"&gt;https://www.amundsen.io/amundsen/installation/&lt;/a&gt; is what people actually recommend to evaluate? Are there better resources for installing/testing the extractors? Are there any good tutorials on writing a custom extractor or using the extractors that exist that aren&amp;#39;t just code examples to copy? I would like to know more about how the specific apis in databuilder work because I generally want a good idea of how I would interact or modify one of the examples to fit our needs. I read through the Dashboard Ingestion guidance (&lt;a href=\"https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/\"&gt;https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/&lt;/a&gt;) and this basically just said use the databuilder and check the examples for how to do that. Considering the examples aren&amp;#39;t working I am unsure of how to proceed.&lt;/p&gt;\n\n&lt;p&gt;Is there a more up to date guide on the databuilder or a better way to install it? Is it contained in a pip package or do I actually have to build from source with setup tools? Is there a good and recent tutorial outside of the docs that shows the steps today to get up and running? Are there good deployment guides from this year?&lt;/p&gt;\n\n&lt;p&gt;As much as I want to give it a fair shot, I don&amp;#39;t know how to really evaluate at this point because I can&amp;#39;t really give my team a good estimate on how much work it would be to set this up. I am also not super confident looking at the state of databuilder that it has been well maintained. From what I can see there are quite a few package versions it requires that are deprecated, or nearing end of life.  I must be missing something right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18154ki", "is_robot_indexable": true, "report_reasons": null, "author": "miscbits", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18154ki/amundsen_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18154ki/amundsen_resources/", "subreddit_subscribers": 141077, "created_utc": 1700644635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nMy company has on-premises databases.\n\nI get data from those tables and I have made SSRS reports which uses that data.\n\nNow they want to move that database to cloud (Oracle Cloud).\n\nI want the SSRS reports to get the data from that database on the cloud database.\n\nThe machine I use as a server for SSRS Server is also on-premise.\n\nHow I will connect SSRS reports to cloud database, is it possible to do all this.\n\nI have to present a report about all this in 2 hours.", "author_fullname": "t2_3k9gevl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migration of SSRS &amp; Database from On-Prem to Oracle Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1814cnq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700641250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has on-premises databases.&lt;/p&gt;\n\n&lt;p&gt;I get data from those tables and I have made SSRS reports which uses that data.&lt;/p&gt;\n\n&lt;p&gt;Now they want to move that database to cloud (Oracle Cloud).&lt;/p&gt;\n\n&lt;p&gt;I want the SSRS reports to get the data from that database on the cloud database.&lt;/p&gt;\n\n&lt;p&gt;The machine I use as a server for SSRS Server is also on-premise.&lt;/p&gt;\n\n&lt;p&gt;How I will connect SSRS reports to cloud database, is it possible to do all this.&lt;/p&gt;\n\n&lt;p&gt;I have to present a report about all this in 2 hours.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1814cnq", "is_robot_indexable": true, "report_reasons": null, "author": "Pillstyr", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1814cnq/migration_of_ssrs_database_from_onprem_to_oracle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1814cnq/migration_of_ssrs_database_from_onprem_to_oracle/", "subreddit_subscribers": 141077, "created_utc": 1700641250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone working with databricks, and have implemented CI/CD? \nI'm struggling to come up with an good approach that covers development of Notebooks, DLT workflows and jobs. Things seem to separated atm. Any tips or advice? \n\nCheers", "author_fullname": "t2_7p1fczdj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD in Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180u635", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700607538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone working with databricks, and have implemented CI/CD? \nI&amp;#39;m struggling to come up with an good approach that covers development of Notebooks, DLT workflows and jobs. Things seem to separated atm. Any tips or advice? &lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180u635", "is_robot_indexable": true, "report_reasons": null, "author": "Far-Inspection9930", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180u635/cicd_in_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180u635/cicd_in_databricks/", "subreddit_subscribers": 141077, "created_utc": 1700607538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe have a Databricks env where we have been following the documented Databricks model, where we extract data from a relational data source into our bronze table.   We then do additional processing to create the silver table for the appropriate use case.  We extract the data from a read replica to ensure that we don't impact our transactional work.  This is a beginner question, but what are the main reasons to use the bronze table when doing processing for our silver table versus just pulling directly from the read replica?  Thanks! ", "author_fullname": "t2_b3ncdhop6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Silver from bronze vs Silver pulling directly from read replicas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180rzhz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700602027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We have a Databricks env where we have been following the documented Databricks model, where we extract data from a relational data source into our bronze table.   We then do additional processing to create the silver table for the appropriate use case.  We extract the data from a read replica to ensure that we don&amp;#39;t impact our transactional work.  This is a beginner question, but what are the main reasons to use the bronze table when doing processing for our silver table versus just pulling directly from the read replica?  Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180rzhz", "is_robot_indexable": true, "report_reasons": null, "author": "Annual-Scallion-4888", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180rzhz/databricks_silver_from_bronze_vs_silver_pulling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180rzhz/databricks_silver_from_bronze_vs_silver_pulling/", "subreddit_subscribers": 141077, "created_utc": 1700602027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm interviewing for a role with that title next week, the company in question is only asking for 1+ years of experience working with SQL and Python and my work will revolve around working with SIS in schools. \n\nIn your role, do you find that you have room to grow into a proper Data Engineer?  \n\nI've been cramming data engineering concepts, so in the event of a coding exercise, what depth of complexity should I expect, given that they're only asking for a year of experience? I need to brush up on my SQL and Python.\n", "author_fullname": "t2_lp2ob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have the job title \"Data Integration Engineer\" that can speak more on their duties?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180nl7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700602032.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700590906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interviewing for a role with that title next week, the company in question is only asking for 1+ years of experience working with SQL and Python and my work will revolve around working with SIS in schools. &lt;/p&gt;\n\n&lt;p&gt;In your role, do you find that you have room to grow into a proper Data Engineer?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been cramming data engineering concepts, so in the event of a coding exercise, what depth of complexity should I expect, given that they&amp;#39;re only asking for a year of experience? I need to brush up on my SQL and Python.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "180nl7b", "is_robot_indexable": true, "report_reasons": null, "author": "mrbrucel33", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180nl7b/does_anyone_have_the_job_title_data_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180nl7b/does_anyone_have_the_job_title_data_integration/", "subreddit_subscribers": 141077, "created_utc": 1700590906.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello folks, I had some thoughts about what are the quality indicators of a data team. In software engineering, it's all about DORA. It's not purely software engineering, but more about devops. And there was a large study to correlate success of companies to those indicators (not sure it would apply to data :D). \n\nI found this thread [https://www.reddit.com/r/dataengineering/comments/17e377n/tech\\_kpis/](https://www.reddit.com/r/dataengineering/comments/17e377n/tech_kpis/) but I did not like the \"Number of user per report\", because the data team cannot really directly impact this.\n\nMy naive thoughts of a parallel of DORA are these, let us consider a report:\n\n* Deploy frequency =&gt; Data freshness\n* Lead time for change =&gt; Cutoff time\n* Change Failure Rate =&gt; Stability of your metric\n* Mean time to recovery =&gt; Meantime between data issue introduction and resolution \n\nData freshness is straightforward, I just need a updated\\_at column, or a refreshed\\_at\n\nLead time for change =&gt; the \\*cutoff time\\*, like the time between the end of the month, and the day the report is up to date (We wait \"3 day\" to have the data), not sure how to get that from a table.\n\nChange Failure Rate =&gt; Stability of your metric: when the report says \"january 100\u20ac\", but then after some weeks  \"january 99\" (because some fix or data update). In this case the data has some kind of volatility of 1%.\n\nMeantime to recovery =&gt; Say you have a drop in a column but it takes a week to see it and solve it, recovery is 1w.\n\n&amp;#x200B;\n\nI think that to compute those indicators I need a regular snapshot of our report, so we can detect how long before the new data is append, we could also see the a problem in the report (like a big drop in a metric some day, then a big pump, the time in between would be the recovery time) and the stability.  \n\n\nDo any of you do something like this ? Snapshoting reports ? Or maybe computing those indicators somehow ?", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Performance indicators of a data team ? Can we compute some DORA metrics ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180l3o9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700584480.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks, I had some thoughts about what are the quality indicators of a data team. In software engineering, it&amp;#39;s all about DORA. It&amp;#39;s not purely software engineering, but more about devops. And there was a large study to correlate success of companies to those indicators (not sure it would apply to data :D). &lt;/p&gt;\n\n&lt;p&gt;I found this thread &lt;a href=\"https://www.reddit.com/r/dataengineering/comments/17e377n/tech_kpis/\"&gt;https://www.reddit.com/r/dataengineering/comments/17e377n/tech_kpis/&lt;/a&gt; but I did not like the &amp;quot;Number of user per report&amp;quot;, because the data team cannot really directly impact this.&lt;/p&gt;\n\n&lt;p&gt;My naive thoughts of a parallel of DORA are these, let us consider a report:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Deploy frequency =&amp;gt; Data freshness&lt;/li&gt;\n&lt;li&gt;Lead time for change =&amp;gt; Cutoff time&lt;/li&gt;\n&lt;li&gt;Change Failure Rate =&amp;gt; Stability of your metric&lt;/li&gt;\n&lt;li&gt;Mean time to recovery =&amp;gt; Meantime between data issue introduction and resolution &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Data freshness is straightforward, I just need a updated_at column, or a refreshed_at&lt;/p&gt;\n\n&lt;p&gt;Lead time for change =&amp;gt; the *cutoff time*, like the time between the end of the month, and the day the report is up to date (We wait &amp;quot;3 day&amp;quot; to have the data), not sure how to get that from a table.&lt;/p&gt;\n\n&lt;p&gt;Change Failure Rate =&amp;gt; Stability of your metric: when the report says &amp;quot;january 100\u20ac&amp;quot;, but then after some weeks  &amp;quot;january 99&amp;quot; (because some fix or data update). In this case the data has some kind of volatility of 1%.&lt;/p&gt;\n\n&lt;p&gt;Meantime to recovery =&amp;gt; Say you have a drop in a column but it takes a week to see it and solve it, recovery is 1w.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I think that to compute those indicators I need a regular snapshot of our report, so we can detect how long before the new data is append, we could also see the a problem in the report (like a big drop in a metric some day, then a big pump, the time in between would be the recovery time) and the stability.  &lt;/p&gt;\n\n&lt;p&gt;Do any of you do something like this ? Snapshoting reports ? Or maybe computing those indicators somehow ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180l3o9", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180l3o9/performance_indicators_of_a_data_team_can_we/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180l3o9/performance_indicators_of_a_data_team_can_we/", "subreddit_subscribers": 141077, "created_utc": 1700584480.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Edit: the code is already in bq syntax. \n\nHi all, just moved to bigquery, I am wondering if there is a tool like dbt-core or dataform that requires little to no refactoring? None of the existing code to build datamarts is in CTEs and there are multiple DDL statements where we store intermediate results in temp tables.\n\nI could write a novel explaining how terrible the current process to execute and promote code is but suffice to say it's not good at all and I am desperate to find something better.", "author_fullname": "t2_39nrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Something like dbt/dataform that requires minimum refactoring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180kvqa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700588601.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700583928.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Edit: the code is already in bq syntax. &lt;/p&gt;\n\n&lt;p&gt;Hi all, just moved to bigquery, I am wondering if there is a tool like dbt-core or dataform that requires little to no refactoring? None of the existing code to build datamarts is in CTEs and there are multiple DDL statements where we store intermediate results in temp tables.&lt;/p&gt;\n\n&lt;p&gt;I could write a novel explaining how terrible the current process to execute and promote code is but suffice to say it&amp;#39;s not good at all and I am desperate to find something better.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180kvqa", "is_robot_indexable": true, "report_reasons": null, "author": "arborealguy", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180kvqa/something_like_dbtdataform_that_requires_minimum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180kvqa/something_like_dbtdataform_that_requires_minimum/", "subreddit_subscribers": 141077, "created_utc": 1700583928.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello data engineers!\n\nI am in my first DE project and I will be giving my fellow PBI Devs survey data. My question is: how to serve it properly, looking at big picture and\n\n\\&amp;#x200B;\n\n&amp;#x200B;\n\nThe raw data is in xlsx format, with two worksheets. One with numerical data, and one with descriptions what is hidden behind the numbers as well as Question's\n\n&amp;#x200B;\n\nThis is sample of the data\n\n| RespID | Age | Loc | Gen | PRD | BRD | Q1 | Q2 | Q3 | Q4 | Q5 |\n|--------|-----|-----|-----|-----|-----|----|----|----|----|----|\n| 1      | 1   | 1   | 1   | 4   | 3   | 4  | 1  | 6  | 8  | 5  |\n| 2      | 1   | 2   | 2   | 4   | 1   | 1  | 4  | 9  | 9  | 7  |\n| 3      | 4   | 3   | 2   | 1   | 1   | 3  | 4  | 6  | 9  | 1  |\n| 4      | 1   | 2   | 2   | 4   | 5   | 7  | 4  | 3  | 8  | 6  |\n| 5      | 2   | 4   | 1   | 4   | 5   | 1  | 4  | 3  | 2  | 6  |\n| 6      | 1   | 4   | 2   | 2   | 7   | 2  | 4  | 2  | 1  | 5  |\n| 7      | 2   | 2   | 2   | 3   | 5   | 7  | 2  | 10 | 2  | 6  |\n| 8      | 4   | 2   | 1   | 4   | 1   | 4  | 2  | 6  | 5  | 4  |\n| 9      | 3   | 1   | 2   | 2   | 6   | 4  | 3  | 9  | 4  | 6  |\n| 10     | 1   | 4   | 1   | 4   | 1   | 7  | 7  | 7  | 7  | 6  |\n\nand in the second worksheet i'd have something like\n\nRespID - RespondentID\n\nAge - Age of respondent\n\nLoc - Location of respondent\n\nPRD - Product\n\n..\n\nQ1 - In a scale of 1-7 how do you like XX\n\nthen values\n\nage: 1 - '19-24' .. 4 - '60+'\n\ngender: 1-'female',2-'male'\n\netc\n\nMy considerations:\n\nThis is for one country and all countries will be there. So how can I distinguish then countries? Lets say RespID = 1 from that survey (GB) will not equal to RespID = 1 of USA survey. Also the age buckets could be different, questions etc. Should I append everything and create a new ID for each respondent?\n\nIn the future, data from other department will need to be attached there. So how can I proceed right now, bring some artificial key and let them attach it to the files they want to merge with these surveys?\n\nShould I keep numerical answers and have some sort of dictionary in separate table or should I run python dict over it so all numbers become answers?\n\nThank you for any tips", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Properly delivering for BI", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180gwoz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700573097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello data engineers!&lt;/p&gt;\n\n&lt;p&gt;I am in my first DE project and I will be giving my fellow PBI Devs survey data. My question is: how to serve it properly, looking at big picture and&lt;/p&gt;\n\n&lt;p&gt;&amp;amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The raw data is in xlsx format, with two worksheets. One with numerical data, and one with descriptions what is hidden behind the numbers as well as Question&amp;#39;s&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This is sample of the data&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;RespID&lt;/th&gt;\n&lt;th&gt;Age&lt;/th&gt;\n&lt;th&gt;Loc&lt;/th&gt;\n&lt;th&gt;Gen&lt;/th&gt;\n&lt;th&gt;PRD&lt;/th&gt;\n&lt;th&gt;BRD&lt;/th&gt;\n&lt;th&gt;Q1&lt;/th&gt;\n&lt;th&gt;Q2&lt;/th&gt;\n&lt;th&gt;Q3&lt;/th&gt;\n&lt;th&gt;Q4&lt;/th&gt;\n&lt;th&gt;Q5&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;8&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;8&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;10&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;8&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;5&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;3&lt;/td&gt;\n&lt;td&gt;9&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;10&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;7&lt;/td&gt;\n&lt;td&gt;6&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;and in the second worksheet i&amp;#39;d have something like&lt;/p&gt;\n\n&lt;p&gt;RespID - RespondentID&lt;/p&gt;\n\n&lt;p&gt;Age - Age of respondent&lt;/p&gt;\n\n&lt;p&gt;Loc - Location of respondent&lt;/p&gt;\n\n&lt;p&gt;PRD - Product&lt;/p&gt;\n\n&lt;p&gt;..&lt;/p&gt;\n\n&lt;p&gt;Q1 - In a scale of 1-7 how do you like XX&lt;/p&gt;\n\n&lt;p&gt;then values&lt;/p&gt;\n\n&lt;p&gt;age: 1 - &amp;#39;19-24&amp;#39; .. 4 - &amp;#39;60+&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;gender: 1-&amp;#39;female&amp;#39;,2-&amp;#39;male&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;etc&lt;/p&gt;\n\n&lt;p&gt;My considerations:&lt;/p&gt;\n\n&lt;p&gt;This is for one country and all countries will be there. So how can I distinguish then countries? Lets say RespID = 1 from that survey (GB) will not equal to RespID = 1 of USA survey. Also the age buckets could be different, questions etc. Should I append everything and create a new ID for each respondent?&lt;/p&gt;\n\n&lt;p&gt;In the future, data from other department will need to be attached there. So how can I proceed right now, bring some artificial key and let them attach it to the files they want to merge with these surveys?&lt;/p&gt;\n\n&lt;p&gt;Should I keep numerical answers and have some sort of dictionary in separate table or should I run python dict over it so all numbers become answers?&lt;/p&gt;\n\n&lt;p&gt;Thank you for any tips&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180gwoz", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180gwoz/properly_delivering_for_bi/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180gwoz/properly_delivering_for_bi/", "subreddit_subscribers": 141077, "created_utc": 1700573097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\n\n[View Poll](https://www.reddit.com/poll/180fwa8)", "author_fullname": "t2_d9zudt5m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will LLMs replace the data transformation layer when building ETL pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180fwa8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700569896.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/180fwa8\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180fwa8", "is_robot_indexable": true, "report_reasons": null, "author": "Data_Berry", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1701174696530, "options": [{"text": "Yes (Full automation)", "id": "26096551"}, {"text": "Yes (Partial automation with some configuration required)", "id": "26096552"}, {"text": "No", "id": "26096553"}, {"text": "Don't know", "id": "26096554"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 49, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180fwa8/will_llms_replace_the_data_transformation_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/180fwa8/will_llms_replace_the_data_transformation_layer/", "subreddit_subscribers": 141077, "created_utc": 1700569896.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a couple of stored procedures that re-compute daily. These contain aggregate functions like max, distinct, window functions, etc. I know this is quite vague, but is there a way to structure your queries/models to enable you to run incremental refreshes?", "author_fullname": "t2_c2f6kxll", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental aggregations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180e2pn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700562913.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a couple of stored procedures that re-compute daily. These contain aggregate functions like max, distinct, window functions, etc. I know this is quite vague, but is there a way to structure your queries/models to enable you to run incremental refreshes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180e2pn", "is_robot_indexable": true, "report_reasons": null, "author": "unstable_label", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180e2pn/incremental_aggregations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180e2pn/incremental_aggregations/", "subreddit_subscribers": 141077, "created_utc": 1700562913.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Current process:\n\nProcure customer requirements verbally and understand. Change the configurations (in json) according to the template available for the hardware of an automobile. The json files have a specific format and we can change the working of the devices (lights, fridge, bathroom etc.) based on the json template (json schema) available. Basically this template has data types. You can consider this as a library.\n\nFuture process: \n\nProcure the customer requirements online on a Portal. Convert it into xml format. This xml file has information about the switches and what they control and how they control. For example : Kitchen light, lounge light and some ambient lighting scenarios. Now this file has to be automatically converted into the json file based on the json schema. This is in order to remove manual work which takes a lot of time. Need to automate, standardise the entire process.\n\n\nCan someone help me with this process? How do I start with extracting info from the XML file? How do I save this json schema as a data model? And based on this how do I create a json file? I'd like to also create a knowledge graph so that it's easy to query and look for something specific. \n\nAny help would be appreciated! :)", "author_fullname": "t2_gdjxkf2v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with Data Mapping", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180hzoc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700576225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Current process:&lt;/p&gt;\n\n&lt;p&gt;Procure customer requirements verbally and understand. Change the configurations (in json) according to the template available for the hardware of an automobile. The json files have a specific format and we can change the working of the devices (lights, fridge, bathroom etc.) based on the json template (json schema) available. Basically this template has data types. You can consider this as a library.&lt;/p&gt;\n\n&lt;p&gt;Future process: &lt;/p&gt;\n\n&lt;p&gt;Procure the customer requirements online on a Portal. Convert it into xml format. This xml file has information about the switches and what they control and how they control. For example : Kitchen light, lounge light and some ambient lighting scenarios. Now this file has to be automatically converted into the json file based on the json schema. This is in order to remove manual work which takes a lot of time. Need to automate, standardise the entire process.&lt;/p&gt;\n\n&lt;p&gt;Can someone help me with this process? How do I start with extracting info from the XML file? How do I save this json schema as a data model? And based on this how do I create a json file? I&amp;#39;d like to also create a knowledge graph so that it&amp;#39;s easy to query and look for something specific. &lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated! :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180hzoc", "is_robot_indexable": true, "report_reasons": null, "author": "Informal_Poem_4394", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180hzoc/need_help_with_data_mapping/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180hzoc/need_help_with_data_mapping/", "subreddit_subscribers": 141077, "created_utc": 1700576225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I want to extract the match scores of my favorite basketball teams with Python and load them to a created database. \n\nFor example, all matches played by the relevant teams starting from 2023. \n\nThere is an API Client called \"nba\\_api\". It might work for me but not sure. I will check further.\n\nAny suggestions or ideas would be great!", "author_fullname": "t2_sirw75zj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an NBA API for free that has teams match score?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180gm5f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700572160.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I want to extract the match scores of my favorite basketball teams with Python and load them to a created database. &lt;/p&gt;\n\n&lt;p&gt;For example, all matches played by the relevant teams starting from 2023. &lt;/p&gt;\n\n&lt;p&gt;There is an API Client called &amp;quot;nba_api&amp;quot;. It might work for me but not sure. I will check further.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or ideas would be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180gm5f", "is_robot_indexable": true, "report_reasons": null, "author": "duende37s", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180gm5f/is_there_an_nba_api_for_free_that_has_teams_match/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180gm5f/is_there_an_nba_api_for_free_that_has_teams_match/", "subreddit_subscribers": 141077, "created_utc": 1700572160.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have seen text to sql tools based on chatgpt out there that can help in sql development etc.\n\nJust wanted to see if you use, how are you using and how beneficial it has been for your teams.", "author_fullname": "t2_dhgy4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are you using chatgpt in your sql workflows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180mbx3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700587603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen text to sql tools based on chatgpt out there that can help in sql development etc.&lt;/p&gt;\n\n&lt;p&gt;Just wanted to see if you use, how are you using and how beneficial it has been for your teams.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180mbx3", "is_robot_indexable": true, "report_reasons": null, "author": "mjfnd", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180mbx3/are_you_using_chatgpt_in_your_sql_workflows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180mbx3/are_you_using_chatgpt_in_your_sql_workflows/", "subreddit_subscribers": 141077, "created_utc": 1700587603.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}