{"kind": "Listing", "data": {"after": "t3_180nl7b", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I'm currently learning about orchestrating data pipeline with Airflow. With all the free resources I have seen, it seems that a majority of the demos are showing developers to save the python code in the `dags/` folder and then call PythonOperator to run the task. Note that the python code would not only include the Dags and tasks code, but they also have the actual ETL codes in there. To be honest, I am not usually comfortable in jamming codes for different purpose in one .py file because I prefer separating codes for ETL from codes that defines a DAG and tasks in other places.\n\nSo, some questions I have, maybe a stupid question, are:\n\nIn real practice, are we really saving the codes in the `dags` folder?  \n\nSay that I have a function that does simple data cleansing with pandas. in real practice, should I just define a function in a .py and then also define the dags and tasks in the same python file? Would the `dags` folder be flooded? Is there any ways to just save the dags and tasks codes in the dags folder, but import the actual ETL codes from other .py files saved in another folder?\n\nI feel the question may be leaning towards a 'no', but what I'm looking to learn is the \\`why\\` and \\`how is that being done in real practice\\`.\n\n&amp;#x200B;\n\nThanks!!!", "author_fullname": "t2_8vauvmym", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How is Airflow really used in real practice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180lef1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 84, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 84, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700585257.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I&amp;#39;m currently learning about orchestrating data pipeline with Airflow. With all the free resources I have seen, it seems that a majority of the demos are showing developers to save the python code in the &lt;code&gt;dags/&lt;/code&gt; folder and then call PythonOperator to run the task. Note that the python code would not only include the Dags and tasks code, but they also have the actual ETL codes in there. To be honest, I am not usually comfortable in jamming codes for different purpose in one .py file because I prefer separating codes for ETL from codes that defines a DAG and tasks in other places.&lt;/p&gt;\n\n&lt;p&gt;So, some questions I have, maybe a stupid question, are:&lt;/p&gt;\n\n&lt;p&gt;In real practice, are we really saving the codes in the &lt;code&gt;dags&lt;/code&gt; folder?  &lt;/p&gt;\n\n&lt;p&gt;Say that I have a function that does simple data cleansing with pandas. in real practice, should I just define a function in a .py and then also define the dags and tasks in the same python file? Would the &lt;code&gt;dags&lt;/code&gt; folder be flooded? Is there any ways to just save the dags and tasks codes in the dags folder, but import the actual ETL codes from other .py files saved in another folder?&lt;/p&gt;\n\n&lt;p&gt;I feel the question may be leaning towards a &amp;#39;no&amp;#39;, but what I&amp;#39;m looking to learn is the `why` and `how is that being done in real practice`.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180lef1", "is_robot_indexable": true, "report_reasons": null, "author": "sevkw", "discussion_type": null, "num_comments": 57, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180lef1/how_is_airflow_really_used_in_real_practice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180lef1/how_is_airflow_really_used_in_real_practice/", "subreddit_subscribers": 141114, "created_utc": 1700585257.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thinking of switching to data engineering. I currently make $91k in clinical informatics with amazing WLB. \n\nWondering, what is the WLB like as a data engineer? How does it compare to SWE WLB?", "author_fullname": "t2_7x2alm42", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the work life balance like as a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180ysrr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700620924.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thinking of switching to data engineering. I currently make $91k in clinical informatics with amazing WLB. &lt;/p&gt;\n\n&lt;p&gt;Wondering, what is the WLB like as a data engineer? How does it compare to SWE WLB?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180ysrr", "is_robot_indexable": true, "report_reasons": null, "author": "InstaMastery", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180ysrr/what_is_the_work_life_balance_like_as_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180ysrr/what_is_the_work_life_balance_like_as_a_data/", "subreddit_subscribers": 141114, "created_utc": 1700620924.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I find that most of the material out there is beginner level for data modeling for both relational data modeling and warehousing. Is there a good course that you can recommend that\u2019s at least intermediate or advanced. Thank you! \ud83d\ude4f\ud83c\udffb", "author_fullname": "t2_g7w5zhnv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best data modelling course?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180wmae", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700614288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I find that most of the material out there is beginner level for data modeling for both relational data modeling and warehousing. Is there a good course that you can recommend that\u2019s at least intermediate or advanced. Thank you! \ud83d\ude4f\ud83c\udffb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180wmae", "is_robot_indexable": true, "report_reasons": null, "author": "long_spy200", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180wmae/what_is_the_best_data_modelling_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180wmae/what_is_the_best_data_modelling_course/", "subreddit_subscribers": 141114, "created_utc": 1700614288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I lurked in the sub and saw posts talking about transitioning from data engineering to other roles like backend , dataops , data analysis, BI  , cloud engineering and devops \n\n\nI think there are no roles 100% data engineering and the fact i didn't find alot of jobs or interns so it's best to be more rounded? \n\nSo what skills are mostly common between them and data engineering. i know it depends on the company and the role but what you guys think?", "author_fullname": "t2_hxue1umo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what roles are close to data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1817rwr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700655796.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700655302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I lurked in the sub and saw posts talking about transitioning from data engineering to other roles like backend , dataops , data analysis, BI  , cloud engineering and devops &lt;/p&gt;\n\n&lt;p&gt;I think there are no roles 100% data engineering and the fact i didn&amp;#39;t find alot of jobs or interns so it&amp;#39;s best to be more rounded? &lt;/p&gt;\n\n&lt;p&gt;So what skills are mostly common between them and data engineering. i know it depends on the company and the role but what you guys think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1817rwr", "is_robot_indexable": true, "report_reasons": null, "author": "Single-Sound-1865", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1817rwr/what_roles_are_close_to_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1817rwr/what_roles_are_close_to_data_engineering/", "subreddit_subscribers": 141114, "created_utc": 1700655302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently when we merge into our protected `development` or `production` branches, our CI/CD pipeline gets triggered and it builds one or more docker containers and pushes them to a repo where our run tasks pick them up. At each task invocation dbt starts by parsing the entire project that is contained within that particular image. \n\nA few weeks ago I passed some non-existent tags to our task runner, which is all in all 8 different dbt operations, and it took about 7 minutes to complete. All of that time is spent spinning up the task, parsing, finding no matches for these tags, and shutting down. \n\nI would like to incorporate parsing into our CI/CD pipelines so that parsing happens only when it's required, which is when new code is merged in, and otherwise successive operations on unchanged codebases just reference the same saved manifest to save some time during runs. Even if is just a couple of minutes.  \n\nBut I haven't come up with a clean way to do it. I could include `dbt parse` as a `RUN` command in each project's Dockerfile. But then I need to get the database credentials to the docker build, as passing a large number of arguments seems clunky. Additionally, I then need to include the database credentials as CI/CD variables in my pipeline which will make key rotation difficult. \n\nHas anyone found a clean way to do this?", "author_fullname": "t2_6nf2i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone had success incorporating dbt's parse step into your CI/CD pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180ml58", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700588268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently when we merge into our protected &lt;code&gt;development&lt;/code&gt; or &lt;code&gt;production&lt;/code&gt; branches, our CI/CD pipeline gets triggered and it builds one or more docker containers and pushes them to a repo where our run tasks pick them up. At each task invocation dbt starts by parsing the entire project that is contained within that particular image. &lt;/p&gt;\n\n&lt;p&gt;A few weeks ago I passed some non-existent tags to our task runner, which is all in all 8 different dbt operations, and it took about 7 minutes to complete. All of that time is spent spinning up the task, parsing, finding no matches for these tags, and shutting down. &lt;/p&gt;\n\n&lt;p&gt;I would like to incorporate parsing into our CI/CD pipelines so that parsing happens only when it&amp;#39;s required, which is when new code is merged in, and otherwise successive operations on unchanged codebases just reference the same saved manifest to save some time during runs. Even if is just a couple of minutes.  &lt;/p&gt;\n\n&lt;p&gt;But I haven&amp;#39;t come up with a clean way to do it. I could include &lt;code&gt;dbt parse&lt;/code&gt; as a &lt;code&gt;RUN&lt;/code&gt; command in each project&amp;#39;s Dockerfile. But then I need to get the database credentials to the docker build, as passing a large number of arguments seems clunky. Additionally, I then need to include the database credentials as CI/CD variables in my pipeline which will make key rotation difficult. &lt;/p&gt;\n\n&lt;p&gt;Has anyone found a clean way to do this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180ml58", "is_robot_indexable": true, "report_reasons": null, "author": "radil", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180ml58/has_anyone_had_success_incorporating_dbts_parse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180ml58/has_anyone_had_success_incorporating_dbts_parse/", "subreddit_subscribers": 141114, "created_utc": 1700588268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, \nI'll be taking my test next week and I' took Alan Rodrigues's course and saw a lot of videos of YouTube. \nIf anyone has fresh tips for me it would be highly appreciated", "author_fullname": "t2_38po62bx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DP 203 Nov 23", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180umnf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700608751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, \nI&amp;#39;ll be taking my test next week and I&amp;#39; took Alan Rodrigues&amp;#39;s course and saw a lot of videos of YouTube. \nIf anyone has fresh tips for me it would be highly appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "180umnf", "is_robot_indexable": true, "report_reasons": null, "author": "Esteban_Rdz", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180umnf/dp_203_nov_23/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180umnf/dp_203_nov_23/", "subreddit_subscribers": 141114, "created_utc": 1700608751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Would great expectations/dbt and a viz tool on top to display the metrics ? How do you handle logging and monitoring of chemistry drift of raw files when landing ? Is monitoring / messaging via slack and cloud watch enough ?\n\nJust curious as to what people think works well, ideally open source. That can cover a product and analytic pipelines.", "author_fullname": "t2_13551s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is your favorite method tools and metrics to implement data quality checks / monitors for data pipelines that feed product database (Postgres) and data science teams (snowflake / data bricks) (s3 for both)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1810lqk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700626709.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Would great expectations/dbt and a viz tool on top to display the metrics ? How do you handle logging and monitoring of chemistry drift of raw files when landing ? Is monitoring / messaging via slack and cloud watch enough ?&lt;/p&gt;\n\n&lt;p&gt;Just curious as to what people think works well, ideally open source. That can cover a product and analytic pipelines.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1810lqk", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance2", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1810lqk/what_is_your_favorite_method_tools_and_metrics_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1810lqk/what_is_your_favorite_method_tools_and_metrics_to/", "subreddit_subscribers": 141114, "created_utc": 1700626709.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Coming from a background of piping in data into snowflake from apis, I manage ci/cd via dbt / circle CI / terraform / GitHub. Ie  a dev/qa/prod env / schemas that let\u2019s say a new column is added to a dbt model and in turn a table now has a new column and then it gets merged to prod and the prod schema has new Column. \n\n\nWhat\u2019s the best way to do the above and can I leverage any of my knowledge from implementing a process for a dwh rather than a prod db. Also any chance I can kill two birds with one stone ?", "author_fullname": "t2_xo4dr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python / AWS / Postgres data pipeline that ingests data from ali and writes to Postgres that is backed for a saas app. How do best manage ci/cd", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1810qpk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700627204.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Coming from a background of piping in data into snowflake from apis, I manage ci/cd via dbt / circle CI / terraform / GitHub. Ie  a dev/qa/prod env / schemas that let\u2019s say a new column is added to a dbt model and in turn a table now has a new column and then it gets merged to prod and the prod schema has new Column. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s the best way to do the above and can I leverage any of my knowledge from implementing a process for a dwh rather than a prod db. Also any chance I can kill two birds with one stone ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1810qpk", "is_robot_indexable": true, "report_reasons": null, "author": "citizenofacceptance", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1810qpk/python_aws_postgres_data_pipeline_that_ingests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1810qpk/python_aws_postgres_data_pipeline_that_ingests/", "subreddit_subscribers": 141114, "created_utc": 1700627204.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have 5 years of json application log files stored in 60 second folder partitions with one json log file per event in S3. So a single 60 second partition could have 100\u2019s of json files. This is obviously unusable for analytics. \n\nI need to consolidate the log data and repartition into nested year/month/day partitions with one json log file per day\n\nAny thoughts on the best way to do this within AWS or outside of AWS, that\u2019s not I/O bound? ", "author_fullname": "t2_r168sf7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to repartition json data in s3 bucket", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180q34j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700597454.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700597255.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 5 years of json application log files stored in 60 second folder partitions with one json log file per event in S3. So a single 60 second partition could have 100\u2019s of json files. This is obviously unusable for analytics. &lt;/p&gt;\n\n&lt;p&gt;I need to consolidate the log data and repartition into nested year/month/day partitions with one json log file per day&lt;/p&gt;\n\n&lt;p&gt;Any thoughts on the best way to do this within AWS or outside of AWS, that\u2019s not I/O bound? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180q34j", "is_robot_indexable": true, "report_reasons": null, "author": "johncena9519", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180q34j/best_way_to_repartition_json_data_in_s3_bucket/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180q34j/best_way_to_repartition_json_data_in_s3_bucket/", "subreddit_subscribers": 141114, "created_utc": 1700597255.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_bxjjl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Change Data Capture Breaks Encapsulation\". Does it, though?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 90, "top_awarded_type": null, "hide_score": false, "name": "t3_180m4ib", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/lb_31UM3cfi5BJLJ7V8rI9LuJZDwEVlI8UMRcWYEUxM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1700587093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "decodable.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.decodable.co/blog/change-data-capture-breaks-encapsulation-does-it-though", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?auto=webp&amp;s=76d0fb3735486ab22191f7d87d4f31f534c0b3ba", "width": 1898, "height": 1233}, "resolutions": [{"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c620453ffb0a56ab41a2188e1956d8a52f4c078", "width": 108, "height": 70}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ca2423f02ac40f19b1b79f2731eb418ff11ec83", "width": 216, "height": 140}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=09eaca410b99c679520de72384272bfa3a935393", "width": 320, "height": 207}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=298d58ab70c8baa397e0e3f20b55b4c8d35ae7c5", "width": 640, "height": 415}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7542d65a374213316e86de0f856f5f789cec9f69", "width": 960, "height": 623}, {"url": "https://external-preview.redd.it/pUHWsfF6Gyo6eltcfkv27Xw-Zrj6pT0O6Bb-t_y9e68.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=22b0a0db31c57353d318821141190a39ae8c5b96", "width": 1080, "height": 701}], "variants": {}, "id": "fZR5RCGRbhiP6OBRB8mmdV7xnByAN1Vs7fs2gFHquvU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "180m4ib", "is_robot_indexable": true, "report_reasons": null, "author": "gunnarmorling", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180m4ib/change_data_capture_breaks_encapsulation_does_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.decodable.co/blog/change-data-capture-breaks-encapsulation-does-it-though", "subreddit_subscribers": 141114, "created_utc": 1700587093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Management seems interested in investing in the West Monroe's data bricks accelerator and I'm curious if anyone has any experience with it or Databricks accelerator's in general.\n\nWe have a small data engineering team and the belief is that using that accelerator will increase our speed to market.  I'm curious if anyone has experience with these types of solutions and how they fare relative to other solutions on the market.\n\n[https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator](https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator)", "author_fullname": "t2_3pgbq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do anyone have experience with West Monroe's Databricks Lakehouse Accelerator Mizu?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180yzxe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700621539.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Management seems interested in investing in the West Monroe&amp;#39;s data bricks accelerator and I&amp;#39;m curious if anyone has any experience with it or Databricks accelerator&amp;#39;s in general.&lt;/p&gt;\n\n&lt;p&gt;We have a small data engineering team and the belief is that using that accelerator will increase our speed to market.  I&amp;#39;m curious if anyone has experience with these types of solutions and how they fare relative to other solutions on the market.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator\"&gt;https://www.westmonroe.com/partners/mizu-west-monroe-databricks-lakehouse-accelerator&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?auto=webp&amp;s=f52f3657bb5f3877cbe4a55bd18794778eed9704", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8850f78c52e7797d26c19ad6fda65deb722f263", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=974ce6593363bfbd6c815744281c23d866f10cf2", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=21d02e2d873de235403725ad86e741de9b7fc69e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee266c3f3bf1c2d433acba20b20308e49cffd5ef", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c151d3b5becb70af0a66fe80b20f59902ce1bc2", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/qBJRp9Ved3altwRuRpBPxLzPo5_ejXFOaJdh9EBOEYI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e2e95bfc0db065b7f319e2973b702c60bd9b13f", "width": 1080, "height": 567}], "variants": {}, "id": "2pz3rWcq_ScNWhN7QkxHJWo9cO-lvk6W0rqq9t78ngs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "180yzxe", "is_robot_indexable": true, "report_reasons": null, "author": "tehsandvich", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180yzxe/do_anyone_have_experience_with_west_monroes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180yzxe/do_anyone_have_experience_with_west_monroes/", "subreddit_subscribers": 141114, "created_utc": 1700621539.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Source data is big json objects. We don't use all fields but some of the ones we do use are heavily nested.\n\nMy instinct is to land the json into a raw stage, then extract the pertinent fields to yield flat tables that can feed data marts. \n\nObviously that's a very high lever overview. But wondering if anyone has opinions on that direction vs somehow keeping JSON intact.\n\nThanks for any pointers, links, thoughts.", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dealing with big JSON objects - flatten into tabular or find a way to query JSON efficiently?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_181axwe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700664967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Source data is big json objects. We don&amp;#39;t use all fields but some of the ones we do use are heavily nested.&lt;/p&gt;\n\n&lt;p&gt;My instinct is to land the json into a raw stage, then extract the pertinent fields to yield flat tables that can feed data marts. &lt;/p&gt;\n\n&lt;p&gt;Obviously that&amp;#39;s a very high lever overview. But wondering if anyone has opinions on that direction vs somehow keeping JSON intact.&lt;/p&gt;\n\n&lt;p&gt;Thanks for any pointers, links, thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181axwe", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181axwe/dealing_with_big_json_objects_flatten_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181axwe/dealing_with_big_json_objects_flatten_into/", "subreddit_subscribers": 141114, "created_utc": 1700664967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " \n\nMy company has on-premises databases.\n\nI get data from those tables and I have made SSRS reports which uses that data.\n\nNow they want to move that database to cloud (Oracle Cloud).\n\nI want the SSRS reports to get the data from that database on the cloud database.\n\nThe machine I use as a server for SSRS Server is also on-premise.\n\nHow I will connect SSRS reports to cloud database, is it possible to do all this.\n\nI have to present a report about all this in 2 hours.", "author_fullname": "t2_3k9gevl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Migration of SSRS &amp; Database from On-Prem to Oracle Cloud", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1814cnq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700641250.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has on-premises databases.&lt;/p&gt;\n\n&lt;p&gt;I get data from those tables and I have made SSRS reports which uses that data.&lt;/p&gt;\n\n&lt;p&gt;Now they want to move that database to cloud (Oracle Cloud).&lt;/p&gt;\n\n&lt;p&gt;I want the SSRS reports to get the data from that database on the cloud database.&lt;/p&gt;\n\n&lt;p&gt;The machine I use as a server for SSRS Server is also on-premise.&lt;/p&gt;\n\n&lt;p&gt;How I will connect SSRS reports to cloud database, is it possible to do all this.&lt;/p&gt;\n\n&lt;p&gt;I have to present a report about all this in 2 hours.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1814cnq", "is_robot_indexable": true, "report_reasons": null, "author": "Pillstyr", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1814cnq/migration_of_ssrs_database_from_onprem_to_oracle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1814cnq/migration_of_ssrs_database_from_onprem_to_oracle/", "subreddit_subscribers": 141114, "created_utc": 1700641250.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is Kappa\u2019s simplicity the key to your real-time processing needs, or does Lambda\u2019s hybrid versatility suit your batch and streaming scenarios better? Discover the differences, use cases, and benefits that make each framework unique in this guide \n\n[https://memphis.dev/kappa-vs-lambda/](https://memphis.dev/kappa-vs-lambda/)", "author_fullname": "t2_bqjrmnud", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kappa vs Lambda guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_181bgav", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700666312.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is Kappa\u2019s simplicity the key to your real-time processing needs, or does Lambda\u2019s hybrid versatility suit your batch and streaming scenarios better? Discover the differences, use cases, and benefits that make each framework unique in this guide &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://memphis.dev/kappa-vs-lambda/\"&gt;https://memphis.dev/kappa-vs-lambda/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?auto=webp&amp;s=41573d9ac660082eaeba6ad842c839de9e381218", "width": 1024, "height": 681}, "resolutions": [{"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=62a231fdacf5ee11c67d8c543212d54c74a32d5b", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a4cb76675f01006e97763cb3169495e3de0cb381", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b236b59564bf213abb77081fa03030f894d734aa", "width": 320, "height": 212}, {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=441827d1797e3f4d0dd2a1dcce80cc06cd906b7d", "width": 640, "height": 425}, {"url": "https://external-preview.redd.it/VcXNpmHxogs8D7732mGX2CFJ-Hs68t9arbRtsrT79b0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=454279e23a364b257dab8aafc43e7c18d8305eb6", "width": 960, "height": 638}], "variants": {}, "id": "65E_XwJLXDsdcg8e5-svW1D1aVB5bA4Ka1ewYsvf7qY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "181bgav", "is_robot_indexable": true, "report_reasons": null, "author": "Glittering_Bug105", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181bgav/kappa_vs_lambda_guide/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181bgav/kappa_vs_lambda_guide/", "subreddit_subscribers": 141114, "created_utc": 1700666312.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All, \n\nI've built a pipeline locally that tasks GA4 data, applies some transformations and tests with DBT, and stores the incrementally transformed tables to a GCP dataset. I put my dbt code in a docker container bc I read that this could help me deploy my dbt project to the cloud faster. But how do you go about automating the DBT code from the docker container? Right now I use taskfile locally to automate the building of the image, container and the serving of the dbt documentation but I fail to understand how I go from there to having this pipeline automated in GCP. \n\n&amp;#x200B;\n\nThank you for your time if you read this. ", "author_fullname": "t2_1jh436du", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you deploy your DBT code in a Docker container to GCP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181admm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700663326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve built a pipeline locally that tasks GA4 data, applies some transformations and tests with DBT, and stores the incrementally transformed tables to a GCP dataset. I put my dbt code in a docker container bc I read that this could help me deploy my dbt project to the cloud faster. But how do you go about automating the DBT code from the docker container? Right now I use taskfile locally to automate the building of the image, container and the serving of the dbt documentation but I fail to understand how I go from there to having this pipeline automated in GCP. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time if you read this. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "181admm", "is_robot_indexable": true, "report_reasons": null, "author": "Jeannetton", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181admm/how_do_you_deploy_your_dbt_code_in_a_docker/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181admm/how_do_you_deploy_your_dbt_code_in_a_docker/", "subreddit_subscribers": 141114, "created_utc": 1700663326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking for a site with SQL puzzles that are close to real-world situations, like HackerRank but even more practical. Any go-to recommendations?\n\nIf the puzzles would have data modeling questions, that would be awesome!", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Puzzles Recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181a473", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700662565.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a site with SQL puzzles that are close to real-world situations, like HackerRank but even more practical. Any go-to recommendations?&lt;/p&gt;\n\n&lt;p&gt;If the puzzles would have data modeling questions, that would be awesome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181a473", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181a473/sql_puzzles_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181a473/sql_puzzles_recommendations/", "subreddit_subscribers": 141114, "created_utc": 1700662565.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking for a site with SQL puzzles that are close to real-world situations, like HackerRank but even more practical. Any go-to recommendations?\n\nIf the puzzles would have data modeling questions, that would be awesome!", "author_fullname": "t2_mbbdv7y98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Puzzles Recommendations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_181a45d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700662562.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a site with SQL puzzles that are close to real-world situations, like HackerRank but even more practical. Any go-to recommendations?&lt;/p&gt;\n\n&lt;p&gt;If the puzzles would have data modeling questions, that would be awesome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "181a45d", "is_robot_indexable": true, "report_reasons": null, "author": "Kindly-Screen-2557", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/181a45d/sql_puzzles_recommendations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/181a45d/sql_puzzles_recommendations/", "subreddit_subscribers": 141114, "created_utc": 1700662562.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nwe are currently in the process of modernizing the data stack and piloted a couple of monolithic/old ETL/ELT-Solutions like Informtica, Talend etc. and came to the conclusion than none is fitting our needs and only work well for the simplest of tasks and everything remotely complex requires workarounds etc.. It\u00b4s simply not better than the old solution.\n\nWe therefore consindering \"hand roling\" our platform with with an orchestrator and k8s/docker as the execution environment + python as the language of choice, because we have already k8s/helm/git-ops  and python knowledge for developing and deploying numerous python services/webservices for hosting models and other purposes.\n\nMy biggest concern is on how to decide which orchestrator we should use. We have the following requirements.\n\n* **strictly On-prem deployment (regulated environment):**\n   * At best with an helm-chart on k8s (locally hosted OKD Clusters), any other platform would also be ok but k8s is preferred.\n* **Implementation partner to get us started:**\n   * The partner should at best be a german company/a company in DACH Region.\n* **Community**:\n   * Theres a community where one can find infos about working with the orchestrator etc and it\u00b4s not enough to have docs alone.\n* **Paid/OSS**\n   * Both are ok\n\nI personaly tried dagster and airflow and liked dagster more but could finda german implementation partner and the community is also much small.\n\nIt seems like lately a lot of posts/articles/people state that working with airflow is a pain/its a bad piece of software and one should avoid it. \n\nI\u00b4m therefore asking if airflow is still a valid solution to invest in and start the journey with it and if anyone uses dagster in production and knows german partners etc.?\n\nOr maybe I\u00b4m missing a third solution whe should definetly check out and also has german partners?\n\nBest regards\n\n&amp;#x200B;", "author_fullname": "t2_aue40tqc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Orchestrator/Scheduler - which one is valid in 2023 to run on an on prem k8s environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1819vgc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700661890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;we are currently in the process of modernizing the data stack and piloted a couple of monolithic/old ETL/ELT-Solutions like Informtica, Talend etc. and came to the conclusion than none is fitting our needs and only work well for the simplest of tasks and everything remotely complex requires workarounds etc.. It\u00b4s simply not better than the old solution.&lt;/p&gt;\n\n&lt;p&gt;We therefore consindering &amp;quot;hand roling&amp;quot; our platform with with an orchestrator and k8s/docker as the execution environment + python as the language of choice, because we have already k8s/helm/git-ops  and python knowledge for developing and deploying numerous python services/webservices for hosting models and other purposes.&lt;/p&gt;\n\n&lt;p&gt;My biggest concern is on how to decide which orchestrator we should use. We have the following requirements.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;strictly On-prem deployment (regulated environment):&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;At best with an helm-chart on k8s (locally hosted OKD Clusters), any other platform would also be ok but k8s is preferred.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Implementation partner to get us started:&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The partner should at best be a german company/a company in DACH Region.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Community&lt;/strong&gt;:\n\n&lt;ul&gt;\n&lt;li&gt;Theres a community where one can find infos about working with the orchestrator etc and it\u00b4s not enough to have docs alone.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Paid/OSS&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Both are ok&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I personaly tried dagster and airflow and liked dagster more but could finda german implementation partner and the community is also much small.&lt;/p&gt;\n\n&lt;p&gt;It seems like lately a lot of posts/articles/people state that working with airflow is a pain/its a bad piece of software and one should avoid it. &lt;/p&gt;\n\n&lt;p&gt;I\u00b4m therefore asking if airflow is still a valid solution to invest in and start the journey with it and if anyone uses dagster in production and knows german partners etc.?&lt;/p&gt;\n\n&lt;p&gt;Or maybe I\u00b4m missing a third solution whe should definetly check out and also has german partners?&lt;/p&gt;\n\n&lt;p&gt;Best regards&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1819vgc", "is_robot_indexable": true, "report_reasons": null, "author": "Salfiiii", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1819vgc/orchestratorscheduler_which_one_is_valid_in_2023/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1819vgc/orchestratorscheduler_which_one_is_valid_in_2023/", "subreddit_subscribers": 141114, "created_utc": 1700661890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, does anyone have experience with dbt and synapse spark pools? I know dbt have an adapter for synapse sql pools and an adapter for databricks. Dbt has also a spark adapter and Im wondering if its possible to use that with synapse? Now that Microsoft introducing fabric, they are taking away a lot of adapters, now its not possible to connect to synapse spark pools from VScode without a fabric workspace for example \ud83e\udee0", "author_fullname": "t2_4c41t3vo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt and synapse spark pools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1819gix", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700660729.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, does anyone have experience with dbt and synapse spark pools? I know dbt have an adapter for synapse sql pools and an adapter for databricks. Dbt has also a spark adapter and Im wondering if its possible to use that with synapse? Now that Microsoft introducing fabric, they are taking away a lot of adapters, now its not possible to connect to synapse spark pools from VScode without a fabric workspace for example \ud83e\udee0&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1819gix", "is_robot_indexable": true, "report_reasons": null, "author": "fugas1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1819gix/dbt_and_synapse_spark_pools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1819gix/dbt_and_synapse_spark_pools/", "subreddit_subscribers": 141114, "created_utc": 1700660729.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This must be something devs run into a lot, but I can only find this list on [datahub](https://datahub.io/core/un-locode) or [this list on Kaggle](https://www.kaggle.com/datasets/programmerrdai/unlocode-code-list?select=subdivision-codes.csv), but I'm not sure if they're up to date or maintained.\n\nWhat's the best way of finding and implementing ISO or UN lists into your projects?", "author_fullname": "t2_cstta", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where would you find a full list of UN/LOCODE countries and cities?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18191po", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1700659471.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This must be something devs run into a lot, but I can only find this list on &lt;a href=\"https://datahub.io/core/un-locode\"&gt;datahub&lt;/a&gt; or &lt;a href=\"https://www.kaggle.com/datasets/programmerrdai/unlocode-code-list?select=subdivision-codes.csv\"&gt;this list on Kaggle&lt;/a&gt;, but I&amp;#39;m not sure if they&amp;#39;re up to date or maintained.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way of finding and implementing ISO or UN lists into your projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?auto=webp&amp;s=cece6e86958b39e85d5fbb63c8608ce3e3fe53dd", "width": 1414, "height": 1418}, "resolutions": [{"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4600462a3d997639ddd1ddf74b7c482874d15241", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d19ba67edd405544066bd443d7d46cd70eab498c", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=51ea8b83757621409c3470b045562586e2f34aa8", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d6b1c3be342aff1fa0e97f624699f56789df7c46", "width": 640, "height": 641}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0e1eb7acddfe28f090b21ef460255b08110a653", "width": 960, "height": 962}, {"url": "https://external-preview.redd.it/iToMjAxVtGn3jozIaOYi-ajhBEmAelVnWbqfcyMVmzM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cfd6fae80fcc789dc65e9218d37eb7299937cfc5", "width": 1080, "height": 1083}], "variants": {}, "id": "3DephRZhy7zpapaXeulxTbJvfqGyCVgvyc3obhC9CUo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18191po", "is_robot_indexable": true, "report_reasons": null, "author": "Snugglosaurus", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18191po/where_would_you_find_a_full_list_of_unlocode/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18191po/where_would_you_find_a_full_list_of_unlocode/", "subreddit_subscribers": 141114, "created_utc": 1700659471.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What can you expect from Apache Doris as a data warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1817mpu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1700654775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "doris.apache.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://doris.apache.org/blog/apache-doris-summit-asia-2023-what-can-you-expect-from-apache-doris-as-a-data-warehouse", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "1817mpu", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1817mpu/what_can_you_expect_from_apache_doris_as_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://doris.apache.org/blog/apache-doris-summit-asia-2023-what-can-you-expect-from-apache-doris-as-a-data-warehouse", "subreddit_subscribers": 141114, "created_utc": 1700654775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry in advance for long post. I tried to just put a lot of detail but before you read just know I am overall just asking for some resources on setting up Amundsen in 2023, and just a sanity check on my initial research. I've been working on this for 2 days.  \n\n\nHello. I am currently in the process of evaluating Amundsen but I am somewhat lost. I've tried following the Quickstart and the examples. I tried opting in for Atlas for the push capabilities but that didn't seem to work so I just decided to attempt to run everything as close to the docs as possible. The deploy with docker compose worked with default settings, but since that no step going forward worked.\n\nI wanted to test the databuilder which seems like the defacto api for putting information into the metadata engine, so I spun up a postgres database with docker and loaded a table into the public schema. I then ran the example sample\\_postgres\\_extractor which did not work because the version of SqlAlchemy after installing the requirements file did not use the same execution method in the databuilder. As much as I wanted to follow the steps as closely as possible I attempted to downpatch SqlAlchemy. This worked after downpatching a few other requirements that were incompatible with the older version of SqlAlchemy but I simply got an elasticsearch.exceptions.ConnectionError. I tried to test running the example dags which also didn't work. \"apache-airflow-providers-elasticsearch 5.1.0 requires elasticsearch&lt;9,&gt;8, but you have elasticsearch 7.17.9 which is incompatible.\" It seems that the airflow elasticsearch provider uses a higher version of the elasticsearch package that amundsen does so the installation fails. For demo purposes I just downpatched the airflow provider so no more conflicts. I ran the dag and it seems to work, though I still get warnings in my logs that I have incompatible versions of elasticsearch. For the local demo airflow this isn't an issue but for our production environment we are using this provider and cannot remove it or downpatch that package.  \n\n\nWith that experience, I feel like I must be missing some resources? I've heard around, even on this subreddit, that many are using Amundsen and very happy with it. I have to ask if the documentation here [https://www.amundsen.io/amundsen/installation/](https://www.amundsen.io/amundsen/installation/) is what people actually recommend to evaluate? Are there better resources for installing/testing the extractors? Are there any good tutorials on writing a custom extractor or using the extractors that exist that aren't just code examples to copy? I would like to know more about how the specific apis in databuilder work because I generally want a good idea of how I would interact or modify one of the examples to fit our needs. I read through the Dashboard Ingestion guidance ([https://www.amundsen.io/amundsen/databuilder/docs/dashboard\\_ingestion\\_guide/](https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/)) and this basically just said use the databuilder and check the examples for how to do that. Considering the examples aren't working I am unsure of how to proceed.\n\nIs there a more up to date guide on the databuilder or a better way to install it? Is it contained in a pip package or do I actually have to build from source with setup tools? Is there a good and recent tutorial outside of the docs that shows the steps today to get up and running? Are there good deployment guides from this year?\n\nAs much as I want to give it a fair shot, I don't know how to really evaluate at this point because I can't really give my team a good estimate on how much work it would be to set this up. I am also not super confident looking at the state of databuilder that it has been well maintained. From what I can see there are quite a few package versions it requires that are deprecated, or nearing end of life.  I must be missing something right?", "author_fullname": "t2_ptgbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amundsen resources?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18154ki", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700644635.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry in advance for long post. I tried to just put a lot of detail but before you read just know I am overall just asking for some resources on setting up Amundsen in 2023, and just a sanity check on my initial research. I&amp;#39;ve been working on this for 2 days.  &lt;/p&gt;\n\n&lt;p&gt;Hello. I am currently in the process of evaluating Amundsen but I am somewhat lost. I&amp;#39;ve tried following the Quickstart and the examples. I tried opting in for Atlas for the push capabilities but that didn&amp;#39;t seem to work so I just decided to attempt to run everything as close to the docs as possible. The deploy with docker compose worked with default settings, but since that no step going forward worked.&lt;/p&gt;\n\n&lt;p&gt;I wanted to test the databuilder which seems like the defacto api for putting information into the metadata engine, so I spun up a postgres database with docker and loaded a table into the public schema. I then ran the example sample_postgres_extractor which did not work because the version of SqlAlchemy after installing the requirements file did not use the same execution method in the databuilder. As much as I wanted to follow the steps as closely as possible I attempted to downpatch SqlAlchemy. This worked after downpatching a few other requirements that were incompatible with the older version of SqlAlchemy but I simply got an elasticsearch.exceptions.ConnectionError. I tried to test running the example dags which also didn&amp;#39;t work. &amp;quot;apache-airflow-providers-elasticsearch 5.1.0 requires elasticsearch&amp;lt;9,&amp;gt;8, but you have elasticsearch 7.17.9 which is incompatible.&amp;quot; It seems that the airflow elasticsearch provider uses a higher version of the elasticsearch package that amundsen does so the installation fails. For demo purposes I just downpatched the airflow provider so no more conflicts. I ran the dag and it seems to work, though I still get warnings in my logs that I have incompatible versions of elasticsearch. For the local demo airflow this isn&amp;#39;t an issue but for our production environment we are using this provider and cannot remove it or downpatch that package.  &lt;/p&gt;\n\n&lt;p&gt;With that experience, I feel like I must be missing some resources? I&amp;#39;ve heard around, even on this subreddit, that many are using Amundsen and very happy with it. I have to ask if the documentation here &lt;a href=\"https://www.amundsen.io/amundsen/installation/\"&gt;https://www.amundsen.io/amundsen/installation/&lt;/a&gt; is what people actually recommend to evaluate? Are there better resources for installing/testing the extractors? Are there any good tutorials on writing a custom extractor or using the extractors that exist that aren&amp;#39;t just code examples to copy? I would like to know more about how the specific apis in databuilder work because I generally want a good idea of how I would interact or modify one of the examples to fit our needs. I read through the Dashboard Ingestion guidance (&lt;a href=\"https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/\"&gt;https://www.amundsen.io/amundsen/databuilder/docs/dashboard_ingestion_guide/&lt;/a&gt;) and this basically just said use the databuilder and check the examples for how to do that. Considering the examples aren&amp;#39;t working I am unsure of how to proceed.&lt;/p&gt;\n\n&lt;p&gt;Is there a more up to date guide on the databuilder or a better way to install it? Is it contained in a pip package or do I actually have to build from source with setup tools? Is there a good and recent tutorial outside of the docs that shows the steps today to get up and running? Are there good deployment guides from this year?&lt;/p&gt;\n\n&lt;p&gt;As much as I want to give it a fair shot, I don&amp;#39;t know how to really evaluate at this point because I can&amp;#39;t really give my team a good estimate on how much work it would be to set this up. I am also not super confident looking at the state of databuilder that it has been well maintained. From what I can see there are quite a few package versions it requires that are deprecated, or nearing end of life.  I must be missing something right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18154ki", "is_robot_indexable": true, "report_reasons": null, "author": "miscbits", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18154ki/amundsen_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18154ki/amundsen_resources/", "subreddit_subscribers": 141114, "created_utc": 1700644635.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Anyone working with databricks, and have implemented CI/CD? \nI'm struggling to come up with an good approach that covers development of Notebooks, DLT workflows and jobs. Things seem to separated atm. Any tips or advice? \n\nCheers", "author_fullname": "t2_7p1fczdj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD in Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180u635", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700607538.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone working with databricks, and have implemented CI/CD? \nI&amp;#39;m struggling to come up with an good approach that covers development of Notebooks, DLT workflows and jobs. Things seem to separated atm. Any tips or advice? &lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180u635", "is_robot_indexable": true, "report_reasons": null, "author": "Far-Inspection9930", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180u635/cicd_in_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180u635/cicd_in_databricks/", "subreddit_subscribers": 141114, "created_utc": 1700607538.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nWe have a Databricks env where we have been following the documented Databricks model, where we extract data from a relational data source into our bronze table.   We then do additional processing to create the silver table for the appropriate use case.  We extract the data from a read replica to ensure that we don't impact our transactional work.  This is a beginner question, but what are the main reasons to use the bronze table when doing processing for our silver table versus just pulling directly from the read replica?  Thanks! ", "author_fullname": "t2_b3ncdhop6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Silver from bronze vs Silver pulling directly from read replicas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180rzhz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700602027.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;We have a Databricks env where we have been following the documented Databricks model, where we extract data from a relational data source into our bronze table.   We then do additional processing to create the silver table for the appropriate use case.  We extract the data from a read replica to ensure that we don&amp;#39;t impact our transactional work.  This is a beginner question, but what are the main reasons to use the bronze table when doing processing for our silver table versus just pulling directly from the read replica?  Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "180rzhz", "is_robot_indexable": true, "report_reasons": null, "author": "Annual-Scallion-4888", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180rzhz/databricks_silver_from_bronze_vs_silver_pulling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180rzhz/databricks_silver_from_bronze_vs_silver_pulling/", "subreddit_subscribers": 141114, "created_utc": 1700602027.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm interviewing for a role with that title next week, the company in question is only asking for 1+ years of experience working with SQL and Python and my work will revolve around working with SIS in schools. \n\nIn your role, do you find that you have room to grow into a proper Data Engineer?  \n\nI've been cramming data engineering concepts, so in the event of a coding exercise, what depth of complexity should I expect, given that they're only asking for a year of experience? I need to brush up on my SQL and Python.\n", "author_fullname": "t2_lp2ob", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have the job title \"Data Integration Engineer\" that can speak more on their duties?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_180nl7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1700602032.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1700590906.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m interviewing for a role with that title next week, the company in question is only asking for 1+ years of experience working with SQL and Python and my work will revolve around working with SIS in schools. &lt;/p&gt;\n\n&lt;p&gt;In your role, do you find that you have room to grow into a proper Data Engineer?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been cramming data engineering concepts, so in the event of a coding exercise, what depth of complexity should I expect, given that they&amp;#39;re only asking for a year of experience? I need to brush up on my SQL and Python.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "180nl7b", "is_robot_indexable": true, "report_reasons": null, "author": "mrbrucel33", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/180nl7b/does_anyone_have_the_job_title_data_integration/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/180nl7b/does_anyone_have_the_job_title_data_integration/", "subreddit_subscribers": 141114, "created_utc": 1700590906.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}