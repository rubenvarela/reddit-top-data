{"kind": "Listing", "data": {"after": "t3_187cnf4", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1cjqfkwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Update on the Tokyo Lab Film situation: \"Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away!\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_187iixe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 205, "total_awards_received": 0, "media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away! &lt;a href=\"https://t.co/YBZ4VDupty\"&gt;https://t.co/YBZ4VDupty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Go\u2605Tanks (@tanks404) &lt;a href=\"https://twitter.com/tanks404/status/1730202703102353526?ref_src=twsrc%5Etfw\"&gt;November 30, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "width": 350, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": "76c43f34-b98b-11e2-b55c-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/tanks404/status/1730202703102353526", "author_name": "Go\u2605Tanks", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away! &lt;a href=\"https://t.co/YBZ4VDupty\"&gt;https://t.co/YBZ4VDupty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Go\u2605Tanks (@tanks404) &lt;a href=\"https://twitter.com/tanks404/status/1730202703102353526?ref_src=twsrc%5Etfw\"&gt;November 30, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "author_url": "https://twitter.com/tanks404", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away! &lt;a href=\"https://t.co/YBZ4VDupty\"&gt;https://t.co/YBZ4VDupty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Go\u2605Tanks (@tanks404) &lt;a href=\"https://twitter.com/tanks404/status/1730202703102353526?ref_src=twsrc%5Etfw\"&gt;November 30, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "width": 350, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/187iixe", "height": 200}, "link_flair_text": "News", "can_mod_post": false, "score": 205, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/HUDdx7lLHSkxWJN_9IefbOV1AOXLfy7P9VdaDSQ2vCk.jpg", "edited": false, "author_flair_css_class": "dvd", "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701350903.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "twitter.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://twitter.com/tanks404/status/1730202703102353526", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/DwnJo79WeQcmI1ZPa9wXr-ul5WOo3hJFRGHqWQZ1x0k.jpg?auto=webp&amp;s=df5ff545e212b3ceab86560758132b52efec12ac", "width": 140, "height": 140}, "resolutions": [{"url": "https://external-preview.redd.it/DwnJo79WeQcmI1ZPa9wXr-ul5WOo3hJFRGHqWQZ1x0k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cde168eca60c12c204f34ca931507c07ebd143c1", "width": 108, "height": 108}], "variants": {}, "id": "_SMiF5GKmYLGf4y56QLZ9rC9av05q7OKN2PNbSg4Zg0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "DVD", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187iixe", "is_robot_indexable": true, "report_reasons": null, "author": "koempleh", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/187iixe/update_on_the_tokyo_lab_film_situation_tokyo_lab/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://twitter.com/tanks404/status/1730202703102353526", "subreddit_subscribers": 715344, "created_utc": 1701350903.0, "num_crossposts": 0, "media": {"type": "twitter.com", "oembed": {"provider_url": "https://twitter.com", "version": "1.0", "url": "https://twitter.com/tanks404/status/1730202703102353526", "author_name": "Go\u2605Tanks", "height": null, "width": 350, "html": "&lt;blockquote class=\"twitter-video\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Tokyo Lab orphaned film update! Toho has announced they will be taking over management of the remaining archive. No films will be thrown away! &lt;a href=\"https://t.co/YBZ4VDupty\"&gt;https://t.co/YBZ4VDupty&lt;/a&gt;&lt;/p&gt;&amp;mdash; Go\u2605Tanks (@tanks404) &lt;a href=\"https://twitter.com/tanks404/status/1730202703102353526?ref_src=twsrc%5Etfw\"&gt;November 30, 2023&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\n", "author_url": "https://twitter.com/tanks404", "provider_name": "Twitter", "cache_age": 3153600000, "type": "rich"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I run a video production company. We have 300TB of archived projects (and growing daily).\n\nMany years ago, our old solution for archiving was simply to dump old projects off onto an external drive, duplicate that, and have one drive at the office, one offsite elsewhere. This was ok, but not ideal. Relatively expensive per TB, and just a shit ton of physical drives.\n\nA few years ago, we had an unlimited Google Drive and 1000/1000 fibre internet. So we moved to a system where we would drop a project onto an external drive, keep that offsite, and have a duplicate of it uploaded to Google Drive. This worked ok until we reached a hidden file number limit on Google Drive. Then they removed the unlimited sizing of Google Drive accounts completely. So that was a dead end.\n\nSo then we moved that system to Dropbox a couple of years ago, as they were offering an unlimited account. This was the perfect situation. Dropbox was feature rich, fast, integrated beautifully into finder/explorer and just a great solution all round. It meant it was easy to give clients access to old data directly if they needed, etc. Anyway, as you all know, that gravy train has come to an end recently, and we now have 12 months grace with out storage on there before we have to have this sorted back to another sytem.\n\nOur options seem to be:\n\n* Go back to our old system of duplicated external drives, with one living offsite. We'd need \\~$7500AUD worth of new drives to duplicate what we currently have.\n* Buy a couple of LTO-9 tape drives (2 offices in different cities) and keep one copy on an external drive and one copy on a tape archive. This would be \\~$20000AUD of hardware upfront + media costs of \\~$2000AUD (assuming we'd get maybe 30TB per tape on the 18TB raw LTO 9 tapes). So more expensive upfront but would maybe pay off eventually?\n* Build a linustechtips style beast of a NAS. Raw drive cost would be similar to the external drives, but would have the advantage of being accessible remotely. Would then need to spend $5000-10000AUD on the actual hardware on top of the drives. Also have the problem of ever growing storage needs. This solution we could potentially not duplicate the data to external drives though and live with RAID as only form of redundancy...\n* Another clour storage service? Anything fast and decent enough that comes at a reasonable cost?\n\nAny advice here would be appreciated!", "author_fullname": "t2_8o00p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "300TB of data. Dropbox and Google are dead to me. Next options. Cloud? Tape? NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187b44e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 76, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 76, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701322947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I run a video production company. We have 300TB of archived projects (and growing daily).&lt;/p&gt;\n\n&lt;p&gt;Many years ago, our old solution for archiving was simply to dump old projects off onto an external drive, duplicate that, and have one drive at the office, one offsite elsewhere. This was ok, but not ideal. Relatively expensive per TB, and just a shit ton of physical drives.&lt;/p&gt;\n\n&lt;p&gt;A few years ago, we had an unlimited Google Drive and 1000/1000 fibre internet. So we moved to a system where we would drop a project onto an external drive, keep that offsite, and have a duplicate of it uploaded to Google Drive. This worked ok until we reached a hidden file number limit on Google Drive. Then they removed the unlimited sizing of Google Drive accounts completely. So that was a dead end.&lt;/p&gt;\n\n&lt;p&gt;So then we moved that system to Dropbox a couple of years ago, as they were offering an unlimited account. This was the perfect situation. Dropbox was feature rich, fast, integrated beautifully into finder/explorer and just a great solution all round. It meant it was easy to give clients access to old data directly if they needed, etc. Anyway, as you all know, that gravy train has come to an end recently, and we now have 12 months grace with out storage on there before we have to have this sorted back to another sytem.&lt;/p&gt;\n\n&lt;p&gt;Our options seem to be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Go back to our old system of duplicated external drives, with one living offsite. We&amp;#39;d need ~$7500AUD worth of new drives to duplicate what we currently have.&lt;/li&gt;\n&lt;li&gt;Buy a couple of LTO-9 tape drives (2 offices in different cities) and keep one copy on an external drive and one copy on a tape archive. This would be ~$20000AUD of hardware upfront + media costs of ~$2000AUD (assuming we&amp;#39;d get maybe 30TB per tape on the 18TB raw LTO 9 tapes). So more expensive upfront but would maybe pay off eventually?&lt;/li&gt;\n&lt;li&gt;Build a linustechtips style beast of a NAS. Raw drive cost would be similar to the external drives, but would have the advantage of being accessible remotely. Would then need to spend $5000-10000AUD on the actual hardware on top of the drives. Also have the problem of ever growing storage needs. This solution we could potentially not duplicate the data to external drives though and live with RAID as only form of redundancy...&lt;/li&gt;\n&lt;li&gt;Another clour storage service? Anything fast and decent enough that comes at a reasonable cost?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice here would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187b44e", "is_robot_indexable": true, "report_reasons": null, "author": "campster123", "discussion_type": null, "num_comments": 134, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187b44e/300tb_of_data_dropbox_and_google_are_dead_to_me/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187b44e/300tb_of_data_dropbox_and_google_are_dead_to_me/", "subreddit_subscribers": 715344, "created_utc": 1701322947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_j3gnkmjz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "How are brand new Exos drives sold/packaged/shipped?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"xty7zed4ke3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/xty7zed4ke3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc7072302c9ec2e50e510e00dd8d1cd2f1dbdea6"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/xty7zed4ke3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=791dc7f346b67530489e41ea7213ec73f9935166"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/xty7zed4ke3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4446b98d0a733eafefbbb560a2225b33ccad08cb"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/xty7zed4ke3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=95e83f25056e378fb6ceeaf6da1dc5a09d03f6db"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/xty7zed4ke3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b4561d8fb8da894588caf6f7165f5492e53555d"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/xty7zed4ke3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=53e106910754645283697690edb112dc0ba6af58"}], "s": {"y": 2048, "x": 1536, "u": "https://preview.redd.it/xty7zed4ke3c1.jpg?width=1536&amp;format=pjpg&amp;auto=webp&amp;s=794abdbd98fcd4bfd5c1100f07f179a1ab4b9163"}, "id": "xty7zed4ke3c1"}, "wo9zmed4ke3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/wo9zmed4ke3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49d73203d31257417e29ef2fc8bc9edc58a1b8d1"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/wo9zmed4ke3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=569b760a31f806735b0a203f6a2a87fc272ecf83"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/wo9zmed4ke3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf262c59ef94b39aab7403f38c831b28987ae352"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/wo9zmed4ke3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=58bf50eadd3b95875fc429003a96d3fd785a2859"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/wo9zmed4ke3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=20d279b966500b8f0e30853ada10999f8dd8e498"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/wo9zmed4ke3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8a695193354a2af613266db1f9fa1067db02c978"}], "s": {"y": 1536, "x": 2048, "u": "https://preview.redd.it/wo9zmed4ke3c1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=4d7daf1d0ae6e4b087785c1722e5293295a66532"}, "id": "wo9zmed4ke3c1"}, "3xciicd4ke3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 144, "x": 108, "u": "https://preview.redd.it/3xciicd4ke3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=291368d98f26198b8bb113b4dde391246ecb2e74"}, {"y": 288, "x": 216, "u": "https://preview.redd.it/3xciicd4ke3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9cbd32359b8e7e37dd01738583df9fd9643c38dc"}, {"y": 426, "x": 320, "u": "https://preview.redd.it/3xciicd4ke3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=564cebd23e489eb2fd907ed337934aa53f8012fe"}, {"y": 853, "x": 640, "u": "https://preview.redd.it/3xciicd4ke3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4975c3e74602e654c048b386ae9e9f018111b54c"}, {"y": 1280, "x": 960, "u": "https://preview.redd.it/3xciicd4ke3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d6d638d45ef9f6058bf1f75d633aff293a5b7f7"}, {"y": 1440, "x": 1080, "u": "https://preview.redd.it/3xciicd4ke3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=13f6b5066399af14ab6749ce1bbc6ce7d0ce4d18"}], "s": {"y": 2048, "x": 1536, "u": "https://preview.redd.it/3xciicd4ke3c1.jpg?width=1536&amp;format=pjpg&amp;auto=webp&amp;s=6105c8ecc773176d23050a190a3502ec1f1748e3"}, "id": "3xciicd4ke3c1"}}, "name": "t3_1878c2k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "ups": 39, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "picture 1: two boxes like this in a bigger box with lots of bubble wrap.", "media_id": "xty7zed4ke3c1", "id": 367352063}, {"caption": "picture 2: inside the box, was the hard drive in fat bubble wrap", "media_id": "wo9zmed4ke3c1", "id": 367352064}, {"caption": "picture 3: out of the fat bubble wrap is the hard drive in a sealed anti-static bag.", "media_id": "3xciicd4ke3c1", "id": 367352065}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/i8fhV29y5mREvkgFUjhaNZluFujvs6eOKjcb0h8dYD0.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701314109.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/1878c2k", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1878c2k", "is_robot_indexable": true, "report_reasons": null, "author": "fatboycraig", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1878c2k/how_are_brand_new_exos_drives_soldpackagedshipped/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/1878c2k", "subreddit_subscribers": 715344, "created_utc": 1701314109.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Here is a fairly robust way to ensure a drive safe to put into service. I have tested this before and caught drives that would have failed shortly after put into prod, and some that would of after it was more than half full.\n\n1) Check S.M.A.R.T Info: Confirm no (0) Seek Error Rate, Read Error Rate, Reallocated Sector Count, Uncorrectable Sector Count\n\n2) Run Short S.M.A.R.T test\n\n3) Repeat Step 1\n\n4) Run Conveyance S.M.A.R.T test\n\n5) Repeat Step 1\n\n6) Run Destructive Badblocks test (read and write)\n\n7) Repeat Step 1\n\n8) Perform a FULL Format (Overwrite with Zeros)\n\n9) Repeat Step 1\n\n10) Run Extended S.M.A.R.T test\n\n11)  Repeat Step 1\n\nReturn the drive if either of the following is true:\n\nA) The formatting speed drops below 80MB/s by more than 10MB/s (my defective one was ~40MB/s from first power-on)\n\nB) The S.M.A.R.T tests show error count increasing at any step\n\n\n\nIt is also highly advisable to stagger the testing (and repeat some) if you plan on using multiple drives in a pool/raid config. This way the wear on the drives differ, to reduce the likelihood of them failing at the same time. For example, I re-ran either the Full format or badblocks test on some of the drives so some drives have 48 hours of testing, some have 72, some have 96. This way, the chances of a multiple drive failures during rebuild is lower.", "author_fullname": "t2_8oh1uiuf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Must do's for a new hard drive", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1873un5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701301666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is a fairly robust way to ensure a drive safe to put into service. I have tested this before and caught drives that would have failed shortly after put into prod, and some that would of after it was more than half full.&lt;/p&gt;\n\n&lt;p&gt;1) Check S.M.A.R.T Info: Confirm no (0) Seek Error Rate, Read Error Rate, Reallocated Sector Count, Uncorrectable Sector Count&lt;/p&gt;\n\n&lt;p&gt;2) Run Short S.M.A.R.T test&lt;/p&gt;\n\n&lt;p&gt;3) Repeat Step 1&lt;/p&gt;\n\n&lt;p&gt;4) Run Conveyance S.M.A.R.T test&lt;/p&gt;\n\n&lt;p&gt;5) Repeat Step 1&lt;/p&gt;\n\n&lt;p&gt;6) Run Destructive Badblocks test (read and write)&lt;/p&gt;\n\n&lt;p&gt;7) Repeat Step 1&lt;/p&gt;\n\n&lt;p&gt;8) Perform a FULL Format (Overwrite with Zeros)&lt;/p&gt;\n\n&lt;p&gt;9) Repeat Step 1&lt;/p&gt;\n\n&lt;p&gt;10) Run Extended S.M.A.R.T test&lt;/p&gt;\n\n&lt;p&gt;11)  Repeat Step 1&lt;/p&gt;\n\n&lt;p&gt;Return the drive if either of the following is true:&lt;/p&gt;\n\n&lt;p&gt;A) The formatting speed drops below 80MB/s by more than 10MB/s (my defective one was ~40MB/s from first power-on)&lt;/p&gt;\n\n&lt;p&gt;B) The S.M.A.R.T tests show error count increasing at any step&lt;/p&gt;\n\n&lt;p&gt;It is also highly advisable to stagger the testing (and repeat some) if you plan on using multiple drives in a pool/raid config. This way the wear on the drives differ, to reduce the likelihood of them failing at the same time. For example, I re-ran either the Full format or badblocks test on some of the drives so some drives have 48 hours of testing, some have 72, some have 96. This way, the chances of a multiple drive failures during rebuild is lower.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1873un5", "is_robot_indexable": true, "report_reasons": null, "author": "EMC2DATA592", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1873un5/must_dos_for_a_new_hard_drive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1873un5/must_dos_for_a_new_hard_drive/", "subreddit_subscribers": 715344, "created_utc": 1701301666.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im wondering if anyone has ever scraped pricing data for the major supermarkets?\n\nWith the cost of living crisis currently im interested to see which products have increased dramatically and if theres any correlation across brands or sectors.\n\nThanks!", "author_fullname": "t2_eq196", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UK Datahoarders - Has anyone ever scraped supermarket price files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187ivbk", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701351907.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im wondering if anyone has ever scraped pricing data for the major supermarkets?&lt;/p&gt;\n\n&lt;p&gt;With the cost of living crisis currently im interested to see which products have increased dramatically and if theres any correlation across brands or sectors.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187ivbk", "is_robot_indexable": true, "report_reasons": null, "author": "jpjapers", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187ivbk/uk_datahoarders_has_anyone_ever_scraped/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187ivbk/uk_datahoarders_has_anyone_ever_scraped/", "subreddit_subscribers": 715344, "created_utc": 1701351907.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_32ywoabe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How often does server part deals restock / does it restock on recertified drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 62, "top_awarded_type": null, "hide_score": false, "name": "t3_187bar0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/iwCBiRezStZy1nV0nrfwes-tcwpuobFCoagbHyQe4kw.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701323579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/ebuzyuw8cf3c1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?auto=webp&amp;s=0effe9c41dc3e4ae189bab531c0d23d4ef7f21d3", "width": 1892, "height": 848}, "resolutions": [{"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6a0584255087489f9c1fb9b7dd91cae942c93b8c", "width": 108, "height": 48}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c787f1c3ea4535aba5eacc760512f31be6189260", "width": 216, "height": 96}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b098bc52e3afdb699f739b6c4c44bbd2c70ce50", "width": 320, "height": 143}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8be3668603d7fe909e7253220bd484fd5e7e7799", "width": 640, "height": 286}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b374578fa8f6620e443c94603391ac8f9c2a3e4", "width": 960, "height": 430}, {"url": "https://preview.redd.it/ebuzyuw8cf3c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2ee083859db6095bf2d13a935dbbe0830534e72c", "width": 1080, "height": 484}], "variants": {}, "id": "pL5FDZKw5hYWcV0CxlhT5h9J4amN0SyKGWZLUdLT0H0"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "14TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187bar0", "is_robot_indexable": true, "report_reasons": null, "author": "Beanconscriptog", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/187bar0/how_often_does_server_part_deals_restock_does_it/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/ebuzyuw8cf3c1.png", "subreddit_subscribers": 715344, "created_utc": 1701323579.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My HDD is failing and manually copying files to an external HDD is taking forever. We're talking about KB/s transfer speeds. I have around 1.3TB in it, but only really need around 400-500gb. I'm afraid this method is only hastening its death.\n\nI used to use a dos program, Norton Ghost, to clone partitions. Would a similar program be better for backing up my files vs copy/pasting? I feel like the manual copying is wearing the drive out faster. On the other hand, cloning might just copy over corrupted files on bad sectors and all (I know nothing about this so I could be wrong).\n\nThanks in advance.", "author_fullname": "t2_bgpdn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it better to clone a failing HDD than manually backing it up via copy/paste?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187aus1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701322095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My HDD is failing and manually copying files to an external HDD is taking forever. We&amp;#39;re talking about KB/s transfer speeds. I have around 1.3TB in it, but only really need around 400-500gb. I&amp;#39;m afraid this method is only hastening its death.&lt;/p&gt;\n\n&lt;p&gt;I used to use a dos program, Norton Ghost, to clone partitions. Would a similar program be better for backing up my files vs copy/pasting? I feel like the manual copying is wearing the drive out faster. On the other hand, cloning might just copy over corrupted files on bad sectors and all (I know nothing about this so I could be wrong).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187aus1", "is_robot_indexable": true, "report_reasons": null, "author": "imanol1898", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187aus1/is_it_better_to_clone_a_failing_hdd_than_manually/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187aus1/is_it_better_to_clone_a_failing_hdd_than_manually/", "subreddit_subscribers": 715344, "created_utc": 1701322095.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello,\n\nSo I've had this janky QNAP NAS for years. It's janky because my apartment got hit by lightning a few years ago and fried one of the ethernet ports.\n\nEver since then, I had been using it (against my better judgment) because the other ethernet port still worked so I was able to keep using it.\n\nI finally got a new NAS (Synology) and two new drives and migrated all my data over. Whew!\n\nBut now I've got two empty slots so I figured I would move over the newer of 2 of the 4 drives from my old QNAP NAS.\n\nBut the QNAP has a weird issue... it never shuts down properly, possibly related to the lightning. Any time you tell it to shut down, it just reboots. So I basically told it to shut down and once it seemed close to rebooting, I just pulled the power cord.\n\nThen I moved those two newer drives over to the Synology NAS which is when it told me that one drive has *six* bad sectors. The drives are encrypted by QNAP's OS if that matters. So I was planning to wipe them anyway.\n\n1. Is it likely that the improper shutdowns contributed to the bad sectors?\n2. Is it likely that wiping the drives could solve this somehow?\n3. Should I not use that drive?\n\nThanks", "author_fullname": "t2_1ppymy6t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drive reports *6* bad sectors after moving from old NAS to new NAS.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187abcp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701320306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;ve had this janky QNAP NAS for years. It&amp;#39;s janky because my apartment got hit by lightning a few years ago and fried one of the ethernet ports.&lt;/p&gt;\n\n&lt;p&gt;Ever since then, I had been using it (against my better judgment) because the other ethernet port still worked so I was able to keep using it.&lt;/p&gt;\n\n&lt;p&gt;I finally got a new NAS (Synology) and two new drives and migrated all my data over. Whew!&lt;/p&gt;\n\n&lt;p&gt;But now I&amp;#39;ve got two empty slots so I figured I would move over the newer of 2 of the 4 drives from my old QNAP NAS.&lt;/p&gt;\n\n&lt;p&gt;But the QNAP has a weird issue... it never shuts down properly, possibly related to the lightning. Any time you tell it to shut down, it just reboots. So I basically told it to shut down and once it seemed close to rebooting, I just pulled the power cord.&lt;/p&gt;\n\n&lt;p&gt;Then I moved those two newer drives over to the Synology NAS which is when it told me that one drive has &lt;em&gt;six&lt;/em&gt; bad sectors. The drives are encrypted by QNAP&amp;#39;s OS if that matters. So I was planning to wipe them anyway.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is it likely that the improper shutdowns contributed to the bad sectors?&lt;/li&gt;\n&lt;li&gt;Is it likely that wiping the drives could solve this somehow?&lt;/li&gt;\n&lt;li&gt;Should I not use that drive?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187abcp", "is_robot_indexable": true, "report_reasons": null, "author": "CactusBoyScout", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187abcp/drive_reports_6_bad_sectors_after_moving_from_old/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187abcp/drive_reports_6_bad_sectors_after_moving_from_old/", "subreddit_subscribers": 715344, "created_utc": 1701320306.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have an Unraid system with 14 drives in the NAS, and a DAS with 8 drives connected.\n\nCurrently I have 6 drives in NAS connected via motherboard sata, and the other 8 via a pcie to sata card.\n\nI then have an 8e HBA in IT mode connected to a SFF-8087 to SFF-8088 adapter in the DAS, connecting to the 8 drives.\n\nI am changing the motherboard and CPU, and stupidly brought a motherboard with only 4 sata, 1 x pcie x16, and 1 pcie x4.\n\nIs there a SAS expander I could get, along with an internal HBA, that could connect to the internal drives of the NAS, and then have external ports to go to the DAS? I looked at the Adaptec 82885T, and apparently the 2 external ports are input only. Would it maybe better to get 2 of those, 1 in NAS, and 1 in the DAS, and use my current HBA 8e to connect to them both?", "author_fullname": "t2_aodccc8g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SAS expander options", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187br0y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701325077.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an Unraid system with 14 drives in the NAS, and a DAS with 8 drives connected.&lt;/p&gt;\n\n&lt;p&gt;Currently I have 6 drives in NAS connected via motherboard sata, and the other 8 via a pcie to sata card.&lt;/p&gt;\n\n&lt;p&gt;I then have an 8e HBA in IT mode connected to a SFF-8087 to SFF-8088 adapter in the DAS, connecting to the 8 drives.&lt;/p&gt;\n\n&lt;p&gt;I am changing the motherboard and CPU, and stupidly brought a motherboard with only 4 sata, 1 x pcie x16, and 1 pcie x4.&lt;/p&gt;\n\n&lt;p&gt;Is there a SAS expander I could get, along with an internal HBA, that could connect to the internal drives of the NAS, and then have external ports to go to the DAS? I looked at the Adaptec 82885T, and apparently the 2 external ports are input only. Would it maybe better to get 2 of those, 1 in NAS, and 1 in the DAS, and use my current HBA 8e to connect to them both?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "To the Cloud!", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187br0y", "is_robot_indexable": true, "report_reasons": null, "author": "minimaddnz", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/187br0y/sas_expander_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187br0y/sas_expander_options/", "subreddit_subscribers": 715344, "created_utc": 1701325077.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m in search of a small, travel-sized way to store massive amounts of data, primarily 4K remux movies. I\u2019ll be going to Antarctica, where my internet will be limited. I\u2019ve been told that they have plenty of standard-def movies, a handful of blue rays, and a small-ish collection of HD movies digitally. What they\u2019re missing, they say, is full-bitrate 4K movies, and I want to bring some. I know these films can grow upwards of 30-50GB.\n\nI have been looking at M.2 NVME drives\u2026 grabbing a few 2TB drives and enclosures seem like a good option when you compare $/GB, I think I should be able to squeeze about 40 movies on each (assuming 50gb) drive. Is there a better option? Spinning disk drives are not an option due to the size, weight, and fragility, otherwise I\u2019d grab a 20tb HDD and be done with it.", "author_fullname": "t2_o87j0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Travel-Sized Data Hoarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_187smk5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701377059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in search of a small, travel-sized way to store massive amounts of data, primarily 4K remux movies. I\u2019ll be going to Antarctica, where my internet will be limited. I\u2019ve been told that they have plenty of standard-def movies, a handful of blue rays, and a small-ish collection of HD movies digitally. What they\u2019re missing, they say, is full-bitrate 4K movies, and I want to bring some. I know these films can grow upwards of 30-50GB.&lt;/p&gt;\n\n&lt;p&gt;I have been looking at M.2 NVME drives\u2026 grabbing a few 2TB drives and enclosures seem like a good option when you compare $/GB, I think I should be able to squeeze about 40 movies on each (assuming 50gb) drive. Is there a better option? Spinning disk drives are not an option due to the size, weight, and fragility, otherwise I\u2019d grab a 20tb HDD and be done with it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187smk5", "is_robot_indexable": true, "report_reasons": null, "author": "shootingcharlie8", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187smk5/travelsized_data_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187smk5/travelsized_data_hoarding/", "subreddit_subscribers": 715344, "created_utc": 1701377059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My issue is very simple.\n\nI run a small sports team and I would like to set up a cloud storage for videos of practice, that the athletes could watch in their spare time.\n\nWhat are my options? So far it has been unbelievably complicated to do and I am not that old.\n\nTrying to be the most efficient data hoarder that has ever lived.\n\nThanks again.", "author_fullname": "t2_19hby7xp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most simple cloud storage option?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_187rng8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701374603.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My issue is very simple.&lt;/p&gt;\n\n&lt;p&gt;I run a small sports team and I would like to set up a cloud storage for videos of practice, that the athletes could watch in their spare time.&lt;/p&gt;\n\n&lt;p&gt;What are my options? So far it has been unbelievably complicated to do and I am not that old.&lt;/p&gt;\n\n&lt;p&gt;Trying to be the most efficient data hoarder that has ever lived.&lt;/p&gt;\n\n&lt;p&gt;Thanks again.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187rng8", "is_robot_indexable": true, "report_reasons": null, "author": "imaverycringeguy", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187rng8/most_simple_cloud_storage_option/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187rng8/most_simple_cloud_storage_option/", "subreddit_subscribers": 715344, "created_utc": 1701374603.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m trying to expand my ZFS pool on the cheap, and am torn between shucking the $150 dollar Seagate 14TB externals vs buying a manufactured refurbished Seagate 16TB from ServerPartsDeals for the same price. What would y\u2019all\u2019s recommendation be based on experience or longevity data? \n\nThanks for your help!", "author_fullname": "t2_2mooplap", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shucked vs Refurbished", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187lveh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701359934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to expand my ZFS pool on the cheap, and am torn between shucking the $150 dollar Seagate 14TB externals vs buying a manufactured refurbished Seagate 16TB from ServerPartsDeals for the same price. What would y\u2019all\u2019s recommendation be based on experience or longevity data? &lt;/p&gt;\n\n&lt;p&gt;Thanks for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187lveh", "is_robot_indexable": true, "report_reasons": null, "author": "techromancer1", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187lveh/shucked_vs_refurbished/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187lveh/shucked_vs_refurbished/", "subreddit_subscribers": 715344, "created_utc": 1701359934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone,\n\n&amp;#x200B;\n\nI would like some advice on how to archive a large number of download links from the [https://uloz.to](https://uloz.to) website.\n\n&amp;#x200B;\n\nIt's a public file sharing website which unfortunately disables searching between their files and then it should only be possible to download if you have a link to the file you want to download.\n\nIs there any way to back up the links to the files that are in their database? Is there a script that can go through it one by one and write them to an excel file?\n\n&amp;#x200B;\n\nThank you for your help.", "author_fullname": "t2_f6ydydphm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to backup links to files on uloz.to", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187epf1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701340394.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701336801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I would like some advice on how to archive a large number of download links from the &lt;a href=\"https://uloz.to\"&gt;https://uloz.to&lt;/a&gt; website.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a public file sharing website which unfortunately disables searching between their files and then it should only be possible to download if you have a link to the file you want to download.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to back up the links to the files that are in their database? Is there a script that can go through it one by one and write them to an excel file?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3Q97mNEkTbNb6kLsRXcZnSnjZDfXCltsjWeb7wsPKMs.jpg?auto=webp&amp;s=1c3764a4072f31b6ea14d42c813aee98e03f8c6a", "width": 600, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/3Q97mNEkTbNb6kLsRXcZnSnjZDfXCltsjWeb7wsPKMs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0c3c1ed6e2d39c413f1fc4a7628138ad38863d3", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/3Q97mNEkTbNb6kLsRXcZnSnjZDfXCltsjWeb7wsPKMs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7094c96b5ad9a7c958820029341084039e88097e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/3Q97mNEkTbNb6kLsRXcZnSnjZDfXCltsjWeb7wsPKMs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24bc78655d394adaba1e32e7faad7399bed4dbf6", "width": 320, "height": 320}], "variants": {}, "id": "pRqK59bWm7VS-3qKQR_j8gMutFJRCzJjMa9gYDelECk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187epf1", "is_robot_indexable": true, "report_reasons": null, "author": "FeelsGoodBlok", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187epf1/how_to_backup_links_to_files_on_ulozto/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187epf1/how_to_backup_links_to_files_on_ulozto/", "subreddit_subscribers": 715344, "created_utc": 1701336801.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone shucked one of these?\n\nAre you able to use the drive or is it encrypted?\n\nWhat drive is inside? One site says it's an Exos X20z which is HSMR.\n\n&amp;#x200B;\n\nThanks!", "author_fullname": "t2_3t236", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "20TB Seagate One Touch Hub shuck", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18715ux", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701294754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone shucked one of these?&lt;/p&gt;\n\n&lt;p&gt;Are you able to use the drive or is it encrypted?&lt;/p&gt;\n\n&lt;p&gt;What drive is inside? One site says it&amp;#39;s an Exos X20z which is HSMR.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "18715ux", "is_robot_indexable": true, "report_reasons": null, "author": "xkegsx", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/18715ux/20tb_seagate_one_touch_hub_shuck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/18715ux/20tb_seagate_one_touch_hub_shuck/", "subreddit_subscribers": 715344, "created_utc": 1701294754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't know of a repair tool for an imgc file and here is the error message I'm getting when trying to write with the HDD Raw Copy Tool  Access violation at address 0040608 in module \"HDDRawCopy1.20Portable.exe\". Read of address 8591F84A. My question is how do I find that and repair it so it can get past the 74% mark when writing?\n\nFor more context [https://www.reddit.com/r/datarecovery/comments/186yhtt/how\\_do\\_i\\_repair\\_this\\_corrupted\\_chunk\\_of\\_data/](https://www.reddit.com/r/datarecovery/comments/186yhtt/how_do_i_repair_this_corrupted_chunk_of_data/)", "author_fullname": "t2_bkin1n0j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I use WinHex to repair a corrupted chunk of data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1870k9a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701293226.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know of a repair tool for an imgc file and here is the error message I&amp;#39;m getting when trying to write with the HDD Raw Copy Tool  Access violation at address 0040608 in module &amp;quot;HDDRawCopy1.20Portable.exe&amp;quot;. Read of address 8591F84A. My question is how do I find that and repair it so it can get past the 74% mark when writing?&lt;/p&gt;\n\n&lt;p&gt;For more context &lt;a href=\"https://www.reddit.com/r/datarecovery/comments/186yhtt/how_do_i_repair_this_corrupted_chunk_of_data/\"&gt;https://www.reddit.com/r/datarecovery/comments/186yhtt/how_do_i_repair_this_corrupted_chunk_of_data/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1870k9a", "is_robot_indexable": true, "report_reasons": null, "author": "BikesOnDykes", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1870k9a/how_do_i_use_winhex_to_repair_a_corrupted_chunk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1870k9a/how_do_i_use_winhex_to_repair_a_corrupted_chunk/", "subreddit_subscribers": 715344, "created_utc": 1701293226.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Recently I have ordered two Seagate 20tb Exos x20 drives. I bought them as [manufacturer recertified drives](https://serverpartdeals.com/products/seagate-exos-x20-st20000nm007d-20tb-7-2k-rpm-sata-6gb-s-3-5-recertified-hard-drive) from [serverpartdeals.com](https://serverpartdeals.com). I actually own 4 other 20tb Exos drives that I did not purchase recertified. When I installed the first drive, I turned on my PC and it stalled at the BIOS for a minute or two, I could hear the hard drive spinning and making interesting noises. It sounding like it was scanning the entirety of the drive. After a few moments, the drive appeared in the BIOS. I made my way into Windows and the disc management application refused to initialize the drive stating and I/O error. This was not a big problem because Windows didn't want to initialize my other Exos drives and I needed to use Partition Wizard to initialize and format them. However Partition Wizard just flat out says Bad Drive and won't let me even try to do anything with the drive. I installed SeaTools and it wouldn't even try to run a single test. I realize that bad hard drives do exist, and this is something that happens. But my issue is that BOTH of the hard drives that I purchased behave exactly the same. Am I missing something obvious? Did I really get two bad hard drives at once? I'm still within the return period so I'm not really concerned about being able to recoup my investment. I would just like to know if I'm missing something obvious. Any help or wisdom is appreciated. Thank you all in advance.", "author_fullname": "t2_9nxdp6ve", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did I receive TWO bad drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_187sh4x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701376964.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701376676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently I have ordered two Seagate 20tb Exos x20 drives. I bought them as &lt;a href=\"https://serverpartdeals.com/products/seagate-exos-x20-st20000nm007d-20tb-7-2k-rpm-sata-6gb-s-3-5-recertified-hard-drive\"&gt;manufacturer recertified drives&lt;/a&gt; from &lt;a href=\"https://serverpartdeals.com\"&gt;serverpartdeals.com&lt;/a&gt;. I actually own 4 other 20tb Exos drives that I did not purchase recertified. When I installed the first drive, I turned on my PC and it stalled at the BIOS for a minute or two, I could hear the hard drive spinning and making interesting noises. It sounding like it was scanning the entirety of the drive. After a few moments, the drive appeared in the BIOS. I made my way into Windows and the disc management application refused to initialize the drive stating and I/O error. This was not a big problem because Windows didn&amp;#39;t want to initialize my other Exos drives and I needed to use Partition Wizard to initialize and format them. However Partition Wizard just flat out says Bad Drive and won&amp;#39;t let me even try to do anything with the drive. I installed SeaTools and it wouldn&amp;#39;t even try to run a single test. I realize that bad hard drives do exist, and this is something that happens. But my issue is that BOTH of the hard drives that I purchased behave exactly the same. Am I missing something obvious? Did I really get two bad hard drives at once? I&amp;#39;m still within the return period so I&amp;#39;m not really concerned about being able to recoup my investment. I would just like to know if I&amp;#39;m missing something obvious. Any help or wisdom is appreciated. Thank you all in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/i0jmgpbukSNSov8Pj7F7-vcZtzxjWx2Eu7q6ELsHN5k.jpg?auto=webp&amp;s=bb53d22d3fe387e810d859b20c7ea1dba5579525", "width": 1000, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/i0jmgpbukSNSov8Pj7F7-vcZtzxjWx2Eu7q6ELsHN5k.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1eb8ce1774db56924fdeaee4c86cd29427554baf", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/i0jmgpbukSNSov8Pj7F7-vcZtzxjWx2Eu7q6ELsHN5k.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6893f4210cd8fda233a5bff966a788103a0ede1e", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/i0jmgpbukSNSov8Pj7F7-vcZtzxjWx2Eu7q6ELsHN5k.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=164c0ab2ff3a0b398951c83e1bbdb861b6286cbd", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/i0jmgpbukSNSov8Pj7F7-vcZtzxjWx2Eu7q6ELsHN5k.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d89d9bab333e3bab1f88cd5cb6672c1833864039", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/i0jmgpbukSNSov8Pj7F7-vcZtzxjWx2Eu7q6ELsHN5k.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3555cf24c8fdf8db7d8cd94e6c3aaecb95af8c99", "width": 960, "height": 960}], "variants": {}, "id": "tgE7Ol8sbAjViviG8qXATdT_pNs_MO8-hAMI0nwz3ek"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187sh4x", "is_robot_indexable": true, "report_reasons": null, "author": "ACParker", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187sh4x/did_i_receive_two_bad_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187sh4x/did_i_receive_two_bad_drives/", "subreddit_subscribers": 715344, "created_utc": 1701376676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have just got a CRC error on an SSD running as a cache pool, raid 0, on unraid. Should I be concerned and RMA the SSD or ignore it? I have just started an extended SMART test.", "author_fullname": "t2_ju7i9anj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CRC error", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_187s1ni", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701375602.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have just got a CRC error on an SSD running as a cache pool, raid 0, on unraid. Should I be concerned and RMA the SSD or ignore it? I have just started an extended SMART test.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187s1ni", "is_robot_indexable": true, "report_reasons": null, "author": "ParticularGiraffe174", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187s1ni/crc_error/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187s1ni/crc_error/", "subreddit_subscribers": 715344, "created_utc": 1701375602.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently added a Sabrent Dual Bay Raid HDD enclosure to my setup for backups. \n\nThe device has a USB-A 3.2 port on the front that I decided to test with a T7 SSD. \n\nWhat I\u2019ve learned is that the read/write speeds are not equivalent when using a random USB A adaptor that I had laying on a shelf. As in, the Read ~700MBs and the Write ~125MBs.\n\nThe port performs well in other scenarios (R-700 W-700) So I\u2019m curious, can the same USB adaptor have differing R/W performance or is mine a dud?", "author_fullname": "t2_4h0rbq1j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "USB Read/Write", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_187rxj5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701375316.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently added a Sabrent Dual Bay Raid HDD enclosure to my setup for backups. &lt;/p&gt;\n\n&lt;p&gt;The device has a USB-A 3.2 port on the front that I decided to test with a T7 SSD. &lt;/p&gt;\n\n&lt;p&gt;What I\u2019ve learned is that the read/write speeds are not equivalent when using a random USB A adaptor that I had laying on a shelf. As in, the Read ~700MBs and the Write ~125MBs.&lt;/p&gt;\n\n&lt;p&gt;The port performs well in other scenarios (R-700 W-700) So I\u2019m curious, can the same USB adaptor have differing R/W performance or is mine a dud?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187rxj5", "is_robot_indexable": true, "report_reasons": null, "author": "ZakkLacksRhythm", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187rxj5/usb_readwrite/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187rxj5/usb_readwrite/", "subreddit_subscribers": 715344, "created_utc": 1701375316.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just got a WD 18TB External Hard Drive that I basically want to use as part of my \"server\". I put it in quotes, because it's not what you would call a proper server. It's pretty much just an external drive connected to a 2010 MacBook Pro (2010MBP) that is connected to our home network. \n\nI'm a MacOS user. My main laptop is a M1 MacBook Pro. I currently have two 2TB drives connected to the 2010MBP via USB. This new 18TB drive will be replacing the two 2TB drives. These drives are where I store my music files for iTunes Match and my movie files for my local Plex server. The rest of the files are random work files and family files from over the years.\n\nMy overall question is what do I need to do to test this new 18TB drive to make sure it's OK to start using? I've see a lot of talk about testing drives before using. But please keep in mind, I'm not really a command line guy. To be honest, a lot of the command line talk goes over my head. I can run commands in Terminal if I'm given the exact commands to type or I can follow guides step by step. But I am very much a beginner when it comes to the command line. \n\nI've seen mention of CrystalDiskInfo. But unfortunately, there is no Mac app. I did see some software that PC Magazine recommended for the Mac called DriveDX (http://binaryfruit.com/driverx). \n\nIf DriveDX isn't good enough, is anyone able to guide me step by step in Terminal to test the drive? If that's asking a lot, I apologize. Or even a site that I can read how to do this on a Mac? \n\nThank you. I appreciate any help or guidance. \n\nTL;DR How can I test a new WD 18TB External Hard Drive using a MacOS or Terminal before using it?", "author_fullname": "t2_lu9n5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I prepare and/or test a new WD 18TB External Hard Drive? (MacOS/Unix)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_187rwam", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701375230.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just got a WD 18TB External Hard Drive that I basically want to use as part of my &amp;quot;server&amp;quot;. I put it in quotes, because it&amp;#39;s not what you would call a proper server. It&amp;#39;s pretty much just an external drive connected to a 2010 MacBook Pro (2010MBP) that is connected to our home network. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a MacOS user. My main laptop is a M1 MacBook Pro. I currently have two 2TB drives connected to the 2010MBP via USB. This new 18TB drive will be replacing the two 2TB drives. These drives are where I store my music files for iTunes Match and my movie files for my local Plex server. The rest of the files are random work files and family files from over the years.&lt;/p&gt;\n\n&lt;p&gt;My overall question is what do I need to do to test this new 18TB drive to make sure it&amp;#39;s OK to start using? I&amp;#39;ve see a lot of talk about testing drives before using. But please keep in mind, I&amp;#39;m not really a command line guy. To be honest, a lot of the command line talk goes over my head. I can run commands in Terminal if I&amp;#39;m given the exact commands to type or I can follow guides step by step. But I am very much a beginner when it comes to the command line. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen mention of CrystalDiskInfo. But unfortunately, there is no Mac app. I did see some software that PC Magazine recommended for the Mac called DriveDX (&lt;a href=\"http://binaryfruit.com/driverx\"&gt;http://binaryfruit.com/driverx&lt;/a&gt;). &lt;/p&gt;\n\n&lt;p&gt;If DriveDX isn&amp;#39;t good enough, is anyone able to guide me step by step in Terminal to test the drive? If that&amp;#39;s asking a lot, I apologize. Or even a site that I can read how to do this on a Mac? &lt;/p&gt;\n\n&lt;p&gt;Thank you. I appreciate any help or guidance. &lt;/p&gt;\n\n&lt;p&gt;TL;DR How can I test a new WD 18TB External Hard Drive using a MacOS or Terminal before using it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187rwam", "is_robot_indexable": true, "report_reasons": null, "author": "blackicehawk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187rwam/how_can_i_prepare_andor_test_a_new_wd_18tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187rwam/how_can_i_prepare_andor_test_a_new_wd_18tb/", "subreddit_subscribers": 715344, "created_utc": 1701375230.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m in search of a small, travel-sized way to store massive amounts of data, primarily 4K remux movies. I\u2019ll be going to Antarctica, where my internet will be limited. I\u2019ve been told that they have plenty of standard-def movies, a handful of blue rays, and a small-ish collection of HD movies digitally. What they\u2019re missing, they say, is full-bitrate 4K movies, and I want to bring some. I know these films can grow upwards of 30-50GB.\n\nI have been looking at M.2 NVME drives\u2026 grabbing a few 2TB drives and enclosures seem like a good option, I think I should be able to squeeze about 40 movies on each (assuming 50gb) drive. Is there a better option? Spinning disk drives are not an option, otherwise I\u2019d grab a 20tb HDD and be done with it.\nEdit: I see the typo in the title\u2026 I can\u2019t change it\u2026 it\u2019s supposed to say Travel-Sized Data Hoarding. ", "author_fullname": "t2_vdxjkybi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Travel-Sized says hoarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_187rse3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701374953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m in search of a small, travel-sized way to store massive amounts of data, primarily 4K remux movies. I\u2019ll be going to Antarctica, where my internet will be limited. I\u2019ve been told that they have plenty of standard-def movies, a handful of blue rays, and a small-ish collection of HD movies digitally. What they\u2019re missing, they say, is full-bitrate 4K movies, and I want to bring some. I know these films can grow upwards of 30-50GB.&lt;/p&gt;\n\n&lt;p&gt;I have been looking at M.2 NVME drives\u2026 grabbing a few 2TB drives and enclosures seem like a good option, I think I should be able to squeeze about 40 movies on each (assuming 50gb) drive. Is there a better option? Spinning disk drives are not an option, otherwise I\u2019d grab a 20tb HDD and be done with it.\nEdit: I see the typo in the title\u2026 I can\u2019t change it\u2026 it\u2019s supposed to say Travel-Sized Data Hoarding. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187rse3", "is_robot_indexable": true, "report_reasons": null, "author": "DisposabIeHuman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187rse3/travelsized_says_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187rse3/travelsized_says_hoarding/", "subreddit_subscribers": 715344, "created_utc": 1701374953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all,\n\nI'm looking for recommendations for a relatively affordable, ultra high capacity, 3.5\" HDD. The one listed below that seems to fit the bill is the WD 22TB Gold. I know there are some enterprise level HDDs in the $40k range but thats far outside my budget.  \n\nThe thing I'm not sure about is stability. I've had WD portable HDDs fail on me before, so, honestly, I'm not a huge fan. There were mitigating factors which could have played a part so I'm willing to listen to the crowd. I do like Seagate. I've only used thair SDDs, though, so it's apples and oranges. \n\nStability, Largest capacity, Price. \n\n&amp;#x200B;\n\nThis is for backup but it really needs to be reliable.\n\n&amp;#x200B;\n\nThoughts?\n\n&amp;#x200B;\n\n[https://www.amazon.com/Seagate-IronWolf-Enterprise-Internal-Drive/dp/B0B94MF4LP/ref=psdc\\_1254762011\\_t1\\_B0CCPPQG1W?th=1](https://www.amazon.com/Seagate-IronWolf-Enterprise-Internal-Drive/dp/B0B94MF4LP/ref=psdc_1254762011_t1_B0CCPPQG1W?th=1)\n\n[https://www.amazon.com/Western-Digital-20TB-Internal-Drive/dp/B0B5W1CQ8W/ref=psdc\\_1254762011\\_t2\\_B0CCPPQG1W?th=1](https://www.amazon.com/Western-Digital-20TB-Internal-Drive/dp/B0B5W1CQ8W/ref=psdc_1254762011_t2_B0CCPPQG1W?th=1)\n\n[https://www.amazon.com/Western-Digital-Enterprise-Class-Internal/dp/B0B5W2ZM58/ref=psdc\\_1254762011\\_t1\\_B0B5W1CQ8W?th=1](https://www.amazon.com/Western-Digital-Enterprise-Class-Internal/dp/B0B5W2ZM58/ref=psdc_1254762011_t1_B0B5W1CQ8W?th=1)\n\n&amp;#x200B;", "author_fullname": "t2_7hpjfkdgq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for recommendations for Large Capacity 3.5\" HDD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187q05b", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701370421.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for recommendations for a relatively affordable, ultra high capacity, 3.5&amp;quot; HDD. The one listed below that seems to fit the bill is the WD 22TB Gold. I know there are some enterprise level HDDs in the $40k range but thats far outside my budget.  &lt;/p&gt;\n\n&lt;p&gt;The thing I&amp;#39;m not sure about is stability. I&amp;#39;ve had WD portable HDDs fail on me before, so, honestly, I&amp;#39;m not a huge fan. There were mitigating factors which could have played a part so I&amp;#39;m willing to listen to the crowd. I do like Seagate. I&amp;#39;ve only used thair SDDs, though, so it&amp;#39;s apples and oranges. &lt;/p&gt;\n\n&lt;p&gt;Stability, Largest capacity, Price. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This is for backup but it really needs to be reliable.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thoughts?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/Seagate-IronWolf-Enterprise-Internal-Drive/dp/B0B94MF4LP/ref=psdc_1254762011_t1_B0CCPPQG1W?th=1\"&gt;https://www.amazon.com/Seagate-IronWolf-Enterprise-Internal-Drive/dp/B0B94MF4LP/ref=psdc_1254762011_t1_B0CCPPQG1W?th=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/Western-Digital-20TB-Internal-Drive/dp/B0B5W1CQ8W/ref=psdc_1254762011_t2_B0CCPPQG1W?th=1\"&gt;https://www.amazon.com/Western-Digital-20TB-Internal-Drive/dp/B0B5W1CQ8W/ref=psdc_1254762011_t2_B0CCPPQG1W?th=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.amazon.com/Western-Digital-Enterprise-Class-Internal/dp/B0B5W2ZM58/ref=psdc_1254762011_t1_B0B5W1CQ8W?th=1\"&gt;https://www.amazon.com/Western-Digital-Enterprise-Class-Internal/dp/B0B5W2ZM58/ref=psdc_1254762011_t1_B0B5W1CQ8W?th=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187q05b", "is_robot_indexable": true, "report_reasons": null, "author": "BrokieTrader", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187q05b/looking_for_recommendations_for_large_capacity_35/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187q05b/looking_for_recommendations_for_large_capacity_35/", "subreddit_subscribers": 715344, "created_utc": 1701370421.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, in purchasing or re-purposing used drives to add to a large veeam repository, I've run into various ways that drives have been locked. So far, I've seen three different ways that sas drives have been locked in a way that I could not initially use them in my KTL3 shelves attached to TrueNAS SCALE servers.\n\n&amp;nbsp;\n\n**1. SED locking:**\nI have disks from retired tegile disk arrays. I went to spin these drives up, and they were basically unreadable, and dmesg was spitting out a bunch of errors. You can identify these disks physically because they have a PSID printed on the top of them.\nTo clear these locks, you can use the sedutil-cli utility to wipe the disk and clear the lock:\n\n    sedutil-cli --yesIreallywanttoERASEALLmydatausingthePSID &lt;PSID key goes here&gt; /dev/sdr\n\nThis process ran in seconds, and access was gained to the drive.\n\n&amp;nbsp;\n\n**2. T10-DIF:**\nThis wasn't obvious to me, as I've seen netapp formatted drives that won't read because linux won't read 520 byte block size drives. These drives, tho, reported as 512B formatted drives, but I couldn't read/write them. Upon further inspection, I found that they were indeed formatted at 512B blocks, but then there was another 8 bits of data, resulting in 520B blocks. I was able to reformat them with:\n\n    sg_format --format --size=512 /dev/sdl\n\nThis process takes time, nearly 12-24 hours, per drive (12TB disks). After the format was complete, i pulled and reinserted the drives into the shelf and was able to access them successfully. tmux was helpful in formatting the disks in parallel so i didn't have to wait for each one to finish before starting the next.\n\n&amp;nbsp;\n\n**3. SCSI reservation:**\nThis was even more obscure. I popped a drive in the shelf, and unlike the other drives that spun up and the activity light went out, these drives out of another tegile array came up and the activity light came on, blinked a few times as linux identified it, and then remained on. I was seeing the following in dmesg:\n\n    [174050.317023]  sdt: unable to read partition table\n    [174050.318401] sd 8:0:25:0: [sdt] Attached SCSI disk\n    [174050.899555] hpsa 0000:05:00.0: cp 0000000004843087 has status 0x18 Sense: 0xff, ASC: 0xff, ASCQ: 0xff, Returning result: 0x18\n    [174050.901621] sd 8:0:25:0: reservation conflict\n\nThese were locked in a way that the disk was inaccessible because the access was reserved at a SCSI command level by the firmware of the drive, so this wasn't about the format of the disk, or an encryption key, but at a lower level where the host simply is denied access to the drive because another host as some time set a reservation, and the current host can't automatically clear it to gain access. I was able to gain access by using sg_persist to set a new reservation, and then clear all reservations with it:\n\n    sg_persist --out --register-ignore  --param-sark=abc1234 /dev/sdr\n    sg_persist /dev/sdr\n    No service action given; assume Persistent Reserve In command\n    with Read Keys service action\n      HGST      HUS726040ALS211   BD05\n      Peripheral device type: disk\n      PR generation=0x2, 2 registered reservation keys follow:\n        0xabc1234\n        0x5bf43c4200000001\n    sg_persist --out -C --param-rk=abc1234 /dev/sdr\n\n&amp;nbsp;&amp;nbsp;\n\nFor the tegile drives, they were ALL sed locked, but three disk out of each system had scsi reservations set. I had to clear the scsi reservation, then clear the disk with SED PSID, and then the drives were accessable. I could not see, via sedutil-cli, that they were SED locked until the scsi reservation was cleared.\n\n&amp;nbsp;&amp;nbsp;\n\nI don't know who this will help, but thought I'd throw it out there for us folks not paying for new drives.", "author_fullname": "t2_16lp28n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "read only / locked hard drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187lcm0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701360530.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701358571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, in purchasing or re-purposing used drives to add to a large veeam repository, I&amp;#39;ve run into various ways that drives have been locked. So far, I&amp;#39;ve seen three different ways that sas drives have been locked in a way that I could not initially use them in my KTL3 shelves attached to TrueNAS SCALE servers.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. SED locking:&lt;/strong&gt;\nI have disks from retired tegile disk arrays. I went to spin these drives up, and they were basically unreadable, and dmesg was spitting out a bunch of errors. You can identify these disks physically because they have a PSID printed on the top of them.\nTo clear these locks, you can use the sedutil-cli utility to wipe the disk and clear the lock:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sedutil-cli --yesIreallywanttoERASEALLmydatausingthePSID &amp;lt;PSID key goes here&amp;gt; /dev/sdr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This process ran in seconds, and access was gained to the drive.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. T10-DIF:&lt;/strong&gt;\nThis wasn&amp;#39;t obvious to me, as I&amp;#39;ve seen netapp formatted drives that won&amp;#39;t read because linux won&amp;#39;t read 520 byte block size drives. These drives, tho, reported as 512B formatted drives, but I couldn&amp;#39;t read/write them. Upon further inspection, I found that they were indeed formatted at 512B blocks, but then there was another 8 bits of data, resulting in 520B blocks. I was able to reformat them with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sg_format --format --size=512 /dev/sdl\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This process takes time, nearly 12-24 hours, per drive (12TB disks). After the format was complete, i pulled and reinserted the drives into the shelf and was able to access them successfully. tmux was helpful in formatting the disks in parallel so i didn&amp;#39;t have to wait for each one to finish before starting the next.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3. SCSI reservation:&lt;/strong&gt;\nThis was even more obscure. I popped a drive in the shelf, and unlike the other drives that spun up and the activity light went out, these drives out of another tegile array came up and the activity light came on, blinked a few times as linux identified it, and then remained on. I was seeing the following in dmesg:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[174050.317023]  sdt: unable to read partition table\n[174050.318401] sd 8:0:25:0: [sdt] Attached SCSI disk\n[174050.899555] hpsa 0000:05:00.0: cp 0000000004843087 has status 0x18 Sense: 0xff, ASC: 0xff, ASCQ: 0xff, Returning result: 0x18\n[174050.901621] sd 8:0:25:0: reservation conflict\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;These were locked in a way that the disk was inaccessible because the access was reserved at a SCSI command level by the firmware of the drive, so this wasn&amp;#39;t about the format of the disk, or an encryption key, but at a lower level where the host simply is denied access to the drive because another host as some time set a reservation, and the current host can&amp;#39;t automatically clear it to gain access. I was able to gain access by using sg_persist to set a new reservation, and then clear all reservations with it:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;sg_persist --out --register-ignore  --param-sark=abc1234 /dev/sdr\nsg_persist /dev/sdr\nNo service action given; assume Persistent Reserve In command\nwith Read Keys service action\n  HGST      HUS726040ALS211   BD05\n  Peripheral device type: disk\n  PR generation=0x2, 2 registered reservation keys follow:\n    0xabc1234\n    0x5bf43c4200000001\nsg_persist --out -C --param-rk=abc1234 /dev/sdr\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;For the tegile drives, they were ALL sed locked, but three disk out of each system had scsi reservations set. I had to clear the scsi reservation, then clear the disk with SED PSID, and then the drives were accessable. I could not see, via sedutil-cli, that they were SED locked until the scsi reservation was cleared.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know who this will help, but thought I&amp;#39;d throw it out there for us folks not paying for new drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187lcm0", "is_robot_indexable": true, "report_reasons": null, "author": "gmc_5303", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187lcm0/read_only_locked_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187lcm0/read_only_locked_hard_drives/", "subreddit_subscribers": 715344, "created_utc": 1701358571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've read quite a few threads here and on [digitalfaq.com](https://digitalfaq.com), but it seems that people often comment under the assumption that everyone uses the same operating system as them (whichever that may be). \n\nI use a Mac (apple silicon) and am trying to digitize some Video8 tapes. I have a camera that can output S-Video but need a capture card. I've seen this card linked [https://streameez.com/product/610/](https://streameez.com/product/610/) but the software does not support Mac. I've seen people recommend open source software such as FFmpeg which work on Mac, but I'm not sure if I'll run into compatibility issues between the card and software.\n\nI've seen the blackmagic cards recommended, but they're out of my price range. I'd like to stay under $150 for this link in the chain. \n\nDoes anyone here have experience with S-Video digitization on Mac? Any advice will be appreciated!", "author_fullname": "t2_5hwbmov1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Video8 tapes via S-Video to Mac. Which capture card can I use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187ihgc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701350778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve read quite a few threads here and on &lt;a href=\"https://digitalfaq.com\"&gt;digitalfaq.com&lt;/a&gt;, but it seems that people often comment under the assumption that everyone uses the same operating system as them (whichever that may be). &lt;/p&gt;\n\n&lt;p&gt;I use a Mac (apple silicon) and am trying to digitize some Video8 tapes. I have a camera that can output S-Video but need a capture card. I&amp;#39;ve seen this card linked &lt;a href=\"https://streameez.com/product/610/\"&gt;https://streameez.com/product/610/&lt;/a&gt; but the software does not support Mac. I&amp;#39;ve seen people recommend open source software such as FFmpeg which work on Mac, but I&amp;#39;m not sure if I&amp;#39;ll run into compatibility issues between the card and software.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve seen the blackmagic cards recommended, but they&amp;#39;re out of my price range. I&amp;#39;d like to stay under $150 for this link in the chain. &lt;/p&gt;\n\n&lt;p&gt;Does anyone here have experience with S-Video digitization on Mac? Any advice will be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187ihgc", "is_robot_indexable": true, "report_reasons": null, "author": "Large-Childhood", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187ihgc/video8_tapes_via_svideo_to_mac_which_capture_card/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187ihgc/video8_tapes_via_svideo_to_mac_which_capture_card/", "subreddit_subscribers": 715344, "created_utc": 1701350778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_4ohl4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thinking of a simple upgrade for cheap, what do y'all think?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_187hke4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1701347852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "govdeals.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.govdeals.com/asset/341/12217", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187hke4", "is_robot_indexable": true, "report_reasons": null, "author": "Nicker", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/187hke4/thinking_of_a_simple_upgrade_for_cheap_what_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.govdeals.com/asset/341/12217", "subreddit_subscribers": 715344, "created_utc": 1701347852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a bunch of older 2.5\" SSDs and would like to use those. I already have a SAS capable Debian Linux host and would like to attach a disk shelf / jbod case to it. A SAS2 solution would be fine since bandwidth is not critical for my application. I'm not sure whether an older used EMC or NetApp case would be a good option since those (imho) tend to be setup with proprietary hardware?\n\nOptions I'm considering:\n\n[HP 19\\\\\" Storserv 8000 Storage 24x 2,5\\\\\"](https://preview.redd.it/kevj5oucqf3c1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=ef4b94d7b4bf0516c0077340789ca1c8f19f3c67)\n\n&amp;#x200B;\n\n[DELL PowerVault MD1200](https://preview.redd.it/vai47l8jqf3c1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=c5e9b07d1f14070424c9330b7b8dcde4cc4c785d)\n\nFeedback and suggestions are welcome.\n\n  \n\n\n&amp;#x200B;", "author_fullname": "t2_xsvaa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "2.5\" Storage POD / JBOD / Disk Shelf suggestions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 80, "top_awarded_type": null, "hide_score": false, "media_metadata": {"vai47l8jqf3c1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 58, "x": 108, "u": "https://preview.redd.it/vai47l8jqf3c1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d2e52b14af5ec0075b871d1a312bb29004075234"}, {"y": 116, "x": 216, "u": "https://preview.redd.it/vai47l8jqf3c1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0a8873a74a8133ec1286f11349cc1e511e3faefb"}, {"y": 172, "x": 320, "u": "https://preview.redd.it/vai47l8jqf3c1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3770cbb0b9a5382e9e5ec6e6ab321284a54bf878"}, {"y": 344, "x": 640, "u": "https://preview.redd.it/vai47l8jqf3c1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a8989c897a66944949a81d53fb6a075c7e01ec87"}, {"y": 516, "x": 960, "u": "https://preview.redd.it/vai47l8jqf3c1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=faf976b43b459d61167ab44413ad58b73375b1fa"}, {"y": 580, "x": 1080, "u": "https://preview.redd.it/vai47l8jqf3c1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b7adf02f2a94748bee2e9d245d0b7dd34d6af5a"}], "s": {"y": 645, "x": 1200, "u": "https://preview.redd.it/vai47l8jqf3c1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=c5e9b07d1f14070424c9330b7b8dcde4cc4c785d"}, "id": "vai47l8jqf3c1"}, "kevj5oucqf3c1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 62, "x": 108, "u": "https://preview.redd.it/kevj5oucqf3c1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e495b1bd5a73d1bdeb6b4db55bfe0dbadd7f1a60"}, {"y": 124, "x": 216, "u": "https://preview.redd.it/kevj5oucqf3c1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb749e368a775a2767c147ff48b851fe5d699f66"}, {"y": 184, "x": 320, "u": "https://preview.redd.it/kevj5oucqf3c1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a4514b7161c537955521011d06a1828ea1cc58eb"}, {"y": 368, "x": 640, "u": "https://preview.redd.it/kevj5oucqf3c1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b9ace8d7e899dae60da8c649b35bfe3ab6cbd39"}, {"y": 552, "x": 960, "u": "https://preview.redd.it/kevj5oucqf3c1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa64968d2d4b66889ee8393d2516d4803052d7bb"}, {"y": 621, "x": 1080, "u": "https://preview.redd.it/kevj5oucqf3c1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=05f185d824fc9104c7799de24d151dc236454080"}], "s": {"y": 691, "x": 1200, "u": "https://preview.redd.it/kevj5oucqf3c1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=ef4b94d7b4bf0516c0077340789ca1c8f19f3c67"}, "id": "kevj5oucqf3c1"}}, "name": "t3_187cnf4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/kfomtneHSzQxDUljcaTesgMJe3i2WlVgDb38Qgu095E.jpg", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701328462.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of older 2.5&amp;quot; SSDs and would like to use those. I already have a SAS capable Debian Linux host and would like to attach a disk shelf / jbod case to it. A SAS2 solution would be fine since bandwidth is not critical for my application. I&amp;#39;m not sure whether an older used EMC or NetApp case would be a good option since those (imho) tend to be setup with proprietary hardware?&lt;/p&gt;\n\n&lt;p&gt;Options I&amp;#39;m considering:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kevj5oucqf3c1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef4b94d7b4bf0516c0077340789ca1c8f19f3c67\"&gt;HP 19\\&amp;quot; Storserv 8000 Storage 24x 2,5\\&amp;quot;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vai47l8jqf3c1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c5e9b07d1f14070424c9330b7b8dcde4cc4c785d\"&gt;DELL PowerVault MD1200&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Feedback and suggestions are welcome.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "187cnf4", "is_robot_indexable": true, "report_reasons": null, "author": "Jotschi", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/187cnf4/25_storage_pod_jbod_disk_shelf_suggestions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/187cnf4/25_storage_pod_jbod_disk_shelf_suggestions/", "subreddit_subscribers": 715344, "created_utc": 1701328462.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}