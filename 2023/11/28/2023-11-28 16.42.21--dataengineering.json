{"kind": "Listing", "data": {"after": "t3_1859zpn", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After some holiday reporting, I noticed my reports were off. This is basically worst fear of mine and I sent the report to leadership! The root cause was ultimately a dataset issue, don't want to get into the complexity but some customers trade with each other and this was causing some duplicates when suming values. \n\nNoone has said anything but I feel like a big dumb dumb. I want to avoid this in the future... How are you all validating your datasets? The reporting team is just yours truly so I open to any ideas I can implement in an automated fashion. If it matters I do most of my reports in SQL views then bring them into powerbi and add the relationships. The view is what was wrong.", "author_fullname": "t2_jrmn04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you test your data and reports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185gxaj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701126900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After some holiday reporting, I noticed my reports were off. This is basically worst fear of mine and I sent the report to leadership! The root cause was ultimately a dataset issue, don&amp;#39;t want to get into the complexity but some customers trade with each other and this was causing some duplicates when suming values. &lt;/p&gt;\n\n&lt;p&gt;Noone has said anything but I feel like a big dumb dumb. I want to avoid this in the future... How are you all validating your datasets? The reporting team is just yours truly so I open to any ideas I can implement in an automated fashion. If it matters I do most of my reports in SQL views then bring them into powerbi and add the relationships. The view is what was wrong.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185gxaj", "is_robot_indexable": true, "report_reasons": null, "author": "soricellia", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185gxaj/how_do_you_test_your_data_and_reports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185gxaj/how_do_you_test_your_data_and_reports/", "subreddit_subscribers": 142314, "created_utc": 1701126900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! Don't know if this is the best sub to ask but here we go.  \n\n\nI work at a medium sized company (200 office employees, 750 on site) as a Business Analyst, I have around 5/6 years experience as a Data Analyst but well the transition is a long story for another day.  \n\n\nCurrently I'm alone here basically with my boss, and we want to deploy a Datawarehouse, I've worked with those before many times, but never implemented one. We will probably go with Azure since the company is very Microsoft-y and such.   \n\n\nThe thing is that my boss recommended me to learn to do this stuff by myself, instead of hiring a consultor to do it for is. It's not that the company doesn't want to pay, they are very keen, but my boss is a great dude and he had the idea that it would be an awesome learning experience if we try to do it ourselves and, before investing too much time or going to production, only then hire a consultor to basically make sure we did everything fine.  \n\n\nOur data comes from different (but not too many) sources: an HSE system with which we connect through an API, an operational (we use it to schedule shifts and all that, basically our main) system with which we connect directly to their data blob, a finance system (Pronto XI) and then some other random stuff.  \n\n\nCan you guys recommend any learning path that may help me with this? I'm very excited to do this and want to get it right, we have access to LinkedIn learn so I thought about starting a Data Engineering course or something like that...  \n\n\nThanks a lot!!", "author_fullname": "t2_66n34smm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help a dude install a DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185jbtq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701133209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Don&amp;#39;t know if this is the best sub to ask but here we go.  &lt;/p&gt;\n\n&lt;p&gt;I work at a medium sized company (200 office employees, 750 on site) as a Business Analyst, I have around 5/6 years experience as a Data Analyst but well the transition is a long story for another day.  &lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m alone here basically with my boss, and we want to deploy a Datawarehouse, I&amp;#39;ve worked with those before many times, but never implemented one. We will probably go with Azure since the company is very Microsoft-y and such.   &lt;/p&gt;\n\n&lt;p&gt;The thing is that my boss recommended me to learn to do this stuff by myself, instead of hiring a consultor to do it for is. It&amp;#39;s not that the company doesn&amp;#39;t want to pay, they are very keen, but my boss is a great dude and he had the idea that it would be an awesome learning experience if we try to do it ourselves and, before investing too much time or going to production, only then hire a consultor to basically make sure we did everything fine.  &lt;/p&gt;\n\n&lt;p&gt;Our data comes from different (but not too many) sources: an HSE system with which we connect through an API, an operational (we use it to schedule shifts and all that, basically our main) system with which we connect directly to their data blob, a finance system (Pronto XI) and then some other random stuff.  &lt;/p&gt;\n\n&lt;p&gt;Can you guys recommend any learning path that may help me with this? I&amp;#39;m very excited to do this and want to get it right, we have access to LinkedIn learn so I thought about starting a Data Engineering course or something like that...  &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185jbtq", "is_robot_indexable": true, "report_reasons": null, "author": "Palpitation-Itchy", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185jbtq/help_a_dude_install_a_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185jbtq/help_a_dude_install_a_dwh/", "subreddit_subscribers": 142314, "created_utc": 1701133209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to implement Apache Airflow to our data management systems, particularly focusing on integrating it with our existing infrastructure which includes a mix of cloud and on-premises data sources. The goal is to streamline our complex ETL tasks. However, I'm encountering issues with scaling Airflow to handle large datasets and ensuring integration with data sources and tools we use. \n\nHas anyone faced similar obstacles while using Apache Airflow? I'm looking for insights on best practices, tools, or strategies that could help in smoothing out these issues. Any experiences or advice you could share would be great!", "author_fullname": "t2_bcq4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Challenges with Implementing Apache Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1857dh5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701103664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to implement Apache Airflow to our data management systems, particularly focusing on integrating it with our existing infrastructure which includes a mix of cloud and on-premises data sources. The goal is to streamline our complex ETL tasks. However, I&amp;#39;m encountering issues with scaling Airflow to handle large datasets and ensuring integration with data sources and tools we use. &lt;/p&gt;\n\n&lt;p&gt;Has anyone faced similar obstacles while using Apache Airflow? I&amp;#39;m looking for insights on best practices, tools, or strategies that could help in smoothing out these issues. Any experiences or advice you could share would be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1857dh5", "is_robot_indexable": true, "report_reasons": null, "author": "xDarkOne", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1857dh5/challenges_with_implementing_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1857dh5/challenges_with_implementing_apache_airflow/", "subreddit_subscribers": 142314, "created_utc": 1701103664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As in title, I am wondering what people are seeing/charging for contracting rates as Senior Data Engineers in the UK currently. From the looks of things I am seeing rates advertised between \u00a3500 - \u00a3600, but I am wondering if this is the normal going rate or if people are charging more.\n\n&amp;#x200B;\n\nContext I currently work for Amazon as a Senior DE earning 130k total comp but want to move into contracting to maximise my income however I want to ensure that I am not leaving money on the table so to speak in any rate negotiations.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_75if3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UK Senior Data Engineer Contracting Rates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185pyzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701154678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As in title, I am wondering what people are seeing/charging for contracting rates as Senior Data Engineers in the UK currently. From the looks of things I am seeing rates advertised between \u00a3500 - \u00a3600, but I am wondering if this is the normal going rate or if people are charging more.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Context I currently work for Amazon as a Senior DE earning 130k total comp but want to move into contracting to maximise my income however I want to ensure that I am not leaving money on the table so to speak in any rate negotiations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "185pyzm", "is_robot_indexable": true, "report_reasons": null, "author": "Wtf_Pinkelephants", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185pyzm/uk_senior_data_engineer_contracting_rates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185pyzm/uk_senior_data_engineer_contracting_rates/", "subreddit_subscribers": 142314, "created_utc": 1701154678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you were interested in Iceberg tables, they are finally available for all accounts on snowflake \ud83e\uddca\nIt's time to test them.  \nTo those who already tried them : how was the query performance for you?\n\nhttps://docs.snowflake.com/en/release-notes/2023/7_42", "author_fullname": "t2_w6z0w1b6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg tables are now available in public preview on snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185tgn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701169209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you were interested in Iceberg tables, they are finally available for all accounts on snowflake \ud83e\uddca\nIt&amp;#39;s time to test them.&lt;br/&gt;\nTo those who already tried them : how was the query performance for you?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.snowflake.com/en/release-notes/2023/7_42\"&gt;https://docs.snowflake.com/en/release-notes/2023/7_42&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185tgn0", "is_robot_indexable": true, "report_reasons": null, "author": "sdc-msimon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185tgn0/iceberg_tables_are_now_available_in_public/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185tgn0/iceberg_tables_are_now_available_in_public/", "subreddit_subscribers": 142314, "created_utc": 1701169209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks \n\nso i\u2019m designing a data model purely for analysis and BI, i thought of normalising the data first to reduce the size in each table, and to make it easier to modify the tables and easy data governance.\nThen, de-normalise the data again into a star schema based on the business need ofc.. to make it simple to understand and perform analysis and to reduce the number of joins \n\nI agree this depends on business requirements but i want to hear from another person what they think about this approach. \n\nthnx", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Normalisation &gt; Denormalisation star schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185d1h9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701117593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks &lt;/p&gt;\n\n&lt;p&gt;so i\u2019m designing a data model purely for analysis and BI, i thought of normalising the data first to reduce the size in each table, and to make it easier to modify the tables and easy data governance.\nThen, de-normalise the data again into a star schema based on the business need ofc.. to make it simple to understand and perform analysis and to reduce the number of joins &lt;/p&gt;\n\n&lt;p&gt;I agree this depends on business requirements but i want to hear from another person what they think about this approach. &lt;/p&gt;\n\n&lt;p&gt;thnx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185d1h9", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185d1h9/normalisation_denormalisation_star_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185d1h9/normalisation_denormalisation_star_schema/", "subreddit_subscribers": 142314, "created_utc": 1701117593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Are data warehouses a bad choice for data apps because they lack freshness, latency, and query concurrency needs?  \n\nOr if we're able to improve ingestion latency, query response time, and concurrency/scaleability, maybe it can be a good choice?  ", "author_fullname": "t2_jg3w8gbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should data warehouses serve as application backends?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185uz7j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701174754.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are data warehouses a bad choice for data apps because they lack freshness, latency, and query concurrency needs?  &lt;/p&gt;\n\n&lt;p&gt;Or if we&amp;#39;re able to improve ingestion latency, query response time, and concurrency/scaleability, maybe it can be a good choice?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185uz7j", "is_robot_indexable": true, "report_reasons": null, "author": "tamargal91", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185uz7j/should_data_warehouses_serve_as_application/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185uz7j/should_data_warehouses_serve_as_application/", "subreddit_subscribers": 142314, "created_utc": 1701174754.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working as a data engineer for a company that after a merger operates in three countries with different currencies. We need to support local currency reporting, and reporting across the company using all three currencies.\n\nI'm thinking a bit about how to model this in an effective way in the data platform, and there are some options, to which I wanted to see if anyone has good/bad experiences working with multiple currencies. For the record our tech stack is dbt, Databricks and Power BI.\n\nI found this data modelling option from [Kimball](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/multiple-currencies/) that says to store the local unconverted currency (what's coming from the source) along with a chosen single \"main\" currency. Then we provide an exchange rate table and let users/analysts convert currencies in queries/notebooks/Power BI reports when needed. \n\nI imagine to prevent creating a mess of conversions in the platform I would suggest users to use the \"main\" currency in their work and only convert to local currencies in their presentation layers. That way we don't have to convert numbers back and forwards, and it's easy to know when to convert the currency.\n\nDoes anyone here have any experience modelling multiple currencies in this way? Any good or bad experiences? Or are there any other options we should consider?", "author_fullname": "t2_11b4ct", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle reporting in multiple currencies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185sl1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701165652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working as a data engineer for a company that after a merger operates in three countries with different currencies. We need to support local currency reporting, and reporting across the company using all three currencies.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking a bit about how to model this in an effective way in the data platform, and there are some options, to which I wanted to see if anyone has good/bad experiences working with multiple currencies. For the record our tech stack is dbt, Databricks and Power BI.&lt;/p&gt;\n\n&lt;p&gt;I found this data modelling option from &lt;a href=\"https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/multiple-currencies/\"&gt;Kimball&lt;/a&gt; that says to store the local unconverted currency (what&amp;#39;s coming from the source) along with a chosen single &amp;quot;main&amp;quot; currency. Then we provide an exchange rate table and let users/analysts convert currencies in queries/notebooks/Power BI reports when needed. &lt;/p&gt;\n\n&lt;p&gt;I imagine to prevent creating a mess of conversions in the platform I would suggest users to use the &amp;quot;main&amp;quot; currency in their work and only convert to local currencies in their presentation layers. That way we don&amp;#39;t have to convert numbers back and forwards, and it&amp;#39;s easy to know when to convert the currency.&lt;/p&gt;\n\n&lt;p&gt;Does anyone here have any experience modelling multiple currencies in this way? Any good or bad experiences? Or are there any other options we should consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185sl1j", "is_robot_indexable": true, "report_reasons": null, "author": "mmammies", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185sl1j/how_do_you_handle_reporting_in_multiple_currencies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185sl1j/how_do_you_handle_reporting_in_multiple_currencies/", "subreddit_subscribers": 142314, "created_utc": 1701165652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We built an ML-powered Databricks cluster optimization tool to help remove the tedious work of tuning Jobs clusters to lower costs.\n\nWe recently shipped a huge upgrade to our product, Gradient, that can automatically improve your job cluster settings to hit your business goals.\n\nThis just came out of private preview where we had awesome results with early testers.  We're looking for more feedback and would love to have folks here give it a spin!\n\nCheck out our demo video here:  [https://www.youtube.com/watch?v=i09x6B6mtjg](https://www.youtube.com/watch?v=i09x6B6mtjg)\n\nTo get started and try for yourself, [see our docs](https://docs.synccomputing.com/sync-gradient/readme)!", "author_fullname": "t2_vk94wnpj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Self-Improving Databricks Jobs Clusters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185wvhp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701180489.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We built an ML-powered Databricks cluster optimization tool to help remove the tedious work of tuning Jobs clusters to lower costs.&lt;/p&gt;\n\n&lt;p&gt;We recently shipped a huge upgrade to our product, Gradient, that can automatically improve your job cluster settings to hit your business goals.&lt;/p&gt;\n\n&lt;p&gt;This just came out of private preview where we had awesome results with early testers.  We&amp;#39;re looking for more feedback and would love to have folks here give it a spin!&lt;/p&gt;\n\n&lt;p&gt;Check out our demo video here:  &lt;a href=\"https://www.youtube.com/watch?v=i09x6B6mtjg\"&gt;https://www.youtube.com/watch?v=i09x6B6mtjg&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To get started and try for yourself, &lt;a href=\"https://docs.synccomputing.com/sync-gradient/readme\"&gt;see our docs&lt;/a&gt;!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RcWOQxguZtmLJpDr7Fm9YTFW4oNLMFpGdN7dq8vO2v4.jpg?auto=webp&amp;s=05ffd69102d5dddccb997f8ca93ac9307db11e9e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/RcWOQxguZtmLJpDr7Fm9YTFW4oNLMFpGdN7dq8vO2v4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c7aa6eabf1b072b8fef656206ca7554e6d221dd", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/RcWOQxguZtmLJpDr7Fm9YTFW4oNLMFpGdN7dq8vO2v4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2743e0abe3a30633e2e4eada7906ea4ea6b32ae", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/RcWOQxguZtmLJpDr7Fm9YTFW4oNLMFpGdN7dq8vO2v4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aa313b38afb9b8f729582bea89d46839845428b", "width": 320, "height": 240}], "variants": {}, "id": "YONw8Gtri81AzcPYVOVfaJY6Sqx6CR_2aqpiGSQiky0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "185wvhp", "is_robot_indexable": true, "report_reasons": null, "author": "sync_jeff", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185wvhp/selfimproving_databricks_jobs_clusters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185wvhp/selfimproving_databricks_jobs_clusters/", "subreddit_subscribers": 142314, "created_utc": 1701180489.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my industry, we use the term metadata to mean the data that describes data. We have business metadata and technical metadata to describe our data. As an example, for data element \u201ccustomer name\u201d, we have business metadata like name of data element, description, owner of data, data source, domain, etc. We also have technical metadata like column, database, length, data type. \n\nI noticed in the news that law enforcement use \u201cmetadata\u201d differently and basically what they\u2019re referring to is just data. I see this with phone records:\n\n\u201cA phone's metadata describes key facts about an individual data file such as phone calls, photographs, texts, etc. With this data, you might reveal habits, activities and interests, or even uncover a lie.\u201d\n\nIsn\u2019t a lot of phone calls just data?\n\nIs metadata just a nebulous term and it gets used differently in different industries like law enforcement vs corporations, etc. Any thoughts on this discrepancy?", "author_fullname": "t2_6f5ggoc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nebulous usage of term \u201cmetadata\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185ce5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701116016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my industry, we use the term metadata to mean the data that describes data. We have business metadata and technical metadata to describe our data. As an example, for data element \u201ccustomer name\u201d, we have business metadata like name of data element, description, owner of data, data source, domain, etc. We also have technical metadata like column, database, length, data type. &lt;/p&gt;\n\n&lt;p&gt;I noticed in the news that law enforcement use \u201cmetadata\u201d differently and basically what they\u2019re referring to is just data. I see this with phone records:&lt;/p&gt;\n\n&lt;p&gt;\u201cA phone&amp;#39;s metadata describes key facts about an individual data file such as phone calls, photographs, texts, etc. With this data, you might reveal habits, activities and interests, or even uncover a lie.\u201d&lt;/p&gt;\n\n&lt;p&gt;Isn\u2019t a lot of phone calls just data?&lt;/p&gt;\n\n&lt;p&gt;Is metadata just a nebulous term and it gets used differently in different industries like law enforcement vs corporations, etc. Any thoughts on this discrepancy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185ce5s", "is_robot_indexable": true, "report_reasons": null, "author": "wackomama", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185ce5s/nebulous_usage_of_term_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185ce5s/nebulous_usage_of_term_metadata/", "subreddit_subscribers": 142314, "created_utc": 1701116016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anybody else had recruiters ask for every damn think in their mental toolbox, when all the client actually wants is somebody to hold their hand and tell them what data is? AWS, GCP, Python, Redshift,SQL, TSQL, PSQL, cloud infrastructure, terraform etc.... come on give us a chance. Give us a use case and we will go to work.", "author_fullname": "t2_8p2u7cmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Knobhead recruiters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185tdkb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701168849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anybody else had recruiters ask for every damn think in their mental toolbox, when all the client actually wants is somebody to hold their hand and tell them what data is? AWS, GCP, Python, Redshift,SQL, TSQL, PSQL, cloud infrastructure, terraform etc.... come on give us a chance. Give us a use case and we will go to work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185tdkb", "is_robot_indexable": true, "report_reasons": null, "author": "Jamese0", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185tdkb/knobhead_recruiters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185tdkb/knobhead_recruiters/", "subreddit_subscribers": 142314, "created_utc": 1701168849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Community Folks, \n\nI have been reading up on differential privacy, k-anonymity, etc. What is their usecases in large organisations who are not really sharing data for public use however their use cases for data shares internally within the org might be high. This is based on the assumption that directly identifiable attributes like name, email, SSN are masked, are there any further steps being taken to protect quasi identifiers.   \nI'm working in an org which has adopted data mesh approach with multiple dbt projects across various use cases running on Snowflake. All production datasets have to be column level tagged as a requirement for Immuta to take care of cataloging and security.   \nI'm keen to understand any frameworks or patterns being used to enable data privacy beyond masking. ", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use Cases for Data Privacy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185ktgw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701137431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Community Folks, &lt;/p&gt;\n\n&lt;p&gt;I have been reading up on differential privacy, k-anonymity, etc. What is their usecases in large organisations who are not really sharing data for public use however their use cases for data shares internally within the org might be high. This is based on the assumption that directly identifiable attributes like name, email, SSN are masked, are there any further steps being taken to protect quasi identifiers.&lt;br/&gt;\nI&amp;#39;m working in an org which has adopted data mesh approach with multiple dbt projects across various use cases running on Snowflake. All production datasets have to be column level tagged as a requirement for Immuta to take care of cataloging and security.&lt;br/&gt;\nI&amp;#39;m keen to understand any frameworks or patterns being used to enable data privacy beyond masking. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185ktgw", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185ktgw/use_cases_for_data_privacy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185ktgw/use_cases_for_data_privacy/", "subreddit_subscribers": 142314, "created_utc": 1701137431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to use TableauOperator to refresh our workbook at the end of my pipeline but it fails. I get an error saying I have a connection issue - too many retries. Anyone came across this issue?\nI\u2019m also open to suggestions using other type of operators", "author_fullname": "t2_8dnn00ks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone ever used airflow to refresh a tableau workbook?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185flgh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701123631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use TableauOperator to refresh our workbook at the end of my pipeline but it fails. I get an error saying I have a connection issue - too many retries. Anyone came across this issue?\nI\u2019m also open to suggestions using other type of operators&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185flgh", "is_robot_indexable": true, "report_reasons": null, "author": "Se7enEl11ven", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185flgh/anyone_ever_used_airflow_to_refresh_a_tableau/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185flgh/anyone_ever_used_airflow_to_refresh_a_tableau/", "subreddit_subscribers": 142314, "created_utc": 1701123631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How common is it to reuse spark batch function in spark streaming ? Specifically when migration to windowing and states. \n\nAnd what would be the best practice for doing that from experience ?", "author_fullname": "t2_j15inhpup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reusing spark batch and spark streaming functions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185buch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701114620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How common is it to reuse spark batch function in spark streaming ? Specifically when migration to windowing and states. &lt;/p&gt;\n\n&lt;p&gt;And what would be the best practice for doing that from experience ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185buch", "is_robot_indexable": true, "report_reasons": null, "author": "springRock88", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185buch/reusing_spark_batch_and_spark_streaming_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185buch/reusing_spark_batch_and_spark_streaming_functions/", "subreddit_subscribers": 142314, "created_utc": 1701114620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_fwerb2uw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best practices for working with dbt and BigQuery - A practitioner's guide", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_185z98k", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1701186894.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "y42.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.y42.com/blog/dbt-bigquery-best-practices", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "185z98k", "is_robot_indexable": true, "report_reasons": null, "author": "Pleasant-Guidance599", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185z98k/best_practices_for_working_with_dbt_and_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.y42.com/blog/dbt-bigquery-best-practices", "subreddit_subscribers": 142314, "created_utc": 1701186894.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm in a bit of a pickle on making a decision for my team.  \n\ncurrent stack/process:\n\nAZ subscription is connected to a bunch of on-prem DB servers (oracle/vertica/teradata/MS SQL server/Snowflake)\n\nAz VM running an in house restful API listener program for file delivery from different teams in the org which gets moved to a storage container. storage container is connected to snowflake as an external stage.\n\nAz VM schedules python scripts to do the movement (connects to on prem DBs and does ETL/ELT to snowflake DB) if extracts from outside DBs need to be written to files the VM handles that. \n\npython scripts will load files or dataframe data to snowflake stages.\n\nsnowflake processes copy into and merge/upsert statements. \n\nour python scripts mainly use PYODBC to connect to the myriad environments we use. \n\n\\-----\n\nall of this has worked very well for our departmental workspace. we only use 1 smallish VM and our subscription in total runs around $1500/month between compute and storage, we also have a couple of internal websites for users to run some basic analytics.\n\n\\----\n\nI have been requested to move to some serverless solution, i guess the powers that be dont like an \"always running VM\"\n\nA sister team has landed on using synapse. OK sure, why not go serverless?\n\nhowever, as i start looking into synapse, for ease of migration, i keep landing on notebooks with spark pools, but even the smallest spark pool runs between $1.50 - $3.50 / 15 minutes. \n\ni am also having issues trying to find out how to make ODBC/JDBC drivers for all of these other environments work within the spark pools, but that is another story. \n\n\\-----\n\nAm i on the right path here? it just feels like our \"little VM that could\" is doing all the work and will end up still being cheaper to run than a synapse process.  \n\nShould we pursue synapse, or is there something better for basic ETL processes.  The enterprise is using PBI for dashboarding, and tends to use databricks for analysis and ML. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_5ch80", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "synapse for ETL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_185yid1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701184954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in a bit of a pickle on making a decision for my team.  &lt;/p&gt;\n\n&lt;p&gt;current stack/process:&lt;/p&gt;\n\n&lt;p&gt;AZ subscription is connected to a bunch of on-prem DB servers (oracle/vertica/teradata/MS SQL server/Snowflake)&lt;/p&gt;\n\n&lt;p&gt;Az VM running an in house restful API listener program for file delivery from different teams in the org which gets moved to a storage container. storage container is connected to snowflake as an external stage.&lt;/p&gt;\n\n&lt;p&gt;Az VM schedules python scripts to do the movement (connects to on prem DBs and does ETL/ELT to snowflake DB) if extracts from outside DBs need to be written to files the VM handles that. &lt;/p&gt;\n\n&lt;p&gt;python scripts will load files or dataframe data to snowflake stages.&lt;/p&gt;\n\n&lt;p&gt;snowflake processes copy into and merge/upsert statements. &lt;/p&gt;\n\n&lt;p&gt;our python scripts mainly use PYODBC to connect to the myriad environments we use. &lt;/p&gt;\n\n&lt;p&gt;-----&lt;/p&gt;\n\n&lt;p&gt;all of this has worked very well for our departmental workspace. we only use 1 smallish VM and our subscription in total runs around $1500/month between compute and storage, we also have a couple of internal websites for users to run some basic analytics.&lt;/p&gt;\n\n&lt;p&gt;----&lt;/p&gt;\n\n&lt;p&gt;I have been requested to move to some serverless solution, i guess the powers that be dont like an &amp;quot;always running VM&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;A sister team has landed on using synapse. OK sure, why not go serverless?&lt;/p&gt;\n\n&lt;p&gt;however, as i start looking into synapse, for ease of migration, i keep landing on notebooks with spark pools, but even the smallest spark pool runs between $1.50 - $3.50 / 15 minutes. &lt;/p&gt;\n\n&lt;p&gt;i am also having issues trying to find out how to make ODBC/JDBC drivers for all of these other environments work within the spark pools, but that is another story. &lt;/p&gt;\n\n&lt;p&gt;-----&lt;/p&gt;\n\n&lt;p&gt;Am i on the right path here? it just feels like our &amp;quot;little VM that could&amp;quot; is doing all the work and will end up still being cheaper to run than a synapse process.  &lt;/p&gt;\n\n&lt;p&gt;Should we pursue synapse, or is there something better for basic ETL processes.  The enterprise is using PBI for dashboarding, and tends to use databricks for analysis and ML. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185yid1", "is_robot_indexable": true, "report_reasons": null, "author": "circusboy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185yid1/synapse_for_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185yid1/synapse_for_etl/", "subreddit_subscribers": 142314, "created_utc": 1701184954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does one,  effectively clean dates and timestamps of different format to one format?? There are so many combinations and feels like the type of formats are endless.", "author_fullname": "t2_76x4aitl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WTF date formats.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_185yh0m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701184855.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does one,  effectively clean dates and timestamps of different format to one format?? There are so many combinations and feels like the type of formats are endless.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185yh0m", "is_robot_indexable": true, "report_reasons": null, "author": "Scratch_that_Iich", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185yh0m/wtf_date_formats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185yh0m/wtf_date_formats/", "subreddit_subscribers": 142314, "created_utc": 1701184855.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently facing a challenge at work and wanted to gauge how common my problem is in other companies or projects. Our backend data is quite fragmented, so we have a harder time updating data and finding it", "author_fullname": "t2_u7cx7908", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do any of you have trouble updating/debugging fragmented backend data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185wntg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701179921.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently facing a challenge at work and wanted to gauge how common my problem is in other companies or projects. Our backend data is quite fragmented, so we have a harder time updating data and finding it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185wntg", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Equivalent_1689", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185wntg/do_any_of_you_have_trouble_updatingdebugging/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185wntg/do_any_of_you_have_trouble_updatingdebugging/", "subreddit_subscribers": 142314, "created_utc": 1701179921.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi \ud83d\udc4b \n\nthis is an entry level question: \n\nif i\u2019m connecting a tool to my data warehouse, such as hubspot or any other saas service. \n\nis it gonna be designed already and ready to query or it will be normalised and i need to model the data and design the schema in a star model for the analysis?\n\nbest,", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connecting tools to DW", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185rhal", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701160949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi \ud83d\udc4b &lt;/p&gt;\n\n&lt;p&gt;this is an entry level question: &lt;/p&gt;\n\n&lt;p&gt;if i\u2019m connecting a tool to my data warehouse, such as hubspot or any other saas service. &lt;/p&gt;\n\n&lt;p&gt;is it gonna be designed already and ready to query or it will be normalised and i need to model the data and design the schema in a star model for the analysis?&lt;/p&gt;\n\n&lt;p&gt;best,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185rhal", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185rhal/connecting_tools_to_dw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185rhal/connecting_tools_to_dw/", "subreddit_subscribers": 142314, "created_utc": 1701160949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am breaking my head over this problem. My next part of the problem is to connect different connectors and consume their metadata. But, I am not able to do it via Postgres, I am not sure whether I can scale it to connect other RDBMS or any other database.\n\nThere are bits and pieces of information like using sqoop or something. But I don't want to create a clone of database and read metadata. I simply want to connect to postgres db, read entire metadata, and load into Apache Atlas. \n\nHow would I do it?", "author_fullname": "t2_1k0o7czh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Importing Postgres Metadata to Apache Atlas - Open Source Method only", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185rfyy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701160795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am breaking my head over this problem. My next part of the problem is to connect different connectors and consume their metadata. But, I am not able to do it via Postgres, I am not sure whether I can scale it to connect other RDBMS or any other database.&lt;/p&gt;\n\n&lt;p&gt;There are bits and pieces of information like using sqoop or something. But I don&amp;#39;t want to create a clone of database and read metadata. I simply want to connect to postgres db, read entire metadata, and load into Apache Atlas. &lt;/p&gt;\n\n&lt;p&gt;How would I do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185rfyy", "is_robot_indexable": true, "report_reasons": null, "author": "DesmonMiles07", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185rfyy/importing_postgres_metadata_to_apache_atlas_open/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185rfyy/importing_postgres_metadata_to_apache_atlas_open/", "subreddit_subscribers": 142314, "created_utc": 1701160795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys\u2026 hope you doing great.\nI\u2019ve this certification exam soon and I am studying with some past questions materials. If you\u2019ve had this exam before, how often are questions repeated on a scale of 1 - 10 (lowest to highest)\n\nLooking forward to your responses \ud83d\ude4f", "author_fullname": "t2_n5qii71ai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP professional data engineer exam question repetition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185mfcp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701142104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys\u2026 hope you doing great.\nI\u2019ve this certification exam soon and I am studying with some past questions materials. If you\u2019ve had this exam before, how often are questions repeated on a scale of 1 - 10 (lowest to highest)&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your responses \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185mfcp", "is_robot_indexable": true, "report_reasons": null, "author": "ImaginationOdd3610", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185mfcp/gcp_professional_data_engineer_exam_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185mfcp/gcp_professional_data_engineer_exam_question/", "subreddit_subscribers": 142314, "created_utc": 1701142104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi community, I m currently trying to implement dbt model for dumping a view data from snowflake to bigquery. One approach which I m following is use copy command and load it to GCS and then use bq load to bigquery. But are there any other approaches or dbt utilities available.", "author_fullname": "t2_602lxjl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt utilities for easy data dump to bigquery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185k7mu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701135687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi community, I m currently trying to implement dbt model for dumping a view data from snowflake to bigquery. One approach which I m following is use copy command and load it to GCS and then use bq load to bigquery. But are there any other approaches or dbt utilities available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185k7mu", "is_robot_indexable": true, "report_reasons": null, "author": "ankititachi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185k7mu/dbt_utilities_for_easy_data_dump_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185k7mu/dbt_utilities_for_easy_data_dump_to_bigquery/", "subreddit_subscribers": 142314, "created_utc": 1701135687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nThe team i work in uses cloud functions (sometimes with workflows) to do ingestions from SFTPs back into BigQuery\n\n\nUsually they run out of compute and it fails. How do you ingest data in your teams in GCP?", "author_fullname": "t2_851if4wo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud functions for data ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185agjf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701111184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;The team i work in uses cloud functions (sometimes with workflows) to do ingestions from SFTPs back into BigQuery&lt;/p&gt;\n\n&lt;p&gt;Usually they run out of compute and it fails. How do you ingest data in your teams in GCP?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185agjf", "is_robot_indexable": true, "report_reasons": null, "author": "No-Dress-3160", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185agjf/cloud_functions_for_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185agjf/cloud_functions_for_data_ingestion/", "subreddit_subscribers": 142314, "created_utc": 1701111184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So we model the data and create data marts. Data from those tables need to be exposed to end users within a web app. \n\nIt strikes me as bad practice to have web traffic hit the DWH directly or even through an internal API that might be load balanced. At the nd of the day the DWH data mart tables are being queried once per user action. On top of broader analytics workloads.\n\nWould it be better to export those data marts to a secondary DWH/DB that only holds final tables? And choose it/tune it specifically for read heavy workloads?\n\nThis is more of a system design question I imagine but I'd appreciate any pointers.", "author_fullname": "t2_gua18k7sg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Prod web app traffic directly through DWH, or move modeled data to prod web app DB?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185aela", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701111047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So we model the data and create data marts. Data from those tables need to be exposed to end users within a web app. &lt;/p&gt;\n\n&lt;p&gt;It strikes me as bad practice to have web traffic hit the DWH directly or even through an internal API that might be load balanced. At the nd of the day the DWH data mart tables are being queried once per user action. On top of broader analytics workloads.&lt;/p&gt;\n\n&lt;p&gt;Would it be better to export those data marts to a secondary DWH/DB that only holds final tables? And choose it/tune it specifically for read heavy workloads?&lt;/p&gt;\n\n&lt;p&gt;This is more of a system design question I imagine but I&amp;#39;d appreciate any pointers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185aela", "is_robot_indexable": true, "report_reasons": null, "author": "alexcontrerasdppl", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185aela/prod_web_app_traffic_directly_through_dwh_or_move/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185aela/prod_web_app_traffic_directly_through_dwh_or_move/", "subreddit_subscribers": 142314, "created_utc": 1701111047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering community,\n\nI'm gearing up to tackle the Google Data Engineer Certification, and I'm torn between going for the official Google course or relying on resources from Udemy (specifically Dan Sullivan and/or Jose Portilla's). I plan on doing a lot to prepare, including exam practices etc, but I'm curious about the depth/material of the courses.\n\nFor those here that have taken the official Google course to prepare for the exam, do you think it's a must? Alternatively, for those who went the Udemy or Coursera route, did you find it was enough to cover the exam?\n\nAlso a follow up question. Do the labs help much for the exam? Or is is more relevant for practical / real world experience.\n\nThanks in advance!", "author_fullname": "t2_6khra", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP Data Engineer Certification - Official Course vs. Udemy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1859zpn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701121223.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701110033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; community,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m gearing up to tackle the Google Data Engineer Certification, and I&amp;#39;m torn between going for the official Google course or relying on resources from Udemy (specifically Dan Sullivan and/or Jose Portilla&amp;#39;s). I plan on doing a lot to prepare, including exam practices etc, but I&amp;#39;m curious about the depth/material of the courses.&lt;/p&gt;\n\n&lt;p&gt;For those here that have taken the official Google course to prepare for the exam, do you think it&amp;#39;s a must? Alternatively, for those who went the Udemy or Coursera route, did you find it was enough to cover the exam?&lt;/p&gt;\n\n&lt;p&gt;Also a follow up question. Do the labs help much for the exam? Or is is more relevant for practical / real world experience.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1859zpn", "is_robot_indexable": true, "report_reasons": null, "author": "mmilli", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1859zpn/gcp_data_engineer_certification_official_course/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1859zpn/gcp_data_engineer_certification_official_course/", "subreddit_subscribers": 142314, "created_utc": 1701110033.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}