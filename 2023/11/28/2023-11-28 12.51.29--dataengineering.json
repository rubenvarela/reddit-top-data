{"kind": "Listing", "data": {"after": "t3_185agjf", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7lvmv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Me as an ETL engineer watching people build data tables with no regard to what goes in them.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_18567aq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "ups": 150, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 150, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sz9-bBG2MqUFoLruoU0gIMIAlXGWn6fB4VywBKcnfiA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701100697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/9ZJkPvV", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/o4naRaSSLJjYgSUFgUtIDf2XEKB5v0QDL68efT-i0-8.jpg?auto=webp&amp;s=a0409ac9fe00d820e35ee11b30726619a53bb830", "width": 600, "height": 315}, "resolutions": [{"url": "https://external-preview.redd.it/o4naRaSSLJjYgSUFgUtIDf2XEKB5v0QDL68efT-i0-8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d65aa08d31eb1ba26fb133f7935cf28ee170a83c", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/o4naRaSSLJjYgSUFgUtIDf2XEKB5v0QDL68efT-i0-8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0483e77d4a72889ba5e28f6044032ec7de7afc88", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/o4naRaSSLJjYgSUFgUtIDf2XEKB5v0QDL68efT-i0-8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7bbe577c010686e5ab69bdf22a5b59c69b571fb", "width": 320, "height": 168}], "variants": {}, "id": "ZUFg4bSnyDR3YZyUibB0z7GMKsmGWJDSocIjUs10m9s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "18567aq", "is_robot_indexable": true, "report_reasons": null, "author": "claytonjr", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18567aq/me_as_an_etl_engineer_watching_people_build_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/9ZJkPvV", "subreddit_subscribers": 142264, "created_utc": 1701100697.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_40ihn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Packages: a Primer for Data People", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_1854luu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WhzpsEWYeT_y_94hDBjHfAlyb3DKS85QlmsVLC2Jc8M.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701096408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dagster.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dagster.io/blog/python-packages-primer-1", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?auto=webp&amp;s=6d2eb199f7c455f81e966bee40d7df02837744aa", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=440fde9802be891f6d4d7572c7d3fe51e2f49d15", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=af4f21f8e36603776251933cb48393e3800615f2", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd90858425839b19c339231b7abe87e23db7d410", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fd3295b342dbdd8860fb89ad4bc0e4b9069b372", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e5a3acac644d062d829533502e4f92886a222cb", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/bh-8FegeKhQ6B9SM_8Q0sjaFCBbROa7Nrk_24cu2eCs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=caaa6c497963a3b1aa7271ac3966b2283d867bd3", "width": 1080, "height": 567}], "variants": {}, "id": "HkPo8G4Z5BNccgNizTqmPsBKJEh2N7-tMWViJKnIl2s"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1854luu", "is_robot_indexable": true, "report_reasons": null, "author": "floydophone", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1854luu/python_packages_a_primer_for_data_people/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dagster.io/blog/python-packages-primer-1", "subreddit_subscribers": 142264, "created_utc": 1701096408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am creating a Node JS web app - and have a requirement to text search up to 1 billion XML files. The XML files have text in them from PDFs (OCRd)...so the premise of the app is to search PDFs.\n\nI found [https://www.dtsearch.com/](https://www.dtsearch.com/) but not convinced it's the best solution. I explored other ideas like AWS Glue, but think the cost would be huge.  Has anyone come across anything similar that would be both quick (performance) and affordable (less than couple of hundred dollars per month)?\n\nThe goal would be to have data retrieved in less than 3seconds. I'd love to hear some feedback/ ideas/ experiences!", "author_fullname": "t2_i2wnfy6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Web app to search 500 million to 1 billion XML files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1854v5u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701097120.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am creating a Node JS web app - and have a requirement to text search up to 1 billion XML files. The XML files have text in them from PDFs (OCRd)...so the premise of the app is to search PDFs.&lt;/p&gt;\n\n&lt;p&gt;I found &lt;a href=\"https://www.dtsearch.com/\"&gt;https://www.dtsearch.com/&lt;/a&gt; but not convinced it&amp;#39;s the best solution. I explored other ideas like AWS Glue, but think the cost would be huge.  Has anyone come across anything similar that would be both quick (performance) and affordable (less than couple of hundred dollars per month)?&lt;/p&gt;\n\n&lt;p&gt;The goal would be to have data retrieved in less than 3seconds. I&amp;#39;d love to hear some feedback/ ideas/ experiences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1854v5u", "is_robot_indexable": true, "report_reasons": null, "author": "Brave_Argument627", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1854v5u/web_app_to_search_500_million_to_1_billion_xml/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1854v5u/web_app_to_search_500_million_to_1_billion_xml/", "subreddit_subscribers": 142264, "created_utc": 1701097120.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "After some holiday reporting, I noticed my reports were off. This is basically worst fear of mine and I sent the report to leadership! The root cause was ultimately a dataset issue, don't want to get into the complexity but some customers trade with each other and this was causing some duplicates when suming values. \n\nNoone has said anything but I feel like a big dumb dumb. I want to avoid this in the future... How are you all validating your datasets? The reporting team is just yours truly so I open to any ideas I can implement in an automated fashion. If it matters I do most of my reports in SQL views then bring them into powerbi and add the relationships. The view is what was wrong.", "author_fullname": "t2_jrmn04", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you test your data and reports", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185gxaj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701126900.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After some holiday reporting, I noticed my reports were off. This is basically worst fear of mine and I sent the report to leadership! The root cause was ultimately a dataset issue, don&amp;#39;t want to get into the complexity but some customers trade with each other and this was causing some duplicates when suming values. &lt;/p&gt;\n\n&lt;p&gt;Noone has said anything but I feel like a big dumb dumb. I want to avoid this in the future... How are you all validating your datasets? The reporting team is just yours truly so I open to any ideas I can implement in an automated fashion. If it matters I do most of my reports in SQL views then bring them into powerbi and add the relationships. The view is what was wrong.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185gxaj", "is_robot_indexable": true, "report_reasons": null, "author": "soricellia", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185gxaj/how_do_you_test_your_data_and_reports/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185gxaj/how_do_you_test_your_data_and_reports/", "subreddit_subscribers": 142264, "created_utc": 1701126900.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm trying to implement Apache Airflow to our data management systems, particularly focusing on integrating it with our existing infrastructure which includes a mix of cloud and on-premises data sources. The goal is to streamline our complex ETL tasks. However, I'm encountering issues with scaling Airflow to handle large datasets and ensuring integration with data sources and tools we use. \n\nHas anyone faced similar obstacles while using Apache Airflow? I'm looking for insights on best practices, tools, or strategies that could help in smoothing out these issues. Any experiences or advice you could share would be great!", "author_fullname": "t2_bcq4l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Challenges with Implementing Apache Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1857dh5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701103664.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to implement Apache Airflow to our data management systems, particularly focusing on integrating it with our existing infrastructure which includes a mix of cloud and on-premises data sources. The goal is to streamline our complex ETL tasks. However, I&amp;#39;m encountering issues with scaling Airflow to handle large datasets and ensuring integration with data sources and tools we use. &lt;/p&gt;\n\n&lt;p&gt;Has anyone faced similar obstacles while using Apache Airflow? I&amp;#39;m looking for insights on best practices, tools, or strategies that could help in smoothing out these issues. Any experiences or advice you could share would be great!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1857dh5", "is_robot_indexable": true, "report_reasons": null, "author": "xDarkOne", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1857dh5/challenges_with_implementing_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1857dh5/challenges_with_implementing_apache_airflow/", "subreddit_subscribers": 142264, "created_utc": 1701103664.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The medallion architectural pattern partitions data as follows:\n\n* Bronze: raw data (e.g. events from the Windows Event Log)\n* Silver: transformed, normalized data (e.g. events from the Windows Event Log normalized to Elastic Common Schema (ECS) format)\n* Gold: enriched, joined, aggregated data - effectively materialized views (e.g. process trees constructed using events from the Windows Event Log that have been normalized to ECS format, events correlated to alerts or other telemetry)\n\nIn some cases, data in the silver layer is ready to be queried, and should be forwarded to another system for consumption by analysts (e.g. piped into a full-text search engine, imported into a relational database as-is, etc.).\n\nShould I copy any data that doesn't require further processing to the gold layer, even if it's unchanged from silver to gold?\n\nI'm looking to do as much in a serverless context as possible to minimize costs while allowing data to be analyzed in relational or graph format with ETL off of data in the gold layer.\n\nI'm still designing the stack, but am thinking of forming a Kappa architecture consisting of RabbitMQ / Kafka -&gt; S3 -&gt; \\[TimescaleDB | Neo4j | ElasticSearch\\] where data is immediately written to S3, a set of serverless functions is used to go from bronze, silver, gold, and then another set of serverless functions is used to import data in the gold layer into a given data store.\n\nThe serverless functions would be triggered by RabbitMQ, Kafka, SQS, etc.\n\nThe datasets aren't very big, so simply copying data from silver to gold and keeping the silver layer private seems like it'd make the most sense?", "author_fullname": "t2_lwr1wvz8q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Under the medallion pattern, what do you do when data doesn't change from bronze to gold, or silver to gold?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1853fe8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1701095338.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701093119.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The medallion architectural pattern partitions data as follows:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Bronze: raw data (e.g. events from the Windows Event Log)&lt;/li&gt;\n&lt;li&gt;Silver: transformed, normalized data (e.g. events from the Windows Event Log normalized to Elastic Common Schema (ECS) format)&lt;/li&gt;\n&lt;li&gt;Gold: enriched, joined, aggregated data - effectively materialized views (e.g. process trees constructed using events from the Windows Event Log that have been normalized to ECS format, events correlated to alerts or other telemetry)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In some cases, data in the silver layer is ready to be queried, and should be forwarded to another system for consumption by analysts (e.g. piped into a full-text search engine, imported into a relational database as-is, etc.).&lt;/p&gt;\n\n&lt;p&gt;Should I copy any data that doesn&amp;#39;t require further processing to the gold layer, even if it&amp;#39;s unchanged from silver to gold?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking to do as much in a serverless context as possible to minimize costs while allowing data to be analyzed in relational or graph format with ETL off of data in the gold layer.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m still designing the stack, but am thinking of forming a Kappa architecture consisting of RabbitMQ / Kafka -&amp;gt; S3 -&amp;gt; [TimescaleDB | Neo4j | ElasticSearch] where data is immediately written to S3, a set of serverless functions is used to go from bronze, silver, gold, and then another set of serverless functions is used to import data in the gold layer into a given data store.&lt;/p&gt;\n\n&lt;p&gt;The serverless functions would be triggered by RabbitMQ, Kafka, SQS, etc.&lt;/p&gt;\n\n&lt;p&gt;The datasets aren&amp;#39;t very big, so simply copying data from silver to gold and keeping the silver layer private seems like it&amp;#39;d make the most sense?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1853fe8", "is_robot_indexable": true, "report_reasons": null, "author": "Fun-Importance-1605", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1853fe8/under_the_medallion_pattern_what_do_you_do_when/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1853fe8/under_the_medallion_pattern_what_do_you_do_when/", "subreddit_subscribers": 142264, "created_utc": 1701093119.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Setting up a change-data-capture (CDC) pipeline can be a real pain.\n\nCompanies that use Lassoo Headless Analytics to collect first-party behavioral data, often stream it to destinations like Snowflake.\n\nAfter hearing David Yaffe speak at a data event, I learned about Estuary.\n\nSo, I thought I'd see if I could quickly set up a behavioral data pipeline using Estuary, as an alternative to some of the more complex setups I've encountered.\n\nMost importantly, I looked to do so without help from the technical folks on either team.\n\nHere was my experience. Interested in hearing about any you've had with CDC.\n\n  \n[https://lassoo.io/blog/2023/11/20/quick-cdc-pipeline-with-eestuary-lassoo/](https://lassoo.io/blog/2023/11/20/quick-cdc-pipeline-with-eestuary-lassoo/?ref=reddit_de)\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_8s6trahv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Setting up a CDC pipeline with Estuary and Lassoo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1853qic", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701094003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Setting up a change-data-capture (CDC) pipeline can be a real pain.&lt;/p&gt;\n\n&lt;p&gt;Companies that use Lassoo Headless Analytics to collect first-party behavioral data, often stream it to destinations like Snowflake.&lt;/p&gt;\n\n&lt;p&gt;After hearing David Yaffe speak at a data event, I learned about Estuary.&lt;/p&gt;\n\n&lt;p&gt;So, I thought I&amp;#39;d see if I could quickly set up a behavioral data pipeline using Estuary, as an alternative to some of the more complex setups I&amp;#39;ve encountered.&lt;/p&gt;\n\n&lt;p&gt;Most importantly, I looked to do so without help from the technical folks on either team.&lt;/p&gt;\n\n&lt;p&gt;Here was my experience. Interested in hearing about any you&amp;#39;ve had with CDC.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://lassoo.io/blog/2023/11/20/quick-cdc-pipeline-with-eestuary-lassoo/?ref=reddit_de\"&gt;https://lassoo.io/blog/2023/11/20/quick-cdc-pipeline-with-eestuary-lassoo/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?auto=webp&amp;s=8f2cdf65ab04d656d09fa95f2010374b3a134977", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=91bb2e1714752353ffc2154340b4d12ce21f556d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=182afcec5846bf0a5fdafacdc3f14ccba30179d0", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e46cb77288ea03f0125e5feb506b0383eac8db5", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2c52d4b419821437165b6b1aa74043425cdbc15a", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7669a34964623e8ac15182b9718f3071ce5fc73e", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/7GeA1jc5SdEsoBVAhlmMphIshSh1LHfVk_p3hjnWP6g.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a622935fd526776af61ddd487acfb9838859bda", "width": 1080, "height": 607}], "variants": {}, "id": "h9abt165JkSxltaHAMtwAm6n0NuicYXDCGZoWpMk5vU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1853qic", "is_robot_indexable": true, "report_reasons": null, "author": "Crafty_Combination54", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1853qic/setting_up_a_cdc_pipeline_with_estuary_and_lassoo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1853qic/setting_up_a_cdc_pipeline_with_estuary_and_lassoo/", "subreddit_subscribers": 142264, "created_utc": 1701094003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi! Don't know if this is the best sub to ask but here we go.  \n\n\nI work at a medium sized company (200 office employees, 750 on site) as a Business Analyst, I have around 5/6 years experience as a Data Analyst but well the transition is a long story for another day.  \n\n\nCurrently I'm alone here basically with my boss, and we want to deploy a Datawarehouse, I've worked with those before many times, but never implemented one. We will probably go with Azure since the company is very Microsoft-y and such.   \n\n\nThe thing is that my boss recommended me to learn to do this stuff by myself, instead of hiring a consultor to do it for is. It's not that the company doesn't want to pay, they are very keen, but my boss is a great dude and he had the idea that it would be an awesome learning experience if we try to do it ourselves and, before investing too much time or going to production, only then hire a consultor to basically make sure we did everything fine.  \n\n\nOur data comes from different (but not too many) sources: an HSE system with which we connect through an API, an operational (we use it to schedule shifts and all that, basically our main) system with which we connect directly to their data blob, a finance system (Pronto XI) and then some other random stuff.  \n\n\nCan you guys recommend any learning path that may help me with this? I'm very excited to do this and want to get it right, we have access to LinkedIn learn so I thought about starting a Data Engineering course or something like that...  \n\n\nThanks a lot!!", "author_fullname": "t2_66n34smm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help a dude install a DWH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185jbtq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701133209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! Don&amp;#39;t know if this is the best sub to ask but here we go.  &lt;/p&gt;\n\n&lt;p&gt;I work at a medium sized company (200 office employees, 750 on site) as a Business Analyst, I have around 5/6 years experience as a Data Analyst but well the transition is a long story for another day.  &lt;/p&gt;\n\n&lt;p&gt;Currently I&amp;#39;m alone here basically with my boss, and we want to deploy a Datawarehouse, I&amp;#39;ve worked with those before many times, but never implemented one. We will probably go with Azure since the company is very Microsoft-y and such.   &lt;/p&gt;\n\n&lt;p&gt;The thing is that my boss recommended me to learn to do this stuff by myself, instead of hiring a consultor to do it for is. It&amp;#39;s not that the company doesn&amp;#39;t want to pay, they are very keen, but my boss is a great dude and he had the idea that it would be an awesome learning experience if we try to do it ourselves and, before investing too much time or going to production, only then hire a consultor to basically make sure we did everything fine.  &lt;/p&gt;\n\n&lt;p&gt;Our data comes from different (but not too many) sources: an HSE system with which we connect through an API, an operational (we use it to schedule shifts and all that, basically our main) system with which we connect directly to their data blob, a finance system (Pronto XI) and then some other random stuff.  &lt;/p&gt;\n\n&lt;p&gt;Can you guys recommend any learning path that may help me with this? I&amp;#39;m very excited to do this and want to get it right, we have access to LinkedIn learn so I thought about starting a Data Engineering course or something like that...  &lt;/p&gt;\n\n&lt;p&gt;Thanks a lot!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185jbtq", "is_robot_indexable": true, "report_reasons": null, "author": "Palpitation-Itchy", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185jbtq/help_a_dude_install_a_dwh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185jbtq/help_a_dude_install_a_dwh/", "subreddit_subscribers": 142264, "created_utc": 1701133209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks \n\nso i\u2019m designing a data model purely for analysis and BI, i thought of normalising the data first to reduce the size in each table, and to make it easier to modify the tables and easy data governance.\nThen, de-normalise the data again into a star schema based on the business need ofc.. to make it simple to understand and perform analysis and to reduce the number of joins \n\nI agree this depends on business requirements but i want to hear from another person what they think about this approach. \n\nthnx", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Normalisation &gt; Denormalisation star schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185d1h9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701117593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks &lt;/p&gt;\n\n&lt;p&gt;so i\u2019m designing a data model purely for analysis and BI, i thought of normalising the data first to reduce the size in each table, and to make it easier to modify the tables and easy data governance.\nThen, de-normalise the data again into a star schema based on the business need ofc.. to make it simple to understand and perform analysis and to reduce the number of joins &lt;/p&gt;\n\n&lt;p&gt;I agree this depends on business requirements but i want to hear from another person what they think about this approach. &lt;/p&gt;\n\n&lt;p&gt;thnx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185d1h9", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185d1h9/normalisation_denormalisation_star_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185d1h9/normalisation_denormalisation_star_schema/", "subreddit_subscribers": 142264, "created_utc": 1701117593.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to code a CLI that adds a layer above snapshoted metrics. And I want to open source it. The goal of that CLI is to generate a changelog report for a metric, in a dbt-native project (showing how it moved, opening issue for custom alerts, other stuff). \n\nGo: its new, fun, performant. There is a smaller community than python\n\nPython: Well everybody in data does python... And dbt is in python. And everything leads to python\n\n**If a fun open source project was in Go, would you contribute to it ? even if that meant learning go ?**\n\n[I have already started working on it](https://github.com/data-drift/data-drift), I started in Go for some part, but I needed python to deploy a Pypi lib. Now its hybrid, and I prefer working with go \ud83d\ude2c but the most rational thinking leads to python. ", "author_fullname": "t2_10uv2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would learn Go to contribute to an OS project ? Or should I stick to python ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1853kfp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1701093558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to code a CLI that adds a layer above snapshoted metrics. And I want to open source it. The goal of that CLI is to generate a changelog report for a metric, in a dbt-native project (showing how it moved, opening issue for custom alerts, other stuff). &lt;/p&gt;\n\n&lt;p&gt;Go: its new, fun, performant. There is a smaller community than python&lt;/p&gt;\n\n&lt;p&gt;Python: Well everybody in data does python... And dbt is in python. And everything leads to python&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;If a fun open source project was in Go, would you contribute to it ? even if that meant learning go ?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/data-drift/data-drift\"&gt;I have already started working on it&lt;/a&gt;, I started in Go for some part, but I needed python to deploy a Pypi lib. Now its hybrid, and I prefer working with go \ud83d\ude2c but the most rational thinking leads to python. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?auto=webp&amp;s=dd4bfd0f3ef1d90e367bbc5b3340444947cbaaee", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4b7f6f01a0d7b64cfd98a6395091b75f0fc1932", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0898d4f8d99b1aeedaa56ab08cc8a0aca9e65dac", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c6c27afd4a08250ef2d6d816ce59436ce207e9b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e4fbdf5012ddda5a8c5b1b392c7bc75b04ef4b8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ba3bd6ad7d2c4403d5eb16d02b80a5b6337ca6b", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/SDjByCsdmynAAWG7Ap9Spp9oxIGrFTH_nVguO1E9-Gs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=537297000bcccec2a747cf9d26ce342a52a6d239", "width": 1080, "height": 540}], "variants": {}, "id": "jT7i9UbQy0olPDx7s6vkLISm41g17NGiI0pwgrS1YM8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1853kfp", "is_robot_indexable": true, "report_reasons": null, "author": "Srammmy", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1853kfp/would_learn_go_to_contribute_to_an_os_project_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1853kfp/would_learn_go_to_contribute_to_an_os_project_or/", "subreddit_subscribers": 142264, "created_utc": 1701093558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As in title, I am wondering what people are seeing/charging for contracting rates as Senior Data Engineers in the UK currently. From the looks of things I am seeing rates advertised between \u00a3500 - \u00a3600, but I am wondering if this is the normal going rate or if people are charging more.\n\n&amp;#x200B;\n\nContext I currently work for Amazon as a Senior DE earning 130k total comp but want to move into contracting to maximise my income however I want to ensure that I am not leaving money on the table so to speak in any rate negotiations.\n\n&amp;#x200B;\n\n&amp;#x200B;", "author_fullname": "t2_75if3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "UK Senior Data Engineer Contracting Rates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185pyzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701154678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As in title, I am wondering what people are seeing/charging for contracting rates as Senior Data Engineers in the UK currently. From the looks of things I am seeing rates advertised between \u00a3500 - \u00a3600, but I am wondering if this is the normal going rate or if people are charging more.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Context I currently work for Amazon as a Senior DE earning 130k total comp but want to move into contracting to maximise my income however I want to ensure that I am not leaving money on the table so to speak in any rate negotiations.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "185pyzm", "is_robot_indexable": true, "report_reasons": null, "author": "Wtf_Pinkelephants", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185pyzm/uk_senior_data_engineer_contracting_rates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185pyzm/uk_senior_data_engineer_contracting_rates/", "subreddit_subscribers": 142264, "created_utc": 1701154678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to use TableauOperator to refresh our workbook at the end of my pipeline but it fails. I get an error saying I have a connection issue - too many retries. Anyone came across this issue?\nI\u2019m also open to suggestions using other type of operators", "author_fullname": "t2_8dnn00ks", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone ever used airflow to refresh a tableau workbook?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185flgh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701123631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use TableauOperator to refresh our workbook at the end of my pipeline but it fails. I get an error saying I have a connection issue - too many retries. Anyone came across this issue?\nI\u2019m also open to suggestions using other type of operators&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185flgh", "is_robot_indexable": true, "report_reasons": null, "author": "Se7enEl11ven", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185flgh/anyone_ever_used_airflow_to_refresh_a_tableau/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185flgh/anyone_ever_used_airflow_to_refresh_a_tableau/", "subreddit_subscribers": 142264, "created_utc": 1701123631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi folks. In an effort to learn more about streaming I was planning to set up a small personal project using Washington, DC's transit api (https://developer.wmata.com/docs/services/). \n\nMy initial idea (knowing very little about streaming) would be using Kafka/Spark to stream the data, store it in affordable cloud storage/data warehouse, and transform and serve the data in either a dashboard or a streamlit app to provide near real-time updates on transit. \n\nHowever, since the APIs refresh their data every 10-20 seconds I imagine storage could get expensive quickly and I really don't want to wake up to a $1000 bill. \n\nDoes anyone have advice on how to structure this project to be affordable? I'm not opposed to some small amount of cost (e.g. ~$10/month).", "author_fullname": "t2_550fb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Affordable ways to set up a streaming personal project?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_18567u0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701100738.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks. In an effort to learn more about streaming I was planning to set up a small personal project using Washington, DC&amp;#39;s transit api (&lt;a href=\"https://developer.wmata.com/docs/services/\"&gt;https://developer.wmata.com/docs/services/&lt;/a&gt;). &lt;/p&gt;\n\n&lt;p&gt;My initial idea (knowing very little about streaming) would be using Kafka/Spark to stream the data, store it in affordable cloud storage/data warehouse, and transform and serve the data in either a dashboard or a streamlit app to provide near real-time updates on transit. &lt;/p&gt;\n\n&lt;p&gt;However, since the APIs refresh their data every 10-20 seconds I imagine storage could get expensive quickly and I really don&amp;#39;t want to wake up to a $1000 bill. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have advice on how to structure this project to be affordable? I&amp;#39;m not opposed to some small amount of cost (e.g. ~$10/month).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "18567u0", "is_robot_indexable": true, "report_reasons": null, "author": "PureOhms", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/18567u0/affordable_ways_to_set_up_a_streaming_personal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/18567u0/affordable_ways_to_set_up_a_streaming_personal/", "subreddit_subscribers": 142264, "created_utc": 1701100738.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you were interested in Iceberg tables, they are finally available for all accounts on snowflake \ud83e\uddca\nIt's time to test them.  \nTo those who already tried them : how was the query performance for you?\n\nhttps://docs.snowflake.com/en/release-notes/2023/7_42", "author_fullname": "t2_w6z0w1b6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Iceberg tables are now available in public preview on snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_185tgn0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701169209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you were interested in Iceberg tables, they are finally available for all accounts on snowflake \ud83e\uddca\nIt&amp;#39;s time to test them.&lt;br/&gt;\nTo those who already tried them : how was the query performance for you?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.snowflake.com/en/release-notes/2023/7_42\"&gt;https://docs.snowflake.com/en/release-notes/2023/7_42&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185tgn0", "is_robot_indexable": true, "report_reasons": null, "author": "sdc-msimon", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185tgn0/iceberg_tables_are_now_available_in_public/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185tgn0/iceberg_tables_are_now_available_in_public/", "subreddit_subscribers": 142264, "created_utc": 1701169209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anybody else had recruiters ask for every damn think in their mental toolbox, when all the client actually wants is somebody to hold their hand and tell them what data is? AWS, GCP, Python, Redshift,SQL, TSQL, PSQL, cloud infrastructure, terraform etc.... come on give us a chance. Give us a use case and we will go to work.", "author_fullname": "t2_8p2u7cmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Knobhead recruiters", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_185tdkb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701168849.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anybody else had recruiters ask for every damn think in their mental toolbox, when all the client actually wants is somebody to hold their hand and tell them what data is? AWS, GCP, Python, Redshift,SQL, TSQL, PSQL, cloud infrastructure, terraform etc.... come on give us a chance. Give us a use case and we will go to work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185tdkb", "is_robot_indexable": true, "report_reasons": null, "author": "Jamese0", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185tdkb/knobhead_recruiters/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185tdkb/knobhead_recruiters/", "subreddit_subscribers": 142264, "created_utc": 1701168849.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working as a data engineer for a company that after a merger operates in three countries with different currencies. We need to support local currency reporting, and reporting across the company using all three currencies.\n\nI'm thinking a bit about how to model this in an effective way in the data platform, and there are some options, to which I wanted to see if anyone has good/bad experiences working with multiple currencies. For the record our tech stack is dbt, Databricks and Power BI.\n\nI found this data modelling option from [Kimball](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/multiple-currencies/) that says to store the local unconverted currency (what's coming from the source) along with a chosen single \"main\" currency. Then we provide an exchange rate table and let users/analysts convert currencies in queries/notebooks/Power BI reports when needed. \n\nI imagine to prevent creating a mess of conversions in the platform I would suggest users to use the \"main\" currency in their work and only convert to local currencies in their presentation layers. That way we don't have to convert numbers back and forwards, and it's easy to know when to convert the currency.\n\nDoes anyone here have any experience modelling multiple currencies in this way? Any good or bad experiences? Or are there any other options we should consider?", "author_fullname": "t2_11b4ct", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle reporting in multiple currencies?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185sl1j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701165652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working as a data engineer for a company that after a merger operates in three countries with different currencies. We need to support local currency reporting, and reporting across the company using all three currencies.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking a bit about how to model this in an effective way in the data platform, and there are some options, to which I wanted to see if anyone has good/bad experiences working with multiple currencies. For the record our tech stack is dbt, Databricks and Power BI.&lt;/p&gt;\n\n&lt;p&gt;I found this data modelling option from &lt;a href=\"https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/multiple-currencies/\"&gt;Kimball&lt;/a&gt; that says to store the local unconverted currency (what&amp;#39;s coming from the source) along with a chosen single &amp;quot;main&amp;quot; currency. Then we provide an exchange rate table and let users/analysts convert currencies in queries/notebooks/Power BI reports when needed. &lt;/p&gt;\n\n&lt;p&gt;I imagine to prevent creating a mess of conversions in the platform I would suggest users to use the &amp;quot;main&amp;quot; currency in their work and only convert to local currencies in their presentation layers. That way we don&amp;#39;t have to convert numbers back and forwards, and it&amp;#39;s easy to know when to convert the currency.&lt;/p&gt;\n\n&lt;p&gt;Does anyone here have any experience modelling multiple currencies in this way? Any good or bad experiences? Or are there any other options we should consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185sl1j", "is_robot_indexable": true, "report_reasons": null, "author": "mmammies", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185sl1j/how_do_you_handle_reporting_in_multiple_currencies/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185sl1j/how_do_you_handle_reporting_in_multiple_currencies/", "subreddit_subscribers": 142264, "created_utc": 1701165652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_h557nj7lc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "70 Free Online Courses for Data Science to Advance Your Skills in 2024", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_185oqzd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/gwr0_-AUrmXA5cVRWIalFdHrL400g06VJ5hwnLZ19yw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1701149962.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "mltut.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.mltut.com/free-online-courses-for-data-science/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/HOaS2OUV9bdQjUKzInlijgHCBXFfxdCnC7j5HariO58.jpg?auto=webp&amp;s=1fca9a29266e692014028b149298c7d1247c5051", "width": 2240, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/HOaS2OUV9bdQjUKzInlijgHCBXFfxdCnC7j5HariO58.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb1fa4072f82b2aec6b8585fa5202ce717948810", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/HOaS2OUV9bdQjUKzInlijgHCBXFfxdCnC7j5HariO58.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e1301917f67715bd8b8c0b9d95b387458449a713", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/HOaS2OUV9bdQjUKzInlijgHCBXFfxdCnC7j5HariO58.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6c180aae51d18fa96c188c7384654f60c4f272fb", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/HOaS2OUV9bdQjUKzInlijgHCBXFfxdCnC7j5HariO58.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c6b3ff762c6c69cd91406aa42bdbd148968d22b8", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/HOaS2OUV9bdQjUKzInlijgHCBXFfxdCnC7j5HariO58.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e01afd0073d002de857e2288662b9271b5f8095", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/HOaS2OUV9bdQjUKzInlijgHCBXFfxdCnC7j5HariO58.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9dfcfa2dc9476cda2037f1b6d240b07529e3a89", "width": 1080, "height": 607}], "variants": {}, "id": "PZId2E0FyDwqEJStxKklFxXSQjnGz1tprLX0s3UM5Ow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "185oqzd", "is_robot_indexable": true, "report_reasons": null, "author": "Aqsa81", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185oqzd/70_free_online_courses_for_data_science_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.mltut.com/free-online-courses-for-data-science/", "subreddit_subscribers": 142264, "created_utc": 1701149962.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello Community Folks, \n\nI have been reading up on differential privacy, k-anonymity, etc. What is their usecases in large organisations who are not really sharing data for public use however their use cases for data shares internally within the org might be high. This is based on the assumption that directly identifiable attributes like name, email, SSN are masked, are there any further steps being taken to protect quasi identifiers.   \nI'm working in an org which has adopted data mesh approach with multiple dbt projects across various use cases running on Snowflake. All production datasets have to be column level tagged as a requirement for Immuta to take care of cataloging and security.   \nI'm keen to understand any frameworks or patterns being used to enable data privacy beyond masking. ", "author_fullname": "t2_dv4ply58", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Use Cases for Data Privacy", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185ktgw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701137431.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello Community Folks, &lt;/p&gt;\n\n&lt;p&gt;I have been reading up on differential privacy, k-anonymity, etc. What is their usecases in large organisations who are not really sharing data for public use however their use cases for data shares internally within the org might be high. This is based on the assumption that directly identifiable attributes like name, email, SSN are masked, are there any further steps being taken to protect quasi identifiers.&lt;br/&gt;\nI&amp;#39;m working in an org which has adopted data mesh approach with multiple dbt projects across various use cases running on Snowflake. All production datasets have to be column level tagged as a requirement for Immuta to take care of cataloging and security.&lt;br/&gt;\nI&amp;#39;m keen to understand any frameworks or patterns being used to enable data privacy beyond masking. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185ktgw", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Apple_420", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185ktgw/use_cases_for_data_privacy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185ktgw/use_cases_for_data_privacy/", "subreddit_subscribers": 142264, "created_utc": 1701137431.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my industry, we use the term metadata to mean the data that describes data. We have business metadata and technical metadata to describe our data. As an example, for data element \u201ccustomer name\u201d, we have business metadata like name of data element, description, owner of data, data source, domain, etc. We also have technical metadata like column, database, length, data type. \n\nI noticed in the news that law enforcement use \u201cmetadata\u201d differently and basically what they\u2019re referring to is just data. I see this with phone records:\n\n\u201cA phone's metadata describes key facts about an individual data file such as phone calls, photographs, texts, etc. With this data, you might reveal habits, activities and interests, or even uncover a lie.\u201d\n\nIsn\u2019t a lot of phone calls just data?\n\nIs metadata just a nebulous term and it gets used differently in different industries like law enforcement vs corporations, etc. Any thoughts on this discrepancy?", "author_fullname": "t2_6f5ggoc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Nebulous usage of term \u201cmetadata\u201d", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185ce5s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701116016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my industry, we use the term metadata to mean the data that describes data. We have business metadata and technical metadata to describe our data. As an example, for data element \u201ccustomer name\u201d, we have business metadata like name of data element, description, owner of data, data source, domain, etc. We also have technical metadata like column, database, length, data type. &lt;/p&gt;\n\n&lt;p&gt;I noticed in the news that law enforcement use \u201cmetadata\u201d differently and basically what they\u2019re referring to is just data. I see this with phone records:&lt;/p&gt;\n\n&lt;p&gt;\u201cA phone&amp;#39;s metadata describes key facts about an individual data file such as phone calls, photographs, texts, etc. With this data, you might reveal habits, activities and interests, or even uncover a lie.\u201d&lt;/p&gt;\n\n&lt;p&gt;Isn\u2019t a lot of phone calls just data?&lt;/p&gt;\n\n&lt;p&gt;Is metadata just a nebulous term and it gets used differently in different industries like law enforcement vs corporations, etc. Any thoughts on this discrepancy?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185ce5s", "is_robot_indexable": true, "report_reasons": null, "author": "wackomama", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185ce5s/nebulous_usage_of_term_metadata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185ce5s/nebulous_usage_of_term_metadata/", "subreddit_subscribers": 142264, "created_utc": 1701116016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How common is it to reuse spark batch function in spark streaming ? Specifically when migration to windowing and states. \n\nAnd what would be the best practice for doing that from experience ?", "author_fullname": "t2_j15inhpup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reusing spark batch and spark streaming functions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185buch", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701114620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How common is it to reuse spark batch function in spark streaming ? Specifically when migration to windowing and states. &lt;/p&gt;\n\n&lt;p&gt;And what would be the best practice for doing that from experience ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "185buch", "is_robot_indexable": true, "report_reasons": null, "author": "springRock88", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185buch/reusing_spark_batch_and_spark_streaming_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185buch/reusing_spark_batch_and_spark_streaming_functions/", "subreddit_subscribers": 142264, "created_utc": 1701114620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "hi \ud83d\udc4b \n\nthis is an entry level question: \n\nif i\u2019m connecting a tool to my data warehouse, such as hubspot or any other saas service. \n\nis it gonna be designed already and ready to query or it will be normalised and i need to model the data and design the schema in a star model for the analysis?\n\nbest,", "author_fullname": "t2_flu4lsm6m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Connecting tools to DW", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185rhal", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701160949.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hi \ud83d\udc4b &lt;/p&gt;\n\n&lt;p&gt;this is an entry level question: &lt;/p&gt;\n\n&lt;p&gt;if i\u2019m connecting a tool to my data warehouse, such as hubspot or any other saas service. &lt;/p&gt;\n\n&lt;p&gt;is it gonna be designed already and ready to query or it will be normalised and i need to model the data and design the schema in a star model for the analysis?&lt;/p&gt;\n\n&lt;p&gt;best,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185rhal", "is_robot_indexable": true, "report_reasons": null, "author": "Fuzzy-Example-7326", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185rhal/connecting_tools_to_dw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185rhal/connecting_tools_to_dw/", "subreddit_subscribers": 142264, "created_utc": 1701160949.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am breaking my head over this problem. My next part of the problem is to connect different connectors and consume their metadata. But, I am not able to do it via Postgres, I am not sure whether I can scale it to connect other RDBMS or any other database.\n\nThere are bits and pieces of information like using sqoop or something. But I don't want to create a clone of database and read metadata. I simply want to connect to postgres db, read entire metadata, and load into Apache Atlas. \n\nHow would I do it?", "author_fullname": "t2_1k0o7czh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Importing Postgres Metadata to Apache Atlas - Open Source Method only", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185rfyy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701160795.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am breaking my head over this problem. My next part of the problem is to connect different connectors and consume their metadata. But, I am not able to do it via Postgres, I am not sure whether I can scale it to connect other RDBMS or any other database.&lt;/p&gt;\n\n&lt;p&gt;There are bits and pieces of information like using sqoop or something. But I don&amp;#39;t want to create a clone of database and read metadata. I simply want to connect to postgres db, read entire metadata, and load into Apache Atlas. &lt;/p&gt;\n\n&lt;p&gt;How would I do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185rfyy", "is_robot_indexable": true, "report_reasons": null, "author": "DesmonMiles07", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185rfyy/importing_postgres_metadata_to_apache_atlas_open/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185rfyy/importing_postgres_metadata_to_apache_atlas_open/", "subreddit_subscribers": 142264, "created_utc": 1701160795.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys\u2026 hope you doing great.\nI\u2019ve this certification exam soon and I am studying with some past questions materials. If you\u2019ve had this exam before, how often are questions repeated on a scale of 1 - 10 (lowest to highest)\n\nLooking forward to your responses \ud83d\ude4f", "author_fullname": "t2_n5qii71ai", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP professional data engineer exam question repetition", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185mfcp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701142104.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys\u2026 hope you doing great.\nI\u2019ve this certification exam soon and I am studying with some past questions materials. If you\u2019ve had this exam before, how often are questions repeated on a scale of 1 - 10 (lowest to highest)&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your responses \ud83d\ude4f&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185mfcp", "is_robot_indexable": true, "report_reasons": null, "author": "ImaginationOdd3610", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185mfcp/gcp_professional_data_engineer_exam_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185mfcp/gcp_professional_data_engineer_exam_question/", "subreddit_subscribers": 142264, "created_utc": 1701142104.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi community, I m currently trying to implement dbt model for dumping a view data from snowflake to bigquery. One approach which I m following is use copy command and load it to GCS and then use bq load to bigquery. But are there any other approaches or dbt utilities available.", "author_fullname": "t2_602lxjl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dbt utilities for easy data dump to bigquery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185k7mu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701135687.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi community, I m currently trying to implement dbt model for dumping a view data from snowflake to bigquery. One approach which I m following is use copy command and load it to GCS and then use bq load to bigquery. But are there any other approaches or dbt utilities available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185k7mu", "is_robot_indexable": true, "report_reasons": null, "author": "ankititachi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185k7mu/dbt_utilities_for_easy_data_dump_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185k7mu/dbt_utilities_for_easy_data_dump_to_bigquery/", "subreddit_subscribers": 142264, "created_utc": 1701135687.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all,\n\nThe team i work in uses cloud functions (sometimes with workflows) to do ingestions from SFTPs back into BigQuery\n\n\nUsually they run out of compute and it fails. How do you ingest data in your teams in GCP?", "author_fullname": "t2_851if4wo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud functions for data ingestion", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_185agjf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1701111184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;The team i work in uses cloud functions (sometimes with workflows) to do ingestions from SFTPs back into BigQuery&lt;/p&gt;\n\n&lt;p&gt;Usually they run out of compute and it fails. How do you ingest data in your teams in GCP?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "185agjf", "is_robot_indexable": true, "report_reasons": null, "author": "No-Dress-3160", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/185agjf/cloud_functions_for_data_ingestion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/185agjf/cloud_functions_for_data_ingestion/", "subreddit_subscribers": 142264, "created_utc": 1701111184.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}