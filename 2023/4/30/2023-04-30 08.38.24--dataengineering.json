{"kind": "Listing", "data": {"after": null, "dist": 13, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company uses a lot of cool stuff from the typical MDS but I\u2019m honestly not sure if I have ever completed a ticket that truly \u201crequired\u201d anything beyond SQL, a database vendor specific ETL tool, and maybe some python.  \n\nI feel like most of the work I do is the result of bad business analysts and bad requirements gathering. Building data lakes, giant complicated models, calling API\u2019s constantly, unpacking absurd amounts of flat files, all to build data pipelines to drown teams with \u201creal time\u201d data because that\u2019s what they asked for, when they really just need a few KPI aggregated monthly.  \n\nWondering how common this is? To feel like everything you do is extreme overkill relative to the actual business problem that needs solving", "author_fullname": "t2_4nbvm2s0m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools do you \u201cneed\u201d to do your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1331rzp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 55, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 55, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682789633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company uses a lot of cool stuff from the typical MDS but I\u2019m honestly not sure if I have ever completed a ticket that truly \u201crequired\u201d anything beyond SQL, a database vendor specific ETL tool, and maybe some python.  &lt;/p&gt;\n\n&lt;p&gt;I feel like most of the work I do is the result of bad business analysts and bad requirements gathering. Building data lakes, giant complicated models, calling API\u2019s constantly, unpacking absurd amounts of flat files, all to build data pipelines to drown teams with \u201creal time\u201d data because that\u2019s what they asked for, when they really just need a few KPI aggregated monthly.  &lt;/p&gt;\n\n&lt;p&gt;Wondering how common this is? To feel like everything you do is extreme overkill relative to the actual business problem that needs solving&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1331rzp", "is_robot_indexable": true, "report_reasons": null, "author": "Visible-Tennis4144", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1331rzp/what_tools_do_you_need_to_do_your_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1331rzp/what_tools_do_you_need_to_do_your_job/", "subreddit_subscribers": 103255, "created_utc": 1682789633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a student still in engineering school. I apologize in advance for my ignorance. I've been frustrated with trying to understand the use case of Kafka for over a week so I'm sorry if I come off as too aggressive against Kafka.  \nI have a big data project and my professor would like us to use Kafka for realtime (unsure if realtime ingestion, processing or both) and also use Spark.  \nI've been reading up and learning both of those technologies and they seem to be quite similar.   \nFrom what I understand, the only reason to use Kafka would be to make different components communicate with each other in a microservices architecture or for event driven architecture. That's fine. But I can't for the sake of anything holly understand why Kafka would be considered for 'realtime' or for data ingestion.  \nSo I tried to study the following case: Get tweets from Twitter API =&gt; clean-up =&gt; run sentiment analysis =&gt; store.  \nNow, I've been told that Spark is the god of data processing, especially for high volumes and that Spark Streaming is also powerful.   \nFrom where I see it, I have to options:\n\nOption 1:   \nPoll Twitter API every 5 minutes with a Get request =&gt; use Spark Streaming to cleanup &amp; process data in memory =&gt; store in database/datalake.\n\n  \nOption 2:  \nPoll Twitter API every 5 minutes with a Get request =&gt; store in Kafka topics =&gt; listen to topics =&gt; use Spark Streaming to cleanup &amp; process data in memory =&gt; store in database/datalake.  \n\n\nOption 2 adds a whole level of complexity and failure points for no apparent added value.  If anyone could explain what I am missing here that does not involve the following points :  \n\\-Fault tolerance (already exists in Spark clusters)  \n\\-Real-time (it's practically as real-time as Spark streaming)  \n\\-Communication between components (I admit this is a great added value but not relevant in my project)  \n\\-Storage (I have read over&amp;over again that Kafka should not be used as a real storage system)  \n\n\nI would really appreciate your help, especially if concrete examples are given.", "author_fullname": "t2_agpkokzd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused and frustrated about Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133946d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682808656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a student still in engineering school. I apologize in advance for my ignorance. I&amp;#39;ve been frustrated with trying to understand the use case of Kafka for over a week so I&amp;#39;m sorry if I come off as too aggressive against Kafka.&lt;br/&gt;\nI have a big data project and my professor would like us to use Kafka for realtime (unsure if realtime ingestion, processing or both) and also use Spark.&lt;br/&gt;\nI&amp;#39;ve been reading up and learning both of those technologies and they seem to be quite similar.&lt;br/&gt;\nFrom what I understand, the only reason to use Kafka would be to make different components communicate with each other in a microservices architecture or for event driven architecture. That&amp;#39;s fine. But I can&amp;#39;t for the sake of anything holly understand why Kafka would be considered for &amp;#39;realtime&amp;#39; or for data ingestion.&lt;br/&gt;\nSo I tried to study the following case: Get tweets from Twitter API =&amp;gt; clean-up =&amp;gt; run sentiment analysis =&amp;gt; store.&lt;br/&gt;\nNow, I&amp;#39;ve been told that Spark is the god of data processing, especially for high volumes and that Spark Streaming is also powerful.&lt;br/&gt;\nFrom where I see it, I have to options:&lt;/p&gt;\n\n&lt;p&gt;Option 1:&lt;br/&gt;\nPoll Twitter API every 5 minutes with a Get request =&amp;gt; use Spark Streaming to cleanup &amp;amp; process data in memory =&amp;gt; store in database/datalake.&lt;/p&gt;\n\n&lt;p&gt;Option 2:&lt;br/&gt;\nPoll Twitter API every 5 minutes with a Get request =&amp;gt; store in Kafka topics =&amp;gt; listen to topics =&amp;gt; use Spark Streaming to cleanup &amp;amp; process data in memory =&amp;gt; store in database/datalake.  &lt;/p&gt;\n\n&lt;p&gt;Option 2 adds a whole level of complexity and failure points for no apparent added value.  If anyone could explain what I am missing here that does not involve the following points :&lt;br/&gt;\n-Fault tolerance (already exists in Spark clusters)&lt;br/&gt;\n-Real-time (it&amp;#39;s practically as real-time as Spark streaming)&lt;br/&gt;\n-Communication between components (I admit this is a great added value but not relevant in my project)&lt;br/&gt;\n-Storage (I have read over&amp;amp;over again that Kafka should not be used as a real storage system)  &lt;/p&gt;\n\n&lt;p&gt;I would really appreciate your help, especially if concrete examples are given.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "133946d", "is_robot_indexable": true, "report_reasons": null, "author": "Leather-Ad9576", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133946d/confused_and_frustrated_about_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133946d/confused_and_frustrated_about_kafka/", "subreddit_subscribers": 103255, "created_utc": 1682808656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm gathering data from a large number of sites (e-commerce), and when my scraping scripts run I can either \n\n1. directly insert the scraped data into my postgresql database that is used by the user interfacing parts of this product.\n2. insert the data into S3 (or something similar), have a separate program that extracts from S3 and inserts the data into the postgres db.\n\nPros with number 1:\n\n* less things to manage -&gt; less complexity in a sense\n* less data to store in total\n\nPros with number 2:\n\n* will always have the scraped data from day x stored, and it will in a sense store a backup of the data\n* will come in handy if want to do something else with the data, for example set up a graph database or do some analytics that will need stuff that I decided to not put into the postgres db\n\nWhat are your thoughts and what is the standard approach? I'm currently doing approach 1 and while it works it also feels a little weird to have the scraping and database inserting parts coupled together into the same programs.", "author_fullname": "t2_33835h5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Large scale web scraping - storing data directly in postgres or use S3 as an intermediate step?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132wwue", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682780616.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m gathering data from a large number of sites (e-commerce), and when my scraping scripts run I can either &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;directly insert the scraped data into my postgresql database that is used by the user interfacing parts of this product.&lt;/li&gt;\n&lt;li&gt;insert the data into S3 (or something similar), have a separate program that extracts from S3 and inserts the data into the postgres db.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Pros with number 1:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;less things to manage -&amp;gt; less complexity in a sense&lt;/li&gt;\n&lt;li&gt;less data to store in total&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Pros with number 2:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;will always have the scraped data from day x stored, and it will in a sense store a backup of the data&lt;/li&gt;\n&lt;li&gt;will come in handy if want to do something else with the data, for example set up a graph database or do some analytics that will need stuff that I decided to not put into the postgres db&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What are your thoughts and what is the standard approach? I&amp;#39;m currently doing approach 1 and while it works it also feels a little weird to have the scraping and database inserting parts coupled together into the same programs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "132wwue", "is_robot_indexable": true, "report_reasons": null, "author": "lapurita", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/132wwue/large_scale_web_scraping_storing_data_directly_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/132wwue/large_scale_web_scraping_storing_data_directly_in/", "subreddit_subscribers": 103255, "created_utc": 1682780616.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_z3784il", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13327ll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Ww8xyjmtg3A?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Ww8xyjmtg3A?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)\"&gt;&lt;/iframe&gt;", "author_name": "CMU Database Group", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Ww8xyjmtg3A/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CMUDatabaseGroup"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Ww8xyjmtg3A?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13327ll", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/P2mDw8Z-0Rg8ZV--97XK5ZmgziVC_rv2jsxDV6VWMsY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682790752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=Ww8xyjmtg3A", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/g7aL2Y2WH_CkTEB7cdGpCegCp-HBV8L71D-RYJXbevo.jpg?auto=webp&amp;v=enabled&amp;s=974f76b9c227d742df987223a99a55eae032d0b7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/g7aL2Y2WH_CkTEB7cdGpCegCp-HBV8L71D-RYJXbevo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3aeef8a9aa7b3545177087497121d37b1ce9764d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/g7aL2Y2WH_CkTEB7cdGpCegCp-HBV8L71D-RYJXbevo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bd500c2a8925179978f87f31728e6caf97d9c74", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/g7aL2Y2WH_CkTEB7cdGpCegCp-HBV8L71D-RYJXbevo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=918f1f4cc525ca2db8e56e123384c06d1eded8e9", "width": 320, "height": 240}], "variants": {}, "id": "nFgAxd2naTw_dqfWbKLjnTWgj-VlHobf_1ColE86mHA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13327ll", "is_robot_indexable": true, "report_reasons": null, "author": "boy_named_su", "discussion_type": null, "num_comments": 3, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13327ll/amazon_redshift_internals_cmu_advanced_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=Ww8xyjmtg3A", "subreddit_subscribers": 103255, "created_utc": 1682790752.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Ww8xyjmtg3A?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)\"&gt;&lt;/iframe&gt;", "author_name": "CMU Database Group", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Ww8xyjmtg3A/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CMUDatabaseGroup"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious how many others take this approach. \n\nI don\u2019t comment my code.\n\nI start by writing some placeholders and commenting what I want them to do. Then once all my comments are in place, I write the code to match it.\n\nAm I abnormal? It this an inherently bad approach?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Commenting code, or coding comments?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133b5hk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682814336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how many others take this approach. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t comment my code.&lt;/p&gt;\n\n&lt;p&gt;I start by writing some placeholders and commenting what I want them to do. Then once all my comments are in place, I write the code to match it.&lt;/p&gt;\n\n&lt;p&gt;Am I abnormal? It this an inherently bad approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "133b5hk", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133b5hk/commenting_code_or_coding_comments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133b5hk/commenting_code_or_coding_comments/", "subreddit_subscribers": 103255, "created_utc": 1682814336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys!\nI\u2019m new to Spark Streaming and exploring it. I\u2019m facing an issue right now with respect to performance and hoping to get a solution.\n\nThe details are as follows,\nI believe Metadata logs get compacted after every 10 batches by default for streaming applications under _spark_metadata directory. My trigger window is 60 seconds and pulling in max of 5 million records per window. I notice that after every 10 batches the 11th batch is taking more and more time as it goes on. This compaction batch takes 30x more time than regular batch. Is there any easy resolution to this issue ? \n\nSome of the suggestions I saw are,\n* Shredding the latest compact file by filtering out the old file details which is not an easy process as I have to write custom code.\n* Increasing the compact interval to a bigger number. I tried this by changing the parameter spark.sql.streaming.filesink.compactInterval to 50 but still I don\u2019t see it reflecting. Please suggest.", "author_fullname": "t2_1zefn71o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark streaming - metadata log compaction", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132vf2c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682779187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys!\nI\u2019m new to Spark Streaming and exploring it. I\u2019m facing an issue right now with respect to performance and hoping to get a solution.&lt;/p&gt;\n\n&lt;p&gt;The details are as follows,\nI believe Metadata logs get compacted after every 10 batches by default for streaming applications under _spark_metadata directory. My trigger window is 60 seconds and pulling in max of 5 million records per window. I notice that after every 10 batches the 11th batch is taking more and more time as it goes on. This compaction batch takes 30x more time than regular batch. Is there any easy resolution to this issue ? &lt;/p&gt;\n\n&lt;p&gt;Some of the suggestions I saw are,\n* Shredding the latest compact file by filtering out the old file details which is not an easy process as I have to write custom code.\n* Increasing the compact interval to a bigger number. I tried this by changing the parameter spark.sql.streaming.filesink.compactInterval to 50 but still I don\u2019t see it reflecting. Please suggest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "132vf2c", "is_robot_indexable": true, "report_reasons": null, "author": "sirajz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/132vf2c/spark_streaming_metadata_log_compaction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/132vf2c/spark_streaming_metadata_log_compaction/", "subreddit_subscribers": 103255, "created_utc": 1682779187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently a data engineer with no experience using AWS or any cloud computing and I just want advice is it worth me taking this exam or just doing an AWS course instead ?", "author_fullname": "t2_3xupopvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Aws Associate solution architect, exam", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132ygb0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682782029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently a data engineer with no experience using AWS or any cloud computing and I just want advice is it worth me taking this exam or just doing an AWS course instead ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "132ygb0", "is_robot_indexable": true, "report_reasons": null, "author": "Sulaiman_m97", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/132ygb0/aws_associate_solution_architect_exam/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/132ygb0/aws_associate_solution_architect_exam/", "subreddit_subscribers": 103255, "created_utc": 1682782029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of my recent tasks involves syncing aggregated metrics from out data warehouse to a marketing platform so that the marketing team can make campaigns that target, for example, all users that made 5 or more orders in the last week. \n\nDue to limitations with the marketing platform\u2019s reporting api, we have to download all of the existing data at once instead of by user. This requires downloading a 1 gigabyte text file, filtering out the random attributes we didn\u2019t write, and comparing each user with the existing data in our data warehouse.  \n\nThe goal is to end up with a list of JSON objects to send to the marketing platform that only contain the changed attributes because the platform charges us by the attribute. \n\nWhat\u2019s the best way to compare two giant lists of json objects by user id and only return the attributes that are different?", "author_fullname": "t2_c5ysf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to compare two large new line delimited JSON files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_133i4tr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682837659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of my recent tasks involves syncing aggregated metrics from out data warehouse to a marketing platform so that the marketing team can make campaigns that target, for example, all users that made 5 or more orders in the last week. &lt;/p&gt;\n\n&lt;p&gt;Due to limitations with the marketing platform\u2019s reporting api, we have to download all of the existing data at once instead of by user. This requires downloading a 1 gigabyte text file, filtering out the random attributes we didn\u2019t write, and comparing each user with the existing data in our data warehouse.  &lt;/p&gt;\n\n&lt;p&gt;The goal is to end up with a list of JSON objects to send to the marketing platform that only contain the changed attributes because the platform charges us by the attribute. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s the best way to compare two giant lists of json objects by user id and only return the attributes that are different?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "133i4tr", "is_robot_indexable": true, "report_reasons": null, "author": "cellularcone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133i4tr/whats_the_best_way_to_compare_two_large_new_line/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133i4tr/whats_the_best_way_to_compare_two_large_new_line/", "subreddit_subscribers": 103255, "created_utc": 1682837659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve always wondered about this:\n\nIs it more efficient and secure to call the APIs of a transactional system or connect to the transactional system\u2019s database via ODBC?\n\nMore context:\n\nWe have 2 deployments currently for ServiceNow reporting:\n\n1. An ETL tool calls the APIs of SeviceNow and inserts the output into a relational database which is then connected to a BI tool\n\n2. The BI tool directly connects to the ServiceNow DB via an ODBC driver and runs distributed queries on the DB. This seems like a less of a work to us. \n\nBoth approaches have served well for past 4-5 years so was wondering what the community has to say about both the approaches?\n\nThanks in advance!", "author_fullname": "t2_vls2w775", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which of the below approaches is more secure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1334cfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682796202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve always wondered about this:&lt;/p&gt;\n\n&lt;p&gt;Is it more efficient and secure to call the APIs of a transactional system or connect to the transactional system\u2019s database via ODBC?&lt;/p&gt;\n\n&lt;p&gt;More context:&lt;/p&gt;\n\n&lt;p&gt;We have 2 deployments currently for ServiceNow reporting:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;An ETL tool calls the APIs of SeviceNow and inserts the output into a relational database which is then connected to a BI tool&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The BI tool directly connects to the ServiceNow DB via an ODBC driver and runs distributed queries on the DB. This seems like a less of a work to us. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both approaches have served well for past 4-5 years so was wondering what the community has to say about both the approaches?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1334cfc", "is_robot_indexable": true, "report_reasons": null, "author": "mysterioustechie", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1334cfc/which_of_the_below_approaches_is_more_secure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1334cfc/which_of_the_below_approaches_is_more_secure/", "subreddit_subscribers": 103255, "created_utc": 1682796202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working on a big data quality project for my company, trying to clean up our account and contact data. The MDM team (myself included) believes that since those records were entered by our many sales reps out in the field, they should be the people we submit our DQ reports to, as they would easily be the most likely to know how current the data in their territories is and what updates they need to make to anything that pops up as invalid or questionable.\n\nHowever, we\u2019ve hit a roadblock. The executive we meet with has been a champion of our project in general, but he argues that the sales reps shouldn\u2019t be responsible for cleaning up their data because it would mean they\u2019d be spending time away from selling. Or at the very least, when the project is presented to the execs above him, that\u2019s how he believes they would react. So far he hasn\u2019t given any alternative solution when we ask \u201cwell who else is going to do it?\u201d\n\nSo what we\u2019re looking for is a convincing argument to support why the sales reps should indeed own their data and its quality. It\u2019s not a matter of convincing anyone on the benefits of cleaner data, or the downside to leaving it alone; we all agree on that. But we need to come up with something that will change minds about sales reps taking ownership and spending some time making any updates based on our reports. At this point the reports are planned to go out quarterly, and of course the initial cleanup will be the largest lift before settling into regular maintenance mode.\n\nThoughts? Thanks kindly in advance and sorry for the bad prose, this is the first time I\u2019ve written this all out.", "author_fullname": "t2_6kftfg7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Convincing argument for sales reps to own their data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1338rvs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682807707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working on a big data quality project for my company, trying to clean up our account and contact data. The MDM team (myself included) believes that since those records were entered by our many sales reps out in the field, they should be the people we submit our DQ reports to, as they would easily be the most likely to know how current the data in their territories is and what updates they need to make to anything that pops up as invalid or questionable.&lt;/p&gt;\n\n&lt;p&gt;However, we\u2019ve hit a roadblock. The executive we meet with has been a champion of our project in general, but he argues that the sales reps shouldn\u2019t be responsible for cleaning up their data because it would mean they\u2019d be spending time away from selling. Or at the very least, when the project is presented to the execs above him, that\u2019s how he believes they would react. So far he hasn\u2019t given any alternative solution when we ask \u201cwell who else is going to do it?\u201d&lt;/p&gt;\n\n&lt;p&gt;So what we\u2019re looking for is a convincing argument to support why the sales reps should indeed own their data and its quality. It\u2019s not a matter of convincing anyone on the benefits of cleaner data, or the downside to leaving it alone; we all agree on that. But we need to come up with something that will change minds about sales reps taking ownership and spending some time making any updates based on our reports. At this point the reports are planned to go out quarterly, and of course the initial cleanup will be the largest lift before settling into regular maintenance mode.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Thanks kindly in advance and sorry for the bad prose, this is the first time I\u2019ve written this all out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1338rvs", "is_robot_indexable": true, "report_reasons": null, "author": "usarasa", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1338rvs/convincing_argument_for_sales_reps_to_own_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1338rvs/convincing_argument_for_sales_reps_to_own_their/", "subreddit_subscribers": 103255, "created_utc": 1682807707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI am a grad student in Applied Data Analytics. I'll be graduating this summer, but before doing so I have to take one more elective.\n\nMy goal is to one day transition into data engineering - I know this is difficult as an entry-level job, but I want to position myself so that I'm a good fit one day.\n\nI was set on taking Big Data Analytics as my elective, however, the on-campus class has been cancelled so the only option I have to take it is online for six/seven weeks, which is more intensive...and costs as much as taking it in person.\n\nI was thinking to instead take a Software Engineering class, I come form a non-technical background and have good Python skills, but feel like taking a Software Engineering class might be a better fit for ultimately becoming a Data Engineer.\n\nI'm stuck on what I should pick - a 12-week, on-campus software engineering class or a 6-week online Big Data class? Which will be more benificial if i want to become a data engineer?\n\n**Description for Big Data Class...**\n\n\"This class will focus both on the cluster computing software tools and programming techniques used by data scientists, as well as the important mathematical and statistical models that are used in learning from large-scale data processing. On the tools side, we will cover the basics systems and techniques to store large-volumes of data, as well as modern systems for cluster computing based on Map-Reduce pattern such as Hadoop MapReduce, Apache Spark and Flink. Students will implement data mining algorithms and execute them on real cloud systems like Amazon AWS, Google Cloud or Microsoft Azure by using educational accounts. On the data mining models side, this course will cover the main standard supervised and unsupervised models and will introduce improvement techniques on the model side.\"\n\n**Description for Software Engineering Class...**\n\n\"Overview of techniques and tools to develop high quality software. Topics include software development life cycle such as Agile and DevOps, requirements analysis, software design, programming techniques, refactoring, testing, as well as software management issues. This course features a semester-long group project where students will design and develop a real world software system in groups using Agile methodology and various SE tools, including UML tools, project management tools, programming frameworks, unit and system testing tools , integration tools and version control tools.\"", "author_fullname": "t2_96memylv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which class to choose? Big Data vs Software Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133g229", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682830082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I am a grad student in Applied Data Analytics. I&amp;#39;ll be graduating this summer, but before doing so I have to take one more elective.&lt;/p&gt;\n\n&lt;p&gt;My goal is to one day transition into data engineering - I know this is difficult as an entry-level job, but I want to position myself so that I&amp;#39;m a good fit one day.&lt;/p&gt;\n\n&lt;p&gt;I was set on taking Big Data Analytics as my elective, however, the on-campus class has been cancelled so the only option I have to take it is online for six/seven weeks, which is more intensive...and costs as much as taking it in person.&lt;/p&gt;\n\n&lt;p&gt;I was thinking to instead take a Software Engineering class, I come form a non-technical background and have good Python skills, but feel like taking a Software Engineering class might be a better fit for ultimately becoming a Data Engineer.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m stuck on what I should pick - a 12-week, on-campus software engineering class or a 6-week online Big Data class? Which will be more benificial if i want to become a data engineer?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Description for Big Data Class...&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;This class will focus both on the cluster computing software tools and programming techniques used by data scientists, as well as the important mathematical and statistical models that are used in learning from large-scale data processing. On the tools side, we will cover the basics systems and techniques to store large-volumes of data, as well as modern systems for cluster computing based on Map-Reduce pattern such as Hadoop MapReduce, Apache Spark and Flink. Students will implement data mining algorithms and execute them on real cloud systems like Amazon AWS, Google Cloud or Microsoft Azure by using educational accounts. On the data mining models side, this course will cover the main standard supervised and unsupervised models and will introduce improvement techniques on the model side.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Description for Software Engineering Class...&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Overview of techniques and tools to develop high quality software. Topics include software development life cycle such as Agile and DevOps, requirements analysis, software design, programming techniques, refactoring, testing, as well as software management issues. This course features a semester-long group project where students will design and develop a real world software system in groups using Agile methodology and various SE tools, including UML tools, project management tools, programming frameworks, unit and system testing tools , integration tools and version control tools.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "133g229", "is_robot_indexable": true, "report_reasons": null, "author": "jazzopardi203", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133g229/which_class_to_choose_big_data_vs_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133g229/which_class_to_choose_big_data_vs_software/", "subreddit_subscribers": 103255, "created_utc": 1682830082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2rku02rf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate Data Analysis With Kestra and DuckDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13363rs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682800797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kestra.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kestra.io/blogs/2023-04-25-automate-data-analysis-with-kestra-and-duckdb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13363rs", "is_robot_indexable": true, "report_reasons": null, "author": "tchiotludo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13363rs/automate_data_analysis_with_kestra_and_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kestra.io/blogs/2023-04-25-automate-data-analysis-with-kestra-and-duckdb", "subreddit_subscribers": 103255, "created_utc": 1682800797.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Yet again I've attracted the attention of an employer for a data engineer role and it's time to take the assessment which is usually where everything falls apart. I'm also noticing recruiters don't give me much preparation time to take these assessments.  Anyways, employers are usually interested in me until I take the assessment so, I guess my Python coding skills aren't strong enough.   Since I never making it passed the assessment, that has to be the issue.  I'm trying to figure out how to go about expressing my ideas and solutions to solve these scenarios.  I recall one assessment; I was all over the place with my thoughts and really didn't know how to organize my ideas. I'm looking for advice on how to approach the scenarios below or organize my solutions. I realize knowing the answer isn't the most important thing here, but clearly presenting your ideas is key. \n\n&amp;#x200B;\n\n\\*\\* Debugging a problem with limited information\\*\\*\n\n&amp;#x200B;\n\nIn this scenario, we are looking for things like:\n\n* How do you gather information about a problem in code?\n* How do you research issues?\n* How do you document your learning so others can benefit?\n* How do you communicate technical concepts to other engineers?\n* How do you communicate technical concepts to non-engineers?\n\n&amp;#x200B;\n\n\\*\\*Debug a problem and present options\\*\\*\n\nThis scenario will involve collaborating (via simulated emails) with a non-technical member of your team to resolve an issue with a client's data. You will not need to write any code for this scenario, but you should be able to read some existing code to understand what's going wrong.\n\n&amp;#x200B;\n\n## Create a query to calculate how many hours each department has clocked per day. (using PostgreSQL)\n\nThis might be the easiest one for me, but I'm open to ideas on how to best display my thoughts.", "author_fullname": "t2_6ard8gzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering Assessment tomorrow:(", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132yy5q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.29, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682782928.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682782697.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet again I&amp;#39;ve attracted the attention of an employer for a data engineer role and it&amp;#39;s time to take the assessment which is usually where everything falls apart. I&amp;#39;m also noticing recruiters don&amp;#39;t give me much preparation time to take these assessments.  Anyways, employers are usually interested in me until I take the assessment so, I guess my Python coding skills aren&amp;#39;t strong enough.   Since I never making it passed the assessment, that has to be the issue.  I&amp;#39;m trying to figure out how to go about expressing my ideas and solutions to solve these scenarios.  I recall one assessment; I was all over the place with my thoughts and really didn&amp;#39;t know how to organize my ideas. I&amp;#39;m looking for advice on how to approach the scenarios below or organize my solutions. I realize knowing the answer isn&amp;#39;t the most important thing here, but clearly presenting your ideas is key. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;** Debugging a problem with limited information**&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;In this scenario, we are looking for things like:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;How do you gather information about a problem in code?&lt;/li&gt;\n&lt;li&gt;How do you research issues?&lt;/li&gt;\n&lt;li&gt;How do you document your learning so others can benefit?&lt;/li&gt;\n&lt;li&gt;How do you communicate technical concepts to other engineers?&lt;/li&gt;\n&lt;li&gt;How do you communicate technical concepts to non-engineers?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;**Debug a problem and present options**&lt;/p&gt;\n\n&lt;p&gt;This scenario will involve collaborating (via simulated emails) with a non-technical member of your team to resolve an issue with a client&amp;#39;s data. You will not need to write any code for this scenario, but you should be able to read some existing code to understand what&amp;#39;s going wrong.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;h2&gt;Create a query to calculate how many hours each department has clocked per day. (using PostgreSQL)&lt;/h2&gt;\n\n&lt;p&gt;This might be the easiest one for me, but I&amp;#39;m open to ideas on how to best display my thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "132yy5q", "is_robot_indexable": true, "report_reasons": null, "author": "py_vel26", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/132yy5q/data_engineering_assessment_tomorrow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/132yy5q/data_engineering_assessment_tomorrow/", "subreddit_subscribers": 103255, "created_utc": 1682782697.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}