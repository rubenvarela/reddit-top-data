{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company uses a lot of cool stuff from the typical MDS but I\u2019m honestly not sure if I have ever completed a ticket that truly \u201crequired\u201d anything beyond SQL, a database vendor specific ETL tool, and maybe some python.  \n\nI feel like most of the work I do is the result of bad business analysts and bad requirements gathering. Building data lakes, giant complicated models, calling API\u2019s constantly, unpacking absurd amounts of flat files, all to build data pipelines to drown teams with \u201creal time\u201d data because that\u2019s what they asked for, when they really just need a few KPI aggregated monthly.  \n\nWondering how common this is? To feel like everything you do is extreme overkill relative to the actual business problem that needs solving", "author_fullname": "t2_4nbvm2s0m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What tools do you \u201cneed\u201d to do your job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1331rzp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682789633.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company uses a lot of cool stuff from the typical MDS but I\u2019m honestly not sure if I have ever completed a ticket that truly \u201crequired\u201d anything beyond SQL, a database vendor specific ETL tool, and maybe some python.  &lt;/p&gt;\n\n&lt;p&gt;I feel like most of the work I do is the result of bad business analysts and bad requirements gathering. Building data lakes, giant complicated models, calling API\u2019s constantly, unpacking absurd amounts of flat files, all to build data pipelines to drown teams with \u201creal time\u201d data because that\u2019s what they asked for, when they really just need a few KPI aggregated monthly.  &lt;/p&gt;\n\n&lt;p&gt;Wondering how common this is? To feel like everything you do is extreme overkill relative to the actual business problem that needs solving&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1331rzp", "is_robot_indexable": true, "report_reasons": null, "author": "Visible-Tennis4144", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1331rzp/what_tools_do_you_need_to_do_your_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1331rzp/what_tools_do_you_need_to_do_your_job/", "subreddit_subscribers": 103309, "created_utc": 1682789633.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a student still in engineering school. I apologize in advance for my ignorance. I've been frustrated with trying to understand the use case of Kafka for over a week so I'm sorry if I come off as too aggressive against Kafka.  \nI have a big data project and my professor would like us to use Kafka for realtime (unsure if realtime ingestion, processing or both) and also use Spark.  \nI've been reading up and learning both of those technologies and they seem to be quite similar.   \nFrom what I understand, the only reason to use Kafka would be to make different components communicate with each other in a microservices architecture or for event driven architecture. That's fine. But I can't for the sake of anything holly understand why Kafka would be considered for 'realtime' or for data ingestion.  \nSo I tried to study the following case: Get tweets from Twitter API =&gt; clean-up =&gt; run sentiment analysis =&gt; store.  \nNow, I've been told that Spark is the god of data processing, especially for high volumes and that Spark Streaming is also powerful.   \nFrom where I see it, I have to options:\n\nOption 1:   \nPoll Twitter API every 5 minutes with a Get request =&gt; use Spark Streaming to cleanup &amp; process data in memory =&gt; store in database/datalake.\n\n  \nOption 2:  \nPoll Twitter API every 5 minutes with a Get request =&gt; store in Kafka topics =&gt; listen to topics =&gt; use Spark Streaming to cleanup &amp; process data in memory =&gt; store in database/datalake.  \n\n\nOption 2 adds a whole level of complexity and failure points for no apparent added value.  If anyone could explain what I am missing here that does not involve the following points :  \n\\-Fault tolerance (already exists in Spark clusters)  \n\\-Real-time (it's practically as real-time as Spark streaming)  \n\\-Communication between components (I admit this is a great added value but not relevant in my project)  \n\\-Storage (I have read over&amp;over again that Kafka should not be used as a real storage system)  \n\n\nI would really appreciate your help, especially if concrete examples are given.", "author_fullname": "t2_agpkokzd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Confused and frustrated about Kafka", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133946d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 61, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 61, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682808656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a student still in engineering school. I apologize in advance for my ignorance. I&amp;#39;ve been frustrated with trying to understand the use case of Kafka for over a week so I&amp;#39;m sorry if I come off as too aggressive against Kafka.&lt;br/&gt;\nI have a big data project and my professor would like us to use Kafka for realtime (unsure if realtime ingestion, processing or both) and also use Spark.&lt;br/&gt;\nI&amp;#39;ve been reading up and learning both of those technologies and they seem to be quite similar.&lt;br/&gt;\nFrom what I understand, the only reason to use Kafka would be to make different components communicate with each other in a microservices architecture or for event driven architecture. That&amp;#39;s fine. But I can&amp;#39;t for the sake of anything holly understand why Kafka would be considered for &amp;#39;realtime&amp;#39; or for data ingestion.&lt;br/&gt;\nSo I tried to study the following case: Get tweets from Twitter API =&amp;gt; clean-up =&amp;gt; run sentiment analysis =&amp;gt; store.&lt;br/&gt;\nNow, I&amp;#39;ve been told that Spark is the god of data processing, especially for high volumes and that Spark Streaming is also powerful.&lt;br/&gt;\nFrom where I see it, I have to options:&lt;/p&gt;\n\n&lt;p&gt;Option 1:&lt;br/&gt;\nPoll Twitter API every 5 minutes with a Get request =&amp;gt; use Spark Streaming to cleanup &amp;amp; process data in memory =&amp;gt; store in database/datalake.&lt;/p&gt;\n\n&lt;p&gt;Option 2:&lt;br/&gt;\nPoll Twitter API every 5 minutes with a Get request =&amp;gt; store in Kafka topics =&amp;gt; listen to topics =&amp;gt; use Spark Streaming to cleanup &amp;amp; process data in memory =&amp;gt; store in database/datalake.  &lt;/p&gt;\n\n&lt;p&gt;Option 2 adds a whole level of complexity and failure points for no apparent added value.  If anyone could explain what I am missing here that does not involve the following points :&lt;br/&gt;\n-Fault tolerance (already exists in Spark clusters)&lt;br/&gt;\n-Real-time (it&amp;#39;s practically as real-time as Spark streaming)&lt;br/&gt;\n-Communication between components (I admit this is a great added value but not relevant in my project)&lt;br/&gt;\n-Storage (I have read over&amp;amp;over again that Kafka should not be used as a real storage system)  &lt;/p&gt;\n\n&lt;p&gt;I would really appreciate your help, especially if concrete examples are given.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "133946d", "is_robot_indexable": true, "report_reasons": null, "author": "Leather-Ad9576", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133946d/confused_and_frustrated_about_kafka/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133946d/confused_and_frustrated_about_kafka/", "subreddit_subscribers": 103309, "created_utc": 1682808656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are given the choice to be certified in either Snowflake or Databricks. Which do you guys think is better in terms of career progression? We can only choose to specialize in one (in terms of company sponsorship).", "author_fullname": "t2_6jogidac", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which certification to get, Snowflake or Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133kbq3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682845777.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are given the choice to be certified in either Snowflake or Databricks. Which do you guys think is better in terms of career progression? We can only choose to specialize in one (in terms of company sponsorship).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "133kbq3", "is_robot_indexable": true, "report_reasons": null, "author": "_barnuts", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133kbq3/which_certification_to_get_snowflake_or_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133kbq3/which_certification_to_get_snowflake_or_databricks/", "subreddit_subscribers": 103309, "created_utc": 1682845777.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Curious how many others take this approach. \n\nI don\u2019t comment my code.\n\nI start by writing some placeholders and commenting what I want them to do. Then once all my comments are in place, I write the code to match it.\n\nAm I abnormal? It this an inherently bad approach?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Commenting code, or coding comments?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133b5hk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682814336.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious how many others take this approach. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t comment my code.&lt;/p&gt;\n\n&lt;p&gt;I start by writing some placeholders and commenting what I want them to do. Then once all my comments are in place, I write the code to match it.&lt;/p&gt;\n\n&lt;p&gt;Am I abnormal? It this an inherently bad approach?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "133b5hk", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133b5hk/commenting_code_or_coding_comments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133b5hk/commenting_code_or_coding_comments/", "subreddit_subscribers": 103309, "created_utc": 1682814336.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Reddit,\n\nI am new here. Recently I joined a course from udemy to learn more about databricks. Course instructor was using the F1 race dataset from the Ergast website. The website had a well documented table and all the database details. I have completed the course now and I am looking for some random dataset to test my skills. But it is hard to find dataset like that. \n\nIn search of dataset I am writing this post to get views and comments from community who have been in this field for long time, \nHave you guys come across any dataset like that in your career?", "author_fullname": "t2_738re5e0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datasets for data engineering projects", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133qlow", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682861236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Reddit,&lt;/p&gt;\n\n&lt;p&gt;I am new here. Recently I joined a course from udemy to learn more about databricks. Course instructor was using the F1 race dataset from the Ergast website. The website had a well documented table and all the database details. I have completed the course now and I am looking for some random dataset to test my skills. But it is hard to find dataset like that. &lt;/p&gt;\n\n&lt;p&gt;In search of dataset I am writing this post to get views and comments from community who have been in this field for long time, \nHave you guys come across any dataset like that in your career?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "133qlow", "is_robot_indexable": true, "report_reasons": null, "author": "hpal007", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133qlow/datasets_for_data_engineering_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133qlow/datasets_for_data_engineering_projects/", "subreddit_subscribers": 103309, "created_utc": 1682861236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI am a grad student in Applied Data Analytics. I'll be graduating this summer, but before doing so I have to take one more elective.\n\nMy goal is to one day transition into data engineering - I know this is difficult as an entry-level job, but I want to position myself so that I'm a good fit one day.\n\nI was set on taking Big Data Analytics as my elective, however, the on-campus class has been cancelled so the only option I have to take it is online for six/seven weeks, which is more intensive...and costs as much as taking it in person.\n\nI was thinking to instead take a Software Engineering class, I come form a non-technical background and have good Python skills, but feel like taking a Software Engineering class might be a better fit for ultimately becoming a Data Engineer.\n\nI'm stuck on what I should pick - a 12-week, on-campus software engineering class or a 6-week online Big Data class? Which will be more benificial if i want to become a data engineer?\n\n**Description for Big Data Class...**\n\n\"This class will focus both on the cluster computing software tools and programming techniques used by data scientists, as well as the important mathematical and statistical models that are used in learning from large-scale data processing. On the tools side, we will cover the basics systems and techniques to store large-volumes of data, as well as modern systems for cluster computing based on Map-Reduce pattern such as Hadoop MapReduce, Apache Spark and Flink. Students will implement data mining algorithms and execute them on real cloud systems like Amazon AWS, Google Cloud or Microsoft Azure by using educational accounts. On the data mining models side, this course will cover the main standard supervised and unsupervised models and will introduce improvement techniques on the model side.\"\n\n**Description for Software Engineering Class...**\n\n\"Overview of techniques and tools to develop high quality software. Topics include software development life cycle such as Agile and DevOps, requirements analysis, software design, programming techniques, refactoring, testing, as well as software management issues. This course features a semester-long group project where students will design and develop a real world software system in groups using Agile methodology and various SE tools, including UML tools, project management tools, programming frameworks, unit and system testing tools , integration tools and version control tools.\"", "author_fullname": "t2_96memylv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which class to choose? Big Data vs Software Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133g229", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682830082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I am a grad student in Applied Data Analytics. I&amp;#39;ll be graduating this summer, but before doing so I have to take one more elective.&lt;/p&gt;\n\n&lt;p&gt;My goal is to one day transition into data engineering - I know this is difficult as an entry-level job, but I want to position myself so that I&amp;#39;m a good fit one day.&lt;/p&gt;\n\n&lt;p&gt;I was set on taking Big Data Analytics as my elective, however, the on-campus class has been cancelled so the only option I have to take it is online for six/seven weeks, which is more intensive...and costs as much as taking it in person.&lt;/p&gt;\n\n&lt;p&gt;I was thinking to instead take a Software Engineering class, I come form a non-technical background and have good Python skills, but feel like taking a Software Engineering class might be a better fit for ultimately becoming a Data Engineer.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m stuck on what I should pick - a 12-week, on-campus software engineering class or a 6-week online Big Data class? Which will be more benificial if i want to become a data engineer?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Description for Big Data Class...&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;This class will focus both on the cluster computing software tools and programming techniques used by data scientists, as well as the important mathematical and statistical models that are used in learning from large-scale data processing. On the tools side, we will cover the basics systems and techniques to store large-volumes of data, as well as modern systems for cluster computing based on Map-Reduce pattern such as Hadoop MapReduce, Apache Spark and Flink. Students will implement data mining algorithms and execute them on real cloud systems like Amazon AWS, Google Cloud or Microsoft Azure by using educational accounts. On the data mining models side, this course will cover the main standard supervised and unsupervised models and will introduce improvement techniques on the model side.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Description for Software Engineering Class...&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Overview of techniques and tools to develop high quality software. Topics include software development life cycle such as Agile and DevOps, requirements analysis, software design, programming techniques, refactoring, testing, as well as software management issues. This course features a semester-long group project where students will design and develop a real world software system in groups using Agile methodology and various SE tools, including UML tools, project management tools, programming frameworks, unit and system testing tools , integration tools and version control tools.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "133g229", "is_robot_indexable": true, "report_reasons": null, "author": "jazzopardi203", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133g229/which_class_to_choose_big_data_vs_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133g229/which_class_to_choose_big_data_vs_software/", "subreddit_subscribers": 103309, "created_utc": 1682830082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_z3784il", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_13327ll", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 8, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Ww8xyjmtg3A?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Ww8xyjmtg3A?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)\"&gt;&lt;/iframe&gt;", "author_name": "CMU Database Group", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Ww8xyjmtg3A/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CMUDatabaseGroup"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Ww8xyjmtg3A?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/13327ll", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/P2mDw8Z-0Rg8ZV--97XK5ZmgziVC_rv2jsxDV6VWMsY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682790752.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=Ww8xyjmtg3A", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/g7aL2Y2WH_CkTEB7cdGpCegCp-HBV8L71D-RYJXbevo.jpg?auto=webp&amp;v=enabled&amp;s=974f76b9c227d742df987223a99a55eae032d0b7", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/g7aL2Y2WH_CkTEB7cdGpCegCp-HBV8L71D-RYJXbevo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3aeef8a9aa7b3545177087497121d37b1ce9764d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/g7aL2Y2WH_CkTEB7cdGpCegCp-HBV8L71D-RYJXbevo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bd500c2a8925179978f87f31728e6caf97d9c74", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/g7aL2Y2WH_CkTEB7cdGpCegCp-HBV8L71D-RYJXbevo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=918f1f4cc525ca2db8e56e123384c06d1eded8e9", "width": 320, "height": 240}], "variants": {}, "id": "nFgAxd2naTw_dqfWbKLjnTWgj-VlHobf_1ColE86mHA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "13327ll", "is_robot_indexable": true, "report_reasons": null, "author": "boy_named_su", "discussion_type": null, "num_comments": 4, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13327ll/amazon_redshift_internals_cmu_advanced_databases/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=Ww8xyjmtg3A", "subreddit_subscribers": 103309, "created_utc": 1682790752.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Ww8xyjmtg3A?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"24 - Amazon Redshift Internals (CMU Advanced Databases / Spring 2023)\"&gt;&lt;/iframe&gt;", "author_name": "CMU Database Group", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Ww8xyjmtg3A/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CMUDatabaseGroup"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there,\n\nHoping for some suggestions on DV2.0 automation tools. I\u2019m aware of Wherescape and DbtVault. Are there any others?", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Vault 2.0 automation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133mk9h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682853478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there,&lt;/p&gt;\n\n&lt;p&gt;Hoping for some suggestions on DV2.0 automation tools. I\u2019m aware of Wherescape and DbtVault. Are there any others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "133mk9h", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133mk9h/data_vault_20_automation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133mk9h/data_vault_20_automation/", "subreddit_subscribers": 103309, "created_utc": 1682853478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One of my recent tasks involves syncing aggregated metrics from our data warehouse to a marketing platform so that the marketing team can make campaigns that target, for example, all users that made 5 or more orders in the last week. \n\nDue to limitations with the marketing platform\u2019s reporting api, we have to download all of the existing data at once instead of by user. This requires downloading a 1 gigabyte text file, filtering out the random attributes we didn\u2019t write, and comparing each user with the existing data in our data warehouse.  \n\nThe goal is to end up with a list of JSON objects to send to the marketing platform api that only contain the changed attributes because the platform charges us by the attribute. \n\nWhat\u2019s the best way to compare two giant lists of json objects by user id and only return the attributes that are different?\n\nRight now I\u2019m loading both into pandas dataframes and running a comparison, but it eats up a ton of memory and takes a while.", "author_fullname": "t2_c5ysf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to compare two large new line delimited JSON files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133i4tr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682845885.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682837659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One of my recent tasks involves syncing aggregated metrics from our data warehouse to a marketing platform so that the marketing team can make campaigns that target, for example, all users that made 5 or more orders in the last week. &lt;/p&gt;\n\n&lt;p&gt;Due to limitations with the marketing platform\u2019s reporting api, we have to download all of the existing data at once instead of by user. This requires downloading a 1 gigabyte text file, filtering out the random attributes we didn\u2019t write, and comparing each user with the existing data in our data warehouse.  &lt;/p&gt;\n\n&lt;p&gt;The goal is to end up with a list of JSON objects to send to the marketing platform api that only contain the changed attributes because the platform charges us by the attribute. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s the best way to compare two giant lists of json objects by user id and only return the attributes that are different?&lt;/p&gt;\n\n&lt;p&gt;Right now I\u2019m loading both into pandas dataframes and running a comparison, but it eats up a ton of memory and takes a while.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "133i4tr", "is_robot_indexable": true, "report_reasons": null, "author": "cellularcone", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133i4tr/whats_the_best_way_to_compare_two_large_new_line/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133i4tr/whats_the_best_way_to_compare_two_large_new_line/", "subreddit_subscribers": 103309, "created_utc": 1682837659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve always wondered about this:\n\nIs it more efficient and secure to call the APIs of a transactional system or connect to the transactional system\u2019s database via ODBC?\n\nMore context:\n\nWe have 2 deployments currently for ServiceNow reporting:\n\n1. An ETL tool calls the APIs of SeviceNow and inserts the output into a relational database which is then connected to a BI tool\n\n2. The BI tool directly connects to the ServiceNow DB via an ODBC driver and runs distributed queries on the DB. This seems like a less of a work to us. \n\nBoth approaches have served well for past 4-5 years so was wondering what the community has to say about both the approaches?\n\nThanks in advance!", "author_fullname": "t2_vls2w775", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which of the below approaches is more secure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1334cfc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682796202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve always wondered about this:&lt;/p&gt;\n\n&lt;p&gt;Is it more efficient and secure to call the APIs of a transactional system or connect to the transactional system\u2019s database via ODBC?&lt;/p&gt;\n\n&lt;p&gt;More context:&lt;/p&gt;\n\n&lt;p&gt;We have 2 deployments currently for ServiceNow reporting:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;An ETL tool calls the APIs of SeviceNow and inserts the output into a relational database which is then connected to a BI tool&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The BI tool directly connects to the ServiceNow DB via an ODBC driver and runs distributed queries on the DB. This seems like a less of a work to us. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both approaches have served well for past 4-5 years so was wondering what the community has to say about both the approaches?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1334cfc", "is_robot_indexable": true, "report_reasons": null, "author": "mysterioustechie", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1334cfc/which_of_the_below_approaches_is_more_secure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1334cfc/which_of_the_below_approaches_is_more_secure/", "subreddit_subscribers": 103309, "created_utc": 1682796202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have an use case where I need to pull CDC data from Postgres to Snowflake. We planned on using AWS DMS but looks like our org might not allow it long term. We have events flowing in via Pulsar but they don\u2019t exactly match the schema at the db level, which is causing issues building our DWH. Any other thoughts or processes that could help us? TIA!", "author_fullname": "t2_a8lm525f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pulling CDC data from Postgres", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133sad1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682862898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an use case where I need to pull CDC data from Postgres to Snowflake. We planned on using AWS DMS but looks like our org might not allow it long term. We have events flowing in via Pulsar but they don\u2019t exactly match the schema at the db level, which is causing issues building our DWH. Any other thoughts or processes that could help us? TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "133sad1", "is_robot_indexable": true, "report_reasons": null, "author": "West-Refrigerator-86", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133sad1/pulling_cdc_data_from_postgres/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133sad1/pulling_cdc_data_from_postgres/", "subreddit_subscribers": 103309, "created_utc": 1682862898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I just came across the Apache Spark Research paper and found it to be really interesting. What is a good way to get started reading papers in Data Engineering?", "author_fullname": "t2_c3rgr29z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Research Papers in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133mkov", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682853519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just came across the Apache Spark Research paper and found it to be really interesting. What is a good way to get started reading papers in Data Engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "133mkov", "is_robot_indexable": true, "report_reasons": null, "author": "bluebilloo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133mkov/research_papers_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133mkov/research_papers_in_data_engineering/", "subreddit_subscribers": 103309, "created_utc": 1682853519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_jboiz9k4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Querying microservices in real-time with materialized views", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_133lkqv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/oMRLnn5ocoMAZsQWWEC3qMWEY3m0vOK4gGLpsPz4el8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682850243.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@bumurzaqov2/b876e0aa7115", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/CqigoBzws4MIet3oULLDtclxNmJfB-_4ZCv_vbNAco0.jpg?auto=webp&amp;v=enabled&amp;s=816415d621514c8ba3b5487faef38b3ccec13afa", "width": 1200, "height": 675}, "resolutions": [{"url": "https://external-preview.redd.it/CqigoBzws4MIet3oULLDtclxNmJfB-_4ZCv_vbNAco0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac9e43dac754f8dc57ed30da83932d53c1ac802e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/CqigoBzws4MIet3oULLDtclxNmJfB-_4ZCv_vbNAco0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=736f3e47b9b7f592561861cedb7439b567c5893d", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/CqigoBzws4MIet3oULLDtclxNmJfB-_4ZCv_vbNAco0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed135c4bfdaa711eec0b550f7f67d0c799822389", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/CqigoBzws4MIet3oULLDtclxNmJfB-_4ZCv_vbNAco0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56be1e9d373a213012a12c707995081261246e20", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/CqigoBzws4MIet3oULLDtclxNmJfB-_4ZCv_vbNAco0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b89244edbc77b99b3361c1d022d178f1c951dfee", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/CqigoBzws4MIet3oULLDtclxNmJfB-_4ZCv_vbNAco0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae9aa590df9b8411c7f7681f4e18e06cac186092", "width": 1080, "height": 607}], "variants": {}, "id": "oEtnqG9z64r5jJvZ3M8UXHAk3zWbUdsUKVtKBCyKrq0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "133lkqv", "is_robot_indexable": true, "report_reasons": null, "author": "bumurzokov", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133lkqv/querying_microservices_in_realtime_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@bumurzaqov2/b876e0aa7115", "subreddit_subscribers": 103309, "created_utc": 1682850243.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m working on a big data quality project for my company, trying to clean up our account and contact data. The MDM team (myself included) believes that since those records were entered by our many sales reps out in the field, they should be the people we submit our DQ reports to, as they would easily be the most likely to know how current the data in their territories is and what updates they need to make to anything that pops up as invalid or questionable.\n\nHowever, we\u2019ve hit a roadblock. The executive we meet with has been a champion of our project in general, but he argues that the sales reps shouldn\u2019t be responsible for cleaning up their data because it would mean they\u2019d be spending time away from selling. Or at the very least, when the project is presented to the execs above him, that\u2019s how he believes they would react. So far he hasn\u2019t given any alternative solution when we ask \u201cwell who else is going to do it?\u201d\n\nSo what we\u2019re looking for is a convincing argument to support why the sales reps should indeed own their data and its quality. It\u2019s not a matter of convincing anyone on the benefits of cleaner data, or the downside to leaving it alone; we all agree on that. But we need to come up with something that will change minds about sales reps taking ownership and spending some time making any updates based on our reports. At this point the reports are planned to go out quarterly, and of course the initial cleanup will be the largest lift before settling into regular maintenance mode.\n\nThoughts? Thanks kindly in advance and sorry for the bad prose, this is the first time I\u2019ve written this all out.", "author_fullname": "t2_6kftfg7c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Convincing argument for sales reps to own their data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1338rvs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682807707.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m working on a big data quality project for my company, trying to clean up our account and contact data. The MDM team (myself included) believes that since those records were entered by our many sales reps out in the field, they should be the people we submit our DQ reports to, as they would easily be the most likely to know how current the data in their territories is and what updates they need to make to anything that pops up as invalid or questionable.&lt;/p&gt;\n\n&lt;p&gt;However, we\u2019ve hit a roadblock. The executive we meet with has been a champion of our project in general, but he argues that the sales reps shouldn\u2019t be responsible for cleaning up their data because it would mean they\u2019d be spending time away from selling. Or at the very least, when the project is presented to the execs above him, that\u2019s how he believes they would react. So far he hasn\u2019t given any alternative solution when we ask \u201cwell who else is going to do it?\u201d&lt;/p&gt;\n\n&lt;p&gt;So what we\u2019re looking for is a convincing argument to support why the sales reps should indeed own their data and its quality. It\u2019s not a matter of convincing anyone on the benefits of cleaner data, or the downside to leaving it alone; we all agree on that. But we need to come up with something that will change minds about sales reps taking ownership and spending some time making any updates based on our reports. At this point the reports are planned to go out quarterly, and of course the initial cleanup will be the largest lift before settling into regular maintenance mode.&lt;/p&gt;\n\n&lt;p&gt;Thoughts? Thanks kindly in advance and sorry for the bad prose, this is the first time I\u2019ve written this all out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1338rvs", "is_robot_indexable": true, "report_reasons": null, "author": "usarasa", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1338rvs/convincing_argument_for_sales_reps_to_own_their/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1338rvs/convincing_argument_for_sales_reps_to_own_their/", "subreddit_subscribers": 103309, "created_utc": 1682807707.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI am a newbie in the field and I have just encountered Spring Batch when I searched for batch processing on youtube. I check for it on Wikipedia and there is said that modern batch applications use Spring Batch etc. I was trying to build batch applications using Airflow and etc in Python. As far as I got, Spring Batch is capable of running batch jobs on scheduled periods. Although it seems to me as a first-sighter that Spring Batch can be used for the entire batch processing pipeline, it is pretty strange that I have never ever seen Spring Batch on job ads, published under the \"Data Engineer\" title. (I live in Turkey btw)\n\nI guess that maybe it might be more commonly used among backend developers instead of big data engineers etc.\n\nDid I not understand the use cases of Spring Batch well? Or is it something only not common in my country?\n\nI also wonder if it is not commonly used in big data and data engineering, why? What are the advantages and disadvantages of Spring Batch?", "author_fullname": "t2_t1xjvr55", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Spring Batch a common thing in DE world?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_133v6ef", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682869274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I am a newbie in the field and I have just encountered Spring Batch when I searched for batch processing on youtube. I check for it on Wikipedia and there is said that modern batch applications use Spring Batch etc. I was trying to build batch applications using Airflow and etc in Python. As far as I got, Spring Batch is capable of running batch jobs on scheduled periods. Although it seems to me as a first-sighter that Spring Batch can be used for the entire batch processing pipeline, it is pretty strange that I have never ever seen Spring Batch on job ads, published under the &amp;quot;Data Engineer&amp;quot; title. (I live in Turkey btw)&lt;/p&gt;\n\n&lt;p&gt;I guess that maybe it might be more commonly used among backend developers instead of big data engineers etc.&lt;/p&gt;\n\n&lt;p&gt;Did I not understand the use cases of Spring Batch well? Or is it something only not common in my country?&lt;/p&gt;\n\n&lt;p&gt;I also wonder if it is not commonly used in big data and data engineering, why? What are the advantages and disadvantages of Spring Batch?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "133v6ef", "is_robot_indexable": true, "report_reasons": null, "author": "gxslash", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133v6ef/is_spring_batch_a_common_thing_in_de_world/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/133v6ef/is_spring_batch_a_common_thing_in_de_world/", "subreddit_subscribers": 103309, "created_utc": 1682869274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_sppgx30r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Normalization and Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_133kg14", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Mf6vHGYRuZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Introduction to Normal Forms | First Normal Form(1NF) | DBMS Normalization | Code with Scaler\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Introduction to Normal Forms | First Normal Form(1NF) | DBMS Normalization | Code with Scaler", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Mf6vHGYRuZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Introduction to Normal Forms | First Normal Form(1NF) | DBMS Normalization | Code with Scaler\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Mf6vHGYRuZM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Mf6vHGYRuZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Introduction to Normal Forms | First Normal Form(1NF) | DBMS Normalization | Code with Scaler\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/133kg14", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3qkA8Du3MhBkWm78IZlzjZrEEV1GK8n8OinqjoDmQgQ.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682846208.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/Mf6vHGYRuZM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/0wqZf9UhvAz8EiRvz6MJCzDOgTp6ksWkBoQ1O-mg7u4.jpg?auto=webp&amp;v=enabled&amp;s=38e46c309beb15d5551a00d16edda17737638f75", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/0wqZf9UhvAz8EiRvz6MJCzDOgTp6ksWkBoQ1O-mg7u4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=14dc6c51af10ebf55688e5b37e426c729d71578a", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/0wqZf9UhvAz8EiRvz6MJCzDOgTp6ksWkBoQ1O-mg7u4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35cf5c700bae5ca6004a44e59d437deb91380aaf", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/0wqZf9UhvAz8EiRvz6MJCzDOgTp6ksWkBoQ1O-mg7u4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a529f00e7b6009d1238167ea8c67f65189eaa470", "width": 320, "height": 240}], "variants": {}, "id": "ULlXKHqNhl843O2yOTG9XISLmKN2paxUKOyrqa_QkjA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "133kg14", "is_robot_indexable": true, "report_reasons": null, "author": "thetech_learner", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/133kg14/normalization_and_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/Mf6vHGYRuZM", "subreddit_subscribers": 103309, "created_utc": 1682846208.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Introduction to Normal Forms | First Normal Form(1NF) | DBMS Normalization | Code with Scaler", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/Mf6vHGYRuZM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Introduction to Normal Forms | First Normal Form(1NF) | DBMS Normalization | Code with Scaler\"&gt;&lt;/iframe&gt;", "author_name": " Code with Scaler", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/Mf6vHGYRuZM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@CodewithScaler"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2rku02rf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automate Data Analysis With Kestra and DuckDB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13363rs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682800797.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "kestra.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://kestra.io/blogs/2023-04-25-automate-data-analysis-with-kestra-and-duckdb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "13363rs", "is_robot_indexable": true, "report_reasons": null, "author": "tchiotludo", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/13363rs/automate_data_analysis_with_kestra_and_duckdb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://kestra.io/blogs/2023-04-25-automate-data-analysis-with-kestra-and-duckdb", "subreddit_subscribers": 103309, "created_utc": 1682800797.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}