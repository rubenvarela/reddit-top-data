{"kind": "Listing", "data": {"after": "t3_133yvjv", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_84cmf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "iMessage Exporter 1.4.0: Cliff Aster adds support for iOS Backup parsing and deleted message recovery", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_133ahkj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 258, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 258, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/EuMq_5nztYH4H0z6uxGn-rTm5_SoDSoLqNUFoDJOHB4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682812432.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/ReagentX/imessage-exporter/#", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uxMe5AJ2c9vF2Sl5W71IV5hS6SkeG1mA6ZQmbSnRwmA.jpg?auto=webp&amp;v=enabled&amp;s=2e2dbc179641dfe12a651024a6d9e65d87eaec54", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/uxMe5AJ2c9vF2Sl5W71IV5hS6SkeG1mA6ZQmbSnRwmA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0a7705bf8a8d87880f3ef221360fc23fb20bd42", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/uxMe5AJ2c9vF2Sl5W71IV5hS6SkeG1mA6ZQmbSnRwmA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec24f51a1642a4a3f74cda5c00ef186d5896362b", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/uxMe5AJ2c9vF2Sl5W71IV5hS6SkeG1mA6ZQmbSnRwmA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fa47bd8a1169bb34b10146cb7e4c2f71f87eafc9", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/uxMe5AJ2c9vF2Sl5W71IV5hS6SkeG1mA6ZQmbSnRwmA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3e5228e20f97f05089f5e7e72c2978d6e5d3de6", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/uxMe5AJ2c9vF2Sl5W71IV5hS6SkeG1mA6ZQmbSnRwmA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ab22cdc56e497bab2aef116952144dd112b14b1", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/uxMe5AJ2c9vF2Sl5W71IV5hS6SkeG1mA6ZQmbSnRwmA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7b0ed81033be4d1dd6425de1e6c82ce89bc56b3", "width": 1080, "height": 540}], "variants": {}, "id": "4K8FJDLv_P0yHg15I-0un1wa7xJsp-23L5KsOpZkBRo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133ahkj", "is_robot_indexable": true, "report_reasons": null, "author": "ReagentX", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133ahkj/imessage_exporter_140_cliff_aster_adds_support/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/ReagentX/imessage-exporter/#", "subreddit_subscribers": 680331, "created_utc": 1682812432.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Attention data hoarders! Are you tired of losing your Reddit chats when switching accounts or deleting them altogether? Fear not, because there's now a tool to help you liberate your Reddit chats. Introducing Rexit - the Reddit Brexit tool that exports your Reddit chats into a variety of open formats, such as CSV, JSON, and TXT.\n\nUsing Rexit is simple. Just specify the formats you want to export to using the --formats option, and enter your Reddit username and password when prompted. Rexit will then save your chats to the current directory. If an image was sent in the chat, the filename will be displayed as the message content, prefixed with FILE.\n\nHere's an example usage of Rexit:\n\n    $ rexit --formats csv,json,txt\n    &gt; Your Reddit Username: &lt;USERNAME&gt;\n    &gt; Your Reddit Password: &lt;PASSWORD&gt;\n    \n\n&amp;#x200B;\n\nRexit can be installed via the files provided in the releases page of the GitHub repository, via Cargo homebrew, or build from source. \n\n**To install via Cargo, simply run:**\n\n    $ cargo install rexit\n\n**using homebrew:**\n\n    $ brew tap mpult/mpult \n    $ brew install rexit\n\n**from source:**\n\nyou probably know what you're doing (or I hope so). Use the instructions in the [Readme](https://github.com/MPult/Rexit)\n\nAll contributions are welcome. For documentation on contributing and technical information, run cargo doc --open in your terminal. \n\nRexit is licensed under the GNU General Public License, Version 3.\n\n&amp;#x200B;\n\nIf you have any questions ask me! or checkout the [GitHub](https://github.com/MPult/Rexit).\n\nSay goodbye to lost Reddit chats and hello to data hoarding with Rexit!", "author_fullname": "t2_vkc3rydr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Rexit v1.0.0 - Export your Reddit chats!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133xxqy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682876064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Attention data hoarders! Are you tired of losing your Reddit chats when switching accounts or deleting them altogether? Fear not, because there&amp;#39;s now a tool to help you liberate your Reddit chats. Introducing Rexit - the Reddit Brexit tool that exports your Reddit chats into a variety of open formats, such as CSV, JSON, and TXT.&lt;/p&gt;\n\n&lt;p&gt;Using Rexit is simple. Just specify the formats you want to export to using the --formats option, and enter your Reddit username and password when prompted. Rexit will then save your chats to the current directory. If an image was sent in the chat, the filename will be displayed as the message content, prefixed with FILE.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an example usage of Rexit:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ rexit --formats csv,json,txt\n&amp;gt; Your Reddit Username: &amp;lt;USERNAME&amp;gt;\n&amp;gt; Your Reddit Password: &amp;lt;PASSWORD&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Rexit can be installed via the files provided in the releases page of the GitHub repository, via Cargo homebrew, or build from source. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;To install via Cargo, simply run:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ cargo install rexit\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;using homebrew:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ brew tap mpult/mpult \n$ brew install rexit\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;from source:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;you probably know what you&amp;#39;re doing (or I hope so). Use the instructions in the &lt;a href=\"https://github.com/MPult/Rexit\"&gt;Readme&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;All contributions are welcome. For documentation on contributing and technical information, run cargo doc --open in your terminal. &lt;/p&gt;\n\n&lt;p&gt;Rexit is licensed under the GNU General Public License, Version 3.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;If you have any questions ask me! or checkout the &lt;a href=\"https://github.com/MPult/Rexit\"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Say goodbye to lost Reddit chats and hello to data hoarding with Rexit!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ebjx888g0-LbUrtgh6akhlpIS3ulTMEF5bHkrmHQ2K8.jpg?auto=webp&amp;v=enabled&amp;s=a0e2b097c867f011513addd7de362f08508754ac", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Ebjx888g0-LbUrtgh6akhlpIS3ulTMEF5bHkrmHQ2K8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=248774bdf2dc93dc7587ca947f30b9e444e11e04", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/Ebjx888g0-LbUrtgh6akhlpIS3ulTMEF5bHkrmHQ2K8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=486afa539c8a283241a4c5d834c02c0d0eb500c4", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/Ebjx888g0-LbUrtgh6akhlpIS3ulTMEF5bHkrmHQ2K8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e058e0ca10273340656fe025d5028bbcc3352ad", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/Ebjx888g0-LbUrtgh6akhlpIS3ulTMEF5bHkrmHQ2K8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73069070f2259cc033ee9d054555e002ec45f9f8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/Ebjx888g0-LbUrtgh6akhlpIS3ulTMEF5bHkrmHQ2K8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=54fa41b4525a2fef147e7347aaea95b09aa79bea", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/Ebjx888g0-LbUrtgh6akhlpIS3ulTMEF5bHkrmHQ2K8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db848978440634a5ed0c2a746cb86c63b298cea3", "width": 1080, "height": 540}], "variants": {}, "id": "aZWW6ru-XU5GJmzn86c4duCXetEp84WK3Zc77bF_eD8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133xxqy", "is_robot_indexable": true, "report_reasons": null, "author": "New-Yak-3548", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133xxqy/rexit_v100_export_your_reddit_chats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133xxqy/rexit_v100_export_your_reddit_chats/", "subreddit_subscribers": 680331, "created_utc": 1682876064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Kind of off topic maybe, but where do you guys buy used hard drives, flash drives, sd cards, etc? I've been working on a pet project involving compaction for awhile and its getting expensive to keep ordering off of mercari and the like when i burn out another drive. The project isn't stable enough to justify using new just yet.", "author_fullname": "t2_3x3jc7gd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you guys buy used hard drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133d11a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682820016.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kind of off topic maybe, but where do you guys buy used hard drives, flash drives, sd cards, etc? I&amp;#39;ve been working on a pet project involving compaction for awhile and its getting expensive to keep ordering off of mercari and the like when i burn out another drive. The project isn&amp;#39;t stable enough to justify using new just yet.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133d11a", "is_robot_indexable": true, "report_reasons": null, "author": "NimbleNavigator19", "discussion_type": null, "num_comments": 48, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133d11a/where_do_you_guys_buy_used_hard_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133d11a/where_do_you_guys_buy_used_hard_drives/", "subreddit_subscribers": 680331, "created_utc": 1682820016.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking at scanners for my printed photos instead of paying for one of the digitizing companies. I'm only doing prints, no negatives or other types. What should I look for in the specs and features? The 3 I'm looking at so far are the Epson Perfection V39, Plustek ephoto Z300, and Canon CanoScan Lide 400.\n\n&amp;#x200B;\n\nThanks", "author_fullname": "t2_hahbwvw5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What to look for in a photo scanner?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13386zh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682806149.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking at scanners for my printed photos instead of paying for one of the digitizing companies. I&amp;#39;m only doing prints, no negatives or other types. What should I look for in the specs and features? The 3 I&amp;#39;m looking at so far are the Epson Perfection V39, Plustek ephoto Z300, and Canon CanoScan Lide 400.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13386zh", "is_robot_indexable": true, "report_reasons": null, "author": "DogsAreCool89", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13386zh/what_to_look_for_in_a_photo_scanner/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13386zh/what_to_look_for_in_a_photo_scanner/", "subreddit_subscribers": 680331, "created_utc": 1682806149.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_d3wk5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "is_gallery": true, "title": "Is this a SAS or SATA disk?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"s9z0j10xm1xa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/s9z0j10xm1xa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6f3d975ac95638575db67abc21be5cfd0867e5a"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/s9z0j10xm1xa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a51e4185bb96829b9390c7979924d460a2e662ec"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/s9z0j10xm1xa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68884add6731e345ca9dc3aa54c902d3777fb262"}, {"y": 1280, "x": 640, "u": "https://preview.redd.it/s9z0j10xm1xa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1d1463edb95da942a7ee36baa449cb63284e0e4"}, {"y": 1920, "x": 960, "u": "https://preview.redd.it/s9z0j10xm1xa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57642afc659704f3c887c1b649a38d2c35714a78"}, {"y": 2160, "x": 1080, "u": "https://preview.redd.it/s9z0j10xm1xa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33ad5d5972fc4214c2747d0f0160219d9c4fa69f"}], "s": {"y": 2778, "x": 1284, "u": "https://preview.redd.it/s9z0j10xm1xa1.jpg?width=1284&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6c0756550f839c69728f589c5741ccd0c7b13cfb"}, "id": "s9z0j10xm1xa1"}, "plck010xm1xa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/plck010xm1xa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3d877484872ac23daa439d4860b50668282da38c"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/plck010xm1xa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8db0a57f84ffbfe47e2d6d6c2417cc4c4a0ecb59"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/plck010xm1xa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b411e9be93297a661f358c3ef8c5a19511d4648"}, {"y": 1280, "x": 640, "u": "https://preview.redd.it/plck010xm1xa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af519d99db5b707336b290d4c6478d0a4b56c084"}, {"y": 1920, "x": 960, "u": "https://preview.redd.it/plck010xm1xa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=485b7caf2a6250248e71258cd6e5dfb3b86591db"}, {"y": 2160, "x": 1080, "u": "https://preview.redd.it/plck010xm1xa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ded5440880d24626c7ac89c5abc2abb41d04690d"}], "s": {"y": 2778, "x": 1284, "u": "https://preview.redd.it/plck010xm1xa1.jpg?width=1284&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=924fd64dc466d33f6de5a879d97ea2f898374182"}, "id": "plck010xm1xa1"}, "d3w1i10xm1xa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 216, "x": 108, "u": "https://preview.redd.it/d3w1i10xm1xa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=903b2ebcf64cf5c7a13757974962cb7fbbe8ee83"}, {"y": 432, "x": 216, "u": "https://preview.redd.it/d3w1i10xm1xa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=41decc5079cbc5a77f5cab91e805b07742acdafa"}, {"y": 640, "x": 320, "u": "https://preview.redd.it/d3w1i10xm1xa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b6f4d16165156ca7b618fe35540faad0ca2ffadc"}, {"y": 1280, "x": 640, "u": "https://preview.redd.it/d3w1i10xm1xa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8224fd8916fcefd7b0c074977ccb56c27efd36f5"}, {"y": 1920, "x": 960, "u": "https://preview.redd.it/d3w1i10xm1xa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca3d4eccb7863308fcd92ef5826828a4369f741f"}, {"y": 2160, "x": 1080, "u": "https://preview.redd.it/d3w1i10xm1xa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6eab3dbf6968fec0bd0d5f6f5e104d933ef5d5e6"}], "s": {"y": 2778, "x": 1284, "u": "https://preview.redd.it/d3w1i10xm1xa1.jpg?width=1284&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=dc67971d2a892d4242d39151ec986d84ac2fc3ff"}, "id": "d3w1i10xm1xa1"}}, "name": "t3_133vgvs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "ups": 16, "domain": "old.reddit.com", "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "gallery_data": {"items": [{"caption": "Is says SAS in the title but SATA in the description. ", "media_id": "d3w1i10xm1xa1", "id": 269575437}, {"media_id": "plck010xm1xa1", "id": 269575438}, {"media_id": "s9z0j10xm1xa1", "id": 269575439}]}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/hI-XRq7SsmuAIwskBVy7t_h-GrVL7THNni3ioyTvqfg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682869972.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "total_awards_received": 0, "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.reddit.com/gallery/133vgvs", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133vgvs", "is_robot_indexable": true, "report_reasons": null, "author": "bullerwins", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133vgvs/is_this_a_sas_or_sata_disk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.reddit.com/gallery/133vgvs", "subreddit_subscribers": 680331, "created_utc": 1682869972.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently purchased an external Seagate One Touch Hub 8TB which contains a Seagate Barracuda ST8000DM004, an SMR disk. My intentions were to download torrents directly then keep it running 24/7 to seed, however I've realized this is not the ideal workload for an SMR disk due to slow writing speeds.\n\nI've been trying to read a lot and I've found different answers about the reading speeds as well, some say the writing speeds are fine, others say they're not so great. Would torrenting to a internal CMR drive, then transfer the content to the SMR to let it continously seed many torrents bottleneck me or cause me any problems?", "author_fullname": "t2_z8kgu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Issues with seeding 24/7 from an external SMR disk?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1339tvo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682810600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently purchased an external Seagate One Touch Hub 8TB which contains a Seagate Barracuda ST8000DM004, an SMR disk. My intentions were to download torrents directly then keep it running 24/7 to seed, however I&amp;#39;ve realized this is not the ideal workload for an SMR disk due to slow writing speeds.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been trying to read a lot and I&amp;#39;ve found different answers about the reading speeds as well, some say the writing speeds are fine, others say they&amp;#39;re not so great. Would torrenting to a internal CMR drive, then transfer the content to the SMR to let it continously seed many torrents bottleneck me or cause me any problems?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1339tvo", "is_robot_indexable": true, "report_reasons": null, "author": "smokepasta", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1339tvo/issues_with_seeding_247_from_an_external_smr_disk/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1339tvo/issues_with_seeding_247_from_an_external_smr_disk/", "subreddit_subscribers": 680331, "created_utc": 1682810600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi there, new to this sub, so please be gentle with me.\n\n&amp;#x200B;\n\nCurrent setup: Xpenology NAS with 4 3TB drives in RAID5\n\nLooking to purchase 2 22TB drives with the plan of buying another 2 in the future.\n\nUnsure of the best way to set it up, so that I can add the 2 additional drives later without having to move the data around too much.\n\nAlso unsure even if I bought all 4 at once, if I should run them in RAID5 or not.\n\n&amp;#x200B;\n\nAny help would be much appreciated, I am not tied to Xpenology for the OS.\n\n&amp;#x200B;\n\nEdit: Thinking I will actually end up using TrueNAS Core", "author_fullname": "t2_io7qy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Planning...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133q8vx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682861499.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682860881.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, new to this sub, so please be gentle with me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Current setup: Xpenology NAS with 4 3TB drives in RAID5&lt;/p&gt;\n\n&lt;p&gt;Looking to purchase 2 22TB drives with the plan of buying another 2 in the future.&lt;/p&gt;\n\n&lt;p&gt;Unsure of the best way to set it up, so that I can add the 2 additional drives later without having to move the data around too much.&lt;/p&gt;\n\n&lt;p&gt;Also unsure even if I bought all 4 at once, if I should run them in RAID5 or not.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any help would be much appreciated, I am not tied to Xpenology for the OS.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: Thinking I will actually end up using TrueNAS Core&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "12TB (9TB Usable)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133q8vx", "is_robot_indexable": true, "report_reasons": null, "author": "Connerzzz6", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/133q8vx/planning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133q8vx/planning/", "subreddit_subscribers": 680331, "created_utc": 1682860881.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Whether 'tis nobler to convert my archived files into gzip tarballs or just leave them as-is. I mostly collect FLAC, MP3, JPG, PDF, and EPUB files. I've tried compressing some of them, but the size reduction is insignificant. Any (dis)advantages if I bother archiving them?", "author_fullname": "t2_im7u2xpy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "To compress or not to compress, that is the question.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133qe0e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682861025.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Whether &amp;#39;tis nobler to convert my archived files into gzip tarballs or just leave them as-is. I mostly collect FLAC, MP3, JPG, PDF, and EPUB files. I&amp;#39;ve tried compressing some of them, but the size reduction is insignificant. Any (dis)advantages if I bother archiving them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "500GB (noob)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133qe0e", "is_robot_indexable": true, "report_reasons": null, "author": "HaveOurBaskets", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/133qe0e/to_compress_or_not_to_compress_that_is_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133qe0e/to_compress_or_not_to_compress_that_is_the/", "subreddit_subscribers": 680331, "created_utc": 1682861025.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I don't expect help, as I don't see many AP fans here, so consider this a chronicling.\n\n&amp;#x200B;\n\nTL;DR\n\nUsing Amazon Photos as a photo \\*access &amp; tagging\\* service (not backup, just for access) is good and you can argue me in the comments if you think otherwise. BUT it doesn't offer multi-user access to a central repository and that sucks, since Amazon has multi-user access to prime, books, movies, and purchased content.\n\n&amp;#x200B;\n\nMy spouse recently presented me with the classic user problem \"Where's my stuff?\".\n\n&amp;#x200B;\n\nNow, despite having meticulously organized a OneDrive for cloud access of our local data repository that includes a well-organized but not overly-deep folder structure that I designed specifically based on their recommendations, the sheer volume of photos, video, and other documents that we produce makes search a bit of a challenge, especially when Google Photos as the default has gotten so good at AI-tagging of images that searching \"Flower\" finds plants and \"Flour\" finds baking supplies. \n\n&amp;#x200B;\n\nSo I get it.\n\n&amp;#x200B;\n\nRemembering the near-date of a picture and searching through EXIF-named, data tagged items in a OneDrive is not as easy as just typing in \"cat\" and getting cats. \n\n&amp;#x200B;\n\nSo that leaves me with 2 problems...\n\n\\-1- How do I provide not only a stable backup solution of all our joint photos taken from their phone, my phone, our shared DSLR camera, and all other one-off upload/backup requests we get from family, who know us as the go-to backup dump for data? &lt;&lt;&lt;&lt;&lt; Easy, solved. 3-2-1. Local, Offsite, Encrypted, done.\n\n\\-2- How Do I let my spouse search \"cat\" on their phone and get pictures of our cat. &lt;&lt;&lt; Messier, that one.\n\n&amp;#x200B;\n\nThe default here is going to be Google Photos or iCloud, as most users are going to be on Android or Apple, but I don't love that. I've got problems with both companies as companies and on top of that we already have Pixel phones that are doing *some* photo backups to Google Photos, so we can't use iCloud and using Google Photos would require me untangling the 50k+ photos in the repository that already have a lot of overlap, as well as downloading both accounts' photos to a central repository on my backup server for offsite backups, and it all just gets quite complicated quite quickly.\n\nSo fine, something that is NOT google Photos, is NOT iCloud, searches with AI photo tagging, and has a good user interface.\n\nMy gut said OneDrive - have you tried OneDrive's AI photo search feature? It was raved about as the greatest thing since sliced bread in several articles in 2018..... I assume they fired that engineering team because it's simply non-existent. There is SOME photo tagging with auto-generated tags, but they're random and non-syncing with the Windows file metadata so even manually tagged photos don't show up in search. Complete trash. Literally useless for the one reason I switched. Guess that's my lesson to test a service before hinging everything on it.\n\nSo now I'm on to the next in line on non-self-hosted AI-tagging platforms with neat-o UX - Amazon Photos.\n\n&amp;#x200B;\n\n1) Does it respect my privacy? -- No, absolutely not\n\n2) Do I feel like my photos are in a safe, long term storage environment? -- Again, no. I expect this service to die within the next 2 years\n\n3) Can my spouse now do AI-tagged searches of photos on their phone via a sleek auto-organizing interface? -- Yes!! Mission Accomplished.\n\n&amp;#x200B;\n\nThe only drawback so far - and this is a big one - despite us having 2 separate Amazon accounts linked to the same prime subscription which allows for purchased-content sharing, there is basically no way to both look at the Amazon Photo repository on our individual devices without logging into the same account. The bummer there is that the login syncs between Amazon shopping and Amazon Photos, so I can't stay logged into my shopping account without losing access to view all the photos.\n\nThis sucks and, ultimately, will cause me to abandon the app if it's not fixed before I can find a new service to fill the need, maybe Flickr cloud hosting? (or buck up the money for a self-hosted OpenVPN and the time to build out LibrePhotos).\n\n&amp;#x200B;\n\nI'm open to opinions on this, just know that \\*I know\\* this is not a long-term backup. This is sheerly for the sake of cloud access + AI tagging without me needing to build a local LibrePhotos + OpenVPN solution on my apartment wifi. All off-site backups &amp; multiple copies are settled here.", "author_fullname": "t2_3hqf1pst", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Amazon Photos Multi-User Access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133bf06", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682815128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t expect help, as I don&amp;#39;t see many AP fans here, so consider this a chronicling.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;TL;DR&lt;/p&gt;\n\n&lt;p&gt;Using Amazon Photos as a photo *access &amp;amp; tagging* service (not backup, just for access) is good and you can argue me in the comments if you think otherwise. BUT it doesn&amp;#39;t offer multi-user access to a central repository and that sucks, since Amazon has multi-user access to prime, books, movies, and purchased content.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;My spouse recently presented me with the classic user problem &amp;quot;Where&amp;#39;s my stuff?&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Now, despite having meticulously organized a OneDrive for cloud access of our local data repository that includes a well-organized but not overly-deep folder structure that I designed specifically based on their recommendations, the sheer volume of photos, video, and other documents that we produce makes search a bit of a challenge, especially when Google Photos as the default has gotten so good at AI-tagging of images that searching &amp;quot;Flower&amp;quot; finds plants and &amp;quot;Flour&amp;quot; finds baking supplies. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So I get it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Remembering the near-date of a picture and searching through EXIF-named, data tagged items in a OneDrive is not as easy as just typing in &amp;quot;cat&amp;quot; and getting cats. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;So that leaves me with 2 problems...&lt;/p&gt;\n\n&lt;p&gt;-1- How do I provide not only a stable backup solution of all our joint photos taken from their phone, my phone, our shared DSLR camera, and all other one-off upload/backup requests we get from family, who know us as the go-to backup dump for data? &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; Easy, solved. 3-2-1. Local, Offsite, Encrypted, done.&lt;/p&gt;\n\n&lt;p&gt;-2- How Do I let my spouse search &amp;quot;cat&amp;quot; on their phone and get pictures of our cat. &amp;lt;&amp;lt;&amp;lt; Messier, that one.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The default here is going to be Google Photos or iCloud, as most users are going to be on Android or Apple, but I don&amp;#39;t love that. I&amp;#39;ve got problems with both companies as companies and on top of that we already have Pixel phones that are doing &lt;em&gt;some&lt;/em&gt; photo backups to Google Photos, so we can&amp;#39;t use iCloud and using Google Photos would require me untangling the 50k+ photos in the repository that already have a lot of overlap, as well as downloading both accounts&amp;#39; photos to a central repository on my backup server for offsite backups, and it all just gets quite complicated quite quickly.&lt;/p&gt;\n\n&lt;p&gt;So fine, something that is NOT google Photos, is NOT iCloud, searches with AI photo tagging, and has a good user interface.&lt;/p&gt;\n\n&lt;p&gt;My gut said OneDrive - have you tried OneDrive&amp;#39;s AI photo search feature? It was raved about as the greatest thing since sliced bread in several articles in 2018..... I assume they fired that engineering team because it&amp;#39;s simply non-existent. There is SOME photo tagging with auto-generated tags, but they&amp;#39;re random and non-syncing with the Windows file metadata so even manually tagged photos don&amp;#39;t show up in search. Complete trash. Literally useless for the one reason I switched. Guess that&amp;#39;s my lesson to test a service before hinging everything on it.&lt;/p&gt;\n\n&lt;p&gt;So now I&amp;#39;m on to the next in line on non-self-hosted AI-tagging platforms with neat-o UX - Amazon Photos.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;1) Does it respect my privacy? -- No, absolutely not&lt;/p&gt;\n\n&lt;p&gt;2) Do I feel like my photos are in a safe, long term storage environment? -- Again, no. I expect this service to die within the next 2 years&lt;/p&gt;\n\n&lt;p&gt;3) Can my spouse now do AI-tagged searches of photos on their phone via a sleek auto-organizing interface? -- Yes!! Mission Accomplished.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The only drawback so far - and this is a big one - despite us having 2 separate Amazon accounts linked to the same prime subscription which allows for purchased-content sharing, there is basically no way to both look at the Amazon Photo repository on our individual devices without logging into the same account. The bummer there is that the login syncs between Amazon shopping and Amazon Photos, so I can&amp;#39;t stay logged into my shopping account without losing access to view all the photos.&lt;/p&gt;\n\n&lt;p&gt;This sucks and, ultimately, will cause me to abandon the app if it&amp;#39;s not fixed before I can find a new service to fill the need, maybe Flickr cloud hosting? (or buck up the money for a self-hosted OpenVPN and the time to build out LibrePhotos).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m open to opinions on this, just know that *I know* this is not a long-term backup. This is sheerly for the sake of cloud access + AI tagging without me needing to build a local LibrePhotos + OpenVPN solution on my apartment wifi. All off-site backups &amp;amp; multiple copies are settled here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "133bf06", "is_robot_indexable": true, "report_reasons": null, "author": "FireWithBoxingGloves", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133bf06/amazon_photos_multiuser_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133bf06/amazon_photos_multiuser_access/", "subreddit_subscribers": 680331, "created_utc": 1682815128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Subreddit for sharing AI-composed music preventing its disappearance", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_13425np", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_nku71", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Editable Flair", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "I have a great ear for understanding when something is truly innovative or pushing a boundary/is musically-notable.  Normally, I can simply share my love with the rest of music fans.  However, this is no longer possible with A.I. music and I have to boldly put my curation-ability to the test, or else this music will be lost forever.  The **tl;dr is you can now browse a digital library called r/AIMusicArchive that posts only the best-of-the-best creations**.  I plan to expand as music is released over time.\n\n***\n\n**Originally**, I thought the aversion to the first successful* pop A.I. project, F.N. Meka, was due to perceived racism** and a feeling that the music itself was bad.  Upon closer examination and re-evaluated with a new subject that has none of the former (politically-correct; music is more accessible) I found the reaction to be identical.\n\nA revelation hit me that I am truly in a small number of people listening to these generations as sound, without bias of how they were made.  This is not my first rodeo -- when autotune began to collide with Atlanta, the now-popular rapper Future was largely seen as a novelty.  I had to drop my conceptions of what I viewed as Hip Hop and listen to the sound itself with an open-mind -- the end-result was being ahead of the curve on what everyone now know as melodic rap, and going to cheap, amazing concerts that now cost hundreds (since little demand existed in 2014.)\n\nOver and over in music, the above has happened for me in less tangible ways I can cite like I did above, but I still find myself having my best experiences to music that, on first-reaction, I disliked or thought was poor quality (even redundant.(  Sound waves truly do exist outside of theses mental barriers we erect to ultimately block ourselves from potentially-beneficial experiences.\n\n***\n\n**With A.I.** Once again, listeners are confronted with the question that, if what you are hearing sounds *good*, does it matter: how it was made?  Who made it?  To what extent?  To a point of eradication?  Because that's where we are -- great A.I. music being made by truly creative humans behind-the-scenes that is being labeled as novelty garbage.  I have a vision of a continuous wave of select A.I. music becoming extremely more-and-more worth-hearing.\n\nEvery subreddit, big medium and small, has banned or discouraged all ability to share these songs.  Even discussion gets banned.  In the spirit of Harry Smith, I present r/AIMusicArchive.  I will update it with only the best of the best A.I. music that is truly worth hearing.  It might not update for weeks or months -- I only will update it when really wild stuff gets generated.  As things get faster, this will be an ideal place to have a one-stop shop for the best-of-the-best.\n\n***\n\n * In the context of A.I. music, the bar for success is barely above existence, but by all metrics, F.N. Meka was an infamous success and demonstration A.I. music capacity\n\n** - Key things to understand: F.N. Meka WAS voiced by two separate, black vocalists; the main outcry was based on a misconception that Meka was voiced by a white person who had rapped the n-word; additionally, the project was largely seen as a soulless major label venture that utilized black culture as a puppet, which has literally been the story of the music industry and no one is banning half of music because of how it was made.  I'm not okay with exploitation, but I'm also not okay with throwing away already-created art that is literally good.  \n\nThe above points went out the window when the public reaction to Heart on my Sleeve was just as intense, proving that this was never about anything but a fear of A.I.", "author_fullname": "t2_ux8vdzm9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[RESEARCH] I've set up a subreddit for exceptional AI-generated music, filling a gap in existing music/AI subreddits. It features 20 mins of distinct, high-quality tunes. For open minds, a library of discovery.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "three", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q6izv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Research", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681784009.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a great ear for understanding when something is truly innovative or pushing a boundary/is musically-notable.  Normally, I can simply share my love with the rest of music fans.  However, this is no longer possible with A.I. music and I have to boldly put my curation-ability to the test, or else this music will be lost forever.  The &lt;strong&gt;tl;dr is you can now browse a digital library called &lt;a href=\"/r/AIMusicArchive\"&gt;r/AIMusicArchive&lt;/a&gt; that posts only the best-of-the-best creations&lt;/strong&gt;.  I plan to expand as music is released over time.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;Originally&lt;/strong&gt;, I thought the aversion to the first successful* pop A.I. project, F.N. Meka, was due to perceived racism** and a feeling that the music itself was bad.  Upon closer examination and re-evaluated with a new subject that has none of the former (politically-correct; music is more accessible) I found the reaction to be identical.&lt;/p&gt;\n\n&lt;p&gt;A revelation hit me that I am truly in a small number of people listening to these generations as sound, without bias of how they were made.  This is not my first rodeo -- when autotune began to collide with Atlanta, the now-popular rapper Future was largely seen as a novelty.  I had to drop my conceptions of what I viewed as Hip Hop and listen to the sound itself with an open-mind -- the end-result was being ahead of the curve on what everyone now know as melodic rap, and going to cheap, amazing concerts that now cost hundreds (since little demand existed in 2014.)&lt;/p&gt;\n\n&lt;p&gt;Over and over in music, the above has happened for me in less tangible ways I can cite like I did above, but I still find myself having my best experiences to music that, on first-reaction, I disliked or thought was poor quality (even redundant.(  Sound waves truly do exist outside of theses mental barriers we erect to ultimately block ourselves from potentially-beneficial experiences.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;strong&gt;With A.I.&lt;/strong&gt; Once again, listeners are confronted with the question that, if what you are hearing sounds &lt;em&gt;good&lt;/em&gt;, does it matter: how it was made?  Who made it?  To what extent?  To a point of eradication?  Because that&amp;#39;s where we are -- great A.I. music being made by truly creative humans behind-the-scenes that is being labeled as novelty garbage.  I have a vision of a continuous wave of select A.I. music becoming extremely more-and-more worth-hearing.&lt;/p&gt;\n\n&lt;p&gt;Every subreddit, big medium and small, has banned or discouraged all ability to share these songs.  Even discussion gets banned.  In the spirit of Harry Smith, I present &lt;a href=\"/r/AIMusicArchive\"&gt;r/AIMusicArchive&lt;/a&gt;.  I will update it with only the best of the best A.I. music that is truly worth hearing.  It might not update for weeks or months -- I only will update it when really wild stuff gets generated.  As things get faster, this will be an ideal place to have a one-stop shop for the best-of-the-best.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In the context of A.I. music, the bar for success is barely above existence, but by all metrics, F.N. Meka was an infamous success and demonstration A.I. music capacity&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;** - Key things to understand: F.N. Meka WAS voiced by two separate, black vocalists; the main outcry was based on a misconception that Meka was voiced by a white person who had rapped the n-word; additionally, the project was largely seen as a soulless major label venture that utilized black culture as a puppet, which has literally been the story of the music industry and no one is banning half of music because of how it was made.  I&amp;#39;m not okay with exploitation, but I&amp;#39;m also not okay with throwing away already-created art that is literally good.  &lt;/p&gt;\n\n&lt;p&gt;The above points went out the window when the public reaction to Heart on my Sleeve was just as intense, proving that this was never about anything but a fear of A.I.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12q6izv", "is_robot_indexable": true, "report_reasons": null, "author": "HappyVibration", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/12q6izv/research_ive_set_up_a_subreddit_for_exceptional/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/MachineLearning/comments/12q6izv/research_ive_set_up_a_subreddit_for_exceptional/", "subreddit_subscribers": 2646617, "created_utc": 1681784009.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1682886638.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/MachineLearning/comments/12q6izv/research_ive_set_up_a_subreddit_for_exceptional/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "13425np", "is_robot_indexable": true, "report_reasons": null, "author": "jorvaor", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12q6izv", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13425np/subreddit_for_sharing_aicomposed_music_preventing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/MachineLearning/comments/12q6izv/research_ive_set_up_a_subreddit_for_exceptional/", "subreddit_subscribers": 680331, "created_utc": 1682886638.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just transferred 4 smaller drives to me new 16 TB WD Element drive \n\n\nI watch 2-4 hours of video daily on my drives steamed to the TV via USB but now it\u2019ll just be from 1 drive\n\nShould I worry or just dive in?\n\nI have thought about just transferring daily what I want to watch to my thumb drive and streaming from the thumb.", "author_fullname": "t2_viuwzrr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will streaming video from an external HD 2-4 hours daily put a lot of stress on it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1341ovx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682885460.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just transferred 4 smaller drives to me new 16 TB WD Element drive &lt;/p&gt;\n\n&lt;p&gt;I watch 2-4 hours of video daily on my drives steamed to the TV via USB but now it\u2019ll just be from 1 drive&lt;/p&gt;\n\n&lt;p&gt;Should I worry or just dive in?&lt;/p&gt;\n\n&lt;p&gt;I have thought about just transferring daily what I want to watch to my thumb drive and streaming from the thumb.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1341ovx", "is_robot_indexable": true, "report_reasons": null, "author": "Rotisseriejedi", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1341ovx/will_streaming_video_from_an_external_hd_24_hours/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1341ovx/will_streaming_video_from_an_external_hd_24_hours/", "subreddit_subscribers": 680331, "created_utc": 1682885460.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Total size of the audio library is around 140gb. Here is the link to get a ton of free sounds from sonniss.    \n\n\n[https://sonniss.com/gameaudiogdc](https://sonniss.com/gameaudiogdc)", "author_fullname": "t2_8oqkctwc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sonniss Royalty Free Sound Effects Archive: GameAudioGDC from 2015-2020", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_1341jsf", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682885103.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Total size of the audio library is around 140gb. Here is the link to get a ton of free sounds from sonniss.    &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://sonniss.com/gameaudiogdc\"&gt;https://sonniss.com/gameaudiogdc&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/G3oMmyvE27MvZvb5ZgC_sYWISvjvkMdq-WIavkkc4rk.jpg?auto=webp&amp;v=enabled&amp;s=4dd8794b62ebee5f67e2352de97b2297e41b731c", "width": 800, "height": 347}, "resolutions": [{"url": "https://external-preview.redd.it/G3oMmyvE27MvZvb5ZgC_sYWISvjvkMdq-WIavkkc4rk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1afaa26af04ae17d7be5c00a942ee95e08747c9c", "width": 108, "height": 46}, {"url": "https://external-preview.redd.it/G3oMmyvE27MvZvb5ZgC_sYWISvjvkMdq-WIavkkc4rk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe9c9324bce8865eea8de2960c8cb232fff97743", "width": 216, "height": 93}, {"url": "https://external-preview.redd.it/G3oMmyvE27MvZvb5ZgC_sYWISvjvkMdq-WIavkkc4rk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=75a6c6b1415b2b15a139a8210308da6103690742", "width": 320, "height": 138}, {"url": "https://external-preview.redd.it/G3oMmyvE27MvZvb5ZgC_sYWISvjvkMdq-WIavkkc4rk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=13890ed79f3716dd67b84ccb42620f52942de6d4", "width": 640, "height": 277}], "variants": {}, "id": "Ucu5T3Q1pjrf8jeVKxg4fs5kKqYAzMNAAv-b7S47dE8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "1341jsf", "is_robot_indexable": true, "report_reasons": null, "author": "polymood_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1341jsf/sonniss_royalty_free_sound_effects_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1341jsf/sonniss_royalty_free_sound_effects_archive/", "subreddit_subscribers": 680331, "created_utc": 1682885103.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I will be on the move for some time without access to my main setup, but do need about 4TB on hand. While not ideal, I already have an empty portable 4TB easystore SMR drive and a better drive of this form factor would probably be an SSD, which is therefore outside of my immediate budget.\n\nI'm considering encrypting it with LUKS and setting up a btrfs (may consider ext4 if beneficial) partition inside, then using it rather frequently. I'm having a bit of trouble wrapping my head around how an SMR disk would take this in the long run. Since encryption should make freed and occupied space indistinguishable, am I going to end up with a massively fragmented disk compounded with SMR fragmentation over time? Or is there something that SMR drives can do to deal with encryption?", "author_fullname": "t2_174s4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SMR with LUKS-encrypted btrfs - acceptable or bad idea in the long run?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133zua6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682880766.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will be on the move for some time without access to my main setup, but do need about 4TB on hand. While not ideal, I already have an empty portable 4TB easystore SMR drive and a better drive of this form factor would probably be an SSD, which is therefore outside of my immediate budget.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m considering encrypting it with LUKS and setting up a btrfs (may consider ext4 if beneficial) partition inside, then using it rather frequently. I&amp;#39;m having a bit of trouble wrapping my head around how an SMR disk would take this in the long run. Since encryption should make freed and occupied space indistinguishable, am I going to end up with a massively fragmented disk compounded with SMR fragmentation over time? Or is there something that SMR drives can do to deal with encryption?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133zua6", "is_robot_indexable": true, "report_reasons": null, "author": "Dystributfunctionion", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133zua6/smr_with_luksencrypted_btrfs_acceptable_or_bad/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133zua6/smr_with_luksencrypted_btrfs_acceptable_or_bad/", "subreddit_subscribers": 680331, "created_utc": 1682880766.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_9tlfztgu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Crucial P3 Plus 4TB M.2 PCIe Gen4 NVMe Internal SSD - \u00a3192.99", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133z94s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682879333.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "amazon.co.uk", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.amazon.co.uk/Crucial-Plus-PCIe-Gen4-Internal/dp/B0B25M8FXX/ref=sr_1_5?crid=1GP7DDJ3GTOZY&amp;keywords=crucial%2Bp3&amp;qid=1682879265&amp;refinements=p_n_feature_seven_browse-bin%3A56158693031&amp;rnid=56157908031&amp;s=computers&amp;sprefix=Crucia%2Caps%2C122&amp;sr=1-5&amp;th=1", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "133z94s", "is_robot_indexable": true, "report_reasons": null, "author": "Striking_Sea_7469", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133z94s/crucial_p3_plus_4tb_m2_pcie_gen4_nvme_internal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.amazon.co.uk/Crucial-Plus-PCIe-Gen4-Internal/dp/B0B25M8FXX/ref=sr_1_5?crid=1GP7DDJ3GTOZY&amp;keywords=crucial%2Bp3&amp;qid=1682879265&amp;refinements=p_n_feature_seven_browse-bin%3A56158693031&amp;rnid=56157908031&amp;s=computers&amp;sprefix=Crucia%2Caps%2C122&amp;sr=1-5&amp;th=1", "subreddit_subscribers": 680331, "created_utc": 1682879333.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!  \n\n\nI have Macrium reflect and I have a Win10 image file. I am used to restoring it on new devices as a faster alternative to installing Win10 and downloading all the updates from scratch.  \n\n\nI also have a 4x hotswap bay for 2,5\" SSDs. Can I select multiple target drives in Macrium when restoring an image? I know it would take longer to restore, of course, I just want to leave it to restore instead of having to re-select it after every drive.  \n\n\nThanks!", "author_fullname": "t2_69e78j78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Quick newbie question: with Macrium, can you select multiple target drives at the same time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133yoyd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682877934.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!  &lt;/p&gt;\n\n&lt;p&gt;I have Macrium reflect and I have a Win10 image file. I am used to restoring it on new devices as a faster alternative to installing Win10 and downloading all the updates from scratch.  &lt;/p&gt;\n\n&lt;p&gt;I also have a 4x hotswap bay for 2,5&amp;quot; SSDs. Can I select multiple target drives in Macrium when restoring an image? I know it would take longer to restore, of course, I just want to leave it to restore instead of having to re-select it after every drive.  &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133yoyd", "is_robot_indexable": true, "report_reasons": null, "author": "real_smoky", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133yoyd/quick_newbie_question_with_macrium_can_you_select/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133yoyd/quick_newbie_question_with_macrium_can_you_select/", "subreddit_subscribers": 680331, "created_utc": 1682877934.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I am picking up a 8 TB WD - easystore external hard drive tomorrow. I will be backing up all of the movies, TV shows and sports games I have stored on my other external hard drive (5 TB Seagate 3.5). \n\nI just got the Seagate one last week, but after poking around on here, I realized that relying on that one drive (as well as it being Seagate) was just asking for something to go wrong. So another $150 or so to prevent me from having to re-burn all of my discs back onto my computer is well worth it. \n\nHowever, I skipped some steps with the Seagate drive (I just plugged it in and started dragging files over). I want to make sure I take all of the steps to make my new WD drive work properly.\n\nAny tips or advice you can offer will be greatly appreciated. \n\nPS: Between the 5TB Seagate drive and the 8TB WD one, which one is a better option to host all of my Plex files (if there is a difference at all)?", "author_fullname": "t2_af98i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Beginner's guide to formatting a new external hard drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133st6g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682863397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am picking up a 8 TB WD - easystore external hard drive tomorrow. I will be backing up all of the movies, TV shows and sports games I have stored on my other external hard drive (5 TB Seagate 3.5). &lt;/p&gt;\n\n&lt;p&gt;I just got the Seagate one last week, but after poking around on here, I realized that relying on that one drive (as well as it being Seagate) was just asking for something to go wrong. So another $150 or so to prevent me from having to re-burn all of my discs back onto my computer is well worth it. &lt;/p&gt;\n\n&lt;p&gt;However, I skipped some steps with the Seagate drive (I just plugged it in and started dragging files over). I want to make sure I take all of the steps to make my new WD drive work properly.&lt;/p&gt;\n\n&lt;p&gt;Any tips or advice you can offer will be greatly appreciated. &lt;/p&gt;\n\n&lt;p&gt;PS: Between the 5TB Seagate drive and the 8TB WD one, which one is a better option to host all of my Plex files (if there is a difference at all)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133st6g", "is_robot_indexable": true, "report_reasons": null, "author": "btgio", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133st6g/beginners_guide_to_formatting_a_new_external_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133st6g/beginners_guide_to_formatting_a_new_external_hard/", "subreddit_subscribers": 680331, "created_utc": 1682863397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What kind of setup do you guys have in terms of a big storage array on a server rack? Right now, I have a large chassis with a ton of drive bays, but it's not going to be sustainable as I keep increasing my storage. I'm wondering what the most scalable setup is for a huge amount of storage.", "author_fullname": "t2_imj4i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Server rack setup with storage array?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133nr16", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682857272.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What kind of setup do you guys have in terms of a big storage array on a server rack? Right now, I have a large chassis with a ton of drive bays, but it&amp;#39;s not going to be sustainable as I keep increasing my storage. I&amp;#39;m wondering what the most scalable setup is for a huge amount of storage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133nr16", "is_robot_indexable": true, "report_reasons": null, "author": "Grandfather-Paradox", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133nr16/server_rack_setup_with_storage_array/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133nr16/server_rack_setup_with_storage_array/", "subreddit_subscribers": 680331, "created_utc": 1682857272.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone. Long time reader, first time poster.\n\nA couple years ago, I successfully archived 35K bookmarks (\u22481TB) with Archivebox 0.4.1\n\nBack in January, I installed docker version of Archivebox 0.6.2 in a brand new linux machine, and managed to save another 7K new bookmarks in a single run.\n\nI'm decided to finally merge those two instances, and run it again via docker-compose, to complete my bookmarks collection.\n\nI came across this link, explaining how to merge archives, but I don't fully understand it.\n\nhttps://github.com/ArchiveBox/ArchiveBox/wiki/Upgrading-or-Merging-Archives\n\nI don't know what to do with the original data folder containing 35K bookmarks. I guess I can move the content of \"archive\" folder to it's new location, but the rest of the files (*index.html, index.sqlite3, archivebox.conf and index_old.json*) not really sure where to put them.\n\nCan any of you guys help me out and guide me along every step of the process?", "author_fullname": "t2_j5hu3307", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Archivebox - trying to hoard 112K bookmarks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133mexo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682853014.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. Long time reader, first time poster.&lt;/p&gt;\n\n&lt;p&gt;A couple years ago, I successfully archived 35K bookmarks (\u22481TB) with Archivebox 0.4.1&lt;/p&gt;\n\n&lt;p&gt;Back in January, I installed docker version of Archivebox 0.6.2 in a brand new linux machine, and managed to save another 7K new bookmarks in a single run.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m decided to finally merge those two instances, and run it again via docker-compose, to complete my bookmarks collection.&lt;/p&gt;\n\n&lt;p&gt;I came across this link, explaining how to merge archives, but I don&amp;#39;t fully understand it.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ArchiveBox/ArchiveBox/wiki/Upgrading-or-Merging-Archives\"&gt;https://github.com/ArchiveBox/ArchiveBox/wiki/Upgrading-or-Merging-Archives&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what to do with the original data folder containing 35K bookmarks. I guess I can move the content of &amp;quot;archive&amp;quot; folder to it&amp;#39;s new location, but the rest of the files (&lt;em&gt;index.html, index.sqlite3, archivebox.conf and index_old.json&lt;/em&gt;) not really sure where to put them.&lt;/p&gt;\n\n&lt;p&gt;Can any of you guys help me out and guide me along every step of the process?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aLOzvQh3di5n5iViBK4zfGXUzzYsfeDSG2Op4ocLsMA.jpg?auto=webp&amp;v=enabled&amp;s=27f4cb6929d778d2c560f14a591c9c09ef4c09c1", "width": 1280, "height": 681}, "resolutions": [{"url": "https://external-preview.redd.it/aLOzvQh3di5n5iViBK4zfGXUzzYsfeDSG2Op4ocLsMA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2efb39cef2f9a7e380e4c189885c075044f63a3e", "width": 108, "height": 57}, {"url": "https://external-preview.redd.it/aLOzvQh3di5n5iViBK4zfGXUzzYsfeDSG2Op4ocLsMA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=648c4c7f0f4b684bec9559a0122731661a020933", "width": 216, "height": 114}, {"url": "https://external-preview.redd.it/aLOzvQh3di5n5iViBK4zfGXUzzYsfeDSG2Op4ocLsMA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac818f88887a2b59a4b3350aa7074327372ce5cd", "width": 320, "height": 170}, {"url": "https://external-preview.redd.it/aLOzvQh3di5n5iViBK4zfGXUzzYsfeDSG2Op4ocLsMA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2885cf0550f8753c014ab4e44c973d7a434c8a19", "width": 640, "height": 340}, {"url": "https://external-preview.redd.it/aLOzvQh3di5n5iViBK4zfGXUzzYsfeDSG2Op4ocLsMA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=46c3e878c4be081c04ee7e68cd31ac6c25562f2f", "width": 960, "height": 510}, {"url": "https://external-preview.redd.it/aLOzvQh3di5n5iViBK4zfGXUzzYsfeDSG2Op4ocLsMA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=576d38cd62272bb5163ca7a9ffe2474ecbcfdf46", "width": 1080, "height": 574}], "variants": {}, "id": "-z9vjhMaxpt-_StJBYK3ZTqIebyvFPjVGKerpNepLGM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133mexo", "is_robot_indexable": true, "report_reasons": null, "author": "marywang2022", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133mexo/archivebox_trying_to_hoard_112k_bookmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133mexo/archivebox_trying_to_hoard_112k_bookmarks/", "subreddit_subscribers": 680331, "created_utc": 1682853014.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So yeah, I have accumulated a bunch of external HDDs, been a while since I counted them but theres at least over 15 of them. Some are 3.5 inch, some 2.5.\n\nCouple years ago I built 2 racks for them from wood, which didnt turn out particularly well, but worked for a while until I got even more of them and now they dont fit and instead of trying to upgrade my shitty attempt I would rather start from scratch.\n\nSo do you guys have any ideas, experience with stacking external HDDs in some sort of a rack? Something kinda like a CD/DVD rack or shelve would be neat, but the dimensions are going to be way off of course even if you try to modify it. So at the moment I am wondering if theres something similar, a narrow self-standing shelf with a ton of levels in it which height could be easily adjusted as intended or by doing a bit of modifications to it? Atm the drives are just laying on the table and on a couple of normal shelves which looks ugly and takes way more space than they actually need. I would also like to keep the drives vertically.\n\nAnd I am just after a rack/shelve, I am not going to start building some nas server with them or anything like that as I want to be able transport them easily in a bag if and when needed. A big metal box isn't very ideal for that, which is why I also upgraded to a smaller ITX case from a regular one.", "author_fullname": "t2_jkzbd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DIY rack ideas for a bunch of usb HDDs of different sizes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133ls4r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682850957.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So yeah, I have accumulated a bunch of external HDDs, been a while since I counted them but theres at least over 15 of them. Some are 3.5 inch, some 2.5.&lt;/p&gt;\n\n&lt;p&gt;Couple years ago I built 2 racks for them from wood, which didnt turn out particularly well, but worked for a while until I got even more of them and now they dont fit and instead of trying to upgrade my shitty attempt I would rather start from scratch.&lt;/p&gt;\n\n&lt;p&gt;So do you guys have any ideas, experience with stacking external HDDs in some sort of a rack? Something kinda like a CD/DVD rack or shelve would be neat, but the dimensions are going to be way off of course even if you try to modify it. So at the moment I am wondering if theres something similar, a narrow self-standing shelf with a ton of levels in it which height could be easily adjusted as intended or by doing a bit of modifications to it? Atm the drives are just laying on the table and on a couple of normal shelves which looks ugly and takes way more space than they actually need. I would also like to keep the drives vertically.&lt;/p&gt;\n\n&lt;p&gt;And I am just after a rack/shelve, I am not going to start building some nas server with them or anything like that as I want to be able transport them easily in a bag if and when needed. A big metal box isn&amp;#39;t very ideal for that, which is why I also upgraded to a smaller ITX case from a regular one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133ls4r", "is_robot_indexable": true, "report_reasons": null, "author": "kasetti", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133ls4r/diy_rack_ideas_for_a_bunch_of_usb_hdds_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133ls4r/diy_rack_ideas_for_a_bunch_of_usb_hdds_of/", "subreddit_subscribers": 680331, "created_utc": 1682850957.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So, My best friend accidentally deleted both chats on telegram and it has almost 4 years of memories saved on it. I opened telegram on edge and before it subsequently loaded, I noticed that it still had our messages a few weeks ago. I then removed the internet connection so that it won't fully load and still have those cached messages. I was wondering if it's possible to extract those messages from Edge's cache? If so, how? These messages mean a lot to me and I guess it serves a core memory for me. I would really be grateful if you can help me save these messages. Thank you", "author_fullname": "t2_hjabt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saving cached telegram messages from Edge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133aclo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682812047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, My best friend accidentally deleted both chats on telegram and it has almost 4 years of memories saved on it. I opened telegram on edge and before it subsequently loaded, I noticed that it still had our messages a few weeks ago. I then removed the internet connection so that it won&amp;#39;t fully load and still have those cached messages. I was wondering if it&amp;#39;s possible to extract those messages from Edge&amp;#39;s cache? If so, how? These messages mean a lot to me and I guess it serves a core memory for me. I would really be grateful if you can help me save these messages. Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133aclo", "is_robot_indexable": true, "report_reasons": null, "author": "McJakey", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133aclo/saving_cached_telegram_messages_from_edge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133aclo/saving_cached_telegram_messages_from_edge/", "subreddit_subscribers": 680331, "created_utc": 1682812047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have my entire collection ripped to ISO. I am now using DVD FAB on an old WIN 7 laptop to convert these into x264 files for easy streaming\n\n&amp;#x200B;\n\nLooking for a good program for my WIN 10 PC that will do the same thing only faster possibly", "author_fullname": "t2_viuwzrr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best free program to convert my ISO collection into individual video files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1336vco", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682802751.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have my entire collection ripped to ISO. I am now using DVD FAB on an old WIN 7 laptop to convert these into x264 files for easy streaming&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Looking for a good program for my WIN 10 PC that will do the same thing only faster possibly&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1336vco", "is_robot_indexable": true, "report_reasons": null, "author": "Rotisseriejedi", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1336vco/best_free_program_to_convert_my_iso_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1336vco/best_free_program_to_convert_my_iso_collection/", "subreddit_subscribers": 680331, "created_utc": 1682802751.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey I didn't see anything after a cursory google search, but do any of you know of a bot/integration that I could use to ping my discord server when, for example, the price of the 14TB WDs hit a specific price target at BBY?", "author_fullname": "t2_71ot7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Discord Bot to Track Prices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133xeo8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682874708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey I didn&amp;#39;t see anything after a cursory google search, but do any of you know of a bot/integration that I could use to ping my discord server when, for example, the price of the 14TB WDs hit a specific price target at BBY?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133xeo8", "is_robot_indexable": true, "report_reasons": null, "author": "firefall", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133xeo8/discord_bot_to_track_prices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133xeo8/discord_bot_to_track_prices/", "subreddit_subscribers": 680331, "created_utc": 1682874708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a windows app (preferably open source) with a GUI that makes it possible to queue up large file copies so that they're copied sequentially? In windows explorer, when I initiate a few copies they happy simultaneously which is slow to perform on spinning disks. I'm looking for something that will let me create a queue of simple file copies.\n\nQuicksync kinda does this but it's cumbersome to set up for just one copy, this isn't for regular backups.", "author_fullname": "t2_3kkzr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows app to queue sequential copying of large files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133w7r3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682871764.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a windows app (preferably open source) with a GUI that makes it possible to queue up large file copies so that they&amp;#39;re copied sequentially? In windows explorer, when I initiate a few copies they happy simultaneously which is slow to perform on spinning disks. I&amp;#39;m looking for something that will let me create a queue of simple file copies.&lt;/p&gt;\n\n&lt;p&gt;Quicksync kinda does this but it&amp;#39;s cumbersome to set up for just one copy, this isn&amp;#39;t for regular backups.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133w7r3", "is_robot_indexable": true, "report_reasons": null, "author": "kwirky88", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133w7r3/windows_app_to_queue_sequential_copying_of_large/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133w7r3/windows_app_to_queue_sequential_copying_of_large/", "subreddit_subscribers": 680331, "created_utc": 1682871764.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I can already copy the files manually with SMB but if my comp goes to sleep the copying stops. I want the NAS to take care of of the copying of several terabytes of Data without having to keep my computer awake.\n\nIs this even possible?", "author_fullname": "t2_v1x12bzc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What Synology package is used for unattended backups of specific files and directories from a 18 TB USB external HDD to my new Synology NAS.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133a9fx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682811801.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can already copy the files manually with SMB but if my comp goes to sleep the copying stops. I want the NAS to take care of of the copying of several terabytes of Data without having to keep my computer awake.&lt;/p&gt;\n\n&lt;p&gt;Is this even possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133a9fx", "is_robot_indexable": true, "report_reasons": null, "author": "IllicitHypocrisy", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133a9fx/what_synology_package_is_used_for_unattended/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133a9fx/what_synology_package_is_used_for_unattended/", "subreddit_subscribers": 680331, "created_utc": 1682811801.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I already asked Newmaxx.\n\nI've bought 2 enclosures, one with the ASM2362 bridge chip and the other with the rtl9210b. Couldn't get either to work with my 2 ssds. The crucial p3 plus 4tb was alright with the realtek chip on a 5gb/s port, but I didn't trust it to put my data on, given blue screens and other issues on other Ports. After flashing the firmware, it didn't even work. \n\nIt's in my pc now, and the silicon power 512gb PCIe 3.0 with dram it replaced didn't show up on either enclosure. I like the form factor, but nvme to USB isn't working on ryzen. Works fine on a 12 year old intel board, though. Will a sata iii ssd work? I'm calling it nvme because of the form factor. I know it's incorrect. \n\nI have plenty of Hdd enclosures and docks. Can I assume that a Transcend MTS830S 4 TB will have no detection problems? What's the heat like on sata nvmes? And since commercial drives like the SanDisk extreme v2 and wd my passport use the ASM2362, should I assume they won't work with my pc?\n\nFinally, how bad would a Samsung qvo 8tb drive be for my needs? Because 4tb drives cost half as much, I saw no need for it. I also heard it's really slow when you exit the cache. But Gigabyte's crappy motherboard has forced me to use sata, as I have no nvme Ports left and USB to nvme is a non starter.", "author_fullname": "t2_gti8t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will sata iii 2280 \"nvme\" ssd work on ryzen motherboard?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_133yvjv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682884433.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682878387.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I already asked Newmaxx.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve bought 2 enclosures, one with the ASM2362 bridge chip and the other with the rtl9210b. Couldn&amp;#39;t get either to work with my 2 ssds. The crucial p3 plus 4tb was alright with the realtek chip on a 5gb/s port, but I didn&amp;#39;t trust it to put my data on, given blue screens and other issues on other Ports. After flashing the firmware, it didn&amp;#39;t even work. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s in my pc now, and the silicon power 512gb PCIe 3.0 with dram it replaced didn&amp;#39;t show up on either enclosure. I like the form factor, but nvme to USB isn&amp;#39;t working on ryzen. Works fine on a 12 year old intel board, though. Will a sata iii ssd work? I&amp;#39;m calling it nvme because of the form factor. I know it&amp;#39;s incorrect. &lt;/p&gt;\n\n&lt;p&gt;I have plenty of Hdd enclosures and docks. Can I assume that a Transcend MTS830S 4 TB will have no detection problems? What&amp;#39;s the heat like on sata nvmes? And since commercial drives like the SanDisk extreme v2 and wd my passport use the ASM2362, should I assume they won&amp;#39;t work with my pc?&lt;/p&gt;\n\n&lt;p&gt;Finally, how bad would a Samsung qvo 8tb drive be for my needs? Because 4tb drives cost half as much, I saw no need for it. I also heard it&amp;#39;s really slow when you exit the cache. But Gigabyte&amp;#39;s crappy motherboard has forced me to use sata, as I have no nvme Ports left and USB to nvme is a non starter.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "133yvjv", "is_robot_indexable": true, "report_reasons": null, "author": "wgolding", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/133yvjv/will_sata_iii_2280_nvme_ssd_work_on_ryzen/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/133yvjv/will_sata_iii_2280_nvme_ssd_work_on_ryzen/", "subreddit_subscribers": 680331, "created_utc": 1682878387.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}