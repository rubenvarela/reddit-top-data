{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nI'm considering data engineering as a career (as a sophomore SWE student) and I'd like to hear what actual data engineers do that would be fun...\n\nI've also considered back-end engineering which I think has more job oppurtunities but seems awfully boring for me.", "author_fullname": "t2_70y52411t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineers, what cool stuff do you do at work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ndkfv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681582937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m considering data engineering as a career (as a sophomore SWE student) and I&amp;#39;d like to hear what actual data engineers do that would be fun...&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also considered back-end engineering which I think has more job oppurtunities but seems awfully boring for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ndkfv", "is_robot_indexable": true, "report_reasons": null, "author": "DoNotTipDeliveryBoy", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ndkfv/data_engineers_what_cool_stuff_do_you_do_at_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ndkfv/data_engineers_what_cool_stuff_do_you_do_at_work/", "subreddit_subscribers": 99372, "created_utc": 1681582937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I see a Lead position with a range from 115k to 230K, \n\nHow many YoE does one need to max out that 230K, do DE really make these kind of money? Assuming it's 230K base.\n\nAlso anyone here working here for CVS, or went through their interview process, how hard is it to pass, get an offer and working there?\n\nThanks.", "author_fullname": "t2_1411ira9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CVS - Lead DE 115K to 230K?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mysus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681557074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I see a Lead position with a range from 115k to 230K, &lt;/p&gt;\n\n&lt;p&gt;How many YoE does one need to max out that 230K, do DE really make these kind of money? Assuming it&amp;#39;s 230K base.&lt;/p&gt;\n\n&lt;p&gt;Also anyone here working here for CVS, or went through their interview process, how hard is it to pass, get an offer and working there?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12mysus", "is_robot_indexable": true, "report_reasons": null, "author": "wisegeek57", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mysus/cvs_lead_de_115k_to_230k/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mysus/cvs_lead_de_115k_to_230k/", "subreddit_subscribers": 99372, "created_utc": 1681557074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all! Hope you're all enjoying Saturday :)\n\nI recently accepted a junior DE position starting in 7 weeks from today. The job description was fairly general with the requirements including only python proficiency and PySpark being a desirable skill and familiarity with relational database structures.\n\nThe company had a couple of behavioural interviews and then a take home take on analysing a dataset which I did with Pandas and then prepared a report summarising my findings. The interview then was basically about my background and how I approached the data analysis task, but nothing about PySpark or databases.\n\nI am looking for some advice on how I can familiarise myself with PySpark enough to not make a fool out of myself on day 1 and any simple data pipeline tutorials which are recommended by the community.\n\nI came across the following 2 books which have decent reviews:\n\n* [Essential PySpark for Scalable Data Analytics](https://www.amazon.in/Essential-PySpark-Scalable-Data-Analytics-ebook/dp/B098C3PPSJ/ref=sr_1_1?crid=1YY53KSSBOYBF&amp;keywords=Essential+PySpark+for+Scalable+Data+Analytics&amp;qid=1681459147&amp;sprefix=essential+pyspark+for+scalable+data+analytics%2Caps%2C386&amp;sr=8-1#customerReviews)\n* [Data Algorithms with Spark](https://www.amazon.in/Data-Algorithms-Spark-Patterns-Grayscale/dp/9355420781/ref=cm_cr_arp_d_product_top?ie=UTF8)\n\nThank you very much in advance and any additional insight or tips would be greatly appreciated!\n\nPS: I am self-taught and over the past year have picked up Python and SQL and have worked with Pandas as well.", "author_fullname": "t2_ipo54", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help! I got a junior DE role!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mxv8x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681554623.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! Hope you&amp;#39;re all enjoying Saturday :)&lt;/p&gt;\n\n&lt;p&gt;I recently accepted a junior DE position starting in 7 weeks from today. The job description was fairly general with the requirements including only python proficiency and PySpark being a desirable skill and familiarity with relational database structures.&lt;/p&gt;\n\n&lt;p&gt;The company had a couple of behavioural interviews and then a take home take on analysing a dataset which I did with Pandas and then prepared a report summarising my findings. The interview then was basically about my background and how I approached the data analysis task, but nothing about PySpark or databases.&lt;/p&gt;\n\n&lt;p&gt;I am looking for some advice on how I can familiarise myself with PySpark enough to not make a fool out of myself on day 1 and any simple data pipeline tutorials which are recommended by the community.&lt;/p&gt;\n\n&lt;p&gt;I came across the following 2 books which have decent reviews:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.in/Essential-PySpark-Scalable-Data-Analytics-ebook/dp/B098C3PPSJ/ref=sr_1_1?crid=1YY53KSSBOYBF&amp;amp;keywords=Essential+PySpark+for+Scalable+Data+Analytics&amp;amp;qid=1681459147&amp;amp;sprefix=essential+pyspark+for+scalable+data+analytics%2Caps%2C386&amp;amp;sr=8-1#customerReviews\"&gt;Essential PySpark for Scalable Data Analytics&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.in/Data-Algorithms-Spark-Patterns-Grayscale/dp/9355420781/ref=cm_cr_arp_d_product_top?ie=UTF8\"&gt;Data Algorithms with Spark&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you very much in advance and any additional insight or tips would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;PS: I am self-taught and over the past year have picked up Python and SQL and have worked with Pandas as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mxv8x", "is_robot_indexable": true, "report_reasons": null, "author": "darthvardhan95", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mxv8x/help_i_got_a_junior_de_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mxv8x/help_i_got_a_junior_de_role/", "subreddit_subscribers": 99372, "created_utc": 1681554623.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nWe are extracting Salesforce data using Fivetran to Snowflake and we have problems with orphans - rows hard deleted in the app are not deleted in our tables in snowflake and will always remain there...\n\nSo correct me if I am wrong but salesforce connectors cant spot HARD DELETED records.\n\nIn other words, if records are soft deleted in the Salesforce app , API calls will see them as rows with IS\\_DELETED = true flag. However if the row is hard deleted from the SF app rest api aka integration tools wont handle that and such 'orphans' will remain in our table using delta loads.\n\nWe end up with the situation where our 'analytical views' show 'orphans'- rows of data that no longer exist in the app.\n\n**How could we set a process to make sure our data in EDW = SF App as much as possible ;)?**\n\nWe use Fivetran as extraction tool and Snowflake as EDW.\n\nMy thought is to DROP / TRUNCACE SF tables in Snowflake and use rest api to call fivetran and force historical resync(**full reload**) once a week on saturday (off work not to impact business). This way we will have weekly process to make sure that our data is 'cleaned once every week'\n\n**Pros:**\n\nData cleansed once a week - acceptable by business\n\n**Cons:**\n\nif by any chance something will fail during 'historical resync' and our ladning table in Snowflake will remain empty, this might impact our 'analytical views' where they could show 'no data'\n\n&amp;#x200B;\n\nIs there any easier, better way, just to ensure our data will always be there ?\n\nI was thinking of RAW / STAGE table / schema...but I am getting loops in my thought process.\n\n&amp;#x200B;\n\nSomething like:\n\n1.Create RAW schema - landing point, for all Salesforce objects we extract from Fivetran. Still, DROP/TRUNCATE tables every saturday and perform full load to make sure we avoid orphans.\n\n2.Have STAGE schema. In order to MERGE data from RAW into STAGE (INSERT,UPDATE,DELETE)\n\n3.And I am getting lost here.....because if RAW will get truncated every saturday and will fail to extract data out of SF for god knows why reason....then merge (DELETE command will wipe entire STAGE table because RAW is empty).\n\n&amp;#x200B;\n\nGuys any ideas?", "author_fullname": "t2_do9wxbfu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salesforce data extraction how to handle hard deletes in the app?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n0xnh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681562336.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681562124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;We are extracting Salesforce data using Fivetran to Snowflake and we have problems with orphans - rows hard deleted in the app are not deleted in our tables in snowflake and will always remain there...&lt;/p&gt;\n\n&lt;p&gt;So correct me if I am wrong but salesforce connectors cant spot HARD DELETED records.&lt;/p&gt;\n\n&lt;p&gt;In other words, if records are soft deleted in the Salesforce app , API calls will see them as rows with IS_DELETED = true flag. However if the row is hard deleted from the SF app rest api aka integration tools wont handle that and such &amp;#39;orphans&amp;#39; will remain in our table using delta loads.&lt;/p&gt;\n\n&lt;p&gt;We end up with the situation where our &amp;#39;analytical views&amp;#39; show &amp;#39;orphans&amp;#39;- rows of data that no longer exist in the app.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How could we set a process to make sure our data in EDW = SF App as much as possible ;)?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We use Fivetran as extraction tool and Snowflake as EDW.&lt;/p&gt;\n\n&lt;p&gt;My thought is to DROP / TRUNCACE SF tables in Snowflake and use rest api to call fivetran and force historical resync(&lt;strong&gt;full reload&lt;/strong&gt;) once a week on saturday (off work not to impact business). This way we will have weekly process to make sure that our data is &amp;#39;cleaned once every week&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Data cleansed once a week - acceptable by business&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;if by any chance something will fail during &amp;#39;historical resync&amp;#39; and our ladning table in Snowflake will remain empty, this might impact our &amp;#39;analytical views&amp;#39; where they could show &amp;#39;no data&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there any easier, better way, just to ensure our data will always be there ?&lt;/p&gt;\n\n&lt;p&gt;I was thinking of RAW / STAGE table / schema...but I am getting loops in my thought process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Something like:&lt;/p&gt;\n\n&lt;p&gt;1.Create RAW schema - landing point, for all Salesforce objects we extract from Fivetran. Still, DROP/TRUNCATE tables every saturday and perform full load to make sure we avoid orphans.&lt;/p&gt;\n\n&lt;p&gt;2.Have STAGE schema. In order to MERGE data from RAW into STAGE (INSERT,UPDATE,DELETE)&lt;/p&gt;\n\n&lt;p&gt;3.And I am getting lost here.....because if RAW will get truncated every saturday and will fail to extract data out of SF for god knows why reason....then merge (DELETE command will wipe entire STAGE table because RAW is empty).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Guys any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12n0xnh", "is_robot_indexable": true, "report_reasons": null, "author": "87keicam", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n0xnh/salesforce_data_extraction_how_to_handle_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12n0xnh/salesforce_data_extraction_how_to_handle_hard/", "subreddit_subscribers": 99372, "created_utc": 1681562124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thats all, thats the post. Idk why but it is just simply not intuitive. I feel like Dagster has so many great concepts, but trying to create my simple pipeline into a reality has been a real pain. Does anyone else feel this way? Do I need to keep sticking with it? \n\nIf anyone has any repositories or things/concepts that helped it click for you let me know.", "author_fullname": "t2_88vvqsgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster Documentation Hurts my Brain", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ndwmo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681583580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thats all, thats the post. Idk why but it is just simply not intuitive. I feel like Dagster has so many great concepts, but trying to create my simple pipeline into a reality has been a real pain. Does anyone else feel this way? Do I need to keep sticking with it? &lt;/p&gt;\n\n&lt;p&gt;If anyone has any repositories or things/concepts that helped it click for you let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ndwmo", "is_robot_indexable": true, "report_reasons": null, "author": "roastmecerebrally", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ndwmo/dagster_documentation_hurts_my_brain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ndwmo/dagster_documentation_hurts_my_brain/", "subreddit_subscribers": 99372, "created_utc": 1681583580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Test, fake, abstract, and synthetic data are becoming increasingly popular these days, especially for different types of testing. That's why we've put together some useful tips for generating test data using only SQL in our latest blog post - https://www.synthesized.io/post/test-data-generation-there-and-back\n\nWe also briefly mention [TDK](https://docs.synthesized.io/tdk/latest/?utm_source=reddit&amp;utm_medium=devrel&amp;utm_campaign=datagen) data tool by [Synthesized](https://www.synthesized.io/?utm_source=reddit&amp;utm_medium=devrel&amp;utm_campaign=datagen) at the very end of the post.\n\nFeel free to provide any critiques or feedback in the comments section!", "author_fullname": "t2_blc2ww3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Test Data Generation - DIY", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12myeg9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681556059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Test, fake, abstract, and synthetic data are becoming increasingly popular these days, especially for different types of testing. That&amp;#39;s why we&amp;#39;ve put together some useful tips for generating test data using only SQL in our latest blog post - &lt;a href=\"https://www.synthesized.io/post/test-data-generation-there-and-back\"&gt;https://www.synthesized.io/post/test-data-generation-there-and-back&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We also briefly mention &lt;a href=\"https://docs.synthesized.io/tdk/latest/?utm_source=reddit&amp;amp;utm_medium=devrel&amp;amp;utm_campaign=datagen\"&gt;TDK&lt;/a&gt; data tool by &lt;a href=\"https://www.synthesized.io/?utm_source=reddit&amp;amp;utm_medium=devrel&amp;amp;utm_campaign=datagen\"&gt;Synthesized&lt;/a&gt; at the very end of the post.&lt;/p&gt;\n\n&lt;p&gt;Feel free to provide any critiques or feedback in the comments section!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?auto=webp&amp;v=enabled&amp;s=5afa9f774adce8655a42a016ec74aa8cf57d3424", "width": 1000, "height": 562}, "resolutions": [{"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d4e2ee296cba0e3e8db5a5c46da148eb5fb012d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=446b64730dc3df832b0def72b70726c4427b92e3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df437f503e6c77d5aa91d56060b1be66b83b725f", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70f6e11efa878f0d2328754fdbabc760e529802f", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b5bd900c3b10c41fd95372afa30694b9af7c58", "width": 960, "height": 539}], "variants": {}, "id": "4XhbEotXIgY7_g3Io7-69b1-eAP8t6b-UxPWBNiJols"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12myeg9", "is_robot_indexable": true, "report_reasons": null, "author": "mgramin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12myeg9/test_data_generation_diy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12myeg9/test_data_generation_diy/", "subreddit_subscribers": 99372, "created_utc": 1681556059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm loooking for advice on switching to a more modern data architecture/stack.\nI'm a senior data analyst and currently building analytics capabilities for a bank \u00een the fraud domain. \nI want to challenge the current reference arhitecture from datalake to data lakehouse.\nWe have the datalake on-premise using IBM tools like Netezza and also a analytics platform build on open source where we do a lot of data science stuff (similar with Databricks)\nHow do I convinge my architects to build a data lakehouse on our analytics platform?\nI'm working on creating some slides about the data lakehouse benefits and share with them, what should I keep in mind when discussing this?", "author_fullname": "t2_6l3fp7bm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching to data lakehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mr7or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681536224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m loooking for advice on switching to a more modern data architecture/stack.\nI&amp;#39;m a senior data analyst and currently building analytics capabilities for a bank \u00een the fraud domain. \nI want to challenge the current reference arhitecture from datalake to data lakehouse.\nWe have the datalake on-premise using IBM tools like Netezza and also a analytics platform build on open source where we do a lot of data science stuff (similar with Databricks)\nHow do I convinge my architects to build a data lakehouse on our analytics platform?\nI&amp;#39;m working on creating some slides about the data lakehouse benefits and share with them, what should I keep in mind when discussing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mr7or", "is_robot_indexable": true, "report_reasons": null, "author": "Wise_Solid1904", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mr7or/switching_to_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mr7or/switching_to_data_lakehouse/", "subreddit_subscribers": 99372, "created_utc": 1681536224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working with Teradata for nearly 2 years now.Worked on many things related to it.\nBut I have limited knowledge on performance tuning  with Teradata(aware of basics- explain plan, collecting stats etc, PPIs, secondary indexes etc.)\n\nIs there any online content where I can find more details on performance tuning of SQL queries?  Any recommendation of online blogs, video tutorial links will be appreciated.\n\nThanks.", "author_fullname": "t2_qfsr1gqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Teradata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ncqnn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681581323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working with Teradata for nearly 2 years now.Worked on many things related to it.\nBut I have limited knowledge on performance tuning  with Teradata(aware of basics- explain plan, collecting stats etc, PPIs, secondary indexes etc.)&lt;/p&gt;\n\n&lt;p&gt;Is there any online content where I can find more details on performance tuning of SQL queries?  Any recommendation of online blogs, video tutorial links will be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ncqnn", "is_robot_indexable": true, "report_reasons": null, "author": "Light7986", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ncqnn/teradata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ncqnn/teradata/", "subreddit_subscribers": 99372, "created_utc": 1681581323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI've noticed that there have been a lot of posts discussing Databricks vs Snowflake on this forum, but I'm interested in hearing about your experiences with Redshift. If you've transitioned from Redshift to Snowflake, I would love to hear your reasons for doing so.\n\nI've come across a post that suggests that when properly optimized, Redshift can outperform Snowflake. However, I'm curious to know what advantages Snowflake offers over Redshift.", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift Vs Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mw1yy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681549543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed that there have been a lot of posts discussing Databricks vs Snowflake on this forum, but I&amp;#39;m interested in hearing about your experiences with Redshift. If you&amp;#39;ve transitioned from Redshift to Snowflake, I would love to hear your reasons for doing so.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve come across a post that suggests that when properly optimized, Redshift can outperform Snowflake. However, I&amp;#39;m curious to know what advantages Snowflake offers over Redshift.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12mw1yy", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mw1yy/redshift_vs_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mw1yy/redshift_vs_snowflake/", "subreddit_subscribers": 99372, "created_utc": 1681549543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone ever done migration from cloud postgresql to bigquery?\nI am struggling with this. I would really appreciate if you can share how i can do it or any good tutorial. I can't use third party tool. I tried doing with python but no success. Also can't query from external source in bigquery.", "author_fullname": "t2_vkmvzdm7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to migrate Google cloud SQL data to bigquery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12nnpu5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681603156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever done migration from cloud postgresql to bigquery?\nI am struggling with this. I would really appreciate if you can share how i can do it or any good tutorial. I can&amp;#39;t use third party tool. I tried doing with python but no success. Also can&amp;#39;t query from external source in bigquery.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12nnpu5", "is_robot_indexable": true, "report_reasons": null, "author": "shaikh21", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12nnpu5/how_to_migrate_google_cloud_sql_data_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12nnpu5/how_to_migrate_google_cloud_sql_data_to_bigquery/", "subreddit_subscribers": 99372, "created_utc": 1681603156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working for a company with a very unique use case in that my company receives data from clients on a weekly basis that is... generally the same population over and over but with slight variations in certain fields depending.  Mostly relates to people data, so changes in PII.  Our current tech stack and architecture is... bad.  It's a mostly MS Azure stack using SSIS between stages and WAY too much manual manipulation at each stage.  So much so that our \"bronze\" stage is an excel plug in that pulls from an ob prem ms sql server...and yes we have data sets that exceed excel data limits.\n\nI'm pushing to make major changes to the flow, but wanted to get a brain check before going to our CTO. I'm still limited to our Azure stack... maybe... hopefully the design can be tech agnostic so I can eventually push to replace parts...  \n\nMy biggest problem is that we use dynamics 365 for certain parts of business and right now its our \"gold\" stage.  We've bloated the he'll out of it.  We are moving to using Synapse for analytics but everything is still funneling through CRM first.  \n\nMy biggest recommendation is going to be to push through Synapse first and build a better \"silver\" stage there to manage transformations and enrichment of data that is automated to remove the manual efforts and only feed to the CRM what it needs to do its thing and remove the bloat.  Then use a microservices approach for periphery systems through apps and feed back most results to crm as needed and also back to a proper gold stage to use for analytics and BI reporting.\n\nClearly there is a lot of detail to fill in, but... am I off base here?  Are there other recommendations  i should consider?", "author_fullname": "t2_2v1p3nx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CRM as source of truth?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n1a3j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681562861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working for a company with a very unique use case in that my company receives data from clients on a weekly basis that is... generally the same population over and over but with slight variations in certain fields depending.  Mostly relates to people data, so changes in PII.  Our current tech stack and architecture is... bad.  It&amp;#39;s a mostly MS Azure stack using SSIS between stages and WAY too much manual manipulation at each stage.  So much so that our &amp;quot;bronze&amp;quot; stage is an excel plug in that pulls from an ob prem ms sql server...and yes we have data sets that exceed excel data limits.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m pushing to make major changes to the flow, but wanted to get a brain check before going to our CTO. I&amp;#39;m still limited to our Azure stack... maybe... hopefully the design can be tech agnostic so I can eventually push to replace parts...  &lt;/p&gt;\n\n&lt;p&gt;My biggest problem is that we use dynamics 365 for certain parts of business and right now its our &amp;quot;gold&amp;quot; stage.  We&amp;#39;ve bloated the he&amp;#39;ll out of it.  We are moving to using Synapse for analytics but everything is still funneling through CRM first.  &lt;/p&gt;\n\n&lt;p&gt;My biggest recommendation is going to be to push through Synapse first and build a better &amp;quot;silver&amp;quot; stage there to manage transformations and enrichment of data that is automated to remove the manual efforts and only feed to the CRM what it needs to do its thing and remove the bloat.  Then use a microservices approach for periphery systems through apps and feed back most results to crm as needed and also back to a proper gold stage to use for analytics and BI reporting.&lt;/p&gt;\n\n&lt;p&gt;Clearly there is a lot of detail to fill in, but... am I off base here?  Are there other recommendations  i should consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12n1a3j", "is_robot_indexable": true, "report_reasons": null, "author": "No-Current-7884", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n1a3j/crm_as_source_of_truth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12n1a3j/crm_as_source_of_truth/", "subreddit_subscribers": 99372, "created_utc": 1681562861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_17hfml", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business Intelligence 101: Data within Multidimensional View - Part 2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 118, "top_awarded_type": null, "hide_score": false, "name": "t3_12mo5oe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/B7NDWZLQaKnSVHRZ5rOhBEd2SmxpB816jc5X8EEw2O8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681528836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datafriends.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datafriends.co/categories/business-intelligence/business-intelligence-101-data-within-multidimensional-view-part-2/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?auto=webp&amp;v=enabled&amp;s=301554cca1dc6a23e5d53ec021f47f630abc40ed", "width": 1264, "height": 1073}, "resolutions": [{"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abe2774539d625801cfe30db6a52d76dd7cc49f5", "width": 108, "height": 91}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fcdbe89d7c6e7e5e08cab15e9f97890f2162741", "width": 216, "height": 183}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3025c5ad8f96b061aa86f3385dc4e408a4cbaf0", "width": 320, "height": 271}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47c849a7937894bf66c9ea75fdff3ae0331bd332", "width": 640, "height": 543}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=484f2a81494fde0e4f96d9d9c04e3c210772780a", "width": 960, "height": 814}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67eb40c789eec371fa96be888ddcf4bc101a7522", "width": 1080, "height": 916}], "variants": {}, "id": "9cmMtWi_38dCpM-4sDVMXNrBJgGYfdInfQ4omvFbmwc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12mo5oe", "is_robot_indexable": true, "report_reasons": null, "author": "harlkwin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12mo5oe/business_intelligence_101_data_within/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datafriends.co/categories/business-intelligence/business-intelligence-101-data-within-multidimensional-view-part-2/", "subreddit_subscribers": 99372, "created_utc": 1681528836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all!\nI have a question regarding this weird situation I am encountering.\nI am running a python script which makes a db connection to a Vertica database.\nOnce the connection is established, the script has 1 string which contains the following queries in the order below:\n\n- Delete records from the table (Commit after this)\n- 3 insert queries into a local temp session tables (with on commit preserve rows)\n- Final insert query selecting the data from the session tables into a database table. (commit after this)\n\nWhen I run the script locally through Airflow, the script runs perfectly fine.\nOn Production, the script completes but the database table does not have any records inserted.\n\nAny idea why this is happening?", "author_fullname": "t2_ahi836bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DB Query Execution from Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12nprcv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681607678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all!\nI have a question regarding this weird situation I am encountering.\nI am running a python script which makes a db connection to a Vertica database.\nOnce the connection is established, the script has 1 string which contains the following queries in the order below:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Delete records from the table (Commit after this)&lt;/li&gt;\n&lt;li&gt;3 insert queries into a local temp session tables (with on commit preserve rows)&lt;/li&gt;\n&lt;li&gt;Final insert query selecting the data from the session tables into a database table. (commit after this)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When I run the script locally through Airflow, the script runs perfectly fine.\nOn Production, the script completes but the database table does not have any records inserted.&lt;/p&gt;\n\n&lt;p&gt;Any idea why this is happening?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12nprcv", "is_robot_indexable": true, "report_reasons": null, "author": "yyforthewin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12nprcv/db_query_execution_from_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12nprcv/db_query_execution_from_python/", "subreddit_subscribers": 99372, "created_utc": 1681607678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I am exploring some ideas in the analytics &amp; data space and wonder what are most of the people doing for analytics once the data is in the datawarehouse. Mixpanel and amplitudes of the world are not designed for datawarehouses and metabase, looker UI/UX is not as user friendly especially for the business stakeholders. Thoughts?", "author_fullname": "t2_3tiq5c7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make the most out of Data Warehouse analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12nkwld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681597254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am exploring some ideas in the analytics &amp;amp; data space and wonder what are most of the people doing for analytics once the data is in the datawarehouse. Mixpanel and amplitudes of the world are not designed for datawarehouses and metabase, looker UI/UX is not as user friendly especially for the business stakeholders. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12nkwld", "is_robot_indexable": true, "report_reasons": null, "author": "ownubie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12nkwld/how_to_make_the_most_out_of_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12nkwld/how_to_make_the_most_out_of_data_warehouse/", "subreddit_subscribers": 99372, "created_utc": 1681597254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently run Spark and Hudi on EMR.  I\u2019ve been asked to do a POC for setting up the same stack on Kubernetes.\n\nIs anyone aware of reliable Helm charts or Docker images that could be leveraged as a baseline?\n\nThis is my first foray into K8s and it feels like a lot to learn. Any tips or guidance?  We will be looking at Fargate and also EKS.", "author_fullname": "t2_fer0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience setting up Spark and Hudi on Kubernetes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12na3wg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681576224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently run Spark and Hudi on EMR.  I\u2019ve been asked to do a POC for setting up the same stack on Kubernetes.&lt;/p&gt;\n\n&lt;p&gt;Is anyone aware of reliable Helm charts or Docker images that could be leveraged as a baseline?&lt;/p&gt;\n\n&lt;p&gt;This is my first foray into K8s and it feels like a lot to learn. Any tips or guidance?  We will be looking at Fargate and also EKS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12na3wg", "is_robot_indexable": true, "report_reasons": null, "author": "TheCauthon", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12na3wg/experience_setting_up_spark_and_hudi_on_kubernetes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12na3wg/experience_setting_up_spark_and_hudi_on_kubernetes/", "subreddit_subscribers": 99372, "created_utc": 1681576224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are adopting bigquery and I have a requirement to export large tables to a single CSV. These are tables well in excess of the 1GB file limit. \n\nThere seems to be many different ways to do this, none of which are particularly good? \n\n1) Recursive compose - I found python code from a google dev blog that allows for compose in excess of 32 files but when I try it soaks up all the cpu and hangs, even on 1 file. Still looking into this. \n\n2) Pyspark - fine to do the export, but it seems like it's very inefficient to run the coalesce on the dataframe to get one partition as that happens on a single node? I am going to try running this in serverless dataproc mode to see if autoscaling helps. \n\n3) Copy the files to the local file system and run cat. Would work but seems kinda old school? \n\nDo you all have other ideas?", "author_fullname": "t2_39nrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "bigquery large tables to csv", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n9nvf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681575555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are adopting bigquery and I have a requirement to export large tables to a single CSV. These are tables well in excess of the 1GB file limit. &lt;/p&gt;\n\n&lt;p&gt;There seems to be many different ways to do this, none of which are particularly good? &lt;/p&gt;\n\n&lt;p&gt;1) Recursive compose - I found python code from a google dev blog that allows for compose in excess of 32 files but when I try it soaks up all the cpu and hangs, even on 1 file. Still looking into this. &lt;/p&gt;\n\n&lt;p&gt;2) Pyspark - fine to do the export, but it seems like it&amp;#39;s very inefficient to run the coalesce on the dataframe to get one partition as that happens on a single node? I am going to try running this in serverless dataproc mode to see if autoscaling helps. &lt;/p&gt;\n\n&lt;p&gt;3) Copy the files to the local file system and run cat. Would work but seems kinda old school? &lt;/p&gt;\n\n&lt;p&gt;Do you all have other ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12n9nvf", "is_robot_indexable": true, "report_reasons": null, "author": "arborealguy", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n9nvf/bigquery_large_tables_to_csv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12n9nvf/bigquery_large_tables_to_csv/", "subreddit_subscribers": 99372, "created_utc": 1681575555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5cgbpdbh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Diving into the Future: Serverless Data Warehouse Platform Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_12n6shn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Dpm5A_UJvVPcJoU5Ej3aqiwZa26424uqj1Iffv6efQY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681571540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databend.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databend.com/blog/2023/04/13/diving-into-the-future-serverless-data-warehouse-platform-architecture/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?auto=webp&amp;v=enabled&amp;s=91391bd25d84a560236232ee3c0ad4c936cc33a4", "width": 1876, "height": 799}, "resolutions": [{"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5e8ce9a98a80da9bf5ab68a5d868c99632b78b5", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2f2c585db55edd51e07bee86ac9d4beebfd0692", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7129633690960b716cdd465a6f0b95e07d2b988c", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4816b53b13ace18152784d12856d82c950f651b4", "width": 640, "height": 272}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d64784ed0d0911d035ce370630a05ea285dbc4d", "width": 960, "height": 408}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bda5ec9483c46ab209ebd0fd852a6c2e306e166f", "width": 1080, "height": 459}], "variants": {}, "id": "rPO5lJ3zPMxPH3wNXtvkYTcOqg6BHOpTKgvVTe_M61k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12n6shn", "is_robot_indexable": true, "report_reasons": null, "author": "PsiACE", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n6shn/diving_into_the_future_serverless_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databend.com/blog/2023/04/13/diving-into-the-future-serverless-data-warehouse-platform-architecture/", "subreddit_subscribers": 99372, "created_utc": 1681571540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I can only think of 1 benefit for federated learning scenarios where sharing a schema with other trusted clients could be beneficial, where machine learning can be carried out on heterogenous datasets that have different formats but require a ML model that solves a specific problem which everyone shares. \n\nThis involves having the data never leaving their internal systems, with a model trained locally. It would require the training set to have the same schema across all clients. \n\nAny other potential uses or benefits for this?", "author_fullname": "t2_bk9ikdyl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Benefits of sharing schemas externally to other trusted parties?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mu721", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681544187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can only think of 1 benefit for federated learning scenarios where sharing a schema with other trusted clients could be beneficial, where machine learning can be carried out on heterogenous datasets that have different formats but require a ML model that solves a specific problem which everyone shares. &lt;/p&gt;\n\n&lt;p&gt;This involves having the data never leaving their internal systems, with a model trained locally. It would require the training set to have the same schema across all clients. &lt;/p&gt;\n\n&lt;p&gt;Any other potential uses or benefits for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12mu721", "is_robot_indexable": true, "report_reasons": null, "author": "yinyanglanguage", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mu721/benefits_of_sharing_schemas_externally_to_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mu721/benefits_of_sharing_schemas_externally_to_other/", "subreddit_subscribers": 99372, "created_utc": 1681544187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm evaluating couchbase for one of my projects. I require some of the features in the enterprise version but as it's just a small personal project I don't want to buy the license.   \nI tried and it seems that they let you download the Couchbase server ee, Couchbase gateway ee and Couchbase lite ee without any license key, so can I just use it without a key forever?\n\nHow would that stop someone from using enterprise edition Couchbase for commercial purposes?", "author_fullname": "t2_99l81eq0h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Couchbase Enterprise license", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12msgxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681539559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m evaluating couchbase for one of my projects. I require some of the features in the enterprise version but as it&amp;#39;s just a small personal project I don&amp;#39;t want to buy the license.&lt;br/&gt;\nI tried and it seems that they let you download the Couchbase server ee, Couchbase gateway ee and Couchbase lite ee without any license key, so can I just use it without a key forever?&lt;/p&gt;\n\n&lt;p&gt;How would that stop someone from using enterprise edition Couchbase for commercial purposes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12msgxv", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Reading72", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12msgxv/couchbase_enterprise_license/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12msgxv/couchbase_enterprise_license/", "subreddit_subscribers": 99372, "created_utc": 1681539559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am wondering a custom Data Catalog vs  existing solution like data hub. Our data schema is not huge but high skus for an e-commerce mart. Has anyone here seen/done building custom Data Catalog and succeeded/failed?", "author_fullname": "t2_6zaja793", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Custom Data Catalog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mp3yb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681531253.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681531054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am wondering a custom Data Catalog vs  existing solution like data hub. Our data schema is not huge but high skus for an e-commerce mart. Has anyone here seen/done building custom Data Catalog and succeeded/failed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mp3yb", "is_robot_indexable": true, "report_reasons": null, "author": "Ambitious_Cucumber96", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mp3yb/custom_data_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mp3yb/custom_data_catalog/", "subreddit_subscribers": 99372, "created_utc": 1681531054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently looking for a on-prem application solutions for end user's to input data into a OLTP database. Something like what you could do with MS Access where you could create a front end. The database could be Azure SQL DB or just SQL server. Would I need to learn how to be a software developer or are there other \"low code\" solutions out there?", "author_fullname": "t2_8yface1u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Front-end for OLTP database options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12njryb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681594959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently looking for a on-prem application solutions for end user&amp;#39;s to input data into a OLTP database. Something like what you could do with MS Access where you could create a front end. The database could be Azure SQL DB or just SQL server. Would I need to learn how to be a software developer or are there other &amp;quot;low code&amp;quot; solutions out there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12njryb", "is_robot_indexable": true, "report_reasons": null, "author": "Benmagz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12njryb/frontend_for_oltp_database_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12njryb/frontend_for_oltp_database_options/", "subreddit_subscribers": 99372, "created_utc": 1681594959.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}