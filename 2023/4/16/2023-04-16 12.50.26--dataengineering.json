{"kind": "Listing", "data": {"after": null, "dist": 16, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Thats all, thats the post. Idk why but it is just simply not intuitive. I feel like Dagster has so many great concepts, but trying to create my simple pipeline into a reality has been a real pain. Does anyone else feel this way? Do I need to keep sticking with it? \n\nIf anyone has any repositories or things/concepts that helped it click for you let me know.", "author_fullname": "t2_88vvqsgc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster Documentation Hurts my Brain", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ndwmo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681583580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thats all, thats the post. Idk why but it is just simply not intuitive. I feel like Dagster has so many great concepts, but trying to create my simple pipeline into a reality has been a real pain. Does anyone else feel this way? Do I need to keep sticking with it? &lt;/p&gt;\n\n&lt;p&gt;If anyone has any repositories or things/concepts that helped it click for you let me know.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ndwmo", "is_robot_indexable": true, "report_reasons": null, "author": "roastmecerebrally", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ndwmo/dagster_documentation_hurts_my_brain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ndwmo/dagster_documentation_hurts_my_brain/", "subreddit_subscribers": 99486, "created_utc": 1681583580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nWe are extracting Salesforce data using Fivetran to Snowflake and we have problems with orphans - rows hard deleted in the app are not deleted in our tables in snowflake and will always remain there...\n\nSo correct me if I am wrong but salesforce connectors cant spot HARD DELETED records.\n\nIn other words, if records are soft deleted in the Salesforce app , API calls will see them as rows with IS\\_DELETED = true flag. However if the row is hard deleted from the SF app rest api aka integration tools wont handle that and such 'orphans' will remain in our table using delta loads.\n\nWe end up with the situation where our 'analytical views' show 'orphans'- rows of data that no longer exist in the app.\n\n**How could we set a process to make sure our data in EDW = SF App as much as possible ;)?**\n\nWe use Fivetran as extraction tool and Snowflake as EDW.\n\nMy thought is to DROP / TRUNCACE SF tables in Snowflake and use rest api to call fivetran and force historical resync(**full reload**) once a week on saturday (off work not to impact business). This way we will have weekly process to make sure that our data is 'cleaned once every week'\n\n**Pros:**\n\nData cleansed once a week - acceptable by business\n\n**Cons:**\n\nif by any chance something will fail during 'historical resync' and our ladning table in Snowflake will remain empty, this might impact our 'analytical views' where they could show 'no data'\n\n&amp;#x200B;\n\nIs there any easier, better way, just to ensure our data will always be there ?\n\nI was thinking of RAW / STAGE table / schema...but I am getting loops in my thought process.\n\n&amp;#x200B;\n\nSomething like:\n\n1.Create RAW schema - landing point, for all Salesforce objects we extract from Fivetran. Still, DROP/TRUNCATE tables every saturday and perform full load to make sure we avoid orphans.\n\n2.Have STAGE schema. In order to MERGE data from RAW into STAGE (INSERT,UPDATE,DELETE)\n\n3.And I am getting lost here.....because if RAW will get truncated every saturday and will fail to extract data out of SF for god knows why reason....then merge (DELETE command will wipe entire STAGE table because RAW is empty).\n\n&amp;#x200B;\n\nGuys any ideas?", "author_fullname": "t2_do9wxbfu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salesforce data extraction how to handle hard deletes in the app?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n0xnh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681562336.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681562124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;We are extracting Salesforce data using Fivetran to Snowflake and we have problems with orphans - rows hard deleted in the app are not deleted in our tables in snowflake and will always remain there...&lt;/p&gt;\n\n&lt;p&gt;So correct me if I am wrong but salesforce connectors cant spot HARD DELETED records.&lt;/p&gt;\n\n&lt;p&gt;In other words, if records are soft deleted in the Salesforce app , API calls will see them as rows with IS_DELETED = true flag. However if the row is hard deleted from the SF app rest api aka integration tools wont handle that and such &amp;#39;orphans&amp;#39; will remain in our table using delta loads.&lt;/p&gt;\n\n&lt;p&gt;We end up with the situation where our &amp;#39;analytical views&amp;#39; show &amp;#39;orphans&amp;#39;- rows of data that no longer exist in the app.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How could we set a process to make sure our data in EDW = SF App as much as possible ;)?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We use Fivetran as extraction tool and Snowflake as EDW.&lt;/p&gt;\n\n&lt;p&gt;My thought is to DROP / TRUNCACE SF tables in Snowflake and use rest api to call fivetran and force historical resync(&lt;strong&gt;full reload&lt;/strong&gt;) once a week on saturday (off work not to impact business). This way we will have weekly process to make sure that our data is &amp;#39;cleaned once every week&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Data cleansed once a week - acceptable by business&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;if by any chance something will fail during &amp;#39;historical resync&amp;#39; and our ladning table in Snowflake will remain empty, this might impact our &amp;#39;analytical views&amp;#39; where they could show &amp;#39;no data&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there any easier, better way, just to ensure our data will always be there ?&lt;/p&gt;\n\n&lt;p&gt;I was thinking of RAW / STAGE table / schema...but I am getting loops in my thought process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Something like:&lt;/p&gt;\n\n&lt;p&gt;1.Create RAW schema - landing point, for all Salesforce objects we extract from Fivetran. Still, DROP/TRUNCATE tables every saturday and perform full load to make sure we avoid orphans.&lt;/p&gt;\n\n&lt;p&gt;2.Have STAGE schema. In order to MERGE data from RAW into STAGE (INSERT,UPDATE,DELETE)&lt;/p&gt;\n\n&lt;p&gt;3.And I am getting lost here.....because if RAW will get truncated every saturday and will fail to extract data out of SF for god knows why reason....then merge (DELETE command will wipe entire STAGE table because RAW is empty).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Guys any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12n0xnh", "is_robot_indexable": true, "report_reasons": null, "author": "87keicam", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n0xnh/salesforce_data_extraction_how_to_handle_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12n0xnh/salesforce_data_extraction_how_to_handle_hard/", "subreddit_subscribers": 99486, "created_utc": 1681562124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "on an architectural level...", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is data lake just a theoretical construct? How does it look on a code level when we say implement in GCP?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12o0euv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681631817.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;on an architectural level...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12o0euv", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12o0euv/is_data_lake_just_a_theoretical_construct_how/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12o0euv/is_data_lake_just_a_theoretical_construct_how/", "subreddit_subscribers": 99486, "created_utc": 1681631817.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We have an s3 based data lake where the data is partitioned on org_id and date. There is a use case where in a customer can choose a time range and export the data. The following is the design I have in mind\n1. The user prompt will trigger a lambda that will take the date range and org id. \n2. The lambda will query against athena. The athena results are stored in s3. \n3. The above s3 put event will return the s3 path and the lambda will create a s3 presigned url. \n4. Send the presigned url to the user as an email. \n\n\nMy question is, will the above architecture work for data exports- esp using athena as a query engine on top of the s3 data lake?  Any idea on the number of concurrent queries athena can handle? Is there anything i need to include such as a mechanism to handle queuing of query requests?", "author_fullname": "t2_icq6ey6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Architecture Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12o15z8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681633695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We have an s3 based data lake where the data is partitioned on org_id and date. There is a use case where in a customer can choose a time range and export the data. The following is the design I have in mind\n1. The user prompt will trigger a lambda that will take the date range and org id. \n2. The lambda will query against athena. The athena results are stored in s3. \n3. The above s3 put event will return the s3 path and the lambda will create a s3 presigned url. \n4. Send the presigned url to the user as an email. &lt;/p&gt;\n\n&lt;p&gt;My question is, will the above architecture work for data exports- esp using athena as a query engine on top of the s3 data lake?  Any idea on the number of concurrent queries athena can handle? Is there anything i need to include such as a mechanism to handle queuing of query requests?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12o15z8", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Wrongdoer-939", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12o15z8/data_architecture_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12o15z8/data_architecture_question/", "subreddit_subscribers": 99486, "created_utc": 1681633695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all!\nI have a question regarding this weird situation I am encountering.\nI am running a python script which makes a db connection to a Vertica database.\nOnce the connection is established, the script has 1 string which contains the following queries in the order below:\n\n- Delete records from the table (Commit after this)\n- 3 insert queries into a local temp session tables (with on commit preserve rows)\n- Final insert query selecting the data from the session tables into a database table. (commit after this)\n\nWhen I run the script locally through Airflow, the script runs perfectly fine.\nOn Production, the script completes but the database table does not have any records inserted.\n\nAny idea why this is happening?", "author_fullname": "t2_ahi836bi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DB Query Execution from Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12nprcv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681607678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all!\nI have a question regarding this weird situation I am encountering.\nI am running a python script which makes a db connection to a Vertica database.\nOnce the connection is established, the script has 1 string which contains the following queries in the order below:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Delete records from the table (Commit after this)&lt;/li&gt;\n&lt;li&gt;3 insert queries into a local temp session tables (with on commit preserve rows)&lt;/li&gt;\n&lt;li&gt;Final insert query selecting the data from the session tables into a database table. (commit after this)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When I run the script locally through Airflow, the script runs perfectly fine.\nOn Production, the script completes but the database table does not have any records inserted.&lt;/p&gt;\n\n&lt;p&gt;Any idea why this is happening?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12nprcv", "is_robot_indexable": true, "report_reasons": null, "author": "yyforthewin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12nprcv/db_query_execution_from_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12nprcv/db_query_execution_from_python/", "subreddit_subscribers": 99486, "created_utc": 1681607678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone ever done migration from cloud postgresql to bigquery?\nI am struggling with this. I would really appreciate if you can share how i can do it or any good tutorial. I can't use third party tool. I tried doing with python but no success. Also can't query from external source in bigquery.", "author_fullname": "t2_vkmvzdm7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to migrate Google cloud SQL data to bigquery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12nnpu5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681603156.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever done migration from cloud postgresql to bigquery?\nI am struggling with this. I would really appreciate if you can share how i can do it or any good tutorial. I can&amp;#39;t use third party tool. I tried doing with python but no success. Also can&amp;#39;t query from external source in bigquery.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12nnpu5", "is_robot_indexable": true, "report_reasons": null, "author": "shaikh21", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12nnpu5/how_to_migrate_google_cloud_sql_data_to_bigquery/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12nnpu5/how_to_migrate_google_cloud_sql_data_to_bigquery/", "subreddit_subscribers": 99486, "created_utc": 1681603156.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working with Teradata for nearly 2 years now.Worked on many things related to it.\nBut I have limited knowledge on performance tuning  with Teradata(aware of basics- explain plan, collecting stats etc, PPIs, secondary indexes etc.)\n\nIs there any online content where I can find more details on performance tuning of SQL queries?  Any recommendation of online blogs, video tutorial links will be appreciated.\n\nThanks.", "author_fullname": "t2_qfsr1gqi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Teradata", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ncqnn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681581323.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working with Teradata for nearly 2 years now.Worked on many things related to it.\nBut I have limited knowledge on performance tuning  with Teradata(aware of basics- explain plan, collecting stats etc, PPIs, secondary indexes etc.)&lt;/p&gt;\n\n&lt;p&gt;Is there any online content where I can find more details on performance tuning of SQL queries?  Any recommendation of online blogs, video tutorial links will be appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ncqnn", "is_robot_indexable": true, "report_reasons": null, "author": "Light7986", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ncqnn/teradata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ncqnn/teradata/", "subreddit_subscribers": 99486, "created_utc": 1681581323.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Everyone,I am trying to run my airflow's cpu/memory task on Fargate (so we only pay for what we use). Currently we are running self-managed airflow in AWS EKS cluster and all my Dags use k8s operator to run POD per task and we don't have auto-scaling yet. I have explored ECS operator but that will require creating Task per image and it will be too much(given we have lot's of different images.\n\nI am wondering if we can do it some other way in which i don't have to modify my existing Dags too much", "author_fullname": "t2_tvjfoaqq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running Airflow task intensive Dags on Fargate.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12o1luy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681635791.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681634799.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Everyone,I am trying to run my airflow&amp;#39;s cpu/memory task on Fargate (so we only pay for what we use). Currently we are running self-managed airflow in AWS EKS cluster and all my Dags use k8s operator to run POD per task and we don&amp;#39;t have auto-scaling yet. I have explored ECS operator but that will require creating Task per image and it will be too much(given we have lot&amp;#39;s of different images.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if we can do it some other way in which i don&amp;#39;t have to modify my existing Dags too much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12o1luy", "is_robot_indexable": true, "report_reasons": null, "author": "msf_venom", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12o1luy/running_airflow_task_intensive_dags_on_fargate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12o1luy/running_airflow_task_intensive_dags_on_fargate/", "subreddit_subscribers": 99486, "created_utc": 1681634799.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working for a company with a very unique use case in that my company receives data from clients on a weekly basis that is... generally the same population over and over but with slight variations in certain fields depending.  Mostly relates to people data, so changes in PII.  Our current tech stack and architecture is... bad.  It's a mostly MS Azure stack using SSIS between stages and WAY too much manual manipulation at each stage.  So much so that our \"bronze\" stage is an excel plug in that pulls from an ob prem ms sql server...and yes we have data sets that exceed excel data limits.\n\nI'm pushing to make major changes to the flow, but wanted to get a brain check before going to our CTO. I'm still limited to our Azure stack... maybe... hopefully the design can be tech agnostic so I can eventually push to replace parts...  \n\nMy biggest problem is that we use dynamics 365 for certain parts of business and right now its our \"gold\" stage.  We've bloated the he'll out of it.  We are moving to using Synapse for analytics but everything is still funneling through CRM first.  \n\nMy biggest recommendation is going to be to push through Synapse first and build a better \"silver\" stage there to manage transformations and enrichment of data that is automated to remove the manual efforts and only feed to the CRM what it needs to do its thing and remove the bloat.  Then use a microservices approach for periphery systems through apps and feed back most results to crm as needed and also back to a proper gold stage to use for analytics and BI reporting.\n\nClearly there is a lot of detail to fill in, but... am I off base here?  Are there other recommendations  i should consider?", "author_fullname": "t2_2v1p3nx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CRM as source of truth?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n1a3j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681562861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working for a company with a very unique use case in that my company receives data from clients on a weekly basis that is... generally the same population over and over but with slight variations in certain fields depending.  Mostly relates to people data, so changes in PII.  Our current tech stack and architecture is... bad.  It&amp;#39;s a mostly MS Azure stack using SSIS between stages and WAY too much manual manipulation at each stage.  So much so that our &amp;quot;bronze&amp;quot; stage is an excel plug in that pulls from an ob prem ms sql server...and yes we have data sets that exceed excel data limits.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m pushing to make major changes to the flow, but wanted to get a brain check before going to our CTO. I&amp;#39;m still limited to our Azure stack... maybe... hopefully the design can be tech agnostic so I can eventually push to replace parts...  &lt;/p&gt;\n\n&lt;p&gt;My biggest problem is that we use dynamics 365 for certain parts of business and right now its our &amp;quot;gold&amp;quot; stage.  We&amp;#39;ve bloated the he&amp;#39;ll out of it.  We are moving to using Synapse for analytics but everything is still funneling through CRM first.  &lt;/p&gt;\n\n&lt;p&gt;My biggest recommendation is going to be to push through Synapse first and build a better &amp;quot;silver&amp;quot; stage there to manage transformations and enrichment of data that is automated to remove the manual efforts and only feed to the CRM what it needs to do its thing and remove the bloat.  Then use a microservices approach for periphery systems through apps and feed back most results to crm as needed and also back to a proper gold stage to use for analytics and BI reporting.&lt;/p&gt;\n\n&lt;p&gt;Clearly there is a lot of detail to fill in, but... am I off base here?  Are there other recommendations  i should consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12n1a3j", "is_robot_indexable": true, "report_reasons": null, "author": "No-Current-7884", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n1a3j/crm_as_source_of_truth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12n1a3j/crm_as_source_of_truth/", "subreddit_subscribers": 99486, "created_utc": 1681562861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "First of all - I want to apologise. I know a large percentage of the posts here are from beginners looking into the field and I know it's annoying. But I think this one is at least a little different from the usual salary expectations or \"how to learn DE\" thread.\n\nI am currently a PhD student in molecular biology, but I have been thinking of mastering out and transitioning to DE/DA/DS. I naturally looked at DE as it seems to be the most coveted/high-paying of the three, but some of the concepts were fuzzy to me. I started reading \"Designing Data-Intensive Applications\" and it has helped a lot, a very well written book. One of the things it makes clear is that the need for data warehousing arises particularly in the context of large businesses where you want to separate the OLTP infrastructure from the OLAP infrastructure, so that analysts aren't making expensive queries on business critical operational databases. \n\nComing from a scientific background where data are essentially *entirely* analytical, this is a bit of an odd distinction to think about. It seems like data warehousing has no real analog in science. I wondered if there is something I am missing about data warehousing as a concept that goes beyond just a \"separate database for analytical queries\", considering it seems to be at the core of DE (which is again seemingly the most coveted of the Dx professions)? \n\nI can certainly see that structuring data in logical schemas and knowing tools to bring data in (and organise it) should be useful in science or anywhere. But having said that, I am now wondering how much of this field specifically is built around a workflow that applies in large organisations with huge customer/transaction databases and their peculiar needs. So, in other words, what do people here think -- are there DE workflows with academic science or other \"pure analytics\" domains in mind, and do warehousing concepts in particular have relevance there?", "author_fullname": "t2_42aau", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non-traditional uses for data warehousing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12o3gz8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681639398.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First of all - I want to apologise. I know a large percentage of the posts here are from beginners looking into the field and I know it&amp;#39;s annoying. But I think this one is at least a little different from the usual salary expectations or &amp;quot;how to learn DE&amp;quot; thread.&lt;/p&gt;\n\n&lt;p&gt;I am currently a PhD student in molecular biology, but I have been thinking of mastering out and transitioning to DE/DA/DS. I naturally looked at DE as it seems to be the most coveted/high-paying of the three, but some of the concepts were fuzzy to me. I started reading &amp;quot;Designing Data-Intensive Applications&amp;quot; and it has helped a lot, a very well written book. One of the things it makes clear is that the need for data warehousing arises particularly in the context of large businesses where you want to separate the OLTP infrastructure from the OLAP infrastructure, so that analysts aren&amp;#39;t making expensive queries on business critical operational databases. &lt;/p&gt;\n\n&lt;p&gt;Coming from a scientific background where data are essentially &lt;em&gt;entirely&lt;/em&gt; analytical, this is a bit of an odd distinction to think about. It seems like data warehousing has no real analog in science. I wondered if there is something I am missing about data warehousing as a concept that goes beyond just a &amp;quot;separate database for analytical queries&amp;quot;, considering it seems to be at the core of DE (which is again seemingly the most coveted of the Dx professions)? &lt;/p&gt;\n\n&lt;p&gt;I can certainly see that structuring data in logical schemas and knowing tools to bring data in (and organise it) should be useful in science or anywhere. But having said that, I am now wondering how much of this field specifically is built around a workflow that applies in large organisations with huge customer/transaction databases and their peculiar needs. So, in other words, what do people here think -- are there DE workflows with academic science or other &amp;quot;pure analytics&amp;quot; domains in mind, and do warehousing concepts in particular have relevance there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12o3gz8", "is_robot_indexable": true, "report_reasons": null, "author": "omgpop", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12o3gz8/nontraditional_uses_for_data_warehousing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12o3gz8/nontraditional_uses_for_data_warehousing/", "subreddit_subscribers": 99486, "created_utc": 1681639398.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "could some one elaborate the cases where its preferred to chose one over the another?", "author_fullname": "t2_7owm6ym1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "why do we load it to GCS and then to bigquery and not directly to bigquery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12o0dar", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681631713.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;could some one elaborate the cases where its preferred to chose one over the another?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12o0dar", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Tradition-3450", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12o0dar/why_do_we_load_it_to_gcs_and_then_to_bigquery_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12o0dar/why_do_we_load_it_to_gcs_and_then_to_bigquery_and/", "subreddit_subscribers": 99486, "created_utc": 1681631713.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey, I am exploring some ideas in the analytics &amp; data space and wonder what are most of the people doing for analytics once the data is in the datawarehouse. Mixpanel and amplitudes of the world are not designed for datawarehouses and metabase, looker UI/UX is not as user friendly especially for the business stakeholders. Thoughts?", "author_fullname": "t2_3tiq5c7i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to make the most out of Data Warehouse analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12nkwld", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681597254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am exploring some ideas in the analytics &amp;amp; data space and wonder what are most of the people doing for analytics once the data is in the datawarehouse. Mixpanel and amplitudes of the world are not designed for datawarehouses and metabase, looker UI/UX is not as user friendly especially for the business stakeholders. Thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12nkwld", "is_robot_indexable": true, "report_reasons": null, "author": "ownubie", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12nkwld/how_to_make_the_most_out_of_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12nkwld/how_to_make_the_most_out_of_data_warehouse/", "subreddit_subscribers": 99486, "created_utc": 1681597254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We currently run Spark and Hudi on EMR.  I\u2019ve been asked to do a POC for setting up the same stack on Kubernetes.\n\nIs anyone aware of reliable Helm charts or Docker images that could be leveraged as a baseline?\n\nThis is my first foray into K8s and it feels like a lot to learn. Any tips or guidance?  We will be looking at Fargate and also EKS.", "author_fullname": "t2_fer0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Experience setting up Spark and Hudi on Kubernetes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12na3wg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681576224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We currently run Spark and Hudi on EMR.  I\u2019ve been asked to do a POC for setting up the same stack on Kubernetes.&lt;/p&gt;\n\n&lt;p&gt;Is anyone aware of reliable Helm charts or Docker images that could be leveraged as a baseline?&lt;/p&gt;\n\n&lt;p&gt;This is my first foray into K8s and it feels like a lot to learn. Any tips or guidance?  We will be looking at Fargate and also EKS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12na3wg", "is_robot_indexable": true, "report_reasons": null, "author": "TheCauthon", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12na3wg/experience_setting_up_spark_and_hudi_on_kubernetes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12na3wg/experience_setting_up_spark_and_hudi_on_kubernetes/", "subreddit_subscribers": 99486, "created_utc": 1681576224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are adopting bigquery and I have a requirement to export large tables to a single CSV. These are tables well in excess of the 1GB file limit. \n\nThere seems to be many different ways to do this, none of which are particularly good? \n\n1) Recursive compose - I found python code from a google dev blog that allows for compose in excess of 32 files but when I try it soaks up all the cpu and hangs, even on 1 file. Still looking into this. \n\n2) Pyspark - fine to do the export, but it seems like it's very inefficient to run the coalesce on the dataframe to get one partition as that happens on a single node? I am going to try running this in serverless dataproc mode to see if autoscaling helps. \n\n3) Copy the files to the local file system and run cat. Would work but seems kinda old school? \n\nDo you all have other ideas?", "author_fullname": "t2_39nrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "bigquery large tables to csv", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n9nvf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681575555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are adopting bigquery and I have a requirement to export large tables to a single CSV. These are tables well in excess of the 1GB file limit. &lt;/p&gt;\n\n&lt;p&gt;There seems to be many different ways to do this, none of which are particularly good? &lt;/p&gt;\n\n&lt;p&gt;1) Recursive compose - I found python code from a google dev blog that allows for compose in excess of 32 files but when I try it soaks up all the cpu and hangs, even on 1 file. Still looking into this. &lt;/p&gt;\n\n&lt;p&gt;2) Pyspark - fine to do the export, but it seems like it&amp;#39;s very inefficient to run the coalesce on the dataframe to get one partition as that happens on a single node? I am going to try running this in serverless dataproc mode to see if autoscaling helps. &lt;/p&gt;\n\n&lt;p&gt;3) Copy the files to the local file system and run cat. Would work but seems kinda old school? &lt;/p&gt;\n\n&lt;p&gt;Do you all have other ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12n9nvf", "is_robot_indexable": true, "report_reasons": null, "author": "arborealguy", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n9nvf/bigquery_large_tables_to_csv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12n9nvf/bigquery_large_tables_to_csv/", "subreddit_subscribers": 99486, "created_utc": 1681575555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5cgbpdbh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Diving into the Future: Serverless Data Warehouse Platform Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": false, "name": "t3_12n6shn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Dpm5A_UJvVPcJoU5Ej3aqiwZa26424uqj1Iffv6efQY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681571540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databend.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databend.com/blog/2023/04/13/diving-into-the-future-serverless-data-warehouse-platform-architecture/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?auto=webp&amp;v=enabled&amp;s=91391bd25d84a560236232ee3c0ad4c936cc33a4", "width": 1876, "height": 799}, "resolutions": [{"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5e8ce9a98a80da9bf5ab68a5d868c99632b78b5", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2f2c585db55edd51e07bee86ac9d4beebfd0692", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7129633690960b716cdd465a6f0b95e07d2b988c", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4816b53b13ace18152784d12856d82c950f651b4", "width": 640, "height": 272}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d64784ed0d0911d035ce370630a05ea285dbc4d", "width": 960, "height": 408}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bda5ec9483c46ab209ebd0fd852a6c2e306e166f", "width": 1080, "height": 459}], "variants": {}, "id": "rPO5lJ3zPMxPH3wNXtvkYTcOqg6BHOpTKgvVTe_M61k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12n6shn", "is_robot_indexable": true, "report_reasons": null, "author": "PsiACE", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n6shn/diving_into_the_future_serverless_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databend.com/blog/2023/04/13/diving-into-the-future-serverless-data-warehouse-platform-architecture/", "subreddit_subscribers": 99486, "created_utc": 1681571540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently looking for a on-prem application solutions for end user's to input data into a OLTP database. Something like what you could do with MS Access where you could create a front end. The database could be Azure SQL DB or just SQL server. Would I need to learn how to be a software developer or are there other \"low code\" solutions out there?", "author_fullname": "t2_8yface1u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Front-end for OLTP database options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12njryb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681594959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently looking for a on-prem application solutions for end user&amp;#39;s to input data into a OLTP database. Something like what you could do with MS Access where you could create a front end. The database could be Azure SQL DB or just SQL server. Would I need to learn how to be a software developer or are there other &amp;quot;low code&amp;quot; solutions out there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12njryb", "is_robot_indexable": true, "report_reasons": null, "author": "Benmagz", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12njryb/frontend_for_oltp_database_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12njryb/frontend_for_oltp_database_options/", "subreddit_subscribers": 99486, "created_utc": 1681594959.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}