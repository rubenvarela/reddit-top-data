{"kind": "Listing", "data": {"after": "t3_13229fy", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1nj2uu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The community group Forest of Illusion, dedicated to preserving Nintendo's history, has announced the end of their work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "name": "t3_131omwm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 573, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 573, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1xLkdU9LjrOdJ0JVcivUCmg3gFoiQYIkLR4ZnJQp0Z4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682680301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "archive.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://archive.org/details/readme_20230428", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YiQRc1Kh_LhZKnW0P8o13a-1VrZudI26GKsmOhr6Glw.jpg?auto=webp&amp;v=enabled&amp;s=6a11bfe99205212e71e4602d351ddac52a48141f", "width": 160, "height": 110}, "resolutions": [{"url": "https://external-preview.redd.it/YiQRc1Kh_LhZKnW0P8o13a-1VrZudI26GKsmOhr6Glw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b80e296b21f03abd06e1d1dd0f93880a18650230", "width": 108, "height": 74}], "variants": {}, "id": "te8dS_8tDwV7a4I5wDwIv-RCBsf1rpL-BYZPIq0-t7k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131omwm", "is_robot_indexable": true, "report_reasons": null, "author": "bmacabeus", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131omwm/the_community_group_forest_of_illusion_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://archive.org/details/readme_20230428", "subreddit_subscribers": 680152, "created_utc": 1682680301.0, "num_crossposts": 2, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_qwl19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Always 1 quarter away", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_1325e06", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 449, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 449, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WPAdM3H_QFlb6OybLJBRd9brhtqaORU6YJL5t3R56Hk.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682705666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/iq3d1ag62owa1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?auto=webp&amp;v=enabled&amp;s=e41a777c91e4a811c9730b18c16d42e67237845f", "width": 679, "height": 367}, "resolutions": [{"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca9c461fb7b7b326eb3f74c73d7183c013a238e2", "width": 108, "height": 58}, {"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=42faf4d41eb14cc851d1ccda0214a6f8f484f53e", "width": 216, "height": 116}, {"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e6792352e7612eb9762debfb8578a2d893cf195", "width": 320, "height": 172}, {"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b86691a85e11a11f703e1209fd25d75bccd0d849", "width": 640, "height": 345}], "variants": {}, "id": "LA3HM47MTukpVPigpb3QnEmUY5k8DuEtAayBhsEcsHc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "18TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1325e06", "is_robot_indexable": true, "report_reasons": null, "author": "ZGorlock", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1325e06/always_1_quarter_away/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/iq3d1ag62owa1.jpg", "subreddit_subscribers": 680152, "created_utc": 1682705666.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone saved this master piece?", "author_fullname": "t2_51p8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Farscape 4K Remaster AI was recently uploaded to youtube. DMCA shut it down.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131f4mp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 193, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 193, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682649054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone saved this master piece?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "131f4mp", "is_robot_indexable": true, "report_reasons": null, "author": "fmjk45a", "discussion_type": null, "num_comments": 143, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131f4mp/farscape_4k_remaster_ai_was_recently_uploaded_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131f4mp/farscape_4k_remaster_ai_was_recently_uploaded_to/", "subreddit_subscribers": 680152, "created_utc": 1682649054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/jp9ci1ommowa1.jpg?width=628&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7fc95f04bba701916da3faedeb9a1b9316883b78", "author_fullname": "t2_76pgn19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My hoarder subtype is 'Purist'.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 111, "top_awarded_type": null, "hide_score": false, "media_metadata": {"jp9ci1ommowa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 85, "x": 108, "u": "https://preview.redd.it/jp9ci1ommowa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36eb008968880f44e9801b269b404aa453079bcf"}, {"y": 171, "x": 216, "u": "https://preview.redd.it/jp9ci1ommowa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05f431d4287239dcff8eb9c01c4786fa199e063b"}, {"y": 254, "x": 320, "u": "https://preview.redd.it/jp9ci1ommowa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b5964b1b74d880809f4522562ead4630e7fd31d"}], "s": {"y": 500, "x": 628, "u": "https://preview.redd.it/jp9ci1ommowa1.jpg?width=628&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7fc95f04bba701916da3faedeb9a1b9316883b78"}, "id": "jp9ci1ommowa1"}}, "name": "t3_1328ca3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 28, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 28, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jJFC6ARZ8jfdkXST19GFI3q1rO4zpIg35EC6A8JW4TY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682712518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jp9ci1ommowa1.jpg?width=628&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7fc95f04bba701916da3faedeb9a1b9316883b78\"&gt;https://preview.redd.it/jp9ci1ommowa1.jpg?width=628&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7fc95f04bba701916da3faedeb9a1b9316883b78&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1328ca3", "is_robot_indexable": true, "report_reasons": null, "author": "AshleyUncia", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1328ca3/my_hoarder_subtype_is_purist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1328ca3/my_hoarder_subtype_is_purist/", "subreddit_subscribers": 680152, "created_utc": 1682712518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since the first release (in December 2021), SCrawler has been expanding and improving. I have implemented many of the user requests. I want to say thank you to everyone who uses my program, who likes it and who finds it useful. I really appreciate your kind words when you DM me. It makes my day :-)\n\nThe new release contains new sites: **YouTube, YouTube music, Mastodon, Pinterest** and **ThisVid**. It also contains many improvements and bug fixes.\n\n[https://github.com/AAndyProgram/SCrawler](https://github.com/AAndyProgram/SCrawler)\n\nThe program is completely free. I hope you will like it ;-)", "author_fullname": "t2_bfhlgnl4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SCrawler. Reddit, Twitter, Instagram, YouTube and any other sites downloader. New grand update.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131m8h3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682672597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since the first release (in December 2021), SCrawler has been expanding and improving. I have implemented many of the user requests. I want to say thank you to everyone who uses my program, who likes it and who finds it useful. I really appreciate your kind words when you DM me. It makes my day :-)&lt;/p&gt;\n\n&lt;p&gt;The new release contains new sites: &lt;strong&gt;YouTube, YouTube music, Mastodon, Pinterest&lt;/strong&gt; and &lt;strong&gt;ThisVid&lt;/strong&gt;. It also contains many improvements and bug fixes.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/AAndyProgram/SCrawler\"&gt;https://github.com/AAndyProgram/SCrawler&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The program is completely free. I hope you will like it ;-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?auto=webp&amp;v=enabled&amp;s=7762203f5e277671722427ba8b8059db62a80ca6", "width": 1283, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd65c9e04800e8b1b9def6d61ba13455cf7836a", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aecc109537c4353bb2aa96edfcd99aabea23614b", "width": 216, "height": 107}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2de3fc7a72ad3e6c9e70b9aa1ad88d5fdd02a213", "width": 320, "height": 159}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92ebb7d1e8d6b78d61545c05ce2cd18cca004443", "width": 640, "height": 319}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55c3b20e58f54a3a864249853303c1c5f62bd4e5", "width": 960, "height": 478}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4da6f9f905c18158e2ccbf373eda80877ad17760", "width": 1080, "height": 538}], "variants": {}, "id": "_pHtIQKoFHtVPlwEO3-8R5Db1nV-A5okGhAwJKRuSSk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "32TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131m8h3", "is_robot_indexable": true, "report_reasons": null, "author": "AndyGay06", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/131m8h3/scrawler_reddit_twitter_instagram_youtube_and_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131m8h3/scrawler_reddit_twitter_instagram_youtube_and_any/", "subreddit_subscribers": 680152, "created_utc": 1682672597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Thanks to the help of a fellow anonymous Redditor I've released a new version of [RedditScrape](https://github.com/NSFWUTILS/RedditScrape). This new version now uses the push shift API to gather gigantic levels of data for you to download. This means we no longer need to provide any form of reddit credentials. \n\nWhile the previous version was hard capped at 1,000 posts using the Reddit API, this new version has no limits at all, other than what resources and disk space you have. \n\nFor example, if you're brave enough to try and scrape something like *gonewild*, you'll find it takes DAYS just to get all of the data back from push shift. The JSON text alone is over 9 gigs (3.3 million posts) and climbing. \n\nRunning this is now a two step process, but results in a substantially larger set of media from your favorite subs. \n\nInstructions can be found [here](https://github.com/NSFWUTILS/RedditScrape#setup-the-script). I hope I've fixed a few of the problems that people had with the first iteration along the way.", "author_fullname": "t2_9ptn7jbjf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improved version of RedditScrape for backing up your favorite subreddits (Now with Push Shift)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1328pmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682713371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thanks to the help of a fellow anonymous Redditor I&amp;#39;ve released a new version of &lt;a href=\"https://github.com/NSFWUTILS/RedditScrape\"&gt;RedditScrape&lt;/a&gt;. This new version now uses the push shift API to gather gigantic levels of data for you to download. This means we no longer need to provide any form of reddit credentials. &lt;/p&gt;\n\n&lt;p&gt;While the previous version was hard capped at 1,000 posts using the Reddit API, this new version has no limits at all, other than what resources and disk space you have. &lt;/p&gt;\n\n&lt;p&gt;For example, if you&amp;#39;re brave enough to try and scrape something like &lt;em&gt;gonewild&lt;/em&gt;, you&amp;#39;ll find it takes DAYS just to get all of the data back from push shift. The JSON text alone is over 9 gigs (3.3 million posts) and climbing. &lt;/p&gt;\n\n&lt;p&gt;Running this is now a two step process, but results in a substantially larger set of media from your favorite subs. &lt;/p&gt;\n\n&lt;p&gt;Instructions can be found &lt;a href=\"https://github.com/NSFWUTILS/RedditScrape#setup-the-script\"&gt;here&lt;/a&gt;. I hope I&amp;#39;ve fixed a few of the problems that people had with the first iteration along the way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?auto=webp&amp;v=enabled&amp;s=a9d046c146e14504b168c70d1517d2f199b346da", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2782224869661f9692c787f848cf59658a1a623", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d713d0d33677e933d08cf907a437af1418c21fb6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8821ffcd392ee268954af03e37077434fa4fba81", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec6c7fa7fd452582dde828a06b7fd94cc151ce36", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e168f78bf0b53ce0898d0b6dece21febaf9db97", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8fb80ce0b9ae3a830f64719e823dd2600cfda42", "width": 1080, "height": 540}], "variants": {}, "id": "voDkpEv7h5mZTSSyu9yag2yl_npE2zRAsJkQ6NkYOz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1328pmj", "is_robot_indexable": true, "report_reasons": null, "author": "nsfwutils", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1328pmj/improved_version_of_redditscrape_for_backing_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1328pmj/improved_version_of_redditscrape_for_backing_up/", "subreddit_subscribers": 680152, "created_utc": 1682713371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not the P3 Plus that's for sure. I have a Hikvision MD202 that at USB 3.2 10gpbs speed doesn't work very well. Fir the first time ever, for a second, it managed to hit 800+mbps 9n as ssd benchmark. Then it slowed to a crawl before the drive disconnected itself. I was told that a dram ssd is essential for portable nvme ssd, but I wanted to double check here that it would fix the problem.", "author_fullname": "t2_gti8t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which 4tb nvme ssd for enclosure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132cp9x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682723335.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not the P3 Plus that&amp;#39;s for sure. I have a Hikvision MD202 that at USB 3.2 10gpbs speed doesn&amp;#39;t work very well. Fir the first time ever, for a second, it managed to hit 800+mbps 9n as ssd benchmark. Then it slowed to a crawl before the drive disconnected itself. I was told that a dram ssd is essential for portable nvme ssd, but I wanted to double check here that it would fix the problem.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132cp9x", "is_robot_indexable": true, "report_reasons": null, "author": "wgolding", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/132cp9x/which_4tb_nvme_ssd_for_enclosure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132cp9x/which_4tb_nvme_ssd_for_enclosure/", "subreddit_subscribers": 680152, "created_utc": 1682723335.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all. Figured this would be the place to ask for this. See if anyone else has a clue what this program was.\n\nSo, there was this contact sheet/video thumbnail program I was using for a bit, then I reinstalled my OS, and was going about installing my usual programs, and realized I wanted to add that one again, but had no idea what it was called. So here's it's description:\n\nI believe it might've been a Windows program, and I used it through wine on Linux. It had a GUI, and it allowed drag and drop with video files, plus batch loaded files. On-top of that, the second you added files into it, it would begin the process of creating the contact sheets (typically creates them within the same folder). The location of the files you uploaded is on the left, with the settings and progress on the right.\n\nBest part is, the output (contact sheet afterwards) didn't have a little watermark or any such logo or reference to the program, unlike something like Video Thumbnail Maker.\n\nThis is a shot in the dark, but I've tried searching for this for a few days now, trying everything, and no single program I've come across is it. Trying my hand at all the experts in this sub to see if anyone came across this or knows of it.\n\nThank you!", "author_fullname": "t2_yirnb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to remember what THIS software is called, and what it's name is", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131ijzu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682659490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. Figured this would be the place to ask for this. See if anyone else has a clue what this program was.&lt;/p&gt;\n\n&lt;p&gt;So, there was this contact sheet/video thumbnail program I was using for a bit, then I reinstalled my OS, and was going about installing my usual programs, and realized I wanted to add that one again, but had no idea what it was called. So here&amp;#39;s it&amp;#39;s description:&lt;/p&gt;\n\n&lt;p&gt;I believe it might&amp;#39;ve been a Windows program, and I used it through wine on Linux. It had a GUI, and it allowed drag and drop with video files, plus batch loaded files. On-top of that, the second you added files into it, it would begin the process of creating the contact sheets (typically creates them within the same folder). The location of the files you uploaded is on the left, with the settings and progress on the right.&lt;/p&gt;\n\n&lt;p&gt;Best part is, the output (contact sheet afterwards) didn&amp;#39;t have a little watermark or any such logo or reference to the program, unlike something like Video Thumbnail Maker.&lt;/p&gt;\n\n&lt;p&gt;This is a shot in the dark, but I&amp;#39;ve tried searching for this for a few days now, trying everything, and no single program I&amp;#39;ve come across is it. Trying my hand at all the experts in this sub to see if anyone came across this or knows of it.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "VHS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131ijzu", "is_robot_indexable": true, "report_reasons": null, "author": "prodigalkal7", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/131ijzu/trying_to_remember_what_this_software_is_called/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131ijzu/trying_to_remember_what_this_software_is_called/", "subreddit_subscribers": 680152, "created_utc": 1682659490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I think I fucked up big time and have to ask for some help. I'm trying to recover data from failed Windows Storage Spaces mirrored setup, which from my research at the time seemed enough against single drive failure (Probably mistake #1). \n\n(Way back in the day one of the ubuntu server updates broke my mdadm setup, which was pain to recover, so I decided to try Windows route)\n\nI have a mirrored Windows Storage Spaces pool with 2 physical drives. One of the drives failed. Raid was showing `\"Error\"` state, one of the drives was showind `\"OK\"` state, and another drive `\"Warning\"` state. The logical raid volume was no longer showing, neither in explorer nor disk manager. \n\nI bought a replacement. Tried to detach the failed drive after marking it as retired, but was prompted I need to attach a healthy replacement first. I attached the new drive, it immediately started `\"optimising\"` but was stuck at 0% with no disk activity for a few hours. I tried to stop optimisation, and now was in `\"stopping optimisation\"` state. \n\nI scrapped the idea of using Storage Spaces in the future, found it should be possible to pull out data from just one of the drives, and because the new drive is the only big one to contain the data in question, detached the new empty drive, marked it as retired, physically removed and wiped it clean (probably mistake #2. I did this because the good drive in the raid was still showing as `\"healthy\"`. I installed UFS Explorer RAID Recovery, but it only finds `\"MS Reserved partition\"` and `\"Ext2/3/4 partition\"` in `\"Invalid Root Folder\"` state, failing to find any data on it. GParted under linux shows the same except secondd partition as `\"Storage pool\"` with no option to mount.\n\nWhat I have now is \n\n- Two-way mirror Storage Spaces raid in `\"error\"` state (\"check physical drives section\")\n\n   - One old drive in `\"OK\"` state, with all the data as I understand it.\n\n   - One old drive in `\"Warning / Preparing for removal\"` state, S.M.A.R.T. showing a few reallocated sectors. I marked it as `\"Retired\"` previously via powershell cmdlet.\n\n   - One new drive in `\"Warning / Retired; add a drive then remove this drive\"` state, also marked as retired by me. The drive is wiped clean after being physically removed.\n\n- `Get-StorageJob` shows  `Storage pool-Rebalance` in `Shutting Down` state.\n\nPlease help. I tried many powershell commands, mostly attempting to force remove all non-OK drives from the raid, planning to reattach a healthy one. Now I just want to rescue the data to a good new drive and go ahead from there.\n\nEdit: Also posted to https://www.reddit.com/r/techsupport/comments/132eo7h/storage_spaces_help_greatly_appreciated/?", "author_fullname": "t2_9yl5j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Spaces nightmare. I'm desparate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132egn5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682728895.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682728138.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think I fucked up big time and have to ask for some help. I&amp;#39;m trying to recover data from failed Windows Storage Spaces mirrored setup, which from my research at the time seemed enough against single drive failure (Probably mistake #1). &lt;/p&gt;\n\n&lt;p&gt;(Way back in the day one of the ubuntu server updates broke my mdadm setup, which was pain to recover, so I decided to try Windows route)&lt;/p&gt;\n\n&lt;p&gt;I have a mirrored Windows Storage Spaces pool with 2 physical drives. One of the drives failed. Raid was showing &lt;code&gt;&amp;quot;Error&amp;quot;&lt;/code&gt; state, one of the drives was showind &lt;code&gt;&amp;quot;OK&amp;quot;&lt;/code&gt; state, and another drive &lt;code&gt;&amp;quot;Warning&amp;quot;&lt;/code&gt; state. The logical raid volume was no longer showing, neither in explorer nor disk manager. &lt;/p&gt;\n\n&lt;p&gt;I bought a replacement. Tried to detach the failed drive after marking it as retired, but was prompted I need to attach a healthy replacement first. I attached the new drive, it immediately started &lt;code&gt;&amp;quot;optimising&amp;quot;&lt;/code&gt; but was stuck at 0% with no disk activity for a few hours. I tried to stop optimisation, and now was in &lt;code&gt;&amp;quot;stopping optimisation&amp;quot;&lt;/code&gt; state. &lt;/p&gt;\n\n&lt;p&gt;I scrapped the idea of using Storage Spaces in the future, found it should be possible to pull out data from just one of the drives, and because the new drive is the only big one to contain the data in question, detached the new empty drive, marked it as retired, physically removed and wiped it clean (probably mistake #2. I did this because the good drive in the raid was still showing as &lt;code&gt;&amp;quot;healthy&amp;quot;&lt;/code&gt;. I installed UFS Explorer RAID Recovery, but it only finds &lt;code&gt;&amp;quot;MS Reserved partition&amp;quot;&lt;/code&gt; and &lt;code&gt;&amp;quot;Ext2/3/4 partition&amp;quot;&lt;/code&gt; in &lt;code&gt;&amp;quot;Invalid Root Folder&amp;quot;&lt;/code&gt; state, failing to find any data on it. GParted under linux shows the same except secondd partition as &lt;code&gt;&amp;quot;Storage pool&amp;quot;&lt;/code&gt; with no option to mount.&lt;/p&gt;\n\n&lt;p&gt;What I have now is &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Two-way mirror Storage Spaces raid in &lt;code&gt;&amp;quot;error&amp;quot;&lt;/code&gt; state (&amp;quot;check physical drives section&amp;quot;)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;One old drive in &lt;code&gt;&amp;quot;OK&amp;quot;&lt;/code&gt; state, with all the data as I understand it.&lt;/li&gt;\n&lt;li&gt;One old drive in &lt;code&gt;&amp;quot;Warning / Preparing for removal&amp;quot;&lt;/code&gt; state, S.M.A.R.T. showing a few reallocated sectors. I marked it as &lt;code&gt;&amp;quot;Retired&amp;quot;&lt;/code&gt; previously via powershell cmdlet.&lt;/li&gt;\n&lt;li&gt;One new drive in &lt;code&gt;&amp;quot;Warning / Retired; add a drive then remove this drive&amp;quot;&lt;/code&gt; state, also marked as retired by me. The drive is wiped clean after being physically removed.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;code&gt;Get-StorageJob&lt;/code&gt; shows  &lt;code&gt;Storage pool-Rebalance&lt;/code&gt; in &lt;code&gt;Shutting Down&lt;/code&gt; state.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Please help. I tried many powershell commands, mostly attempting to force remove all non-OK drives from the raid, planning to reattach a healthy one. Now I just want to rescue the data to a good new drive and go ahead from there.&lt;/p&gt;\n\n&lt;p&gt;Edit: Also posted to &lt;a href=\"https://www.reddit.com/r/techsupport/comments/132eo7h/storage_spaces_help_greatly_appreciated/\"&gt;https://www.reddit.com/r/techsupport/comments/132eo7h/storage_spaces_help_greatly_appreciated/&lt;/a&gt;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132egn5", "is_robot_indexable": true, "report_reasons": null, "author": "alex77456", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/132egn5/storage_spaces_nightmare_im_desparate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132egn5/storage_spaces_nightmare_im_desparate/", "subreddit_subscribers": 680152, "created_utc": 1682728138.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Facebook displays a page's videos on a single screen that keeps adding thumbnail results as you scroll down. Because this page has so many videos, I haven't been able to reach the end of the video list (read: load thumbnails of all the page's videos) without something breaking.\n\nI tried using Youtube-DL and yt-dlp. They can download individual videos from Facebook, but they can't download all videos from a particular Facebook page.\n\nI also tried using JDownloader 2. LinkCrawler only found a handful of videos.\n\nSuggestions?", "author_fullname": "t2_ix7smrj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need to archive all of a Facebook page's public videos... but the page has 9,000+ videos and I can't even load all their thumbnails to grab their URLs. Suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131i2wj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682657978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Facebook displays a page&amp;#39;s videos on a single screen that keeps adding thumbnail results as you scroll down. Because this page has so many videos, I haven&amp;#39;t been able to reach the end of the video list (read: load thumbnails of all the page&amp;#39;s videos) without something breaking.&lt;/p&gt;\n\n&lt;p&gt;I tried using Youtube-DL and yt-dlp. They can download individual videos from Facebook, but they can&amp;#39;t download all videos from a particular Facebook page.&lt;/p&gt;\n\n&lt;p&gt;I also tried using JDownloader 2. LinkCrawler only found a handful of videos.&lt;/p&gt;\n\n&lt;p&gt;Suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131i2wj", "is_robot_indexable": true, "report_reasons": null, "author": "SociopathicProTips", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131i2wj/i_need_to_archive_all_of_a_facebook_pages_public/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131i2wj/i_need_to_archive_all_of_a_facebook_pages_public/", "subreddit_subscribers": 680152, "created_utc": 1682657978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a large drive or set of drives that is as easy as it is to use iCloud drive and also as secure. I'm been doing a lot of reading on here and elsewhere and I think a NAS is what I need but I was told to post here for help first. I consider myself a nerd and gadget geek but wow most of the info in this sub is way over my head. I do NOT want a homebrew setup that I have to code etc. I want an off the shelf option that is plug and play. Drag and drop files seamlessly across all my devices. \n\nI should start out that I'm an apple user. I use a MacBook at work, iMac at home and an Iphone on the go. I use all three for both work and home life. I'm an online retailer so I have lots of photos and videos of products that I need to post online on either of the 3 devices at any given time. I also have LOTS of messages where I've shared photos that I need to keep to look back on to see what we paid for something or what we quoted on something. Icloud Drive has been very helpful for me and I would like a larger version of that. I had the 2TB cloud family plan and blew through that in a few months. I kicked SO off the plan and went 2TB solo. I maxed that out in a year. Since people have asked in the other place I posted, it's comprised of this...\n\n1110 GB Photos (122,000 photos, 8,500 videos)\n\n629.6 GB Messages\n\n52.3 GB iCloud drive\n\n9.5 GB backups\n\nand the rest in other stuff.\n\nI have 20+ external HD's of backups and large folders here and there. My most recent acquisition is a seagate 8tb (STLC8000600) which I'm using for time machine. I wish I could just plug a 10TB External HD into my mac and use that like iCloud drive on all my devices. \n\nI know about apple one but for $36/mo I would rather take the $400/yr and get some hardware. We currently use Dropbox to share photos in the office. We use google docs to share files in the office. I do not want another per month service. \n\n&amp;#x200B;\n\nwhat I'm looking to do: \n\nstore and share photos, videos and files across multiple apple devices. Not as important but would be nice to have a separate folder that a fellow employee can tap into to access and add files.", "author_fullname": "t2_qhc3e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Easy iCloud drive replacement?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_132fq82", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682731822.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a large drive or set of drives that is as easy as it is to use iCloud drive and also as secure. I&amp;#39;m been doing a lot of reading on here and elsewhere and I think a NAS is what I need but I was told to post here for help first. I consider myself a nerd and gadget geek but wow most of the info in this sub is way over my head. I do NOT want a homebrew setup that I have to code etc. I want an off the shelf option that is plug and play. Drag and drop files seamlessly across all my devices. &lt;/p&gt;\n\n&lt;p&gt;I should start out that I&amp;#39;m an apple user. I use a MacBook at work, iMac at home and an Iphone on the go. I use all three for both work and home life. I&amp;#39;m an online retailer so I have lots of photos and videos of products that I need to post online on either of the 3 devices at any given time. I also have LOTS of messages where I&amp;#39;ve shared photos that I need to keep to look back on to see what we paid for something or what we quoted on something. Icloud Drive has been very helpful for me and I would like a larger version of that. I had the 2TB cloud family plan and blew through that in a few months. I kicked SO off the plan and went 2TB solo. I maxed that out in a year. Since people have asked in the other place I posted, it&amp;#39;s comprised of this...&lt;/p&gt;\n\n&lt;p&gt;1110 GB Photos (122,000 photos, 8,500 videos)&lt;/p&gt;\n\n&lt;p&gt;629.6 GB Messages&lt;/p&gt;\n\n&lt;p&gt;52.3 GB iCloud drive&lt;/p&gt;\n\n&lt;p&gt;9.5 GB backups&lt;/p&gt;\n\n&lt;p&gt;and the rest in other stuff.&lt;/p&gt;\n\n&lt;p&gt;I have 20+ external HD&amp;#39;s of backups and large folders here and there. My most recent acquisition is a seagate 8tb (STLC8000600) which I&amp;#39;m using for time machine. I wish I could just plug a 10TB External HD into my mac and use that like iCloud drive on all my devices. &lt;/p&gt;\n\n&lt;p&gt;I know about apple one but for $36/mo I would rather take the $400/yr and get some hardware. We currently use Dropbox to share photos in the office. We use google docs to share files in the office. I do not want another per month service. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;what I&amp;#39;m looking to do: &lt;/p&gt;\n\n&lt;p&gt;store and share photos, videos and files across multiple apple devices. Not as important but would be nice to have a separate folder that a fellow employee can tap into to access and add files.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132fq82", "is_robot_indexable": true, "report_reasons": null, "author": "fatguybike", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/132fq82/easy_icloud_drive_replacement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132fq82/easy_icloud_drive_replacement/", "subreddit_subscribers": 680152, "created_utc": 1682731822.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Im not Shure what's the cheapest way is to archive 4tb of storage wehre 1 disk can die without data lost. Raid 1 with 2 4tb drives or raid 5 with more smaller drives. (English ist not my first language)", "author_fullname": "t2_12psxt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Chepest way to get 4tb of redundant storage sapace.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132d6a3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682724577.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im not Shure what&amp;#39;s the cheapest way is to archive 4tb of storage wehre 1 disk can die without data lost. Raid 1 with 2 4tb drives or raid 5 with more smaller drives. (English ist not my first language)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132d6a3", "is_robot_indexable": true, "report_reasons": null, "author": "Klausmp", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/132d6a3/chepest_way_to_get_4tb_of_redundant_storage_sapace/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132d6a3/chepest_way_to_get_4tb_of_redundant_storage_sapace/", "subreddit_subscribers": 680152, "created_utc": 1682724577.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've written a python script to download all saved images from imgur, and sort them into subreddit/username/album. I called it turd_bot as it's the 'TURDs - The Ultimate Reddit Downloader'\n\nI had it running as a cron job on my Debian server for a while, and have just updated it to run with the latest version of PRAW and python.\n\nIt requires some knowledge of python (which I have only a little), and has evolved over time so the code is untidy, but it works. I can't guarantee it will work, and don't know enough to offer any support, but with imgur removing so much thought I would share this.\n\nyou will need to know how to install python modules to run it.\nyou will need to get API access to reddit\nyou will need to get API access to imgur\n\nFiles can be downloaded here: https://github.com/MadJalapeno/turd_bot\n\nPut them into a directory, and run with python3 \n\nTo unsave after downloading\n\n    python3 turd_bot.py -s\n\nto just download\n\n    python3 turd_bot.py -t", "author_fullname": "t2_31rbe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "turd_bot - a python script to download saved posts from imgur", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_132g6ze", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682733186.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve written a python script to download all saved images from imgur, and sort them into subreddit/username/album. I called it turd_bot as it&amp;#39;s the &amp;#39;TURDs - The Ultimate Reddit Downloader&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;I had it running as a cron job on my Debian server for a while, and have just updated it to run with the latest version of PRAW and python.&lt;/p&gt;\n\n&lt;p&gt;It requires some knowledge of python (which I have only a little), and has evolved over time so the code is untidy, but it works. I can&amp;#39;t guarantee it will work, and don&amp;#39;t know enough to offer any support, but with imgur removing so much thought I would share this.&lt;/p&gt;\n\n&lt;p&gt;you will need to know how to install python modules to run it.\nyou will need to get API access to reddit\nyou will need to get API access to imgur&lt;/p&gt;\n\n&lt;p&gt;Files can be downloaded here: &lt;a href=\"https://github.com/MadJalapeno/turd_bot\"&gt;https://github.com/MadJalapeno/turd_bot&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Put them into a directory, and run with python3 &lt;/p&gt;\n\n&lt;p&gt;To unsave after downloading&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;python3 turd_bot.py -s\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;to just download&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;python3 turd_bot.py -t\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/yQ7FhTm8pyAWQ2exQFPliMfn0mnAp4rLbuSNyA3-e24.jpg?auto=webp&amp;v=enabled&amp;s=f73d2e72d2e2b18806f3c1886f16bc0b1ef83e2d", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/yQ7FhTm8pyAWQ2exQFPliMfn0mnAp4rLbuSNyA3-e24.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=357c98c39b999efbb07184eeecfaca5435857059", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/yQ7FhTm8pyAWQ2exQFPliMfn0mnAp4rLbuSNyA3-e24.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61fca42b94bfed0276ff1c9d40e4df30b6f2e1d1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/yQ7FhTm8pyAWQ2exQFPliMfn0mnAp4rLbuSNyA3-e24.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3000afeeeb81d88924d7b1a1d207fa3c044863e2", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/yQ7FhTm8pyAWQ2exQFPliMfn0mnAp4rLbuSNyA3-e24.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97647fec97e1703d3c2c92cdfedbd0a42ef7a2ac", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/yQ7FhTm8pyAWQ2exQFPliMfn0mnAp4rLbuSNyA3-e24.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2a762bc5ff862a93a06e472f72f282ed51e60f1e", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/yQ7FhTm8pyAWQ2exQFPliMfn0mnAp4rLbuSNyA3-e24.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7fe43bba0d65f976028eb814e10ef221069a71d1", "width": 1080, "height": 540}], "variants": {}, "id": "siMWcamrGEeZgY0nL9jAeTA2Zk-FzrR9ZvC7AJLFD_M"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132g6ze", "is_robot_indexable": true, "report_reasons": null, "author": "madjalapeno", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/132g6ze/turd_bot_a_python_script_to_download_saved_posts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132g6ze/turd_bot_a_python_script_to_download_saved_posts/", "subreddit_subscribers": 680152, "created_utc": 1682733186.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm new to archiving. I don't have everything well organized on my computer, so I started compiling a repository of important data I wanted to ensure was backed up. I have two copies of my backup files, one on an internal drive separate from the drive my OS is on, and one on an external HDD. I'm working on saving up to get more/different drives and eventually follow the 3-2-1 rule, but not there yet.\n\nMy directory organization and file naming process was rapidly changing to meet new needs as they came up. As a result, my internal drive backup had a different structure to the external one, and I soon realized the need for version control so that I don't confuse which files had been backed up already and which hadn't, etc.\n\nI did a little research and found that Git Annex was recommended a few times here. I've worked a bit with Git before (but only for remote repos) so I thought I would be able to figure it out. Despite running Windows, I managed to successfully install and set up Annex with two client repositories.\n\nThen issues started to crop up: The webapp would regularly crash or otherwise disconnect in the middle of syncing. Syncing would take several hours (and more due to crashes) whereas direct copy-pasting my data (around 500GB) took half that amount of time or less. The .git folder at the root directory of my backups would swell in size of hundreds of gigabytes (which maybe is normal for annex/LFS but was not what I expected) and the memory capacity of my drives are already cutting it close. These are probably all normal things for Git Annex, but I quickly realized I was out of my depth, and didn't feel comfortable attempt to use Git Annex anymore.\n\nTrying to start the process of removing Annex, I started deleting one of the two .git folders but stopped part-way through wondering if I was deleting my original files. I read somewhere that Annex turns files into symbolic links, but I wasn't sure if they were hard links and I was actually deleting my original files (tbh I don't understand symbolic links very well) and panicked. Looking around online, there isn't a lot of info about how to remove Git Annex from Windows beyond `git annex uninit` and hours after running the command and waiting, I'm still staring at my console. It doesn't help that there isn't any feedback on what it's doing or if it's even working.\n\nI know I'm making a lot of rookie mistakes. I shouldn't have picked software that wasn't beginner-friendly or designed for my OS. I shouldn't just delete random things, especially when I don't fully understand what I'm working with. I probably should have just backed up a system image first thing. I also probably should have saved up more to shell out for larger drives, etc. But now I'm just trying to safely get Git Annex off of my computer and am struggling to do so.\n\ntdlr: I did some dumb things trying to set up two back ups and now I feel like Git Annex is holding my files hostage.", "author_fullname": "t2_4kfehrii", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Removing Git Annex from Windows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_132fv0d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682732525.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682732199.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to archiving. I don&amp;#39;t have everything well organized on my computer, so I started compiling a repository of important data I wanted to ensure was backed up. I have two copies of my backup files, one on an internal drive separate from the drive my OS is on, and one on an external HDD. I&amp;#39;m working on saving up to get more/different drives and eventually follow the 3-2-1 rule, but not there yet.&lt;/p&gt;\n\n&lt;p&gt;My directory organization and file naming process was rapidly changing to meet new needs as they came up. As a result, my internal drive backup had a different structure to the external one, and I soon realized the need for version control so that I don&amp;#39;t confuse which files had been backed up already and which hadn&amp;#39;t, etc.&lt;/p&gt;\n\n&lt;p&gt;I did a little research and found that Git Annex was recommended a few times here. I&amp;#39;ve worked a bit with Git before (but only for remote repos) so I thought I would be able to figure it out. Despite running Windows, I managed to successfully install and set up Annex with two client repositories.&lt;/p&gt;\n\n&lt;p&gt;Then issues started to crop up: The webapp would regularly crash or otherwise disconnect in the middle of syncing. Syncing would take several hours (and more due to crashes) whereas direct copy-pasting my data (around 500GB) took half that amount of time or less. The .git folder at the root directory of my backups would swell in size of hundreds of gigabytes (which maybe is normal for annex/LFS but was not what I expected) and the memory capacity of my drives are already cutting it close. These are probably all normal things for Git Annex, but I quickly realized I was out of my depth, and didn&amp;#39;t feel comfortable attempt to use Git Annex anymore.&lt;/p&gt;\n\n&lt;p&gt;Trying to start the process of removing Annex, I started deleting one of the two .git folders but stopped part-way through wondering if I was deleting my original files. I read somewhere that Annex turns files into symbolic links, but I wasn&amp;#39;t sure if they were hard links and I was actually deleting my original files (tbh I don&amp;#39;t understand symbolic links very well) and panicked. Looking around online, there isn&amp;#39;t a lot of info about how to remove Git Annex from Windows beyond &lt;code&gt;git annex uninit&lt;/code&gt; and hours after running the command and waiting, I&amp;#39;m still staring at my console. It doesn&amp;#39;t help that there isn&amp;#39;t any feedback on what it&amp;#39;s doing or if it&amp;#39;s even working.&lt;/p&gt;\n\n&lt;p&gt;I know I&amp;#39;m making a lot of rookie mistakes. I shouldn&amp;#39;t have picked software that wasn&amp;#39;t beginner-friendly or designed for my OS. I shouldn&amp;#39;t just delete random things, especially when I don&amp;#39;t fully understand what I&amp;#39;m working with. I probably should have just backed up a system image first thing. I also probably should have saved up more to shell out for larger drives, etc. But now I&amp;#39;m just trying to safely get Git Annex off of my computer and am struggling to do so.&lt;/p&gt;\n\n&lt;p&gt;tdlr: I did some dumb things trying to set up two back ups and now I feel like Git Annex is holding my files hostage.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132fv0d", "is_robot_indexable": true, "report_reasons": null, "author": "KuryCoeur", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/132fv0d/removing_git_annex_from_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132fv0d/removing_git_annex_from_windows/", "subreddit_subscribers": 680152, "created_utc": 1682732199.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to download each playlist into its own folder when archiving an entire yt channel?\n\nThis is the command I'm using\n\n    yt-dlp -f \"bestvideo[height&lt;=720][vcodec=vp9]+bestaudio[acodec=opus]\" https://www.youtube.com/coalcracker/playlists --download-archive FILE\n\nIt's just downloading everything together from each playlist.\n\nAlso, will the program automatically check the archive file the next time I run a download for the whole channel so as to not download videos I already have or do I need to take additional steps. \n\nThanks everyone.", "author_fullname": "t2_7ioyznjq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Individual Folders for Each Playlist on ytdl", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132ebs4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682728304.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682727737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to download each playlist into its own folder when archiving an entire yt channel?&lt;/p&gt;\n\n&lt;p&gt;This is the command I&amp;#39;m using&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;yt-dlp -f &amp;quot;bestvideo[height&amp;lt;=720][vcodec=vp9]+bestaudio[acodec=opus]&amp;quot; https://www.youtube.com/coalcracker/playlists --download-archive FILE\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;It&amp;#39;s just downloading everything together from each playlist.&lt;/p&gt;\n\n&lt;p&gt;Also, will the program automatically check the archive file the next time I run a download for the whole channel so as to not download videos I already have or do I need to take additional steps. &lt;/p&gt;\n\n&lt;p&gt;Thanks everyone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132ebs4", "is_robot_indexable": true, "report_reasons": null, "author": "_not_a_coincidence", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/132ebs4/individual_folders_for_each_playlist_on_ytdl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132ebs4/individual_folders_for_each_playlist_on_ytdl/", "subreddit_subscribers": 680152, "created_utc": 1682727737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone, I have limited experience in this space and am seeking advice. Currently, I am using a Synology DS920+ with 4x 10TB running in SHR-2. However, I have no backup of my data, only redundancy, which I know is not ideal. The problem is that it's not easy to back up 20TB of data. I am looking for solutions to this problem. Additionally, I need to expand my storage capacity soon.\n\nOne solution I am considering is buying another DS920+ and putting it in a separate location in my house (since I have nowhere offsite to put it). Then, I would run one DS920+ with 4 separate volumes, with each volume being mirrored and backed up daily to the other Synology. However, this solution has some downsides. I would have to manage 4 volumes, there would be a 50% loss of storage space, and I would lose the 2-drive redundancy I have now. Additionally, I am not sure if there is an option to merge the drives without necessarily setting up a RAID on both sides.\n\nAnother solution I am considering is buying an 8-bay NAS, such as the DS1821+, and continuing with my current setup. Then, maybe in the future, when I have more funds, I can buy 4x16TB drives and use the DS920+ as a daily backup. The downside to this solution is that it's more expensive upfront, and there would be no backups or redundancy against malware, NAS catching on fire, etc. However, I would have 60TB of storage space, with 20TB dedicated to redundancy.\n\nAre there any other solutions I should consider?", "author_fullname": "t2_nqr3p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice on data storage solutions: Should I buy another Synology DS920+ or upgrade to an 8-bay NAS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132dxd6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682726937.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682726625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I have limited experience in this space and am seeking advice. Currently, I am using a Synology DS920+ with 4x 10TB running in SHR-2. However, I have no backup of my data, only redundancy, which I know is not ideal. The problem is that it&amp;#39;s not easy to back up 20TB of data. I am looking for solutions to this problem. Additionally, I need to expand my storage capacity soon.&lt;/p&gt;\n\n&lt;p&gt;One solution I am considering is buying another DS920+ and putting it in a separate location in my house (since I have nowhere offsite to put it). Then, I would run one DS920+ with 4 separate volumes, with each volume being mirrored and backed up daily to the other Synology. However, this solution has some downsides. I would have to manage 4 volumes, there would be a 50% loss of storage space, and I would lose the 2-drive redundancy I have now. Additionally, I am not sure if there is an option to merge the drives without necessarily setting up a RAID on both sides.&lt;/p&gt;\n\n&lt;p&gt;Another solution I am considering is buying an 8-bay NAS, such as the DS1821+, and continuing with my current setup. Then, maybe in the future, when I have more funds, I can buy 4x16TB drives and use the DS920+ as a daily backup. The downside to this solution is that it&amp;#39;s more expensive upfront, and there would be no backups or redundancy against malware, NAS catching on fire, etc. However, I would have 60TB of storage space, with 20TB dedicated to redundancy.&lt;/p&gt;\n\n&lt;p&gt;Are there any other solutions I should consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132dxd6", "is_robot_indexable": true, "report_reasons": null, "author": "sp3ctr41", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/132dxd6/seeking_advice_on_data_storage_solutions_should_i/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132dxd6/seeking_advice_on_data_storage_solutions_should_i/", "subreddit_subscribers": 680152, "created_utc": 1682726625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/r1cridi4apwa1.png?width=890&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=feb39fc469fb0ec2350cbff140bb7a637ba9b5df\n\nFirst time getting a converter these are not soldered and are ok type right?\n\nAlso I see something about PSU \"rails\" getting mentioned and don't realy know anything about inners of PSU. What should I check exactly to make sure I don't overload it?\n\nI have an rm750x how many drives could I load into single sata/molex port on PSU? By default these are included\n\nhttps://preview.redd.it/5mbvji31bpwa1.png?width=826&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89b6f696b66fcb26f4b58f8d33139eb4fadd2664", "author_fullname": "t2_r687it3e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are these ok type of molex to sata converters? (and question about psu)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": 116, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5mbvji31bpwa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 39, "x": 108, "u": "https://preview.redd.it/5mbvji31bpwa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c5798d3e2fb82d642a896a0f7741a33403080c3"}, {"y": 78, "x": 216, "u": "https://preview.redd.it/5mbvji31bpwa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c9c39af7c3e3eaa30c697765a524978c9582da6"}, {"y": 116, "x": 320, "u": "https://preview.redd.it/5mbvji31bpwa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e113a76d955bbc4574846bc2c68da67ac53937a9"}, {"y": 233, "x": 640, "u": "https://preview.redd.it/5mbvji31bpwa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=06bb9f0dc39c2463461e6f0491c5959d6ccd59be"}], "s": {"y": 302, "x": 826, "u": "https://preview.redd.it/5mbvji31bpwa1.png?width=826&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=89b6f696b66fcb26f4b58f8d33139eb4fadd2664"}, "id": "5mbvji31bpwa1"}, "r1cridi4apwa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 89, "x": 108, "u": "https://preview.redd.it/r1cridi4apwa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b0a48da8769c20fd98ecb2483c2d000200ae7a16"}, {"y": 179, "x": 216, "u": "https://preview.redd.it/r1cridi4apwa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8beec32be821d5661b5b376f996239d80ad7cb89"}, {"y": 266, "x": 320, "u": "https://preview.redd.it/r1cridi4apwa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c17cc3522a55f0570bf62870bc295537a0a80ab"}, {"y": 532, "x": 640, "u": "https://preview.redd.it/r1cridi4apwa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=548689f785531cfa000b099669b4eae043ac4bff"}], "s": {"y": 740, "x": 890, "u": "https://preview.redd.it/r1cridi4apwa1.png?width=890&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=feb39fc469fb0ec2350cbff140bb7a637ba9b5df"}, "id": "r1cridi4apwa1"}}, "name": "t3_132boye", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GMZQS4hCY1uzpbqna7fZw9BrdxtEpy3cfSS_La3ZZ6Q.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682720701.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r1cridi4apwa1.png?width=890&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=feb39fc469fb0ec2350cbff140bb7a637ba9b5df\"&gt;https://preview.redd.it/r1cridi4apwa1.png?width=890&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=feb39fc469fb0ec2350cbff140bb7a637ba9b5df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;First time getting a converter these are not soldered and are ok type right?&lt;/p&gt;\n\n&lt;p&gt;Also I see something about PSU &amp;quot;rails&amp;quot; getting mentioned and don&amp;#39;t realy know anything about inners of PSU. What should I check exactly to make sure I don&amp;#39;t overload it?&lt;/p&gt;\n\n&lt;p&gt;I have an rm750x how many drives could I load into single sata/molex port on PSU? By default these are included&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5mbvji31bpwa1.png?width=826&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89b6f696b66fcb26f4b58f8d33139eb4fadd2664\"&gt;https://preview.redd.it/5mbvji31bpwa1.png?width=826&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=89b6f696b66fcb26f4b58f8d33139eb4fadd2664&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Unraid |  35TB usable", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132boye", "is_robot_indexable": true, "report_reasons": null, "author": "lemmeanon", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/132boye/are_these_ok_type_of_molex_to_sata_converters_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132boye/are_these_ok_type_of_molex_to_sata_converters_and/", "subreddit_subscribers": 680152, "created_utc": 1682720701.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm on Windows 11... Is it possible to generate a single hash for an entire folder of data rather than for each individual file within the folder? There could be hundreds of thousands of files in a folder of a few hundred gigs for example. Ideally I'd like something easy to use with a GUI if possible, that I could run on source files and copied files after doing a backup to an external drive. Thanks.", "author_fullname": "t2_hylomcx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Verifying Hashes/Checksums for Entire Folders on Windows 11", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1325uru", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682706717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m on Windows 11... Is it possible to generate a single hash for an entire folder of data rather than for each individual file within the folder? There could be hundreds of thousands of files in a folder of a few hundred gigs for example. Ideally I&amp;#39;d like something easy to use with a GUI if possible, that I could run on source files and copied files after doing a backup to an external drive. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1325uru", "is_robot_indexable": true, "report_reasons": null, "author": "Celcius_87", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1325uru/verifying_hasheschecksums_for_entire_folders_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1325uru/verifying_hasheschecksums_for_entire_folders_on/", "subreddit_subscribers": 680152, "created_utc": 1682706717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So HDDs are starting to go the way of the dodo, less and less devices come with them and experience tells me thats how it starts before a technology becomes obsolete, just like CDs.\n\nBut now I'm left wondering, if you Google around you'll find thousands of articles telling you SSDs in cold storage do not last, but to me that has been proven somewhat false, I've had a few SSDs unpowered for around a year without issues.\n\nSSDs have been around for more than 10 years now so there should be some actual experimental data around yet I cannot find any actual long term studies or people relating their experience, hence this post, what has been your experience? How long your data lasted? Under what conditions?", "author_fullname": "t2_lcjhu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSDs vs Time, what's your experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131irwi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682660197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So HDDs are starting to go the way of the dodo, less and less devices come with them and experience tells me thats how it starts before a technology becomes obsolete, just like CDs.&lt;/p&gt;\n\n&lt;p&gt;But now I&amp;#39;m left wondering, if you Google around you&amp;#39;ll find thousands of articles telling you SSDs in cold storage do not last, but to me that has been proven somewhat false, I&amp;#39;ve had a few SSDs unpowered for around a year without issues.&lt;/p&gt;\n\n&lt;p&gt;SSDs have been around for more than 10 years now so there should be some actual experimental data around yet I cannot find any actual long term studies or people relating their experience, hence this post, what has been your experience? How long your data lasted? Under what conditions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131irwi", "is_robot_indexable": true, "report_reasons": null, "author": "otoko_no_hito", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131irwi/ssds_vs_time_whats_your_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131irwi/ssds_vs_time_whats_your_experience/", "subreddit_subscribers": 680152, "created_utc": 1682660197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I am very new to this so I'm sorry if this is a dumb question. But I have to format my computer and I have around 750GB of anime that I need to move over to another drive or online. Is there a way for me to calculate how long it would take to transfer it all? If it will take more than a week I will just go scorched earth and not move anything over before formatting.  \n\n\nHDD:  Hitachi HUA722010CLA330 1TB, Internal Hard Drive  \n\n\nAll help is greatly appreciated.", "author_fullname": "t2_7aygqar2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hard Drive Switching", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131f5jl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682649118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am very new to this so I&amp;#39;m sorry if this is a dumb question. But I have to format my computer and I have around 750GB of anime that I need to move over to another drive or online. Is there a way for me to calculate how long it would take to transfer it all? If it will take more than a week I will just go scorched earth and not move anything over before formatting.  &lt;/p&gt;\n\n&lt;p&gt;HDD:  Hitachi HUA722010CLA330 1TB, Internal Hard Drive  &lt;/p&gt;\n\n&lt;p&gt;All help is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131f5jl", "is_robot_indexable": true, "report_reasons": null, "author": "_TheOneTrueBean_", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131f5jl/hard_drive_switching/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131f5jl/hard_drive_switching/", "subreddit_subscribers": 680152, "created_utc": 1682649118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Externals are \u201cE\u201d and I changed the letter to one in disk management but still not recognized when 2 drives are connected to PC", "author_fullname": "t2_viuwzrr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to transfer data to another HD that\u2019s not recognized due to drive letter being the same?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_132dv8q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682726466.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Externals are \u201cE\u201d and I changed the letter to one in disk management but still not recognized when 2 drives are connected to PC&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "132dv8q", "is_robot_indexable": true, "report_reasons": null, "author": "Rotisseriejedi", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/132dv8q/how_to_transfer_data_to_another_hd_thats_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/132dv8q/how_to_transfer_data_to_another_hd_thats_not/", "subreddit_subscribers": 680152, "created_utc": 1682726466.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently I use syncthing to two-way sync a folder on my macbook with a folder on my PC. I then have google drive for desktop sync the same folder on my PC to my google drive so that I can access the files if i'm away from both computers. This seems to work OK for now but I have problems if the PC is off (google drive is not updated). Plus there seems to be a delay in google drive syncing too.\n\nIs there a better solution/software I can use?", "author_fullname": "t2_6zxmlbrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to way to sync computers with cloud access.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1328yop", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682713948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I use syncthing to two-way sync a folder on my macbook with a folder on my PC. I then have google drive for desktop sync the same folder on my PC to my google drive so that I can access the files if i&amp;#39;m away from both computers. This seems to work OK for now but I have problems if the PC is off (google drive is not updated). Plus there seems to be a delay in google drive syncing too.&lt;/p&gt;\n\n&lt;p&gt;Is there a better solution/software I can use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1328yop", "is_robot_indexable": true, "report_reasons": null, "author": "Kommanchi", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1328yop/best_way_to_way_to_sync_computers_with_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1328yop/best_way_to_way_to_sync_computers_with_cloud/", "subreddit_subscribers": 680152, "created_utc": 1682713948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm wanting to download and view 360 videos from YouTube with HD quality but am having trouble finding a downloader OR a video player. I've used \"Video Keeper\" to download the videos from YouTube and Windows media player's 360 function. I'm not sure if it's Video Keeper or windows media player, but something's leaving a swirly at the apex of the video. \n\nI've looked online for suggestions but most posts are 5+ years old and I feel like are a bit outdated. \n\nIf you guys have any suggestions or maybe tips I can do to get better video quality, I would love to try them out!! Thanks a bunch!!", "author_fullname": "t2_4qcjjq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions both free and paid to download AND view 360 videos from YTube.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1326vuw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682708996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wanting to download and view 360 videos from YouTube with HD quality but am having trouble finding a downloader OR a video player. I&amp;#39;ve used &amp;quot;Video Keeper&amp;quot; to download the videos from YouTube and Windows media player&amp;#39;s 360 function. I&amp;#39;m not sure if it&amp;#39;s Video Keeper or windows media player, but something&amp;#39;s leaving a swirly at the apex of the video. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked online for suggestions but most posts are 5+ years old and I feel like are a bit outdated. &lt;/p&gt;\n\n&lt;p&gt;If you guys have any suggestions or maybe tips I can do to get better video quality, I would love to try them out!! Thanks a bunch!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1326vuw", "is_robot_indexable": true, "report_reasons": null, "author": "American-Omar", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1326vuw/suggestions_both_free_and_paid_to_download_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1326vuw/suggestions_both_free_and_paid_to_download_and/", "subreddit_subscribers": 680152, "created_utc": 1682708996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of offline storage. I need to send out files a lot but I only need to send out new or changed files since the last time I sent them out. \n\nI use Powershell to generate SHA256 checksums of the offline media and the full file path and file name in a CSV file.  I first use \"Highlight Duplicates\" and \"Remove Duplicates\" function of Excel in the checksums of the local folder to dedupe it. I then compare the offline media with the Local folder's hash and determine which files are not among the files I sent out using the \"Highlight Duplicate\" function. I then paste the paths into FreeFileSync's exclusion list and copy everything else. \n\nDoes this method copy all the new files or am I missing some files? I did some small-scale tests and it seems to have worked. \n\nThanks.", "author_fullname": "t2_lunshccg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using checksums and complete file paths and names in a CSV file to keeping track of offline files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1325i95", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682705937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of offline storage. I need to send out files a lot but I only need to send out new or changed files since the last time I sent them out. &lt;/p&gt;\n\n&lt;p&gt;I use Powershell to generate SHA256 checksums of the offline media and the full file path and file name in a CSV file.  I first use &amp;quot;Highlight Duplicates&amp;quot; and &amp;quot;Remove Duplicates&amp;quot; function of Excel in the checksums of the local folder to dedupe it. I then compare the offline media with the Local folder&amp;#39;s hash and determine which files are not among the files I sent out using the &amp;quot;Highlight Duplicate&amp;quot; function. I then paste the paths into FreeFileSync&amp;#39;s exclusion list and copy everything else. &lt;/p&gt;\n\n&lt;p&gt;Does this method copy all the new files or am I missing some files? I did some small-scale tests and it seems to have worked. &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1325i95", "is_robot_indexable": true, "report_reasons": null, "author": "Mewto17", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1325i95/using_checksums_and_complete_file_paths_and_names/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1325i95/using_checksums_and_complete_file_paths_and_names/", "subreddit_subscribers": 680152, "created_utc": 1682705937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using ZFS in a RAIDZ pool configuration which I think is discouraged now in favor of mirroring for both my server and backup server.  I'm questioning the whole point of RAID in a home environment right now.   If I have backups, do I really need to waste  50% of my disk space via mirroring?    If a drive goes down, restore from backup.   If the backup goes down, backup your stuff again after repairing the backup server. \n\nMost of my storage is due to media server files.  I was thinking of reconfiguring storage for JBOD &amp; mergerfs.  And maybe doing the same with the backup server but adding redundancy via Snapraid with a parity disk on the backup server.\n\nAnything extremely important like financial records are also out on the cloud.   Is RAID redundancy really that important in the home?", "author_fullname": "t2_11l32p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID - home servers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13229fy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682701396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using ZFS in a RAIDZ pool configuration which I think is discouraged now in favor of mirroring for both my server and backup server.  I&amp;#39;m questioning the whole point of RAID in a home environment right now.   If I have backups, do I really need to waste  50% of my disk space via mirroring?    If a drive goes down, restore from backup.   If the backup goes down, backup your stuff again after repairing the backup server. &lt;/p&gt;\n\n&lt;p&gt;Most of my storage is due to media server files.  I was thinking of reconfiguring storage for JBOD &amp;amp; mergerfs.  And maybe doing the same with the backup server but adding redundancy via Snapraid with a parity disk on the backup server.&lt;/p&gt;\n\n&lt;p&gt;Anything extremely important like financial records are also out on the cloud.   Is RAID redundancy really that important in the home?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13229fy", "is_robot_indexable": true, "report_reasons": null, "author": "mlcarson", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13229fy/raid_home_servers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13229fy/raid_home_servers/", "subreddit_subscribers": 680152, "created_utc": 1682701396.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}