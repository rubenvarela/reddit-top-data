{"kind": "Listing", "data": {"after": "t3_12u0f1d", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I DESPISE live coding interviews. I\u2019m a good engineer and I can talk through skills and whiteboard and data model interview just fine. But seriously ask me a basic select statement in sql live and I barely remember how to do that. Panic sets in immediately and I barely make it through. I promise give me an hour to code something real and it will be done but just don\u2019t make me live code. I have almost 10 years experience and can barely write sql in a coding interview. It\u2019s just really rough.", "author_fullname": "t2_58wh4oyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Live coding interview hatred", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12th15p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 98, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 98, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682028930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I DESPISE live coding interviews. I\u2019m a good engineer and I can talk through skills and whiteboard and data model interview just fine. But seriously ask me a basic select statement in sql live and I barely remember how to do that. Panic sets in immediately and I barely make it through. I promise give me an hour to code something real and it will be done but just don\u2019t make me live code. I have almost 10 years experience and can barely write sql in a coding interview. It\u2019s just really rough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12th15p", "is_robot_indexable": true, "report_reasons": null, "author": "k-dani-b", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12th15p/live_coding_interview_hatred/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12th15p/live_coding_interview_hatred/", "subreddit_subscribers": 101166, "created_utc": 1682028930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! I am thrilled to announce that as part of the [dbt technical writing mentorship program](https://www.getdbt.com/blog/technical-writing-mentorship-program/), I have just published a brand new developer blog article for all my fellow data enthusiasts out there! In this tutorial, I provide a step-by-step guide on how to build a Kimball dimensional model with dbt. \n\n* Blog article: [https://docs.getdbt.com/blog/kimball-dimensional-model](https://docs.getdbt.com/blog/kimball-dimensional-model) \n* Repository: [https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling](https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling) \n\nI had trouble finding clear explanations on this topic myself, which is why I decided to write one and share my knowledge with the community. Check out my latest article and let me know what you think!", "author_fullname": "t2_qhi3dexs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Step-by-step tutorial: Building a Kimball dimensional model with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u2542", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682084519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I am thrilled to announce that as part of the &lt;a href=\"https://www.getdbt.com/blog/technical-writing-mentorship-program/\"&gt;dbt technical writing mentorship program&lt;/a&gt;, I have just published a brand new developer blog article for all my fellow data enthusiasts out there! In this tutorial, I provide a step-by-step guide on how to build a Kimball dimensional model with dbt. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Blog article: &lt;a href=\"https://docs.getdbt.com/blog/kimball-dimensional-model\"&gt;https://docs.getdbt.com/blog/kimball-dimensional-model&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;Repository: &lt;a href=\"https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling\"&gt;https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling&lt;/a&gt; &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I had trouble finding clear explanations on this topic myself, which is why I decided to write one and share my knowledge with the community. Check out my latest article and let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?auto=webp&amp;v=enabled&amp;s=d4169652ab7f36209622ee244f310aefe0cf8ce2", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd185c71f8fd75a33c0c79555ab7aa66ff94f024", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10e2c53de9b2969366504c9ded514bbe1e2eb4d1", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7634b43f18facd6b7a0eaa901c2b135c2c9f47cf", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=487560d27ca62339efe41270d29a0644753c4362", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43637abd63f6c913d8d643f155a22cfee21ab4eb", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7eff2686e5c425ddf4ad53b33e6ccec820db7c7", "width": 1080, "height": 567}], "variants": {}, "id": "2FMzESMf4JTwwEXCI7l6BQ_Y4vxSjUVpLJpZ0tVaorg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Founder of Data Engineer Camp, Data Engineer at Canva", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12u2542", "is_robot_indexable": true, "report_reasons": null, "author": "j__neo", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12u2542/stepbystep_tutorial_building_a_kimball/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u2542/stepbystep_tutorial_building_a_kimball/", "subreddit_subscribers": 101166, "created_utc": 1682084519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anybody feel like a fraud? I joined a DE team 2 years ago as my first DE job. The team mostly uses low code tools and really only knows SQL. I have helped them with starting to employ software engineering best practices. \nI want to continue to learn more of the software engineering side of this role but it won\u2019t happen at my current company. \n\nI have been deciding to interview around and started to realize I still don\u2019t know a lot of stuff for programming. \n\nHas anyone been in this spot before?", "author_fullname": "t2_uww96dnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody feel like a fraud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tzhs7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 23, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 23, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682078478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anybody feel like a fraud? I joined a DE team 2 years ago as my first DE job. The team mostly uses low code tools and really only knows SQL. I have helped them with starting to employ software engineering best practices. \nI want to continue to learn more of the software engineering side of this role but it won\u2019t happen at my current company. &lt;/p&gt;\n\n&lt;p&gt;I have been deciding to interview around and started to realize I still don\u2019t know a lot of stuff for programming. &lt;/p&gt;\n\n&lt;p&gt;Has anyone been in this spot before?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12tzhs7", "is_robot_indexable": true, "report_reasons": null, "author": "anon_data_person", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tzhs7/anybody_feel_like_a_fraud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tzhs7/anybody_feel_like_a_fraud/", "subreddit_subscribers": 101166, "created_utc": 1682078478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Applied for a few new opportunities, including some small or mid-size company. Some job *description* specifically asks for experience about data engineering 'on cloud', gcp/aws for example. But  we mainly make data pipeline in our own platform that are built by the department in charge of clusters and data storage, computation (Hadoop, Spark, Hive and other open-source componments mostly). When I told them such, I got rejected.\n\nI only worked in this one single company. Pertty big in the industry Mainly doing data warehouse, inner data products and analysis. Don't really know why 'On-Cloud' experience is such a must. Because from what I see it's just other kind of big data technology used instead of hadoop or spark. For data engineer, tasks are still the same: Making and optimizing data pipelines to transit, build or query using SQL or scripts. The logic beneath is still the same.", "author_fullname": "t2_oorup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's so special about on-cloud data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tockx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682046143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Applied for a few new opportunities, including some small or mid-size company. Some job &lt;em&gt;description&lt;/em&gt; specifically asks for experience about data engineering &amp;#39;on cloud&amp;#39;, gcp/aws for example. But  we mainly make data pipeline in our own platform that are built by the department in charge of clusters and data storage, computation (Hadoop, Spark, Hive and other open-source componments mostly). When I told them such, I got rejected.&lt;/p&gt;\n\n&lt;p&gt;I only worked in this one single company. Pertty big in the industry Mainly doing data warehouse, inner data products and analysis. Don&amp;#39;t really know why &amp;#39;On-Cloud&amp;#39; experience is such a must. Because from what I see it&amp;#39;s just other kind of big data technology used instead of hadoop or spark. For data engineer, tasks are still the same: Making and optimizing data pipelines to transit, build or query using SQL or scripts. The logic beneath is still the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12tockx", "is_robot_indexable": true, "report_reasons": null, "author": "GeForceKawaiiyo", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tockx/whats_so_special_about_oncloud_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tockx/whats_so_special_about_oncloud_data_engineering/", "subreddit_subscribers": 101166, "created_utc": 1682046143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.archerapi.com/](https://www.archerapi.com/)\n\nI web scraped a lot of the data from an Archer Fandom site using Python.\n\nI then cleaned up the data using both Python and SQL and stored the data in a SQLite database.\n\nFlask/Python was used to build the API and I'm hosting it on Heroku. For the front-end web page I used a Bootstrap template. \n\nAs this is my first API I'm open to any and all feedback. Apologies if this post isn't suitable for the subreddit.\n\nSource code: [https://github.com/ben-n93/archer\\_api](https://github.com/ben-n93/archer_api)\n\nThanks!", "author_fullname": "t2_a7uw8rfx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's not a data engineering pipeline but I'm proud of an API I built that provides data about the animated sitcom Archer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tv82a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682066463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.archerapi.com/\"&gt;https://www.archerapi.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I web scraped a lot of the data from an Archer Fandom site using Python.&lt;/p&gt;\n\n&lt;p&gt;I then cleaned up the data using both Python and SQL and stored the data in a SQLite database.&lt;/p&gt;\n\n&lt;p&gt;Flask/Python was used to build the API and I&amp;#39;m hosting it on Heroku. For the front-end web page I used a Bootstrap template. &lt;/p&gt;\n\n&lt;p&gt;As this is my first API I&amp;#39;m open to any and all feedback. Apologies if this post isn&amp;#39;t suitable for the subreddit.&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/ben-n93/archer_api\"&gt;https://github.com/ben-n93/archer_api&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12tv82a", "is_robot_indexable": true, "report_reasons": null, "author": "ruthlesscattle", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tv82a/its_not_a_data_engineering_pipeline_but_im_proud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tv82a/its_not_a_data_engineering_pipeline_but_im_proud/", "subreddit_subscribers": 101166, "created_utc": 1682066463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I work at a Fortune firm as a data science manager right now. I'm being tagged to take over management for a small data engineering team. It's not my background and I want to make sure I help these folks and don't screw everything up. :)\n\nI've got management experience and understand the basics of programming and productionilization of code (though my experience is all DS side with python &amp; R).\n\nMy question is what should I familiarize myself with to help set my new folks up for success? My experience with data engineering has largely been one of setting up ETL in automation for data processes but it's always been self service for my DS (and my DS team's) needs.\n\nI know a lot of this will have to do with:\n\n* Understanding, end to end, where the data needs for the team live and making sure we have the correct owners designated\n* Data pipelining is a huge part of the job, most of my knowledge here starts and ends with ETL in automation\n* Data quality is a huge consideration, though I've never had to worry about it in the past b/c I have always known exactly what I need (here I'll have to figure out what others will likely need)\n* Security, I believe, is largely owned by a different team\n\nI know there is a ton more that I'm missing. Besides learning new toolsets (react and node being the two primary ones, already familiar with Airflow &amp; DAGs), what else do I need to give myself a crash course in? What do I need to watch out for to protect my team from? What else can I do to help them?", "author_fullname": "t2_7jjayp3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tagged to lead a DE team, my background is data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u349o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682086543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I work at a Fortune firm as a data science manager right now. I&amp;#39;m being tagged to take over management for a small data engineering team. It&amp;#39;s not my background and I want to make sure I help these folks and don&amp;#39;t screw everything up. :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got management experience and understand the basics of programming and productionilization of code (though my experience is all DS side with python &amp;amp; R).&lt;/p&gt;\n\n&lt;p&gt;My question is what should I familiarize myself with to help set my new folks up for success? My experience with data engineering has largely been one of setting up ETL in automation for data processes but it&amp;#39;s always been self service for my DS (and my DS team&amp;#39;s) needs.&lt;/p&gt;\n\n&lt;p&gt;I know a lot of this will have to do with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Understanding, end to end, where the data needs for the team live and making sure we have the correct owners designated&lt;/li&gt;\n&lt;li&gt;Data pipelining is a huge part of the job, most of my knowledge here starts and ends with ETL in automation&lt;/li&gt;\n&lt;li&gt;Data quality is a huge consideration, though I&amp;#39;ve never had to worry about it in the past b/c I have always known exactly what I need (here I&amp;#39;ll have to figure out what others will likely need)&lt;/li&gt;\n&lt;li&gt;Security, I believe, is largely owned by a different team&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I know there is a ton more that I&amp;#39;m missing. Besides learning new toolsets (react and node being the two primary ones, already familiar with Airflow &amp;amp; DAGs), what else do I need to give myself a crash course in? What do I need to watch out for to protect my team from? What else can I do to help them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12u349o", "is_robot_indexable": true, "report_reasons": null, "author": "quantpsychguy", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u349o/tagged_to_lead_a_de_team_my_background_is_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u349o/tagged_to_lead_a_de_team_my_background_is_data/", "subreddit_subscribers": 101166, "created_utc": 1682086543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nQuick career question, I currently work at a fortune 500 as a azure data engineer and we also utilize databricks. \n\nI recently received 2 offers: \n\nOne from Accenture federal services as a azure data engineer (25,000 more than I make now)\n\nOne from a smaller company as a data engineer using palantir foundry (15,000$ more than I make now)\n\nAt my current company I will get to that 25,000 more salary within 3 years. \n\nIs there even a market for palantir foundry ? \n\nAnyone have any experience at Accenture or the WLB?\n\nWhat would you guys do?", "author_fullname": "t2_uf4ne7uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tmyym", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682042730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Quick career question, I currently work at a fortune 500 as a azure data engineer and we also utilize databricks. &lt;/p&gt;\n\n&lt;p&gt;I recently received 2 offers: &lt;/p&gt;\n\n&lt;p&gt;One from Accenture federal services as a azure data engineer (25,000 more than I make now)&lt;/p&gt;\n\n&lt;p&gt;One from a smaller company as a data engineer using palantir foundry (15,000$ more than I make now)&lt;/p&gt;\n\n&lt;p&gt;At my current company I will get to that 25,000 more salary within 3 years. &lt;/p&gt;\n\n&lt;p&gt;Is there even a market for palantir foundry ? &lt;/p&gt;\n\n&lt;p&gt;Anyone have any experience at Accenture or the WLB?&lt;/p&gt;\n\n&lt;p&gt;What would you guys do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12tmyym", "is_robot_indexable": true, "report_reasons": null, "author": "NipsAhoy2", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tmyym/career_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tmyym/career_question/", "subreddit_subscribers": 101166, "created_utc": 1682042730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a massive 11,000+ word blog put together by my colleagues here at StarTree re: these three open source real-time analytics (OLAP) databases. Would love to hear anyone's feedback.\n\nSee more: [https://startree.ai/blog/a-tale-of-three-real-time-olap-databases](https://startree.ai/blog/a-tale-of-three-real-time-olap-databases)", "author_fullname": "t2_jt32w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Tale of Three Real-Time OLAP Databases: Apache Pinot, Apache Druid, and ClickHouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ta6lj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682014501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a massive 11,000+ word blog put together by my colleagues here at StarTree re: these three open source real-time analytics (OLAP) databases. Would love to hear anyone&amp;#39;s feedback.&lt;/p&gt;\n\n&lt;p&gt;See more: &lt;a href=\"https://startree.ai/blog/a-tale-of-three-real-time-olap-databases\"&gt;https://startree.ai/blog/a-tale-of-three-real-time-olap-databases&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?auto=webp&amp;v=enabled&amp;s=900d83cd5f59066e8a997ca05e5dd401b65aa563", "width": 2880, "height": 1620}, "resolutions": [{"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fd7d9a8c0ef829fae0ad30f6452699455eb1b1e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f1535da5d7d68a15334670c8cb48b82cde3cc61", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf51b4f14df330739b3572a0dd0e79cfaa787317", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b3f795497aad444876dfde9daeddd7d7a656780", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4aa07c438b37cde774b65c4a6d7445c4373b6e1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea90ea7cc15158a64cc12d0bd087278e7feed305", "width": 1080, "height": 607}], "variants": {}, "id": "wD1aiR5D-SqApZ0bRwMaKKr7L8tLMnpHn28caOSDhAk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ta6lj", "is_robot_indexable": true, "report_reasons": null, "author": "PeterCorless", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ta6lj/a_tale_of_three_realtime_olap_databases_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ta6lj/a_tale_of_three_realtime_olap_databases_apache/", "subreddit_subscribers": 101166, "created_utc": 1682014501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Been trying to move files in an Azure Data Lake G2 container using airflow and I've faced so many issues, for one there's no operator for moving files in the same container or methods on hooks to move files in the container.\n\nOne way is to Download the files in that container filepath and then once its locally in the machine hosting airflow you upload it back to the target container filepath. \n\nIssue with this is if you're using Azure-Managed Airflow like I am, you can't actually access the filesystem airflow is hosted on, so you don't know what filepath to input to download the file to.\n\n&amp;#x200B;\n\n*So then how are people moving around files in an azure storage container using Azure-Managed Airflow or Google Cloud Composer?*\n\n&amp;#x200B;\n\nI tried the API approach using storage SDKs but it feels so complex rather than using an easy operator. On GCP for instance they have an airflow operator called \"gcs\\_to\\_gcs\", which lets you easily move files from one container to the next. \n\n&amp;#x200B;\n\n*Are people just not doing file movement in Azure Storage Containers with airflow?* \n\n&amp;#x200B;\n\n**TLDR: Trying to figure out how to move files in an azure container with only operators and not downloading files to the local filesystem of a cloud-hosted airflow instance since they don't give access to the file system and wondering if people have any ideas.**", "author_fullname": "t2_4akiu5cg1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are people moving files in an Azure Storage Container using Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12trdan", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682054485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been trying to move files in an Azure Data Lake G2 container using airflow and I&amp;#39;ve faced so many issues, for one there&amp;#39;s no operator for moving files in the same container or methods on hooks to move files in the container.&lt;/p&gt;\n\n&lt;p&gt;One way is to Download the files in that container filepath and then once its locally in the machine hosting airflow you upload it back to the target container filepath. &lt;/p&gt;\n\n&lt;p&gt;Issue with this is if you&amp;#39;re using Azure-Managed Airflow like I am, you can&amp;#39;t actually access the filesystem airflow is hosted on, so you don&amp;#39;t know what filepath to input to download the file to.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;So then how are people moving around files in an azure storage container using Azure-Managed Airflow or Google Cloud Composer?&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I tried the API approach using storage SDKs but it feels so complex rather than using an easy operator. On GCP for instance they have an airflow operator called &amp;quot;gcs_to_gcs&amp;quot;, which lets you easily move files from one container to the next. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Are people just not doing file movement in Azure Storage Containers with airflow?&lt;/em&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR: Trying to figure out how to move files in an azure container with only operators and not downloading files to the local filesystem of a cloud-hosted airflow instance since they don&amp;#39;t give access to the file system and wondering if people have any ideas.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12trdan", "is_robot_indexable": true, "report_reasons": null, "author": "Sunya_ONE", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12trdan/how_are_people_moving_files_in_an_azure_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12trdan/how_are_people_moving_files_in_an_azure_storage/", "subreddit_subscribers": 101166, "created_utc": 1682054485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I would appreciate any experienced feedback here.\n\nI am currently building a data lake using delta tables, but without Databricks. \n\nI have some Spark pipelines that periodically partitionally write, compact, `vacuum`, `z-order`, `analize` and this works quite fine for upserting purposes and I wouldn't change it.\n\nAlso, I have a single node client that reads data directly from the data lake and exposes it to some https endpoints with FastAPI. So far I needed Spark to leverage predicate pushdown and delta metadata (lots of partitions and hundreds of GBs, can't afford to read all the data at once).\n\nRead performances are okay-ish, but not perfect. Especially because I would like to provide pagination and no Databricks means no Delta caching.\n\nI have no experience with Polars whatsoever, how would it compare in a read-only scenario considering that it does support delta tables? \n\nWould it be reasonable to think it may overspeed pyspark for filter-only queries meant to retrieve page-sized chunk of data (i.e., 100-1000 rows at the time)?\n\nThe APIs only expose `SELECT * FROM &lt;&gt; WHERE &lt;&gt;`\nkind of queries, where the conditions uses either partition or z-ordered columns.\n\nWould appreciate any feedback here, thanks in advance.", "author_fullname": "t2_31fa566d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Read-only queries on delta tables. Polars or Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tebg8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682022965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I would appreciate any experienced feedback here.&lt;/p&gt;\n\n&lt;p&gt;I am currently building a data lake using delta tables, but without Databricks. &lt;/p&gt;\n\n&lt;p&gt;I have some Spark pipelines that periodically partitionally write, compact, &lt;code&gt;vacuum&lt;/code&gt;, &lt;code&gt;z-order&lt;/code&gt;, &lt;code&gt;analize&lt;/code&gt; and this works quite fine for upserting purposes and I wouldn&amp;#39;t change it.&lt;/p&gt;\n\n&lt;p&gt;Also, I have a single node client that reads data directly from the data lake and exposes it to some https endpoints with FastAPI. So far I needed Spark to leverage predicate pushdown and delta metadata (lots of partitions and hundreds of GBs, can&amp;#39;t afford to read all the data at once).&lt;/p&gt;\n\n&lt;p&gt;Read performances are okay-ish, but not perfect. Especially because I would like to provide pagination and no Databricks means no Delta caching.&lt;/p&gt;\n\n&lt;p&gt;I have no experience with Polars whatsoever, how would it compare in a read-only scenario considering that it does support delta tables? &lt;/p&gt;\n\n&lt;p&gt;Would it be reasonable to think it may overspeed pyspark for filter-only queries meant to retrieve page-sized chunk of data (i.e., 100-1000 rows at the time)?&lt;/p&gt;\n\n&lt;p&gt;The APIs only expose &lt;code&gt;SELECT * FROM &amp;lt;&amp;gt; WHERE &amp;lt;&amp;gt;&lt;/code&gt;\nkind of queries, where the conditions uses either partition or z-ordered columns.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any feedback here, thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tebg8", "is_robot_indexable": true, "report_reasons": null, "author": "Perfecy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tebg8/readonly_queries_on_delta_tables_polars_or_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tebg8/readonly_queries_on_delta_tables_polars_or_spark/", "subreddit_subscribers": 101166, "created_utc": 1682022965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2bhtmk4t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Data Quality with Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_12thpih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ma6jemfxkJhO7PBZlyCf9prP2PIAq3CX435QBjT8PIc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682030443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ssmertin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ssmertin.com/articles/into-to-data-quality-with-apache-spark/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?auto=webp&amp;v=enabled&amp;s=d3f75f8fb2b7b466ac5387f5eb8e2138515fd21a", "width": 956, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=084a6b0fa0f0dce1526f171fc30a06be309563ff", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8be8319f0804dc32bd9f64fedcfd89c088e01d38", "width": 216, "height": 146}, {"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30daca78d1685d6c8fa9df505d49dd0f96e7fb28", "width": 320, "height": 217}, {"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53fa18a4763425e8da48dc6e1c3e40ffdd7dd5e0", "width": 640, "height": 435}], "variants": {}, "id": "L1Yb0PtWmCThbzzDwWEupJeJQaMgNhDypG04WFeckl4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12thpih", "is_robot_indexable": true, "report_reasons": null, "author": "nf_x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12thpih/introduction_to_data_quality_with_apache_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ssmertin.com/articles/into-to-data-quality-with-apache-spark/", "subreddit_subscribers": 101166, "created_utc": 1682030443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most current OLAP databases are built with a columnar storage engine to process huge data volumes. They take pride in their high throughput, but often underperform in high-concurrency scenarios. As a complement, many data engineers invite Key-Value stores like Apache HBase for point queries, and Redis as a cache layer to ease the burden. The downside is redundant storage and high maintenance costs.\n\nApache Doris has been striving to become a unified database for data queries of all sizes, including ad-hoc queries and point queries. Till now, we have already taken down the monster of high-throughput OLAP scenarios. In the upcoming Apache Doris 2.0, we have optimized it for high-concurrency point queries. Long story short, it can achieve over 30,000 QPS for a single node (a 20-time increase in concurrency), by methods of **partitioning and bucketing, indexing, materialized view, runtime filter, TOP-N optimization, row storage format, short-circuit, prepared statement, and row storage cache.**\n\n# 1. Partioning and Bucketing\n\nApache Doris shards data into a two-tiered structure: Partition and Bucket. You can use time information as the Partition Key. As for bucketing, you distribute the data into various nodes after data hashing. A wise bucketing plan can largely increase concurrency and throughput in data reading, because the system only needs to scan one bucket in one partition before it can locate the needed data.\n\n# 2. Index\n\nApache Doris uses various data indexes to speed up data reading and filtering, including smart indexes and secondary indexes. Smart indexes are auto-generated by Doris upon data ingestion, which requires no action from the user\u2019s side.There are two types of smart indexes:\n\n* **Sorted Index**: Apache Doris stores data in an orderly way. It creates a sorted index for every 1024 rows of data. The Key in the index is the value of the sorted column in the first row of the current 1024 rows. If the query involves the sorted column, the system will locate the first row of the relevant 1024 row group and start scanning there.\n* **ZoneMap Index**: These are indexes on the Segment and Page level. The maximum and minimum values of each column within a Page will be recorded, so are those within a Segment. Hence, in equivalence queries and range queries, the system can narrow down the filter range with the help of the MinMax indexes.\n\nSecondary indexes are created by users. These include Bloom Filter indexes, Bitmap indexes, [Inverted indexes](https://doris.apache.org/docs/dev/data-table/index/inverted-index/), and [NGram Bloom Filter indexes](https://doris.apache.org/docs/dev/data-table/index/ngram-bloomfilter-index/). \n\nExample: `select * from user_table where id &gt; 10 and id &lt; 1024`\n\nSuppose that the user has designated `id` as the Key during table creation, the data will be sorted by `id` on Memtable and the disks. So any queries involving `id` as a filter condition will be executed much faster with the aid of sorted indexes. Specifically, the data in storage will be put into multiple ranges based on `id` , and the system will implement binary search to locate the exact range according to the sorted indexes. But that could still be a large range since the sorted indexes are sparse. You can further narrow it down based on ZoneMap indexes, Bloom Filter indexes, and Bitmap indexes.This is another way to reduce data scanning and improve overall concurrency of the system.\n\n# 3. Materialized View\n\nThe idea of materialized view is to trade space for time: You execute pre-computation with pre-defined SQL statements, and perpetuate the results in a table that is visible to users but occupies some storage space. In this way, Apache Doris can respond much faster to queries for aggregated data and breakdown data and those involve the matching of sorted indexes once it hits a materialized view. This is a good way to lessen computation, improve query performance, and reduce resource consumption.\n\n# 4. Runtime Filter\n\nIn multi-table Join queries, the left table is usually called ProbeTable while the right one is called BuildTable, with the former much bigger than the latter. In query execution, firstly, the system reads the right table and creates a HashTable (Build) in the memory. Then, it starts reading the left table row by row, during which it also compares data between the left table and the HashTable and returns the matched data (Probe).\n\nDuring the creation of HashTable, Apache Doris generates a filter for the columns. It can be a Min/Max filter or an IN filter. Then it pushes down the filter to the left table, which can use the filter to screen out data and thus reduces the amount of data that the Probe node has to transfer and compare.This is how the Runtime Filter works. In most Join queries, the Runtime Filter can be automatically pushed down to the most underlying scan nodes or to the distributed Shuffle Join. In other words, Runtime Filter is able to reduce data reading and shorten response time for most Join queries.\n\n# 5. TOP-N Optimization\n\nTOP-N query is a frequent scenario in data analysis. For example, users want to fetch the most recent 100 orders, or the 5 highest/lowest priced products. For such queries, Apache Doris implements TOP-N optimization:\n\n1. Apache Doris reads the sorted fields and query fields from the Scanner layer, reserves only the TOP-N pieces of data by means of Heapsort, updates the real-time TOP-N results as it continues reading, and dynamically pushes them down to the Scanner.\n2. Combing the received TOP-N range and the indexes, the Scanner can skip a large proportion of irrelevant files and data chunks and only read a small number of rows.\n3. Queries on flat tables usually mean the need to scan massive data, but TOP-N queries only retrieve a small amount of data. The strategy here is to divide the data reading process into two stages. In stage one, the system sorts the data based on a few columns (sorted column, or condition column) and locates the TOP-N rows. In stage two, it fetches the TOP-N rows of data after data sorting, and then it retrieves the target data according to the row numbers.\n\n# 6. Row Storage Format\n\nAs we know, row storage is much more efficient when the user only queries for a single row of data. So we introduced row storage format in Apache Doris 2.0. We chose JSONB as the encoding format for row storage for three reasons:\n\n* **Flexible schema change**: If a user has added or deleted a field, or modified the type of a field, these changes must be updated in row storage in real time. So we choose to adopt the JSONB format and encode columns into JSONB fields. This makes changes in fields very easy.\n* **High performance**: Accessing rows in row-oriented storage is much faster than doing that in columnar storage, and it requires much less disk access in high-concurrency scenarios. Also, in some cases, you can map the column ID to the corresponding JSONB value so you can quickly access a certain column.\n* **Less storage space**: JSONB is a compacted binary format. It consumes less space on the disk and is more cost-effective.\n\nIn the storage engine, row storage will be stored as a hidden column (DORIS\\_ROW\\_STORE\\_COL). During Memtable Flush, the columns will be encoded into JSONB and cached into this hidden column. In data reading, the system uses the Column ID to locate the column, finds the target row based on the row number, and then deserializes the relevant columns.\n\n# 7. Short-Circuit\n\nNormally, an SQL statement is executed in three steps:\n\n1. SQL Parser parses the statement to generate an abstract syntax tree (AST).\n2. The Query Optimizer produces an executable plan.\n3. Execute the plan and return the results.\n\nFor complex queries on massive data, it is better to follow the plan created by the Query Optimizer. However, for high-concurrency point queries requiring low latency, that plan is not only unnecessary but also brings extra overheads. That\u2019s why we implement a short-circuit plan for point queries.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/40g6ww69d9va1.png?width=1606&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ada4306ffe3cc6f9874fba1777d35d32b5c0e1ea\n\nOnce the FE receives a point query request, a short-circuit plan will be produced. It is a lightweight plan that involves no equivalent transformation, logic optimization or physical optimization. Instead, it conducts some basic analysis on the AST, creates a fixed plan accordingly, and finds ways to reduce overhead of the optimizer.\n\nFor a simple point query involving primary keys, such as `select * from tbl where pk1 = 123 and pk2 = 456,` since it only involves one single Tablet, it is better to use a lightweight RPC interface for interaction with the Storage Engine. This avoids the creation of a complicated Fragment Plan and eliminates the performance overhead brought by the scheduling under the MPP query framework.\n\nDetails of the RPC interface are as follows:\n\n    message PTabletKeyLookupRequest {\n      required int64 tablet_id = 1;\n      repeated KeyTuple key_tuples = 2;\n      optional Descriptor desc_tbl = 4;\n      optional ExprList  output_expr = 5;\n    }\n    message PTabletKeyLookupResponse {\n      required PStatus status = 1;\n      optional bytes row_batch = 5;\n    optional bool\n    empty_batch = 6;\n    }\n    rpc tablet_fetch_data(PTabletKeyLookupRequest) returns (PTabletKeyLookupResponse);\n\n`tablet_id` is calculated based on the primary key column, while `key_tuples` is the string format of the primary key. In this example, the `key_tuples` is similar to \\['123', '456'\\]. As BE receives the request, `key_tuples` will be encoded into primary key storage format. Then, it will locate the corresponding row number of the Key in the Segment File with the help of the primary key index, and check if that row exists in `delete bitmap`. If it does, the row number will be returned; if not, the system returns NotFound. The returned row number will be used for point query on `__DORIS_ROW_STORE_COL__`. That means we only need to locate one row in that column, fetch the original value of the JSONB format, and deserialize it.\n\n# 8. Prepared Statement\n\nThe idea of prepared statements is to cache precomputed SQL and expressions in HashMap in memory, so they can be directly used in queries when applicable.\n\nPrepared statements adopt MySQL binary protocol for transmission. The protocol is implemented in the mysql\\_row\\_buffer.\\[h|cpp\\] file, and uses MySQL binary encoding. Under this protocol, the client (for example, JDBC Client) sends a pre-compiled statement to FE via `PREPARE` MySQL Command. Next, FE will parse and analyze the statement and cache it in the HashMap as shown in the figure above. Next, the client, using `EXECUTE` MySQL Command, will replace the placeholder, encode it into binary format, and send it to FE. Then, FE will perform deserialization to obtain the value of the placeholder, and generate query conditions.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/nwmpb1d2e9va1.png?width=1134&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=856b9884206b9fd8628224c1bb5ad30e7cfc5105\n\nApart from caching prepared statements in FE, we also cache reusable structures in BE. These structures include pre-allocated computation blocks, query descriptors, and output expressions. Serializing and deserializing these structures often cause a CPU hotspot, so it makes more sense to cache them. The prepared statement for each query comes with a UUID named CacheID. So when BE executes the point query, it will find the corresponding class based on the CacheID, and then reuse the structure in computation.\n\n# 9. Row Storage Cache\n\nApache Doris has a Page Cache feature, where each page caches the data of one column. \n\nhttps://preview.redd.it/v6fvikn8e9va1.png?width=568&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=833ea248bb86a5af2a2917fc319cd81c7894833f\n\nAs mentioned above, we have introduced row storage in Doris. The problem with this is, one row of data consists of multiple columns, so in the case of big queries, the cached data might be erased. Thus, we also introduced row cache to increase row cache hit rate.\n\nRow cache reuses the LRU Cache mechanism in Apache Doris. When the caching starts, the system will initialize a threshold value. If that threshold is hit, the old cached rows will be phased out. For a primary key query statement, the performance gap between cache hit and cache miss can be huge (we are talking about dozens of times less disk I/O and memory access here). So the introduction of row cache can remarkably enhance point query performance.\n\nFull post link: [https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3](https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3)", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How We Increase Database Query Concurrency by 20 Times", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 67, "top_awarded_type": null, "hide_score": true, "media_metadata": {"40g6ww69d9va1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=243dd96f95fb86e20ced85be8324937398397de6"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d51cfeff1ac8692dd77bf482316569a57ca19cf6"}, {"y": 155, "x": 320, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=608adef3e0df5912de83af4dbe5dd0ba497dd050"}, {"y": 310, "x": 640, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6db26b77609a90c2678d422c74f351d02f4b4f07"}, {"y": 465, "x": 960, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62dbab27cf9bef5156d719cdd0e61cce86d81a98"}, {"y": 523, "x": 1080, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d286a4d8150be3f2c8e484c05889549ab052b95"}], "s": {"y": 778, "x": 1606, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=1606&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ada4306ffe3cc6f9874fba1777d35d32b5c0e1ea"}, "id": "40g6ww69d9va1"}, "v6fvikn8e9va1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 102, "x": 108, "u": "https://preview.redd.it/v6fvikn8e9va1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0892aa7ecebe4ebc62a6ad5974acb7c7f3c72b4"}, {"y": 205, "x": 216, "u": "https://preview.redd.it/v6fvikn8e9va1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f92f965796971b4eaa30217324030fc660f9f76d"}, {"y": 304, "x": 320, "u": "https://preview.redd.it/v6fvikn8e9va1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ba831050cad4be44a2753d6bc35fa766ad94c0e"}], "s": {"y": 540, "x": 568, "u": "https://preview.redd.it/v6fvikn8e9va1.png?width=568&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=833ea248bb86a5af2a2917fc319cd81c7894833f"}, "id": "v6fvikn8e9va1"}, "nwmpb1d2e9va1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 86, "x": 108, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b7c7b5015e9ea0be2fc89a2d64ddf800c0c30c6"}, {"y": 172, "x": 216, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ecdf1be3c197fd3dfad10a23b9afb1df1d10515e"}, {"y": 255, "x": 320, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=334c311f365e0becf291b2c97202d659d68f1515"}, {"y": 510, "x": 640, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65496580bb5c00058cbcf1791532313864f99cff"}, {"y": 765, "x": 960, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a57f03f7187d9710839f1de787b4a0d07795d84c"}, {"y": 860, "x": 1080, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6feef10f64c71454c294184c02c5e5de44753236"}], "s": {"y": 904, "x": 1134, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=1134&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=856b9884206b9fd8628224c1bb5ad30e7cfc5105"}, "id": "nwmpb1d2e9va1"}}, "name": "t3_12u8ske", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4rBvxpZtYRMws1zKv6LL5HGgeOjaGQCesHdLymCRJBs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682092445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most current OLAP databases are built with a columnar storage engine to process huge data volumes. They take pride in their high throughput, but often underperform in high-concurrency scenarios. As a complement, many data engineers invite Key-Value stores like Apache HBase for point queries, and Redis as a cache layer to ease the burden. The downside is redundant storage and high maintenance costs.&lt;/p&gt;\n\n&lt;p&gt;Apache Doris has been striving to become a unified database for data queries of all sizes, including ad-hoc queries and point queries. Till now, we have already taken down the monster of high-throughput OLAP scenarios. In the upcoming Apache Doris 2.0, we have optimized it for high-concurrency point queries. Long story short, it can achieve over 30,000 QPS for a single node (a 20-time increase in concurrency), by methods of &lt;strong&gt;partitioning and bucketing, indexing, materialized view, runtime filter, TOP-N optimization, row storage format, short-circuit, prepared statement, and row storage cache.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;1. Partioning and Bucketing&lt;/h1&gt;\n\n&lt;p&gt;Apache Doris shards data into a two-tiered structure: Partition and Bucket. You can use time information as the Partition Key. As for bucketing, you distribute the data into various nodes after data hashing. A wise bucketing plan can largely increase concurrency and throughput in data reading, because the system only needs to scan one bucket in one partition before it can locate the needed data.&lt;/p&gt;\n\n&lt;h1&gt;2. Index&lt;/h1&gt;\n\n&lt;p&gt;Apache Doris uses various data indexes to speed up data reading and filtering, including smart indexes and secondary indexes. Smart indexes are auto-generated by Doris upon data ingestion, which requires no action from the user\u2019s side.There are two types of smart indexes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Sorted Index&lt;/strong&gt;: Apache Doris stores data in an orderly way. It creates a sorted index for every 1024 rows of data. The Key in the index is the value of the sorted column in the first row of the current 1024 rows. If the query involves the sorted column, the system will locate the first row of the relevant 1024 row group and start scanning there.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;ZoneMap Index&lt;/strong&gt;: These are indexes on the Segment and Page level. The maximum and minimum values of each column within a Page will be recorded, so are those within a Segment. Hence, in equivalence queries and range queries, the system can narrow down the filter range with the help of the MinMax indexes.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Secondary indexes are created by users. These include Bloom Filter indexes, Bitmap indexes, &lt;a href=\"https://doris.apache.org/docs/dev/data-table/index/inverted-index/\"&gt;Inverted indexes&lt;/a&gt;, and &lt;a href=\"https://doris.apache.org/docs/dev/data-table/index/ngram-bloomfilter-index/\"&gt;NGram Bloom Filter indexes&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Example: &lt;code&gt;select * from user_table where id &amp;gt; 10 and id &amp;lt; 1024&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Suppose that the user has designated &lt;code&gt;id&lt;/code&gt; as the Key during table creation, the data will be sorted by &lt;code&gt;id&lt;/code&gt; on Memtable and the disks. So any queries involving &lt;code&gt;id&lt;/code&gt; as a filter condition will be executed much faster with the aid of sorted indexes. Specifically, the data in storage will be put into multiple ranges based on &lt;code&gt;id&lt;/code&gt; , and the system will implement binary search to locate the exact range according to the sorted indexes. But that could still be a large range since the sorted indexes are sparse. You can further narrow it down based on ZoneMap indexes, Bloom Filter indexes, and Bitmap indexes.This is another way to reduce data scanning and improve overall concurrency of the system.&lt;/p&gt;\n\n&lt;h1&gt;3. Materialized View&lt;/h1&gt;\n\n&lt;p&gt;The idea of materialized view is to trade space for time: You execute pre-computation with pre-defined SQL statements, and perpetuate the results in a table that is visible to users but occupies some storage space. In this way, Apache Doris can respond much faster to queries for aggregated data and breakdown data and those involve the matching of sorted indexes once it hits a materialized view. This is a good way to lessen computation, improve query performance, and reduce resource consumption.&lt;/p&gt;\n\n&lt;h1&gt;4. Runtime Filter&lt;/h1&gt;\n\n&lt;p&gt;In multi-table Join queries, the left table is usually called ProbeTable while the right one is called BuildTable, with the former much bigger than the latter. In query execution, firstly, the system reads the right table and creates a HashTable (Build) in the memory. Then, it starts reading the left table row by row, during which it also compares data between the left table and the HashTable and returns the matched data (Probe).&lt;/p&gt;\n\n&lt;p&gt;During the creation of HashTable, Apache Doris generates a filter for the columns. It can be a Min/Max filter or an IN filter. Then it pushes down the filter to the left table, which can use the filter to screen out data and thus reduces the amount of data that the Probe node has to transfer and compare.This is how the Runtime Filter works. In most Join queries, the Runtime Filter can be automatically pushed down to the most underlying scan nodes or to the distributed Shuffle Join. In other words, Runtime Filter is able to reduce data reading and shorten response time for most Join queries.&lt;/p&gt;\n\n&lt;h1&gt;5. TOP-N Optimization&lt;/h1&gt;\n\n&lt;p&gt;TOP-N query is a frequent scenario in data analysis. For example, users want to fetch the most recent 100 orders, or the 5 highest/lowest priced products. For such queries, Apache Doris implements TOP-N optimization:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Apache Doris reads the sorted fields and query fields from the Scanner layer, reserves only the TOP-N pieces of data by means of Heapsort, updates the real-time TOP-N results as it continues reading, and dynamically pushes them down to the Scanner.&lt;/li&gt;\n&lt;li&gt;Combing the received TOP-N range and the indexes, the Scanner can skip a large proportion of irrelevant files and data chunks and only read a small number of rows.&lt;/li&gt;\n&lt;li&gt;Queries on flat tables usually mean the need to scan massive data, but TOP-N queries only retrieve a small amount of data. The strategy here is to divide the data reading process into two stages. In stage one, the system sorts the data based on a few columns (sorted column, or condition column) and locates the TOP-N rows. In stage two, it fetches the TOP-N rows of data after data sorting, and then it retrieves the target data according to the row numbers.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;6. Row Storage Format&lt;/h1&gt;\n\n&lt;p&gt;As we know, row storage is much more efficient when the user only queries for a single row of data. So we introduced row storage format in Apache Doris 2.0. We chose JSONB as the encoding format for row storage for three reasons:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Flexible schema change&lt;/strong&gt;: If a user has added or deleted a field, or modified the type of a field, these changes must be updated in row storage in real time. So we choose to adopt the JSONB format and encode columns into JSONB fields. This makes changes in fields very easy.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: Accessing rows in row-oriented storage is much faster than doing that in columnar storage, and it requires much less disk access in high-concurrency scenarios. Also, in some cases, you can map the column ID to the corresponding JSONB value so you can quickly access a certain column.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Less storage space&lt;/strong&gt;: JSONB is a compacted binary format. It consumes less space on the disk and is more cost-effective.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the storage engine, row storage will be stored as a hidden column (DORIS_ROW_STORE_COL). During Memtable Flush, the columns will be encoded into JSONB and cached into this hidden column. In data reading, the system uses the Column ID to locate the column, finds the target row based on the row number, and then deserializes the relevant columns.&lt;/p&gt;\n\n&lt;h1&gt;7. Short-Circuit&lt;/h1&gt;\n\n&lt;p&gt;Normally, an SQL statement is executed in three steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;SQL Parser parses the statement to generate an abstract syntax tree (AST).&lt;/li&gt;\n&lt;li&gt;The Query Optimizer produces an executable plan.&lt;/li&gt;\n&lt;li&gt;Execute the plan and return the results.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For complex queries on massive data, it is better to follow the plan created by the Query Optimizer. However, for high-concurrency point queries requiring low latency, that plan is not only unnecessary but also brings extra overheads. That\u2019s why we implement a short-circuit plan for point queries.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/40g6ww69d9va1.png?width=1606&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ada4306ffe3cc6f9874fba1777d35d32b5c0e1ea\"&gt;https://preview.redd.it/40g6ww69d9va1.png?width=1606&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ada4306ffe3cc6f9874fba1777d35d32b5c0e1ea&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Once the FE receives a point query request, a short-circuit plan will be produced. It is a lightweight plan that involves no equivalent transformation, logic optimization or physical optimization. Instead, it conducts some basic analysis on the AST, creates a fixed plan accordingly, and finds ways to reduce overhead of the optimizer.&lt;/p&gt;\n\n&lt;p&gt;For a simple point query involving primary keys, such as &lt;code&gt;select * from tbl where pk1 = 123 and pk2 = 456,&lt;/code&gt; since it only involves one single Tablet, it is better to use a lightweight RPC interface for interaction with the Storage Engine. This avoids the creation of a complicated Fragment Plan and eliminates the performance overhead brought by the scheduling under the MPP query framework.&lt;/p&gt;\n\n&lt;p&gt;Details of the RPC interface are as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;message PTabletKeyLookupRequest {\n  required int64 tablet_id = 1;\n  repeated KeyTuple key_tuples = 2;\n  optional Descriptor desc_tbl = 4;\n  optional ExprList  output_expr = 5;\n}\nmessage PTabletKeyLookupResponse {\n  required PStatus status = 1;\n  optional bytes row_batch = 5;\noptional bool\nempty_batch = 6;\n}\nrpc tablet_fetch_data(PTabletKeyLookupRequest) returns (PTabletKeyLookupResponse);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;code&gt;tablet_id&lt;/code&gt; is calculated based on the primary key column, while &lt;code&gt;key_tuples&lt;/code&gt; is the string format of the primary key. In this example, the &lt;code&gt;key_tuples&lt;/code&gt; is similar to [&amp;#39;123&amp;#39;, &amp;#39;456&amp;#39;]. As BE receives the request, &lt;code&gt;key_tuples&lt;/code&gt; will be encoded into primary key storage format. Then, it will locate the corresponding row number of the Key in the Segment File with the help of the primary key index, and check if that row exists in &lt;code&gt;delete bitmap&lt;/code&gt;. If it does, the row number will be returned; if not, the system returns NotFound. The returned row number will be used for point query on &lt;code&gt;__DORIS_ROW_STORE_COL__&lt;/code&gt;. That means we only need to locate one row in that column, fetch the original value of the JSONB format, and deserialize it.&lt;/p&gt;\n\n&lt;h1&gt;8. Prepared Statement&lt;/h1&gt;\n\n&lt;p&gt;The idea of prepared statements is to cache precomputed SQL and expressions in HashMap in memory, so they can be directly used in queries when applicable.&lt;/p&gt;\n\n&lt;p&gt;Prepared statements adopt MySQL binary protocol for transmission. The protocol is implemented in the mysql_row_buffer.[h|cpp] file, and uses MySQL binary encoding. Under this protocol, the client (for example, JDBC Client) sends a pre-compiled statement to FE via &lt;code&gt;PREPARE&lt;/code&gt; MySQL Command. Next, FE will parse and analyze the statement and cache it in the HashMap as shown in the figure above. Next, the client, using &lt;code&gt;EXECUTE&lt;/code&gt; MySQL Command, will replace the placeholder, encode it into binary format, and send it to FE. Then, FE will perform deserialization to obtain the value of the placeholder, and generate query conditions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nwmpb1d2e9va1.png?width=1134&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=856b9884206b9fd8628224c1bb5ad30e7cfc5105\"&gt;https://preview.redd.it/nwmpb1d2e9va1.png?width=1134&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=856b9884206b9fd8628224c1bb5ad30e7cfc5105&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Apart from caching prepared statements in FE, we also cache reusable structures in BE. These structures include pre-allocated computation blocks, query descriptors, and output expressions. Serializing and deserializing these structures often cause a CPU hotspot, so it makes more sense to cache them. The prepared statement for each query comes with a UUID named CacheID. So when BE executes the point query, it will find the corresponding class based on the CacheID, and then reuse the structure in computation.&lt;/p&gt;\n\n&lt;h1&gt;9. Row Storage Cache&lt;/h1&gt;\n\n&lt;p&gt;Apache Doris has a Page Cache feature, where each page caches the data of one column. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v6fvikn8e9va1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=833ea248bb86a5af2a2917fc319cd81c7894833f\"&gt;https://preview.redd.it/v6fvikn8e9va1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=833ea248bb86a5af2a2917fc319cd81c7894833f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As mentioned above, we have introduced row storage in Doris. The problem with this is, one row of data consists of multiple columns, so in the case of big queries, the cached data might be erased. Thus, we also introduced row cache to increase row cache hit rate.&lt;/p&gt;\n\n&lt;p&gt;Row cache reuses the LRU Cache mechanism in Apache Doris. When the caching starts, the system will initialize a threshold value. If that threshold is hit, the old cached rows will be phased out. For a primary key query statement, the performance gap between cache hit and cache miss can be huge (we are talking about dozens of times less disk I/O and memory access here). So the introduction of row cache can remarkably enhance point query performance.&lt;/p&gt;\n\n&lt;p&gt;Full post link: &lt;a href=\"https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3\"&gt;https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12u8ske", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u8ske/how_we_increase_database_query_concurrency_by_20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u8ske/how_we_increase_database_query_concurrency_by_20/", "subreddit_subscribers": 101166, "created_utc": 1682092445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, our database is deployed on AWS RDS in a VPC and can only be accessed via bastion and ssh. \n\nCurrently, we run migrations manually. \n\nHow do you do this on a CI/CD pipeline?", "author_fullname": "t2_6542em0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to run DB migrations in CICD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12u6y53", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682090331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, our database is deployed on AWS RDS in a VPC and can only be accessed via bastion and ssh. &lt;/p&gt;\n\n&lt;p&gt;Currently, we run migrations manually. &lt;/p&gt;\n\n&lt;p&gt;How do you do this on a CI/CD pipeline?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12u6y53", "is_robot_indexable": true, "report_reasons": null, "author": "EngrRhys", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u6y53/how_to_run_db_migrations_in_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u6y53/how_to_run_db_migrations_in_cicd/", "subreddit_subscribers": 101166, "created_utc": 1682090331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data folk: do you use the [write-audit-publish (WAP) pattern](https://www.youtube.com/watch?v=fXHdeBnpXrg&amp;t=1001s) in your pipelines? \n\nIt seems like a great idea\u2014and Netflix has been using it for years\u2014but I'm curious if it's more widely adopted than that. Both Apache Iceberg and Apache Hudi support it. \n\nWould also love to hear comments on how you've done it, successes, problems - and also reasons why you haven't, etc \ud83d\udc47\ud83c\udffb\n\n[View Poll](https://www.reddit.com/poll/12u6oqk)", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do *you* use the Write-Audit-Publish (WAP) pattern in your data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12u6oqk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682090100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data folk: do you use the &lt;a href=\"https://www.youtube.com/watch?v=fXHdeBnpXrg&amp;amp;t=1001s\"&gt;write-audit-publish (WAP) pattern&lt;/a&gt; in your pipelines? &lt;/p&gt;\n\n&lt;p&gt;It seems like a great idea\u2014and Netflix has been using it for years\u2014but I&amp;#39;m curious if it&amp;#39;s more widely adopted than that. Both Apache Iceberg and Apache Hudi support it. &lt;/p&gt;\n\n&lt;p&gt;Would also love to hear comments on how you&amp;#39;ve done it, successes, problems - and also reasons why you haven&amp;#39;t, etc \ud83d\udc47\ud83c\udffb&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/12u6oqk\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?auto=webp&amp;v=enabled&amp;s=e636ce7d35a6ffe63bd8d2bcefd07cf7d55c6392", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1257c3e138f6335e24a1a4cee0d488d27364532d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=078eeabf7b25c2fe02e02714669f3b6cda833417", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5282a32789acaac7bd0c2567442756e2aeb3bcc0", "width": 320, "height": 240}], "variants": {}, "id": "YzD2yhQk3wTSSmdUqerRI6LFu8Dk_iCDwBZvhHo8aek"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12u6oqk", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1682349300982, "options": [{"text": "\u2705 Yep!", "id": "22673764"}, {"text": "\ud83d\ude45\ud83c\udffb Nope", "id": "22673765"}, {"text": "\ud83e\udd14 What even is WAP?", "id": "22673766"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 20, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u6oqk/do_you_use_the_writeauditpublish_wap_pattern_in/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/12u6oqk/do_you_use_the_writeauditpublish_wap_pattern_in/", "subreddit_subscribers": 101166, "created_utc": 1682090100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm kind of an accidental DE. I'd worked various reporting/BI roles for years and then after a restructure I ended up primarily working on data integrations and migrations as my company moved from on-prem SQL Servers over to Cloud solutions. Our basic tech stack is AWS(S3), Informatica (CAI/CDI) and Snowflake.\n\n I actually reaslly enjoy my job, and would like to think that a future career as a DE would be possible (intentionally this time!), but my current team is fairly low code. Our primary pipeline is a shared process which handles most of the heavy lifting for any new ingestion. Most of my work involves bringing in new data sources, typically from APIs (but also from SQL, SFTP, etc). It's functional and it works, but I'm interested from a personal development perspective of how else we could do things.\n\nI'm reasonably confident with Python and we have access to Databricks in my company (as well as access to the free Databricks Academy, for which i've just enrolled in the Data Engineer plan). \n\nI guess what im trying to find out is where i could be/should be using Databricks in my workflow/data pipelines? I see a lot of posts on this subreddit around Databricks, and felt like i should use some periods of downtime at work to get some Databricks accreditations, but it would be good if there were some practical use cases.\n\nApologies if this is a bit of a non-question. More generally, feedback on how others are using Databricks in their data pipelines might help me understand its place a bit more!", "author_fullname": "t2_4smhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to use Databricks in my workflow (Novice DE)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12txryi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682074118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m kind of an accidental DE. I&amp;#39;d worked various reporting/BI roles for years and then after a restructure I ended up primarily working on data integrations and migrations as my company moved from on-prem SQL Servers over to Cloud solutions. Our basic tech stack is AWS(S3), Informatica (CAI/CDI) and Snowflake.&lt;/p&gt;\n\n&lt;p&gt;I actually reaslly enjoy my job, and would like to think that a future career as a DE would be possible (intentionally this time!), but my current team is fairly low code. Our primary pipeline is a shared process which handles most of the heavy lifting for any new ingestion. Most of my work involves bringing in new data sources, typically from APIs (but also from SQL, SFTP, etc). It&amp;#39;s functional and it works, but I&amp;#39;m interested from a personal development perspective of how else we could do things.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m reasonably confident with Python and we have access to Databricks in my company (as well as access to the free Databricks Academy, for which i&amp;#39;ve just enrolled in the Data Engineer plan). &lt;/p&gt;\n\n&lt;p&gt;I guess what im trying to find out is where i could be/should be using Databricks in my workflow/data pipelines? I see a lot of posts on this subreddit around Databricks, and felt like i should use some periods of downtime at work to get some Databricks accreditations, but it would be good if there were some practical use cases.&lt;/p&gt;\n\n&lt;p&gt;Apologies if this is a bit of a non-question. More generally, feedback on how others are using Databricks in their data pipelines might help me understand its place a bit more!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12txryi", "is_robot_indexable": true, "report_reasons": null, "author": "andyby2k26", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12txryi/where_to_use_databricks_in_my_workflow_novice_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12txryi/where_to_use_databricks_in_my_workflow_novice_de/", "subreddit_subscribers": 101166, "created_utc": 1682074118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All. I'm planning to get Azure DP-203 DE Associate certification. I have started the preparations only this week.\n\nThough I have a doubt, I haven't got the DP-900 certification, and I'm going for DP-203 straight away. Is it recommended to take the exams in order?  \n\n\nI have experience with AWS from my previous job", "author_fullname": "t2_w4mta741", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance for Azure certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tu6uq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682065299.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682062959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All. I&amp;#39;m planning to get Azure DP-203 DE Associate certification. I have started the preparations only this week.&lt;/p&gt;\n\n&lt;p&gt;Though I have a doubt, I haven&amp;#39;t got the DP-900 certification, and I&amp;#39;m going for DP-203 straight away. Is it recommended to take the exams in order?  &lt;/p&gt;\n\n&lt;p&gt;I have experience with AWS from my previous job&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tu6uq", "is_robot_indexable": true, "report_reasons": null, "author": "Ketonium10", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tu6uq/guidance_for_azure_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tu6uq/guidance_for_azure_certification/", "subreddit_subscribers": 101166, "created_utc": 1682062959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently I have been hearing a lot about AI and the implementation of Generative AI tools in software engineering, some of my friends from non-tech backgrounds have been speaking about it(mostly ChatGpt).\n\n  \nI want to create this post to ask data engineers here to ask If you have used any such tools at work, and if so what are they and how they are being useful.\n\nAlso, what are the AI tools good to have in a data engineer's portfolio?", "author_fullname": "t2_nie4cn9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AI Tools in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tqvgg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682053032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently I have been hearing a lot about AI and the implementation of Generative AI tools in software engineering, some of my friends from non-tech backgrounds have been speaking about it(mostly ChatGpt).&lt;/p&gt;\n\n&lt;p&gt;I want to create this post to ask data engineers here to ask If you have used any such tools at work, and if so what are they and how they are being useful.&lt;/p&gt;\n\n&lt;p&gt;Also, what are the AI tools good to have in a data engineer&amp;#39;s portfolio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12tqvgg", "is_robot_indexable": true, "report_reasons": null, "author": "sds66", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tqvgg/ai_tools_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tqvgg/ai_tools_in_data_engineering/", "subreddit_subscribers": 101166, "created_utc": 1682053032.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a use case to store (ingest) telemetry data that is formatted as json.  Each message is about 1K.  Data comes in over Kafka.  When the data is stored, I need to query the data (I can use SQL or NoSQL, I am flexible in the interface).\n\nMy architect is asking me to store this data into a mongo cluster.  From what I understand, Druid does the same thing as well.  What do you guys recommend?  What are the pros and cons?", "author_fullname": "t2_bluzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache druid vs Mongo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlwyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682040214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a use case to store (ingest) telemetry data that is formatted as json.  Each message is about 1K.  Data comes in over Kafka.  When the data is stored, I need to query the data (I can use SQL or NoSQL, I am flexible in the interface).&lt;/p&gt;\n\n&lt;p&gt;My architect is asking me to store this data into a mongo cluster.  From what I understand, Druid does the same thing as well.  What do you guys recommend?  What are the pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tlwyl", "is_robot_indexable": true, "report_reasons": null, "author": "Beertimeanytime", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tlwyl/apache_druid_vs_mongo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tlwyl/apache_druid_vs_mongo/", "subreddit_subscribers": 101166, "created_utc": 1682040214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to run a web scraping python script that generates json files on EC2 instance then connect the instance to Kinesis.\n\nI can't wrap my head on how to do this. \n\nI need some resources that could help me do this. Something like a guided project for example. \n\nI need some help please.", "author_fullname": "t2_1f15w5lz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EC2 with Kinesis Data streams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlhsq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682039242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to run a web scraping python script that generates json files on EC2 instance then connect the instance to Kinesis.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t wrap my head on how to do this. &lt;/p&gt;\n\n&lt;p&gt;I need some resources that could help me do this. Something like a guided project for example. &lt;/p&gt;\n\n&lt;p&gt;I need some help please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tlhsq", "is_robot_indexable": true, "report_reasons": null, "author": "I-am_Not_Sure", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tlhsq/ec2_with_kinesis_data_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tlhsq/ec2_with_kinesis_data_streams/", "subreddit_subscribers": 101166, "created_utc": 1682039242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone worked with the \"Operational Data Layer\" architecture? The idea is to have an offload layer of mainframe data in the cloud for transactional consumption via APIs, not for analytical.\n\nRef: https://www.mongodb.com/collateral/implementing-an-operational-data-layer", "author_fullname": "t2_z4ea7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Operational Data Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tj6ly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682033804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone worked with the &amp;quot;Operational Data Layer&amp;quot; architecture? The idea is to have an offload layer of mainframe data in the cloud for transactional consumption via APIs, not for analytical.&lt;/p&gt;\n\n&lt;p&gt;Ref: &lt;a href=\"https://www.mongodb.com/collateral/implementing-an-operational-data-layer\"&gt;https://www.mongodb.com/collateral/implementing-an-operational-data-layer&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?auto=webp&amp;v=enabled&amp;s=4d0e5122de6d4ad87574f5c31c130c2f0816e5ea", "width": 1200, "height": 601}, "resolutions": [{"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e50bf9f8ae0b10c2550f9ee3eac6b6788e725d58", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22acd42800e34b60c780f8e454a502f68a5e1b80", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62ee96dc5c7ee55560cb8e18e03dc7954f7d9e3c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78985a472e163338ceefd97e75a5fc3e2e6937bd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73b969be08b7028fdd476f5e48e8cd8b0d712cc9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f989646a090537bac16869c3f68e36c658baa0b0", "width": 1080, "height": 540}], "variants": {}, "id": "9cMT5a0hMRy5J7cG3lurHYe3JYa222Qg02VNunMDa60"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12tj6ly", "is_robot_indexable": true, "report_reasons": null, "author": "aleebit", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tj6ly/operational_data_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tj6ly/operational_data_layer/", "subreddit_subscribers": 101166, "created_utc": 1682033804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, this is my first post here and I'm a little bit excited.\n\nWhen I first started working with column-oriented DBMS ClickHouse, I struggled to find a tool (other than the one built into the [ClickHouse Cloud](https://clickhouse.cloud/) web-UI) that would create the necessary table with the required columns and data types based on a CSV file or Pandas dataframe. Neither clickhouse-connect nor clickhouse-driver had this functionality, such as the `to_sql` method in SQLAlchemy.\n\nI wanted to load my favorite dataset of open-wheel Formula 1 racing world championship results into ClickHouse, but manually creating 15 tables was too time-consuming.\n\nWhen I previously familiared with PySpark,  I noticed that many data professionals use Pandas to define the data  schema before loading CSV files into PySpark. And I thought, why not use Pandas to define the data types by columns?\n\nThis is how [this script](https://github.com/pvl-k/csv2clickhouse) was born, which I want to share. I hope it saves you some time, and it will give me the opportunity to receive a couple of feedbacks and ideas from you for improvement.\n\nI'm not sure about the complete compatibility of data types between Pandas and ClickHouse: quick research gave conflicting results, so please correct me if you find any discrepancies.\n\nAnd be careful with the `replace_flag` \\- when set to True, the script will recreate tables with the same name if they already exist, so you may lose existing data in your database. To avoid this, but also prevent data duplication, I recommend specifying a non-existent database name as the `database_name`. When set to False in the `replace_flag`, data from your CSV files will be added to existing tables with the same name (of course, the number of columns and their data types must match).", "author_fullname": "t2_hmsnetde", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fast way to upload several CSV files -&gt; ClickHouse with create a database and tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tfq8h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682026033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, this is my first post here and I&amp;#39;m a little bit excited.&lt;/p&gt;\n\n&lt;p&gt;When I first started working with column-oriented DBMS ClickHouse, I struggled to find a tool (other than the one built into the &lt;a href=\"https://clickhouse.cloud/\"&gt;ClickHouse Cloud&lt;/a&gt; web-UI) that would create the necessary table with the required columns and data types based on a CSV file or Pandas dataframe. Neither clickhouse-connect nor clickhouse-driver had this functionality, such as the &lt;code&gt;to_sql&lt;/code&gt; method in SQLAlchemy.&lt;/p&gt;\n\n&lt;p&gt;I wanted to load my favorite dataset of open-wheel Formula 1 racing world championship results into ClickHouse, but manually creating 15 tables was too time-consuming.&lt;/p&gt;\n\n&lt;p&gt;When I previously familiared with PySpark,  I noticed that many data professionals use Pandas to define the data  schema before loading CSV files into PySpark. And I thought, why not use Pandas to define the data types by columns?&lt;/p&gt;\n\n&lt;p&gt;This is how &lt;a href=\"https://github.com/pvl-k/csv2clickhouse\"&gt;this script&lt;/a&gt; was born, which I want to share. I hope it saves you some time, and it will give me the opportunity to receive a couple of feedbacks and ideas from you for improvement.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure about the complete compatibility of data types between Pandas and ClickHouse: quick research gave conflicting results, so please correct me if you find any discrepancies.&lt;/p&gt;\n\n&lt;p&gt;And be careful with the &lt;code&gt;replace_flag&lt;/code&gt; - when set to True, the script will recreate tables with the same name if they already exist, so you may lose existing data in your database. To avoid this, but also prevent data duplication, I recommend specifying a non-existent database name as the &lt;code&gt;database_name&lt;/code&gt;. When set to False in the &lt;code&gt;replace_flag&lt;/code&gt;, data from your CSV files will be added to existing tables with the same name (of course, the number of columns and their data types must match).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12tfq8h", "is_robot_indexable": true, "report_reasons": null, "author": "Pavel_K0", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tfq8h/fast_way_to_upload_several_csv_files_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tfq8h/fast_way_to_upload_several_csv_files_clickhouse/", "subreddit_subscribers": 101166, "created_utc": 1682026033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working with S3 and my team has just tasked me with figuring out how to track versioning of the objects. For example, say that one of the files has a monthly upload, but sometimes those monthly uploads are changed with a new version. So we want the latest version of the \"January\" upload. \n\nI'm looking at the Data Cataglog that comes with S3 and writing metadata when the object is uploaded but there doesn't seem to be any clear cut resources to implement the following: \n\n1) Create an entry in a data catalog with the month upload and version.   \n2) Pull the object  URI from the data catalog based a query of month and latest version  \n3) Bonus points if when a new version is uploaded it can alert downstream consumers (email?) that a new version has been uploaded\n\nApologize if this is a simple ask and/or it is stated poorly. I'm very new to AWS.", "author_fullname": "t2_1yn5o9p2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracking Objects in a Data Lake - Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12te2as", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682022430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working with S3 and my team has just tasked me with figuring out how to track versioning of the objects. For example, say that one of the files has a monthly upload, but sometimes those monthly uploads are changed with a new version. So we want the latest version of the &amp;quot;January&amp;quot; upload. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking at the Data Cataglog that comes with S3 and writing metadata when the object is uploaded but there doesn&amp;#39;t seem to be any clear cut resources to implement the following: &lt;/p&gt;\n\n&lt;p&gt;1) Create an entry in a data catalog with the month upload and version.&lt;br/&gt;\n2) Pull the object  URI from the data catalog based a query of month and latest version&lt;br/&gt;\n3) Bonus points if when a new version is uploaded it can alert downstream consumers (email?) that a new version has been uploaded&lt;/p&gt;\n\n&lt;p&gt;Apologize if this is a simple ask and/or it is stated poorly. I&amp;#39;m very new to AWS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12te2as", "is_robot_indexable": true, "report_reasons": null, "author": "10002Hours", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12te2as/tracking_objects_in_a_data_lake_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12te2as/tracking_objects_in_a_data_lake_help/", "subreddit_subscribers": 101166, "created_utc": 1682022430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone -\n\n&amp;#x200B;\n\ni am running a pipeline that consumes data from continously scheduled api calls, returning json files. These json files are written as rawdata to a datalake, where they get picked up by a script that does some transformations and writes them out as parquet files. This happens on a per file basis, i.e.\n\ndata1001.json -&gt; data1001\\_00000.parquet\n\ndata1002.json -&gt; data1002\\_00000.parquet\n\netc.\n\nSince there are plenty of small files, this results in a bit of an IO mess, as doing it this way results in lots and lots of rather small parquet files. Therefore i have added a subsequent spark job here that reads all the files, repartions them (by a 'type' and 'product' column present in the data) and writes them back to the datalake, now with fewer files but larger file size. In order to keep these repartitioned files up to date, this spark job is run daily and processes the entire dataset.\n\nI was wondering if there was a more effective pipeline architecture that doesn't require re-loading the entire dataset into spark every day, repartitioning it and writing it back to the datalake, but rather performs some sort of upsert, to add new entries and update potentially changed rows", "author_fullname": "t2_11cqnxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "partitioned parquet files upserts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tc6mm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682018492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone -&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;i am running a pipeline that consumes data from continously scheduled api calls, returning json files. These json files are written as rawdata to a datalake, where they get picked up by a script that does some transformations and writes them out as parquet files. This happens on a per file basis, i.e.&lt;/p&gt;\n\n&lt;p&gt;data1001.json -&amp;gt; data1001_00000.parquet&lt;/p&gt;\n\n&lt;p&gt;data1002.json -&amp;gt; data1002_00000.parquet&lt;/p&gt;\n\n&lt;p&gt;etc.&lt;/p&gt;\n\n&lt;p&gt;Since there are plenty of small files, this results in a bit of an IO mess, as doing it this way results in lots and lots of rather small parquet files. Therefore i have added a subsequent spark job here that reads all the files, repartions them (by a &amp;#39;type&amp;#39; and &amp;#39;product&amp;#39; column present in the data) and writes them back to the datalake, now with fewer files but larger file size. In order to keep these repartitioned files up to date, this spark job is run daily and processes the entire dataset.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there was a more effective pipeline architecture that doesn&amp;#39;t require re-loading the entire dataset into spark every day, repartitioning it and writing it back to the datalake, but rather performs some sort of upsert, to add new entries and update potentially changed rows&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tc6mm", "is_robot_indexable": true, "report_reasons": null, "author": "nihi_", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tc6mm/partitioned_parquet_files_upserts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tc6mm/partitioned_parquet_files_upserts/", "subreddit_subscribers": 101166, "created_utc": 1682018492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nFYI, my systems are hosted at AWS (our data is mostly stored in RDS MySQL). However, we are considered small-scaled in GB. I expect my database to grow to 500GB - 1TB (part of data migration).\n\nCurrently, running specific reporting queries on RDS is slow, despite optimisation done. Can't imagine after the data migration, probably worst.\n\nI heard data warehouse may be able to solve my issue.\n\nI need to work on \n\na. data analytics for visualisation (most likely will use quicksight)\n\nb. near real-time dashboard (transactions related e.g. total orders today incremental)\n\nc. reporting (wonder my reports can pull from data warehouse, instead of RDS)\n\nd. AI-related (this is more like a future)\n\nPriority is a, b and c. I do not want to hit my RDS and slow it down (though I think RDS replica may solve this, but slow query is unavoidable i think).\n\nnote: though most of my data resides in RDS, I am also exploring kinesis data stream and maybe some data files stored in S3. \n\nI noticed this article recommended not to use Redshift (and somewhere i read, not mistaken if my data is less than 5TB, no point I touch Redshift)\n\nhttps://www.reddit.com/r/dataengineering/comments/12rueol/what\\_is\\_a\\_good\\_resource\\_to\\_learn\\_how\\_to\\_set\\_up\\_a/\n\nI tried Athena. I think Athena is more like query csv or parquet files in S3. Not sure it can query RDS database directly or store in columnar format for fast retrieval. But I don't think Athena is data warehouse.\n\nI also heard good stuff about BigQuery. But I am not sure transferring data from AWS RDS to BigQuery, is a good idea. Bigquery omni doesn't work for me (which also quite expensive), as my data resides in AWS ap-southeast-1. I think some don't recommend multi cloud.\n\nIf I want to retain in AWS, which can help me to achieve a, b c and d, which AWS service is suitable for me? Or should I consider BigQuery for my data warehouse. \n\nNote: I am not interested in Snowflake or Databricks at this moment. \n\nAny help? Thanks.", "author_fullname": "t2_adysoxz3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to Data Warehouse (currently using AWS), which solution works best for small-scale setup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u0hmw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682080914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;FYI, my systems are hosted at AWS (our data is mostly stored in RDS MySQL). However, we are considered small-scaled in GB. I expect my database to grow to 500GB - 1TB (part of data migration).&lt;/p&gt;\n\n&lt;p&gt;Currently, running specific reporting queries on RDS is slow, despite optimisation done. Can&amp;#39;t imagine after the data migration, probably worst.&lt;/p&gt;\n\n&lt;p&gt;I heard data warehouse may be able to solve my issue.&lt;/p&gt;\n\n&lt;p&gt;I need to work on &lt;/p&gt;\n\n&lt;p&gt;a. data analytics for visualisation (most likely will use quicksight)&lt;/p&gt;\n\n&lt;p&gt;b. near real-time dashboard (transactions related e.g. total orders today incremental)&lt;/p&gt;\n\n&lt;p&gt;c. reporting (wonder my reports can pull from data warehouse, instead of RDS)&lt;/p&gt;\n\n&lt;p&gt;d. AI-related (this is more like a future)&lt;/p&gt;\n\n&lt;p&gt;Priority is a, b and c. I do not want to hit my RDS and slow it down (though I think RDS replica may solve this, but slow query is unavoidable i think).&lt;/p&gt;\n\n&lt;p&gt;note: though most of my data resides in RDS, I am also exploring kinesis data stream and maybe some data files stored in S3. &lt;/p&gt;\n\n&lt;p&gt;I noticed this article recommended not to use Redshift (and somewhere i read, not mistaken if my data is less than 5TB, no point I touch Redshift)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/12rueol/what%5C_is%5C_a%5C_good%5C_resource%5C_to%5C_learn%5C_how%5C_to%5C_set%5C_up%5C_a/\"&gt;https://www.reddit.com/r/dataengineering/comments/12rueol/what\\_is\\_a\\_good\\_resource\\_to\\_learn\\_how\\_to\\_set\\_up\\_a/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I tried Athena. I think Athena is more like query csv or parquet files in S3. Not sure it can query RDS database directly or store in columnar format for fast retrieval. But I don&amp;#39;t think Athena is data warehouse.&lt;/p&gt;\n\n&lt;p&gt;I also heard good stuff about BigQuery. But I am not sure transferring data from AWS RDS to BigQuery, is a good idea. Bigquery omni doesn&amp;#39;t work for me (which also quite expensive), as my data resides in AWS ap-southeast-1. I think some don&amp;#39;t recommend multi cloud.&lt;/p&gt;\n\n&lt;p&gt;If I want to retain in AWS, which can help me to achieve a, b c and d, which AWS service is suitable for me? Or should I consider BigQuery for my data warehouse. &lt;/p&gt;\n\n&lt;p&gt;Note: I am not interested in Snowflake or Databricks at this moment. &lt;/p&gt;\n\n&lt;p&gt;Any help? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12u0hmw", "is_robot_indexable": true, "report_reasons": null, "author": "ericchuawc", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u0hmw/new_to_data_warehouse_currently_using_aws_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u0hmw/new_to_data_warehouse_currently_using_aws_which/", "subreddit_subscribers": 101166, "created_utc": 1682080914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title. \n\nLooking for an alternative for SQL Developer for MacOS Ventura. Something not clunky, free, that has dark mode available, and supports multiple connections for Oracle DB\u2019s.", "author_fullname": "t2_4ehxr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle SQL Developer alternative for MacOS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u0f1d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682080744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. &lt;/p&gt;\n\n&lt;p&gt;Looking for an alternative for SQL Developer for MacOS Ventura. Something not clunky, free, that has dark mode available, and supports multiple connections for Oracle DB\u2019s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12u0f1d", "is_robot_indexable": true, "report_reasons": null, "author": "thenyx", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u0f1d/oracle_sql_developer_alternative_for_macos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u0f1d/oracle_sql_developer_alternative_for_macos/", "subreddit_subscribers": 101166, "created_utc": 1682080744.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}