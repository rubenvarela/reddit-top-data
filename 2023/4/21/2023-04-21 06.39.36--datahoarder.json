{"kind": "Listing", "data": {"after": "t3_12th5q2", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_6kz4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Office Depot offered me a free charcoal grill with my hard drive purchase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12tipvp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 894, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 894, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Wqhy-NagOiu23poPEIEIhnyH2qIBE-yveYILc2jSgMA.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682032730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/nRjYJXN.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?auto=webp&amp;v=enabled&amp;s=877a9a18b8b50d040663871def4a1421df6e6386", "width": 3264, "height": 2448}, "resolutions": [{"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73051bbc5204949741714b34cb4b3b04f7590406", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f2b9690cc37062aef73886911d0f1e42ace0489", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccca79086fb20fe136388167e21bf0d06de5a397", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e6b22c4eb4c8459362e65f071bffd24eceb87db0", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09732ba25ca2f59740344f35746feec4199a9026", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bed1c49de8987d7410fdc80006b433cd17bc39a8", "width": 1080, "height": 810}], "variants": {}, "id": "Jo_AoGZNLMYWGVFreZ_a7VwlLo7DvuTcr0I0Xi7tqb8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1/10 PB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tipvp", "is_robot_indexable": true, "report_reasons": null, "author": "mrtramplefoot", "discussion_type": null, "num_comments": 64, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12tipvp/office_depot_offered_me_a_free_charcoal_grill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/nRjYJXN.jpg", "subreddit_subscribers": 678737, "created_utc": 1682032730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Imgur is updating their TOS on May 15, 2023: All NSFW content to be banned, all content outside of a registered account (no-account uploads) to be removed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sq7ip", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 394, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_of987", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 394, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "apolloapp", "selftext": "", "author_fullname": "t2_5cymt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Imgur is updating their TOS on May 15, 2023: All NSFW content to be banned, all content outside of a registered account (no-account uploads) to be removed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/apolloapp", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sjeo5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1190, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Announcement \ud83d\udce3", "can_mod_post": false, "score": 1190, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681958916.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgurinc.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgurinc.com/rules", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1ba6dd68-f254-11eb-b308-5621ae2f972b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_363lq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12sjeo5", "is_robot_indexable": true, "report_reasons": null, "author": "rekabis", "discussion_type": null, "num_comments": 278, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/apolloapp/comments/12sjeo5/imgur_is_updating_their_tos_on_may_15_2023_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgurinc.com/rules", "subreddit_subscribers": 767244, "created_utc": 1681958916.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1681976321.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgurinc.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgurinc.com/rules", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12sq7ip", "is_robot_indexable": true, "report_reasons": null, "author": "GubmintTroll", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12sjeo5", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sq7ip/imgur_is_updating_their_tos_on_may_15_2023_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgurinc.com/rules", "subreddit_subscribers": 678737, "created_utc": 1681976321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_yaxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shutting down Legit Torrents after 17 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sy755", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 228, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 228, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681996304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "legittorrents.info", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.legittorrents.info/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12sy755", "is_robot_indexable": true, "report_reasons": null, "author": "en3r0", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sy755/shutting_down_legit_torrents_after_17_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.legittorrents.info/", "subreddit_subscribers": 678737, "created_utc": 1681996304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We have to prepare ourselves for the possibility that Reddit might try to pull a Tumblr soon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sqpn6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 154, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_udbds5yu", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 154, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Archiveteam", "selftext": "Reddit has been quietly putting restrictions on NSFW subs for a while now. You can't upload videos in then anymore. You can't use Reddit's native image host in them anymore. They won't show up on [r/all](https://www.reddit.com/r/all) anymore. And now, with the upcoming API change, you soon won't be able to access them through 3rd party tools anymore. \n\nAll of this sounds to me a lot like foreplay. I wouldn't be surprised if, soon, Reddit simply pulls the plug on NSFW content altogether. Just look at what they're doing to Imgur, which might I remind you is owned by many of the same people who own Reddit. \n\nReddit, as it stands, is a relic of an older age of the Internet. Far less sanitized than other social media sites. And far less marketable. For the upcoming IPO, they want Reddit to be clean, squeaky clean.", "author_fullname": "t2_udbds5yu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We have to prepare ourselves for the possibility that Reddit might try to pull a Tumblr soon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Archiveteam", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sqopw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 133, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 133, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681977696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Archiveteam", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reddit has been quietly putting restrictions on NSFW subs for a while now. You can&amp;#39;t upload videos in then anymore. You can&amp;#39;t use Reddit&amp;#39;s native image host in them anymore. They won&amp;#39;t show up on &lt;a href=\"https://www.reddit.com/r/all\"&gt;r/all&lt;/a&gt; anymore. And now, with the upcoming API change, you soon won&amp;#39;t be able to access them through 3rd party tools anymore. &lt;/p&gt;\n\n&lt;p&gt;All of this sounds to me a lot like foreplay. I wouldn&amp;#39;t be surprised if, soon, Reddit simply pulls the plug on NSFW content altogether. Just look at what they&amp;#39;re doing to Imgur, which might I remind you is owned by many of the same people who own Reddit. &lt;/p&gt;\n\n&lt;p&gt;Reddit, as it stands, is a relic of an older age of the Internet. Far less sanitized than other social media sites. And far less marketable. For the upcoming IPO, they want Reddit to be clean, squeaky clean.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sug0", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12sqopw", "is_robot_indexable": true, "report_reasons": null, "author": "I_got_too_silly", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Archiveteam/comments/12sqopw/we_have_to_prepare_ourselves_for_the_possibility/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/Archiveteam/comments/12sqopw/we_have_to_prepare_ourselves_for_the_possibility/", "subreddit_subscribers": 11862, "created_utc": 1681977696.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1681977769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Archiveteam", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/Archiveteam/comments/12sqopw/we_have_to_prepare_ourselves_for_the_possibility/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12sqpn6", "is_robot_indexable": true, "report_reasons": null, "author": "I_got_too_silly", "discussion_type": null, "num_comments": 63, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12sqopw", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sqpn6/we_have_to_prepare_ourselves_for_the_possibility/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/Archiveteam/comments/12sqopw/we_have_to_prepare_ourselves_for_the_possibility/", "subreddit_subscribers": 678737, "created_utc": 1681977769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_3qve30kb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "reddit-img-dl: Tool to download media and comments from subreddits or reddit users. Its a fork of the The-Eye-Team/reddit-dl with additional features. I worked on this afew years ago, but it might be useful to the people here with the recent reddit and imgur changes.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12t4ohz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682004665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/IceWreck/reddit-img-dl", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12t4ohz", "is_robot_indexable": true, "report_reasons": null, "author": "HulkaBurninFudge", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12t4ohz/redditimgdl_tool_to_download_media_and_comments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/IceWreck/reddit-img-dl", "subreddit_subscribers": 678737, "created_utc": 1682004665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have over 100 URLs of bookmarks I\u2019ve collected over the years saved in a file. My question is if I wanted to permanently save the actual pages themselves (articles, essays, blog posts etc) what would be the best and easiest way to do it?\n\nIs making a PDF or an HTML file of the page in question sufficient?", "author_fullname": "t2_gfj7b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to save a webpage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12t6c63", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1682009102.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682006868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have over 100 URLs of bookmarks I\u2019ve collected over the years saved in a file. My question is if I wanted to permanently save the actual pages themselves (articles, essays, blog posts etc) what would be the best and easiest way to do it?&lt;/p&gt;\n\n&lt;p&gt;Is making a PDF or an HTML file of the page in question sufficient?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12t6c63", "is_robot_indexable": true, "report_reasons": null, "author": "Cmyers1980", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12t6c63/whats_the_best_way_to_save_a_webpage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12t6c63/whats_the_best_way_to_save_a_webpage/", "subreddit_subscribers": 678737, "created_utc": 1682006868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm merging dashcam files using bandicam on an HDD that has 400gb of free space. The output file is under 100gb - but I'm getting an error 70% of the way through saying \"not enough disk space on output folder\".\n\nWhat could be the reason for this?", "author_fullname": "t2_n7q75", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Not enough disk space, please change output folder\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12td88j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682020650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m merging dashcam files using bandicam on an HDD that has 400gb of free space. The output file is under 100gb - but I&amp;#39;m getting an error 70% of the way through saying &amp;quot;not enough disk space on output folder&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What could be the reason for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12td88j", "is_robot_indexable": true, "report_reasons": null, "author": "StarSurf", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12td88j/not_enough_disk_space_please_change_output_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12td88j/not_enough_disk_space_please_change_output_folder/", "subreddit_subscribers": 678737, "created_utc": 1682020650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone. I need help downloading multiple streams from a music festival starting tomorrow. There are 5 different streams all going on for the same time and last about 8-9 hours each. I tried testing some livestreams from other channels on YoutubeDL Material, but it gives me an error. Also, I want to know if there is a way to set the times and links to auto record at certain times. I need answers by tomorrow afternoon!! I have an unRAID server w/ a GTX 1660Ti and a Windows PC with an RTX 2060.", "author_fullname": "t2_in3ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Youtube Livestreams (concurrent)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tq2wl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682050761.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I need help downloading multiple streams from a music festival starting tomorrow. There are 5 different streams all going on for the same time and last about 8-9 hours each. I tried testing some livestreams from other channels on YoutubeDL Material, but it gives me an error. Also, I want to know if there is a way to set the times and links to auto record at certain times. I need answers by tomorrow afternoon!! I have an unRAID server w/ a GTX 1660Ti and a Windows PC with an RTX 2060.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tq2wl", "is_robot_indexable": true, "report_reasons": null, "author": "Kobeis2pac", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tq2wl/help_with_youtube_livestreams_concurrent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tq2wl/help_with_youtube_livestreams_concurrent/", "subreddit_subscribers": 678737, "created_utc": 1682050761.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a book scanner to digitize a large collection of books for a project. The scanner should handle various book sizes and types and scan books accurately without causing any damage. The scans need to be high-resolution with clear and sharp images for digital archiving and printing purposes. Open to considering options across different budgets. Please share any experience or recommendations for the best professional book scanner. Thank you!", "author_fullname": "t2_cv6pdkug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing Book Scanner? Need Recommendation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tb9z1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682016650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a book scanner to digitize a large collection of books for a project. The scanner should handle various book sizes and types and scan books accurately without causing any damage. The scans need to be high-resolution with clear and sharp images for digital archiving and printing purposes. Open to considering options across different budgets. Please share any experience or recommendations for the best professional book scanner. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tb9z1", "is_robot_indexable": true, "report_reasons": null, "author": "InterestingEmploy669", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tb9z1/choosing_book_scanner_need_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tb9z1/choosing_book_scanner_need_recommendation/", "subreddit_subscribers": 678737, "created_utc": 1682016650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So i am currently running httrack for pretty large web backup projects (looking for newer alternatives tho)\n\n**What are your recommendations for deduplication of the downloaded files?**  \n(currently saving everything on a zfs pool with dedup/compression enabled)\n\n**Are there scripts for changing the local urls within the downloaded pages afterwards?**  \nLets say i download [example.com](https://example.com) and [forum.example.com](https://forum.example.com) separatly a couple weeks apart, since they are different projects within httrack all links that go to the other one are still refering to the original instead of the download (if an [forum.example.com](https://forum.example.com) site is linked within an [example.com](https://example.com) site it is still linking to the original [forum.example.com](https://forum.example.com) instad of the downloaded one) which makes sense since httrack didnt know it existed when it was downloaded but is there a way to change is later down the line? Basically a script that goes though all html files and replaces the external domains with internal ones?\n\n**What settings do you use? What to look out for to get the best performance and backup?**", "author_fullname": "t2_8jnr5wv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HTTRack Discussion | deduplication, merging &amp; optimal settings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12szkc3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.65, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681999070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i am currently running httrack for pretty large web backup projects (looking for newer alternatives tho)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are your recommendations for deduplication of the downloaded files?&lt;/strong&gt;&lt;br/&gt;\n(currently saving everything on a zfs pool with dedup/compression enabled)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Are there scripts for changing the local urls within the downloaded pages afterwards?&lt;/strong&gt;&lt;br/&gt;\nLets say i download &lt;a href=\"https://example.com\"&gt;example.com&lt;/a&gt; and &lt;a href=\"https://forum.example.com\"&gt;forum.example.com&lt;/a&gt; separatly a couple weeks apart, since they are different projects within httrack all links that go to the other one are still refering to the original instead of the download (if an &lt;a href=\"https://forum.example.com\"&gt;forum.example.com&lt;/a&gt; site is linked within an &lt;a href=\"https://example.com\"&gt;example.com&lt;/a&gt; site it is still linking to the original &lt;a href=\"https://forum.example.com\"&gt;forum.example.com&lt;/a&gt; instad of the downloaded one) which makes sense since httrack didnt know it existed when it was downloaded but is there a way to change is later down the line? Basically a script that goes though all html files and replaces the external domains with internal ones?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What settings do you use? What to look out for to get the best performance and backup?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12szkc3", "is_robot_indexable": true, "report_reasons": null, "author": "Pommes254", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12szkc3/httrack_discussion_deduplication_merging_optimal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12szkc3/httrack_discussion_deduplication_merging_optimal/", "subreddit_subscribers": 678737, "created_utc": 1681999070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not sure if this is the best place to ask, but I'm starting my Jellyfin library and was wondering. I understand the purpose of remuxing a DVD since the original file structure is so messy, but what's the point of going to extra steps to remux a blu-ray when you can just copy the M2TS and rename it? The vast majority of players support M2TS and MKV, why bother with the extra steps? It's only in the edge cases where it seems to make sense, yet it's the standard so there must be a reason. Can someone please enlighten me? Thank you", "author_fullname": "t2_ebdgt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the point of remuxing a blu ray? Why not just copy and rename the M2TS file?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpzaq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": "", "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682050478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure if this is the best place to ask, but I&amp;#39;m starting my Jellyfin library and was wondering. I understand the purpose of remuxing a DVD since the original file structure is so messy, but what&amp;#39;s the point of going to extra steps to remux a blu-ray when you can just copy the M2TS and rename it? The vast majority of players support M2TS and MKV, why bother with the extra steps? It&amp;#39;s only in the edge cases where it seems to make sense, yet it&amp;#39;s the standard so there must be a reason. Can someone please enlighten me? Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tpzaq", "is_robot_indexable": true, "report_reasons": null, "author": "incarrion", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12tpzaq/whats_the_point_of_remuxing_a_blu_ray_why_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpzaq/whats_the_point_of_remuxing_a_blu_ray_why_not/", "subreddit_subscribers": 678737, "created_utc": 1682050478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of \"assets\" for things like video editing a graphic design that I want to not only organize well but navigate and preview content from efficiently. A common workflow for me is to be Photoshopping something, need an icon, and often it could be faster to google for some icon svg than it would be to locate it on my NAS.\n\n\u200b\n\nI also often want to add associated metadata that I could keep or possibly search later. Say I download a photo that requires artist attribution, I need to keep that license information with it.  \n\n\nI probably am looking for a Windows-based desktop utility since I am trying to use it with Photoshop and such, but I could potentially host something that better indexes/searches my content and download individual assets per-project.\n\nEdit: I am realizing that Adobe Bridge is actually looking pretty good", "author_fullname": "t2_1jfc9efx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use any \"Digital Asset Management\" tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlwwt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682049658.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682040211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of &amp;quot;assets&amp;quot; for things like video editing a graphic design that I want to not only organize well but navigate and preview content from efficiently. A common workflow for me is to be Photoshopping something, need an icon, and often it could be faster to google for some icon svg than it would be to locate it on my NAS.&lt;/p&gt;\n\n&lt;p&gt;\u200b&lt;/p&gt;\n\n&lt;p&gt;I also often want to add associated metadata that I could keep or possibly search later. Say I download a photo that requires artist attribution, I need to keep that license information with it.  &lt;/p&gt;\n\n&lt;p&gt;I probably am looking for a Windows-based desktop utility since I am trying to use it with Photoshop and such, but I could potentially host something that better indexes/searches my content and download individual assets per-project.&lt;/p&gt;\n\n&lt;p&gt;Edit: I am realizing that Adobe Bridge is actually looking pretty good&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tlwwt", "is_robot_indexable": true, "report_reasons": null, "author": "TechSquidTV", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tlwwt/do_you_use_any_digital_asset_management_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tlwwt/do_you_use_any_digital_asset_management_tools/", "subreddit_subscribers": 678737, "created_utc": 1682040211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen massive explosion in ML industry and now it has reached peak level of attention. Major powers have jumped in to regulate/control the technology (a cease on research has been called by major figures) and its not far fetched that a lot of content related to ML will be made illegal for general public. Governments and/or Big Tech will try to regulate and take this technology out of general public as they can't function if its accessible to everyone. They need consumers for economy to function and if people become content with what they can produce at home it is threat to their power and control. \n\nAlso a lot of people (who want to protect their jobs) are in favor of regulating the AI which may or may not be a good thing depending on who you ask but all ML related content that we can easily access today can be restricted or removed in future. Also not every country is going to ban this. So it will still be legal in some countries (i.e China, Russia and third world countries). \n\nThe content that is in most demand are models and datasets. HuggingFace is website that is currently providing free access to all tools while datasets can be downloaded from their providers i.e LAION, Facebook, Microsoft or research groups. Future regulations may restrict their access,\n\nI am pretty sure people here will be aware of this. As most open ML research is backed by archivists who are lending space to host datasets and models. I believe this industry is going to change a lot due to power struggle and it may be good idea to backup stuff.", "author_fullname": "t2_24zdsiec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Start Hoarding AI/ML content!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpm40", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682049504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen massive explosion in ML industry and now it has reached peak level of attention. Major powers have jumped in to regulate/control the technology (a cease on research has been called by major figures) and its not far fetched that a lot of content related to ML will be made illegal for general public. Governments and/or Big Tech will try to regulate and take this technology out of general public as they can&amp;#39;t function if its accessible to everyone. They need consumers for economy to function and if people become content with what they can produce at home it is threat to their power and control. &lt;/p&gt;\n\n&lt;p&gt;Also a lot of people (who want to protect their jobs) are in favor of regulating the AI which may or may not be a good thing depending on who you ask but all ML related content that we can easily access today can be restricted or removed in future. Also not every country is going to ban this. So it will still be legal in some countries (i.e China, Russia and third world countries). &lt;/p&gt;\n\n&lt;p&gt;The content that is in most demand are models and datasets. HuggingFace is website that is currently providing free access to all tools while datasets can be downloaded from their providers i.e LAION, Facebook, Microsoft or research groups. Future regulations may restrict their access,&lt;/p&gt;\n\n&lt;p&gt;I am pretty sure people here will be aware of this. As most open ML research is backed by archivists who are lending space to host datasets and models. I believe this industry is going to change a lot due to power struggle and it may be good idea to backup stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12tpm40", "is_robot_indexable": true, "report_reasons": null, "author": "_H_a_c_k_e_r_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tpm40/start_hoarding_aiml_content/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpm40/start_hoarding_aiml_content/", "subreddit_subscribers": 678737, "created_utc": 1682049504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So here\u2019s the problem. Despite my diligence with backing up, I noticed my backup drive has fewer folders than my primary drive and I\u2019m trying to figure out what\u2019s missing, but there are hundreds of folders in the main directory alone and manually comparing is a nightmare. I\u2019m tempted to just delete the backup and re-copy it all, but that\u2019s 3 terabytes of data, I don\u2019t have all day to wait around for it to copy over. \n\nWhat I\u2019m looking for is a program that will compare the directories, and if there\u2019s a whole folder or even just a single file missing from one, it\u2019ll make note of that and report it back. It would be even better if it could compare the checksum of each file and report if one is different. I believe the missing folders are due to my mistake rather than file corruption, but it would be a relief to know for sure.", "author_fullname": "t2_tggommtv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to automatically compare two folder trees and report back the differences between them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpfvr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682049023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So here\u2019s the problem. Despite my diligence with backing up, I noticed my backup drive has fewer folders than my primary drive and I\u2019m trying to figure out what\u2019s missing, but there are hundreds of folders in the main directory alone and manually comparing is a nightmare. I\u2019m tempted to just delete the backup and re-copy it all, but that\u2019s 3 terabytes of data, I don\u2019t have all day to wait around for it to copy over. &lt;/p&gt;\n\n&lt;p&gt;What I\u2019m looking for is a program that will compare the directories, and if there\u2019s a whole folder or even just a single file missing from one, it\u2019ll make note of that and report it back. It would be even better if it could compare the checksum of each file and report if one is different. I believe the missing folders are due to my mistake rather than file corruption, but it would be a relief to know for sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tpfvr", "is_robot_indexable": true, "report_reasons": null, "author": "bobisnotmyuncIe", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tpfvr/is_there_a_way_to_automatically_compare_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpfvr/is_there_a_way_to_automatically_compare_two/", "subreddit_subscribers": 678737, "created_utc": 1682049023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve been collecting the audio files from podcasts and I want to cut out the ads from them. Are there any super simple audio editors that would work for this? All I need is the ability to cut out portions, I don\u2019t need any fancy editing tools. The main criteria would be efficiency, as I will need to do this for many files and it will probably have to be done manually.\n\nThank you for any recommendations!", "author_fullname": "t2_13f9bd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple Audio Editor for Removing Ads from Podcasts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlkzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682039447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been collecting the audio files from podcasts and I want to cut out the ads from them. Are there any super simple audio editors that would work for this? All I need is the ability to cut out portions, I don\u2019t need any fancy editing tools. The main criteria would be efficiency, as I will need to do this for many files and it will probably have to be done manually.&lt;/p&gt;\n\n&lt;p&gt;Thank you for any recommendations!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tlkzl", "is_robot_indexable": true, "report_reasons": null, "author": "rkusty23", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tlkzl/simple_audio_editor_for_removing_ads_from_podcasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tlkzl/simple_audio_editor_for_removing_ads_from_podcasts/", "subreddit_subscribers": 678737, "created_utc": 1682039447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have two large SAS/SATA expanders, a 16-bay and a 24-bay both made by RAID Machine (link to the 24-bay version below) They're great and they do the trick but the fans are like jet engines taking off, they're ridiculously loud. Would anyone happen to know if replacing the fans is a viable option? i dunno if they have temp sensors or something like that. If they are replaceable, would anyone have suggestions on quieter replacement fans? Or if the noise is just what it is, any suggestions on some kind of cabinet or something I could build out to try to mitigate it? Obviously airflow/ventilation would be a primary concern. They're currently rackmounted in a custom ATA style road case on wheels that slides under my desk. Anyway, I'm all ears if anyone has thoughts.\n\n&amp;#x200B;\n\n[https://www.pc-pitstop.com/24-bay-12g-expander-enclosure](https://www.pc-pitstop.com/24-bay-12g-expander-enclosure)", "author_fullname": "t2_aukaiie4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loud fans", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tkp6o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682037370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have two large SAS/SATA expanders, a 16-bay and a 24-bay both made by RAID Machine (link to the 24-bay version below) They&amp;#39;re great and they do the trick but the fans are like jet engines taking off, they&amp;#39;re ridiculously loud. Would anyone happen to know if replacing the fans is a viable option? i dunno if they have temp sensors or something like that. If they are replaceable, would anyone have suggestions on quieter replacement fans? Or if the noise is just what it is, any suggestions on some kind of cabinet or something I could build out to try to mitigate it? Obviously airflow/ventilation would be a primary concern. They&amp;#39;re currently rackmounted in a custom ATA style road case on wheels that slides under my desk. Anyway, I&amp;#39;m all ears if anyone has thoughts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pc-pitstop.com/24-bay-12g-expander-enclosure\"&gt;https://www.pc-pitstop.com/24-bay-12g-expander-enclosure&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?auto=webp&amp;v=enabled&amp;s=da26c84211db6bfe70d900ba26856f0bcf91505c", "width": 400, "height": 170}, "resolutions": [{"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f2f4ff1fca374eb83aa4760f5c7d3e9712af75c", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1730f57cf94ea15b684b80527e7242b25348ae9b", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45d4f4395be91ba11bf9c841d9371c77ae8631a3", "width": 320, "height": 136}], "variants": {}, "id": "Qh5tIF25DB92XS8vDre-aDXLS7_h3WcZhBsWj1sK2DQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tkp6o", "is_robot_indexable": true, "report_reasons": null, "author": "2Ksmooth", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tkp6o/loud_fans/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tkp6o/loud_fans/", "subreddit_subscribers": 678737, "created_utc": 1682037370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello peeps,\n\nUntil today, I thought  Z-Library was a mere mirror of LibGen, apparently this isn't so :)\n\nI have a nice setup with LibGen, it's like this:\n\nInterested in books about say Nutrition\n\n1. Download non-fiction LibGen dump from any of the sites hosting them.\n2. Initialize the DB locally.\n3. Run queries to identify what I need, returning `updated`.`IdentifierWODash` which is the stripped down ISBN in the LibGen dump.\n4. Download said books via Libgen Desktop, just because I haven't found a better software for it.\n\nCan I have something like that for Z-Library?\n\nMy use case is this: \u201cOMG, I should download every PDF book that ever existed on Psychology this weekend, that was published in the past 10 years\u201d.\n\nThen I need a way to make that happen.\n\nDoes Z-Library have a DB dump like Libgen does? What's your setup for getting files from it?\n\nEdit:\n\nDid a search on GitHub:\n\n    zlibrary in:topic pushed:&gt;2023-01-01 stars:&gt;=10\n\nMeans: repositories with zlibrary in their topics, where code was pushed this year and has at least 10 stars.\n\nResulted in\n\n1. [A project in Chinese](https://github.com/Senkita/zLib-Web). I have no idea what it's about.\n2. [ZLibrary CLI](https://github.com/baroxyton/zlibrary-CLI).\n\nZLibrary CLI looks like the only solution these days? It accounts for personal domains, the new Z-Library setup.\n\nIs there anything else?\n\nThanks  \n", "author_fullname": "t2_w0ujml6e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bulk download non-fiction from Z-Library and search Z-Library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tfoud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682029007.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682025947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello peeps,&lt;/p&gt;\n\n&lt;p&gt;Until today, I thought  Z-Library was a mere mirror of LibGen, apparently this isn&amp;#39;t so :)&lt;/p&gt;\n\n&lt;p&gt;I have a nice setup with LibGen, it&amp;#39;s like this:&lt;/p&gt;\n\n&lt;p&gt;Interested in books about say Nutrition&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download non-fiction LibGen dump from any of the sites hosting them.&lt;/li&gt;\n&lt;li&gt;Initialize the DB locally.&lt;/li&gt;\n&lt;li&gt;Run queries to identify what I need, returning &lt;code&gt;updated&lt;/code&gt;.&lt;code&gt;IdentifierWODash&lt;/code&gt; which is the stripped down ISBN in the LibGen dump.&lt;/li&gt;\n&lt;li&gt;Download said books via Libgen Desktop, just because I haven&amp;#39;t found a better software for it.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Can I have something like that for Z-Library?&lt;/p&gt;\n\n&lt;p&gt;My use case is this: \u201cOMG, I should download every PDF book that ever existed on Psychology this weekend, that was published in the past 10 years\u201d.&lt;/p&gt;\n\n&lt;p&gt;Then I need a way to make that happen.&lt;/p&gt;\n\n&lt;p&gt;Does Z-Library have a DB dump like Libgen does? What&amp;#39;s your setup for getting files from it?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;Did a search on GitHub:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;zlibrary in:topic pushed:&amp;gt;2023-01-01 stars:&amp;gt;=10\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Means: repositories with zlibrary in their topics, where code was pushed this year and has at least 10 stars.&lt;/p&gt;\n\n&lt;p&gt;Resulted in&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Senkita/zLib-Web\"&gt;A project in Chinese&lt;/a&gt;. I have no idea what it&amp;#39;s about.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/baroxyton/zlibrary-CLI\"&gt;ZLibrary CLI&lt;/a&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;ZLibrary CLI looks like the only solution these days? It accounts for personal domains, the new Z-Library setup.&lt;/p&gt;\n\n&lt;p&gt;Is there anything else?&lt;/p&gt;\n\n&lt;p&gt;Thanks  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?auto=webp&amp;v=enabled&amp;s=00f73080ff2bf090f2dd5dd09bb7fdced4eda3f4", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3767cf5de977f41e6bd19b5891dc929dfd49a38c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fd88e7c5c2bee32f08b847306aa0c06cefb4982", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba9deed576c5c01fabf614f6d86b9808639e71b7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd8574ae90c27b380a18e94b71f50e4265674eaf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74362ce9fab54f851ec521a4b36cab471d5246e6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd461a5b0618ce06de92e1ce7698bf4aa16f0931", "width": 1080, "height": 540}], "variants": {}, "id": "kogm69pcGJGnWfA-SgY36JvhgpMJjd2m-mJYyI6cGak"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tfoud", "is_robot_indexable": true, "report_reasons": null, "author": "EUTIORti", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tfoud/how_to_bulk_download_nonfiction_from_zlibrary_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tfoud/how_to_bulk_download_nonfiction_from_zlibrary_and/", "subreddit_subscribers": 678737, "created_utc": 1682025947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "They used to be able to grab the .m3u8 file pretty easily and get the file. \nHowever, now they are possibly using DASH or maybe even .m3u8, but no sniffers are able to find it. They use (from what I have seen), two different video players THEOplayer and bitmovin player. They are also now using DRM protection.\n\nMost people I have spoken to are using OBS to just record it, however, unfortunately, my computer kind of sucks for doing stuff like that. Which results in choppy playback at some points.\n\nDoes anyone here (figured this may be one of the only places someone might) know how to still grab the file and download using FFMPEG?", "author_fullname": "t2_h5d1uatv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LivePhish has been updated", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12su2j8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681986853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They used to be able to grab the .m3u8 file pretty easily and get the file. \nHowever, now they are possibly using DASH or maybe even .m3u8, but no sniffers are able to find it. They use (from what I have seen), two different video players THEOplayer and bitmovin player. They are also now using DRM protection.&lt;/p&gt;\n\n&lt;p&gt;Most people I have spoken to are using OBS to just record it, however, unfortunately, my computer kind of sucks for doing stuff like that. Which results in choppy playback at some points.&lt;/p&gt;\n\n&lt;p&gt;Does anyone here (figured this may be one of the only places someone might) know how to still grab the file and download using FFMPEG?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12su2j8", "is_robot_indexable": true, "report_reasons": null, "author": "GoldPhysical", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12su2j8/livephish_has_been_updated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12su2j8/livephish_has_been_updated/", "subreddit_subscribers": 678737, "created_utc": 1681986853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm into searching how to put more storage disks into HP Z240 tower chassis. It has 2 sata conectors and one nvme on board itself. Only 2 SATA on motherboard was surprise since in tower there are plenty of room for internal drives. My guess is that I would need to go via PCIE expansion cards, but could not find consensus by searching the web on what is the best approach. Apparently this motherboard does not support bifurcation so for NVME is only one drive per one PCIE slot? SATA PCIE expansion card is also needed. Ideally I would like to run minimum of 3 SATA drives and 3 NVME...", "author_fullname": "t2_i2xgwndg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HP Z240 tower storage options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tqct7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682051528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m into searching how to put more storage disks into HP Z240 tower chassis. It has 2 sata conectors and one nvme on board itself. Only 2 SATA on motherboard was surprise since in tower there are plenty of room for internal drives. My guess is that I would need to go via PCIE expansion cards, but could not find consensus by searching the web on what is the best approach. Apparently this motherboard does not support bifurcation so for NVME is only one drive per one PCIE slot? SATA PCIE expansion card is also needed. Ideally I would like to run minimum of 3 SATA drives and 3 NVME...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tqct7", "is_robot_indexable": true, "report_reasons": null, "author": "No_Requirement_64OO", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tqct7/hp_z240_tower_storage_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tqct7/hp_z240_tower_storage_options/", "subreddit_subscribers": 678737, "created_utc": 1682051528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to save all the linked photos from a forum for posterity? With imgur removing photos that are not related to an account, it will completely destroy most of the content on a dreamwidth forum (it was originally hosted on LiveJournal) I go on. \n\nEvery day, a number of secrets are posted for people to comment on. They are posted in the main post as embedded images and then are linked directly below one by one for people to discuss. \n\nI don't want all those secrets just lost forever. But I also don't want to save every photo made in the replies.", "author_fullname": "t2_bamgj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to save every linked image from the main body of posts on a site (but not the comments)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tj4p6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682033681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to save all the linked photos from a forum for posterity? With imgur removing photos that are not related to an account, it will completely destroy most of the content on a dreamwidth forum (it was originally hosted on LiveJournal) I go on. &lt;/p&gt;\n\n&lt;p&gt;Every day, a number of secrets are posted for people to comment on. They are posted in the main post as embedded images and then are linked directly below one by one for people to discuss. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want all those secrets just lost forever. But I also don&amp;#39;t want to save every photo made in the replies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tj4p6", "is_robot_indexable": true, "report_reasons": null, "author": "Ashmeadow", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tj4p6/is_there_a_way_to_save_every_linked_image_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tj4p6/is_there_a_way_to_save_every_linked_image_from/", "subreddit_subscribers": 678737, "created_utc": 1682033681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I relocate overseas for a few months each year, and I am looking for a portable solution.\n\nI am restricted by international travel weight restrictions, so would rather not have anything too heavy.\n\nAt home i run a 4 x 3tb Ironwolf softraid which currently backs up my 2 x 5tb seagate portable drives and macbook.\n\nI usually take the macbook and portable drives overseas with me. One of the 5tb for movies and the other for archived data that i may need. The macbook backups onto the data drive while I am away as a I only add a few gigs. This archive drive is nearly full and the movie drive tired so hence the need for expansion.\n\nMy current thought is either get a Synology DS220j with a single 16tb drive for now which I can expand later or the WD 16 TB Elements Desktop External USBC Hard Drive.   \nThis would allow me to add one of my 5tb portable drives to my softraid, and using the other as an offsite backup as it is getting old and isn't much use for anything else.\n\nDoes anyone have any other ideas?", "author_fullname": "t2_1gm1x1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Semi-Portable NAS option? Currently looking at Synology DS220j but would a large external drive be better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sv8lu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681989723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I relocate overseas for a few months each year, and I am looking for a portable solution.&lt;/p&gt;\n\n&lt;p&gt;I am restricted by international travel weight restrictions, so would rather not have anything too heavy.&lt;/p&gt;\n\n&lt;p&gt;At home i run a 4 x 3tb Ironwolf softraid which currently backs up my 2 x 5tb seagate portable drives and macbook.&lt;/p&gt;\n\n&lt;p&gt;I usually take the macbook and portable drives overseas with me. One of the 5tb for movies and the other for archived data that i may need. The macbook backups onto the data drive while I am away as a I only add a few gigs. This archive drive is nearly full and the movie drive tired so hence the need for expansion.&lt;/p&gt;\n\n&lt;p&gt;My current thought is either get a Synology DS220j with a single 16tb drive for now which I can expand later or the WD 16 TB Elements Desktop External USBC Hard Drive.&lt;br/&gt;\nThis would allow me to add one of my 5tb portable drives to my softraid, and using the other as an offsite backup as it is getting old and isn&amp;#39;t much use for anything else.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any other ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12sv8lu", "is_robot_indexable": true, "report_reasons": null, "author": "matmah", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sv8lu/semiportable_nas_option_currently_looking_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12sv8lu/semiportable_nas_option_currently_looking_at/", "subreddit_subscribers": 678737, "created_utc": 1681989723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone ever spoken to Denis Petrov who owns the [archive.is](https://archive.is) website? I've tried so many Emails and haven't received any reply concerning my DCMA request. I've been very polite, but this is getting ridiculous.  \n\n\nI did find a phone number and address when I put the website into ICANN and WhoIs. Has anyone managed to reach him using those?  \n\n\nI don't want to annoy him, just get the pictures of my daughters removed that were put on his site by a person who has been creeping on me. I deleted them off the account they originally were on when things started to happen, but then I found out the creep had archived the pages! I don't want this creep who has been very.... I can't go into details here but just they make their love of underaged things known... I don't want that kind of person to have access to these images that they clearly saved for a sick purpose!  \n\n\nI don't know what to do. If anyone knows anything please let me know!", "author_fullname": "t2_qboeung3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has Anyone Ever Actually Spoken to Denis Petrov of Archive.Is?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12trawt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682054293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever spoken to Denis Petrov who owns the &lt;a href=\"https://archive.is\"&gt;archive.is&lt;/a&gt; website? I&amp;#39;ve tried so many Emails and haven&amp;#39;t received any reply concerning my DCMA request. I&amp;#39;ve been very polite, but this is getting ridiculous.  &lt;/p&gt;\n\n&lt;p&gt;I did find a phone number and address when I put the website into ICANN and WhoIs. Has anyone managed to reach him using those?  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to annoy him, just get the pictures of my daughters removed that were put on his site by a person who has been creeping on me. I deleted them off the account they originally were on when things started to happen, but then I found out the creep had archived the pages! I don&amp;#39;t want this creep who has been very.... I can&amp;#39;t go into details here but just they make their love of underaged things known... I don&amp;#39;t want that kind of person to have access to these images that they clearly saved for a sick purpose!  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what to do. If anyone knows anything please let me know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cMsDEjZcw1yJ2-Oox97YXjr_B80QoYA7KKKBUw8desk.jpg?auto=webp&amp;v=enabled&amp;s=70049ac5e4587eb732d86f2bf3c7f941a6314e91", "width": 144, "height": 144}, "resolutions": [{"url": "https://external-preview.redd.it/cMsDEjZcw1yJ2-Oox97YXjr_B80QoYA7KKKBUw8desk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e81d09b8bf08f6c2e49210b8e709ad2fba21fef5", "width": 108, "height": 108}], "variants": {}, "id": "5WAXcyxu5qzeFIANl2gYNgI5-HT3q1BW7sA5zqtskZE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12trawt", "is_robot_indexable": true, "report_reasons": null, "author": "TheVeganDragon_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12trawt/has_anyone_ever_actually_spoken_to_denis_petrov/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12trawt/has_anyone_ever_actually_spoken_to_denis_petrov/", "subreddit_subscribers": 678737, "created_utc": 1682054293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Do you know any simple program that can backup folders into a \"database of files\", for deduplication, but without breaking files into chunks? Just simply to move the files into the database?\n\nIdeally the program will contain: database of files, identified by their hash + index for each backup cycle.\n\n&amp;#x200B;\n\nThe most important feature that I am looking for is: Deduplication.\n\nWhy I don't want any splitting/chunking? For simplicity, and so I will not be forced to \"trust\" the program for recovery. It should be easy to manually recover files, by checking the index &amp; find the files in the database, even without the program.\n\n&amp;#x200B;\n\nI will run this program manually.\n\nit also has to run on windows.\n\nBonus if it has some GUI.\n\nBonus if it can handle relative paths.\n\n&amp;#x200B;\n\nI checked duplicati, restic and borg and they all have a \"chunking\" feature that cannot be disabled. Their backup database is hard to read without the program.", "author_fullname": "t2_vq86wim9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a very simple deduplication program for backups (no file splitting/chunking, no encryption, no compression, no scheduling)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12tr404", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682055185.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682053720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you know any simple program that can backup folders into a &amp;quot;database of files&amp;quot;, for deduplication, but without breaking files into chunks? Just simply to move the files into the database?&lt;/p&gt;\n\n&lt;p&gt;Ideally the program will contain: database of files, identified by their hash + index for each backup cycle.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The most important feature that I am looking for is: Deduplication.&lt;/p&gt;\n\n&lt;p&gt;Why I don&amp;#39;t want any splitting/chunking? For simplicity, and so I will not be forced to &amp;quot;trust&amp;quot; the program for recovery. It should be easy to manually recover files, by checking the index &amp;amp; find the files in the database, even without the program.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I will run this program manually.&lt;/p&gt;\n\n&lt;p&gt;it also has to run on windows.&lt;/p&gt;\n\n&lt;p&gt;Bonus if it has some GUI.&lt;/p&gt;\n\n&lt;p&gt;Bonus if it can handle relative paths.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I checked duplicati, restic and borg and they all have a &amp;quot;chunking&amp;quot; feature that cannot be disabled. Their backup database is hard to read without the program.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tr404", "is_robot_indexable": true, "report_reasons": null, "author": "algotrader944", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tr404/looking_for_a_very_simple_deduplication_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tr404/looking_for_a_very_simple_deduplication_program/", "subreddit_subscribers": 678737, "created_utc": 1682053720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Link: [https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html](https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html)", "author_fullname": "t2_22bur9io", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BuzzFeed News, Which Dragged Media Into the Digital Age, Shuts Down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tno3h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682044442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Link: &lt;a href=\"https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html\"&gt;https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?auto=webp&amp;v=enabled&amp;s=23240c57aab8133784c52a6677408f9e5bbcae8e", "width": 1050, "height": 549}, "resolutions": [{"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c6302b02d8bf5d9e01ef89d1951c0544f454a0a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c49dbec0d1ad8f5ff6d557048585eae3d2a7c147", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fad551e310ce78029b4e1864946889af23b1d70", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a6297044c9341a30035e3f8337f912a0995302be", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d73ea86ac977f2982068f14de815ec3b4154ea57", "width": 960, "height": 501}], "variants": {}, "id": "b1Knl5I0c-FFco_fthSZJqci2gEa_iHaq3W3AkA04tE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tno3h", "is_robot_indexable": true, "report_reasons": null, "author": "classicalist", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tno3h/buzzfeed_news_which_dragged_media_into_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tno3h/buzzfeed_news_which_dragged_media_into_the/", "subreddit_subscribers": 678737, "created_utc": 1682044442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "my favourite vtuber channel is being deleted in 10 days. Ive archived 1.5TB of vids, shorts, and livestreams but was wondering what the best way to copy community content.  Are there any tools that can help automate this since there are too many memes/photos to manually save.", "author_fullname": "t2_i8ob7wwj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I copy community posts in a good way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12th5q2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682029218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;my favourite vtuber channel is being deleted in 10 days. Ive archived 1.5TB of vids, shorts, and livestreams but was wondering what the best way to copy community content.  Are there any tools that can help automate this since there are too many memes/photos to manually save.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12th5q2", "is_robot_indexable": true, "report_reasons": null, "author": "CRTAutist1337", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12th5q2/how_do_i_copy_community_posts_in_a_good_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12th5q2/how_do_i_copy_community_posts_in_a_good_way/", "subreddit_subscribers": 678737, "created_utc": 1682029218.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}