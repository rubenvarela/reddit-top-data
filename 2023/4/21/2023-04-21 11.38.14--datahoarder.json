{"kind": "Listing", "data": {"after": "t3_12szliz", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_6kz4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Office Depot offered me a free charcoal grill with my hard drive purchase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12tipvp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 1430, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 1430, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Wqhy-NagOiu23poPEIEIhnyH2qIBE-yveYILc2jSgMA.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682032730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/nRjYJXN.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?auto=webp&amp;v=enabled&amp;s=877a9a18b8b50d040663871def4a1421df6e6386", "width": 3264, "height": 2448}, "resolutions": [{"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73051bbc5204949741714b34cb4b3b04f7590406", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f2b9690cc37062aef73886911d0f1e42ace0489", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccca79086fb20fe136388167e21bf0d06de5a397", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e6b22c4eb4c8459362e65f071bffd24eceb87db0", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09732ba25ca2f59740344f35746feec4199a9026", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bed1c49de8987d7410fdc80006b433cd17bc39a8", "width": 1080, "height": 810}], "variants": {}, "id": "Jo_AoGZNLMYWGVFreZ_a7VwlLo7DvuTcr0I0Xi7tqb8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1/10 PB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tipvp", "is_robot_indexable": true, "report_reasons": null, "author": "mrtramplefoot", "discussion_type": null, "num_comments": 102, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12tipvp/office_depot_offered_me_a_free_charcoal_grill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/nRjYJXN.jpg", "subreddit_subscribers": 678877, "created_utc": 1682032730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_yaxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shutting down Legit Torrents after 17 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sy755", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 249, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 249, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681996304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "legittorrents.info", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.legittorrents.info/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12sy755", "is_robot_indexable": true, "report_reasons": null, "author": "en3r0", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sy755/shutting_down_legit_torrents_after_17_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.legittorrents.info/", "subreddit_subscribers": 678877, "created_utc": 1681996304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_3qve30kb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "reddit-img-dl: Tool to download media and comments from subreddits or reddit users. Its a fork of the The-Eye-Team/reddit-dl with additional features. I worked on this afew years ago, but it might be useful to the people here with the recent reddit and imgur changes.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12t4ohz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 86, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 86, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682004665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/IceWreck/reddit-img-dl", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12t4ohz", "is_robot_indexable": true, "report_reasons": null, "author": "HulkaBurninFudge", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12t4ohz/redditimgdl_tool_to_download_media_and_comments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/IceWreck/reddit-img-dl", "subreddit_subscribers": 678877, "created_utc": 1682004665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen massive explosion in ML industry and now it has reached peak level of attention. Major powers have jumped in to regulate/control the technology (a cease on research has been called by major figures) and its not far fetched that a lot of content related to ML will be made illegal for general public. Governments and/or Big Tech will try to regulate and take this technology out of general public as they can't function if its accessible to everyone. They need consumers for economy to function and if people become content with what they can produce at home it is threat to their power and control. \n\nAlso a lot of people (who want to protect their jobs) are in favor of regulating the AI which may or may not be a good thing depending on who you ask but all ML related content that we can easily access today can be restricted or removed in future. Also not every country is going to ban this. So it will still be legal in some countries (i.e China, Russia and third world countries). \n\nThe content that is in most demand are models and datasets. HuggingFace is website that is currently providing free access to all tools while datasets can be downloaded from their providers i.e LAION, Facebook, Microsoft or research groups. Future regulations may restrict their access,\n\nI am pretty sure people here will be aware of this. As most open ML research is backed by archivists who are lending space to host datasets and models. I believe this industry is going to change a lot due to power struggle and it may be good idea to backup stuff.", "author_fullname": "t2_24zdsiec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Start Hoarding AI/ML content!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpm40", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682049504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen massive explosion in ML industry and now it has reached peak level of attention. Major powers have jumped in to regulate/control the technology (a cease on research has been called by major figures) and its not far fetched that a lot of content related to ML will be made illegal for general public. Governments and/or Big Tech will try to regulate and take this technology out of general public as they can&amp;#39;t function if its accessible to everyone. They need consumers for economy to function and if people become content with what they can produce at home it is threat to their power and control. &lt;/p&gt;\n\n&lt;p&gt;Also a lot of people (who want to protect their jobs) are in favor of regulating the AI which may or may not be a good thing depending on who you ask but all ML related content that we can easily access today can be restricted or removed in future. Also not every country is going to ban this. So it will still be legal in some countries (i.e China, Russia and third world countries). &lt;/p&gt;\n\n&lt;p&gt;The content that is in most demand are models and datasets. HuggingFace is website that is currently providing free access to all tools while datasets can be downloaded from their providers i.e LAION, Facebook, Microsoft or research groups. Future regulations may restrict their access,&lt;/p&gt;\n\n&lt;p&gt;I am pretty sure people here will be aware of this. As most open ML research is backed by archivists who are lending space to host datasets and models. I believe this industry is going to change a lot due to power struggle and it may be good idea to backup stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12tpm40", "is_robot_indexable": true, "report_reasons": null, "author": "_H_a_c_k_e_r_", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tpm40/start_hoarding_aiml_content/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpm40/start_hoarding_aiml_content/", "subreddit_subscribers": 678877, "created_utc": 1682049504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have over 100 URLs of bookmarks I\u2019ve collected over the years saved in a file. My question is if I wanted to permanently save the actual pages themselves (articles, essays, blog posts etc) what would be the best and easiest way to do it?\n\nIs making a PDF or an HTML file of the page in question sufficient?", "author_fullname": "t2_gfj7b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to save a webpage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12t6c63", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1682009102.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682006868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have over 100 URLs of bookmarks I\u2019ve collected over the years saved in a file. My question is if I wanted to permanently save the actual pages themselves (articles, essays, blog posts etc) what would be the best and easiest way to do it?&lt;/p&gt;\n\n&lt;p&gt;Is making a PDF or an HTML file of the page in question sufficient?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12t6c63", "is_robot_indexable": true, "report_reasons": null, "author": "Cmyers1980", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12t6c63/whats_the_best_way_to_save_a_webpage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12t6c63/whats_the_best_way_to_save_a_webpage/", "subreddit_subscribers": 678877, "created_utc": 1682006868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, I recently downloaded all the savegames on GameFAQs and uploaded them on [archive.org](https://archive.org). Here's the page [https://archive.org/details/gamefaqs\\_savegames](https://archive.org/details/gamefaqs_savegames). A short description:\n\nAll the savegames that can be downloaded on gamefaqs as of 21 April 2023. Each zip file contains a folder for each game with at least a savegame available and the game's folder contains a series of subfolders for every type of different gamesave available. For example, [here](https://gamefaqs.gamespot.com/snes/519824-super-mario-world/saves) there are save files from gba and wii from different regions. Every folder also contains a description.txt file with the description of the gamesaves.\n\nGames for each console:\n\n* dreamcast - 152\n* ds - 501\n* famicomds - 1\n* gamecube - 313\n* gba - 307\n* genesis - 3\n* n64 - 121\n* nes - 6\n* ps - 698\n* ps2 - 1853\n* ps3 - 548\n* psp - 1131\n* sms - 1\n* snes - 23\n* tg16 - 3\n* turbocd - 2\n* wii - 411\n* xbox - 452\n\nThe games were mostly deduplicated, but sometimes the scraping would fail due to the connection being reset by the server so the same game might be present twice under different names.", "author_fullname": "t2_24wtqt95", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GameFAQs savegames collection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tv29a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682065890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I recently downloaded all the savegames on GameFAQs and uploaded them on &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;. Here&amp;#39;s the page &lt;a href=\"https://archive.org/details/gamefaqs_savegames\"&gt;https://archive.org/details/gamefaqs_savegames&lt;/a&gt;. A short description:&lt;/p&gt;\n\n&lt;p&gt;All the savegames that can be downloaded on gamefaqs as of 21 April 2023. Each zip file contains a folder for each game with at least a savegame available and the game&amp;#39;s folder contains a series of subfolders for every type of different gamesave available. For example, &lt;a href=\"https://gamefaqs.gamespot.com/snes/519824-super-mario-world/saves\"&gt;here&lt;/a&gt; there are save files from gba and wii from different regions. Every folder also contains a description.txt file with the description of the gamesaves.&lt;/p&gt;\n\n&lt;p&gt;Games for each console:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;dreamcast - 152&lt;/li&gt;\n&lt;li&gt;ds - 501&lt;/li&gt;\n&lt;li&gt;famicomds - 1&lt;/li&gt;\n&lt;li&gt;gamecube - 313&lt;/li&gt;\n&lt;li&gt;gba - 307&lt;/li&gt;\n&lt;li&gt;genesis - 3&lt;/li&gt;\n&lt;li&gt;n64 - 121&lt;/li&gt;\n&lt;li&gt;nes - 6&lt;/li&gt;\n&lt;li&gt;ps - 698&lt;/li&gt;\n&lt;li&gt;ps2 - 1853&lt;/li&gt;\n&lt;li&gt;ps3 - 548&lt;/li&gt;\n&lt;li&gt;psp - 1131&lt;/li&gt;\n&lt;li&gt;sms - 1&lt;/li&gt;\n&lt;li&gt;snes - 23&lt;/li&gt;\n&lt;li&gt;tg16 - 3&lt;/li&gt;\n&lt;li&gt;turbocd - 2&lt;/li&gt;\n&lt;li&gt;wii - 411&lt;/li&gt;\n&lt;li&gt;xbox - 452&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The games were mostly deduplicated, but sometimes the scraping would fail due to the connection being reset by the server so the same game might be present twice under different names.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tv29a", "is_robot_indexable": true, "report_reasons": null, "author": "youuuuuuuuuuuuuuuuu", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tv29a/gamefaqs_savegames_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tv29a/gamefaqs_savegames_collection/", "subreddit_subscribers": 678877, "created_utc": 1682065890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone. I need help downloading multiple streams from a music festival starting tomorrow. There are 5 different streams all going on for the same time and last about 8-9 hours each. I tried testing some livestreams from other channels on YoutubeDL Material, but it gives me an error. Also, I want to know if there is a way to set the times and links to auto record at certain times. I need answers by tomorrow afternoon!! I have an unRAID server w/ a GTX 1660Ti and a Windows PC with an RTX 2060.", "author_fullname": "t2_in3ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Youtube Livestreams (concurrent)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tq2wl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682050761.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I need help downloading multiple streams from a music festival starting tomorrow. There are 5 different streams all going on for the same time and last about 8-9 hours each. I tried testing some livestreams from other channels on YoutubeDL Material, but it gives me an error. Also, I want to know if there is a way to set the times and links to auto record at certain times. I need answers by tomorrow afternoon!! I have an unRAID server w/ a GTX 1660Ti and a Windows PC with an RTX 2060.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tq2wl", "is_robot_indexable": true, "report_reasons": null, "author": "Kobeis2pac", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tq2wl/help_with_youtube_livestreams_concurrent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tq2wl/help_with_youtube_livestreams_concurrent/", "subreddit_subscribers": 678877, "created_utc": 1682050761.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of \"assets\" for things like video editing a graphic design that I want to not only organize well but navigate and preview content from efficiently. A common workflow for me is to be Photoshopping something, need an icon, and often it could be faster to google for some icon svg than it would be to locate it on my NAS.\n\n\u200b\n\nI also often want to add associated metadata that I could keep or possibly search later. Say I download a photo that requires artist attribution, I need to keep that license information with it.  \n\n\nI probably am looking for a Windows-based desktop utility since I am trying to use it with Photoshop and such, but I could potentially host something that better indexes/searches my content and download individual assets per-project.\n\nEdit: I am realizing that Adobe Bridge is actually looking pretty good", "author_fullname": "t2_1jfc9efx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use any \"Digital Asset Management\" tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlwwt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682049658.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682040211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of &amp;quot;assets&amp;quot; for things like video editing a graphic design that I want to not only organize well but navigate and preview content from efficiently. A common workflow for me is to be Photoshopping something, need an icon, and often it could be faster to google for some icon svg than it would be to locate it on my NAS.&lt;/p&gt;\n\n&lt;p&gt;\u200b&lt;/p&gt;\n\n&lt;p&gt;I also often want to add associated metadata that I could keep or possibly search later. Say I download a photo that requires artist attribution, I need to keep that license information with it.  &lt;/p&gt;\n\n&lt;p&gt;I probably am looking for a Windows-based desktop utility since I am trying to use it with Photoshop and such, but I could potentially host something that better indexes/searches my content and download individual assets per-project.&lt;/p&gt;\n\n&lt;p&gt;Edit: I am realizing that Adobe Bridge is actually looking pretty good&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tlwwt", "is_robot_indexable": true, "report_reasons": null, "author": "TechSquidTV", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tlwwt/do_you_use_any_digital_asset_management_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tlwwt/do_you_use_any_digital_asset_management_tools/", "subreddit_subscribers": 678877, "created_utc": 1682040211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm merging dashcam files using bandicam on an HDD that has 400gb of free space. The output file is under 100gb - but I'm getting an error 70% of the way through saying \"not enough disk space on output folder\".\n\nWhat could be the reason for this?", "author_fullname": "t2_n7q75", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Not enough disk space, please change output folder\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12td88j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682020650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m merging dashcam files using bandicam on an HDD that has 400gb of free space. The output file is under 100gb - but I&amp;#39;m getting an error 70% of the way through saying &amp;quot;not enough disk space on output folder&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What could be the reason for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12td88j", "is_robot_indexable": true, "report_reasons": null, "author": "StarSurf", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12td88j/not_enough_disk_space_please_change_output_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12td88j/not_enough_disk_space_please_change_output_folder/", "subreddit_subscribers": 678877, "created_utc": 1682020650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not sure if this is the best place to ask, but I'm starting my Jellyfin library and was wondering. I understand the purpose of remuxing a DVD since the original file structure is so messy, but what's the point of going to extra steps to remux a blu-ray when you can just copy the M2TS and rename it? The vast majority of players support M2TS and MKV, why bother with the extra steps? It's only in the edge cases where it seems to make sense, yet it's the standard so there must be a reason. Can someone please enlighten me? Thank you", "author_fullname": "t2_ebdgt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the point of remuxing a blu ray? Why not just copy and rename the M2TS file?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpzaq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": "", "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682050478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure if this is the best place to ask, but I&amp;#39;m starting my Jellyfin library and was wondering. I understand the purpose of remuxing a DVD since the original file structure is so messy, but what&amp;#39;s the point of going to extra steps to remux a blu-ray when you can just copy the M2TS and rename it? The vast majority of players support M2TS and MKV, why bother with the extra steps? It&amp;#39;s only in the edge cases where it seems to make sense, yet it&amp;#39;s the standard so there must be a reason. Can someone please enlighten me? Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tpzaq", "is_robot_indexable": true, "report_reasons": null, "author": "incarrion", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12tpzaq/whats_the_point_of_remuxing_a_blu_ray_why_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpzaq/whats_the_point_of_remuxing_a_blu_ray_why_not/", "subreddit_subscribers": 678877, "created_utc": 1682050478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So here\u2019s the problem. Despite my diligence with backing up, I noticed my backup drive has fewer folders than my primary drive and I\u2019m trying to figure out what\u2019s missing, but there are hundreds of folders in the main directory alone and manually comparing is a nightmare. I\u2019m tempted to just delete the backup and re-copy it all, but that\u2019s 3 terabytes of data, I don\u2019t have all day to wait around for it to copy over. \n\nWhat I\u2019m looking for is a program that will compare the directories, and if there\u2019s a whole folder or even just a single file missing from one, it\u2019ll make note of that and report it back. It would be even better if it could compare the checksum of each file and report if one is different. I believe the missing folders are due to my mistake rather than file corruption, but it would be a relief to know for sure.", "author_fullname": "t2_tggommtv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to automatically compare two folder trees and report back the differences between them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpfvr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682049023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So here\u2019s the problem. Despite my diligence with backing up, I noticed my backup drive has fewer folders than my primary drive and I\u2019m trying to figure out what\u2019s missing, but there are hundreds of folders in the main directory alone and manually comparing is a nightmare. I\u2019m tempted to just delete the backup and re-copy it all, but that\u2019s 3 terabytes of data, I don\u2019t have all day to wait around for it to copy over. &lt;/p&gt;\n\n&lt;p&gt;What I\u2019m looking for is a program that will compare the directories, and if there\u2019s a whole folder or even just a single file missing from one, it\u2019ll make note of that and report it back. It would be even better if it could compare the checksum of each file and report if one is different. I believe the missing folders are due to my mistake rather than file corruption, but it would be a relief to know for sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tpfvr", "is_robot_indexable": true, "report_reasons": null, "author": "bobisnotmyuncIe", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tpfvr/is_there_a_way_to_automatically_compare_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpfvr/is_there_a_way_to_automatically_compare_two/", "subreddit_subscribers": 678877, "created_utc": 1682049023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a book scanner to digitize a large collection of books for a project. The scanner should handle various book sizes and types and scan books accurately without causing any damage. The scans need to be high-resolution with clear and sharp images for digital archiving and printing purposes. Open to considering options across different budgets. Please share any experience or recommendations for the best professional book scanner. Thank you!", "author_fullname": "t2_cv6pdkug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing Book Scanner? Need Recommendation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tb9z1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682016650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a book scanner to digitize a large collection of books for a project. The scanner should handle various book sizes and types and scan books accurately without causing any damage. The scans need to be high-resolution with clear and sharp images for digital archiving and printing purposes. Open to considering options across different budgets. Please share any experience or recommendations for the best professional book scanner. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tb9z1", "is_robot_indexable": true, "report_reasons": null, "author": "InterestingEmploy669", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tb9z1/choosing_book_scanner_need_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tb9z1/choosing_book_scanner_need_recommendation/", "subreddit_subscribers": 678877, "created_utc": 1682016650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So i am currently running httrack for pretty large web backup projects (looking for newer alternatives tho)\n\n**What are your recommendations for deduplication of the downloaded files?**  \n(currently saving everything on a zfs pool with dedup/compression enabled)\n\n**Are there scripts for changing the local urls within the downloaded pages afterwards?**  \nLets say i download [example.com](https://example.com) and [forum.example.com](https://forum.example.com) separatly a couple weeks apart, since they are different projects within httrack all links that go to the other one are still refering to the original instead of the download (if an [forum.example.com](https://forum.example.com) site is linked within an [example.com](https://example.com) site it is still linking to the original [forum.example.com](https://forum.example.com) instad of the downloaded one) which makes sense since httrack didnt know it existed when it was downloaded but is there a way to change is later down the line? Basically a script that goes though all html files and replaces the external domains with internal ones?\n\n**What settings do you use? What to look out for to get the best performance and backup?**", "author_fullname": "t2_8jnr5wv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HTTRack Discussion | deduplication, merging &amp; optimal settings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12szkc3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.66, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681999070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i am currently running httrack for pretty large web backup projects (looking for newer alternatives tho)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are your recommendations for deduplication of the downloaded files?&lt;/strong&gt;&lt;br/&gt;\n(currently saving everything on a zfs pool with dedup/compression enabled)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Are there scripts for changing the local urls within the downloaded pages afterwards?&lt;/strong&gt;&lt;br/&gt;\nLets say i download &lt;a href=\"https://example.com\"&gt;example.com&lt;/a&gt; and &lt;a href=\"https://forum.example.com\"&gt;forum.example.com&lt;/a&gt; separatly a couple weeks apart, since they are different projects within httrack all links that go to the other one are still refering to the original instead of the download (if an &lt;a href=\"https://forum.example.com\"&gt;forum.example.com&lt;/a&gt; site is linked within an &lt;a href=\"https://example.com\"&gt;example.com&lt;/a&gt; site it is still linking to the original &lt;a href=\"https://forum.example.com\"&gt;forum.example.com&lt;/a&gt; instad of the downloaded one) which makes sense since httrack didnt know it existed when it was downloaded but is there a way to change is later down the line? Basically a script that goes though all html files and replaces the external domains with internal ones?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What settings do you use? What to look out for to get the best performance and backup?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12szkc3", "is_robot_indexable": true, "report_reasons": null, "author": "Pommes254", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12szkc3/httrack_discussion_deduplication_merging_optimal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12szkc3/httrack_discussion_deduplication_merging_optimal/", "subreddit_subscribers": 678877, "created_utc": 1681999070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Ok, it took some finding but I found a working link for of RipMe. This is a slightly updated fork of the RipMe software. The link in the description was dead, but some light URL editing lead me to working JAR file.  It's not perfect, and has not been updated since October 2022, but it has an easy to understand GUI that most people can get started right now with. \n\nYes, the 1000 post API limit is present here.\n\nhttps://github.com/ripmeapp2/ripme/releases\n\nGood luck and happy archiving.\n\nEdit, I have it loaded and working, so far archiving both Reddit images and Imgur images from reddit subs.", "author_fullname": "t2_afj5b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RipMe2, For ripping Imgur and Reddit images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tupuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682065976.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682064702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok, it took some finding but I found a working link for of RipMe. This is a slightly updated fork of the RipMe software. The link in the description was dead, but some light URL editing lead me to working JAR file.  It&amp;#39;s not perfect, and has not been updated since October 2022, but it has an easy to understand GUI that most people can get started right now with. &lt;/p&gt;\n\n&lt;p&gt;Yes, the 1000 post API limit is present here.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ripmeapp2/ripme/releases\"&gt;https://github.com/ripmeapp2/ripme/releases&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Good luck and happy archiving.&lt;/p&gt;\n\n&lt;p&gt;Edit, I have it loaded and working, so far archiving both Reddit images and Imgur images from reddit subs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?auto=webp&amp;v=enabled&amp;s=096eeb8dc19b502cc1490d2f012ab14a03d79c5e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c05cf6d23623aaf07578fba58f2526f67bd7c104", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a8ae83a4acb9914d3b2147eb6e97fde0f951c25", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e305d5ebfa072137476305ac12177637160b386", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20c24e5a8f0ef0664121a9d5d4d3a827a4fd4b2a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ac16d9a992941d10b11b2c7b543c9b4de03485a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a86bc54551172eea421e03965f895c2929e230e5", "width": 1080, "height": 540}], "variants": {}, "id": "kmYT9_2i9LCQNRX9rdnzmRppuqRVXBQapgR-p2FDmjk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1300+TB ZFS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tupuh", "is_robot_indexable": true, "report_reasons": null, "author": "EchoGecko795", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12tupuh/ripme2_for_ripping_imgur_and_reddit_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tupuh/ripme2_for_ripping_imgur_and_reddit_images/", "subreddit_subscribers": 678877, "created_utc": 1682064702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone ever spoken to Denis Petrov who owns the [archive.is](https://archive.is) website? I've tried so many Emails and haven't received any reply concerning my DCMA request. I've been very polite, but this is getting ridiculous.  \n\n\nI did find a phone number and address when I put the website into ICANN and WhoIs. Has anyone managed to reach him using those?  \n\n\nI don't want to annoy him, just get the pictures of my daughters removed that were put on his site by a person who has been creeping on me. I deleted them off the account they originally were on when things started to happen, but then I found out the creep had archived the pages! I don't want this creep who has been very.... I can't go into details here but just they make their love of underaged things known... I don't want that kind of person to have access to these images that they clearly saved for a sick purpose!  \n\n\nI don't know what to do. If anyone knows anything please let me know!", "author_fullname": "t2_qboeung3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has Anyone Ever Actually Spoken to Denis Petrov of Archive.Is?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12trawt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682054293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever spoken to Denis Petrov who owns the &lt;a href=\"https://archive.is\"&gt;archive.is&lt;/a&gt; website? I&amp;#39;ve tried so many Emails and haven&amp;#39;t received any reply concerning my DCMA request. I&amp;#39;ve been very polite, but this is getting ridiculous.  &lt;/p&gt;\n\n&lt;p&gt;I did find a phone number and address when I put the website into ICANN and WhoIs. Has anyone managed to reach him using those?  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to annoy him, just get the pictures of my daughters removed that were put on his site by a person who has been creeping on me. I deleted them off the account they originally were on when things started to happen, but then I found out the creep had archived the pages! I don&amp;#39;t want this creep who has been very.... I can&amp;#39;t go into details here but just they make their love of underaged things known... I don&amp;#39;t want that kind of person to have access to these images that they clearly saved for a sick purpose!  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what to do. If anyone knows anything please let me know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cMsDEjZcw1yJ2-Oox97YXjr_B80QoYA7KKKBUw8desk.jpg?auto=webp&amp;v=enabled&amp;s=70049ac5e4587eb732d86f2bf3c7f941a6314e91", "width": 144, "height": 144}, "resolutions": [{"url": "https://external-preview.redd.it/cMsDEjZcw1yJ2-Oox97YXjr_B80QoYA7KKKBUw8desk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e81d09b8bf08f6c2e49210b8e709ad2fba21fef5", "width": 108, "height": 108}], "variants": {}, "id": "5WAXcyxu5qzeFIANl2gYNgI5-HT3q1BW7sA5zqtskZE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12trawt", "is_robot_indexable": true, "report_reasons": null, "author": "TheVeganDragon_", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12trawt/has_anyone_ever_actually_spoken_to_denis_petrov/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12trawt/has_anyone_ever_actually_spoken_to_denis_petrov/", "subreddit_subscribers": 678877, "created_utc": 1682054293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve been collecting the audio files from podcasts and I want to cut out the ads from them. Are there any super simple audio editors that would work for this? All I need is the ability to cut out portions, I don\u2019t need any fancy editing tools. The main criteria would be efficiency, as I will need to do this for many files and it will probably have to be done manually.\n\nThank you for any recommendations!", "author_fullname": "t2_13f9bd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple Audio Editor for Removing Ads from Podcasts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlkzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682039447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been collecting the audio files from podcasts and I want to cut out the ads from them. Are there any super simple audio editors that would work for this? All I need is the ability to cut out portions, I don\u2019t need any fancy editing tools. The main criteria would be efficiency, as I will need to do this for many files and it will probably have to be done manually.&lt;/p&gt;\n\n&lt;p&gt;Thank you for any recommendations!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tlkzl", "is_robot_indexable": true, "report_reasons": null, "author": "rkusty23", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tlkzl/simple_audio_editor_for_removing_ads_from_podcasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tlkzl/simple_audio_editor_for_removing_ads_from_podcasts/", "subreddit_subscribers": 678877, "created_utc": 1682039447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have two large SAS/SATA expanders, a 16-bay and a 24-bay both made by RAID Machine (link to the 24-bay version below) They're great and they do the trick but the fans are like jet engines taking off, they're ridiculously loud. Would anyone happen to know if replacing the fans is a viable option? i dunno if they have temp sensors or something like that. If they are replaceable, would anyone have suggestions on quieter replacement fans? Or if the noise is just what it is, any suggestions on some kind of cabinet or something I could build out to try to mitigate it? Obviously airflow/ventilation would be a primary concern. They're currently rackmounted in a custom ATA style road case on wheels that slides under my desk. Anyway, I'm all ears if anyone has thoughts.\n\n&amp;#x200B;\n\n[https://www.pc-pitstop.com/24-bay-12g-expander-enclosure](https://www.pc-pitstop.com/24-bay-12g-expander-enclosure)", "author_fullname": "t2_aukaiie4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loud fans", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tkp6o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682037370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have two large SAS/SATA expanders, a 16-bay and a 24-bay both made by RAID Machine (link to the 24-bay version below) They&amp;#39;re great and they do the trick but the fans are like jet engines taking off, they&amp;#39;re ridiculously loud. Would anyone happen to know if replacing the fans is a viable option? i dunno if they have temp sensors or something like that. If they are replaceable, would anyone have suggestions on quieter replacement fans? Or if the noise is just what it is, any suggestions on some kind of cabinet or something I could build out to try to mitigate it? Obviously airflow/ventilation would be a primary concern. They&amp;#39;re currently rackmounted in a custom ATA style road case on wheels that slides under my desk. Anyway, I&amp;#39;m all ears if anyone has thoughts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pc-pitstop.com/24-bay-12g-expander-enclosure\"&gt;https://www.pc-pitstop.com/24-bay-12g-expander-enclosure&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?auto=webp&amp;v=enabled&amp;s=da26c84211db6bfe70d900ba26856f0bcf91505c", "width": 400, "height": 170}, "resolutions": [{"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f2f4ff1fca374eb83aa4760f5c7d3e9712af75c", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1730f57cf94ea15b684b80527e7242b25348ae9b", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45d4f4395be91ba11bf9c841d9371c77ae8631a3", "width": 320, "height": 136}], "variants": {}, "id": "Qh5tIF25DB92XS8vDre-aDXLS7_h3WcZhBsWj1sK2DQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tkp6o", "is_robot_indexable": true, "report_reasons": null, "author": "2Ksmooth", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tkp6o/loud_fans/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tkp6o/loud_fans/", "subreddit_subscribers": 678877, "created_utc": 1682037370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello peeps,\n\nUntil today, I thought  Z-Library was a mere mirror of LibGen, apparently this isn't so :)\n\nI have a nice setup with LibGen, it's like this:\n\nInterested in books about say Nutrition\n\n1. Download non-fiction LibGen dump from any of the sites hosting them.\n2. Initialize the DB locally.\n3. Run queries to identify what I need, returning `updated`.`IdentifierWODash` which is the stripped down ISBN in the LibGen dump.\n4. Download said books via Libgen Desktop, just because I haven't found a better software for it.\n\nCan I have something like that for Z-Library?\n\nMy use case is this: \u201cOMG, I should download every PDF book that ever existed on Psychology this weekend, that was published in the past 10 years\u201d.\n\nThen I need a way to make that happen.\n\nDoes Z-Library have a DB dump like Libgen does? What's your setup for getting files from it?\n\nEdit:\n\nDid a search on GitHub:\n\n    zlibrary in:topic pushed:&gt;2023-01-01 stars:&gt;=10\n\nMeans: repositories with zlibrary in their topics, where code was pushed this year and has at least 10 stars.\n\nResulted in\n\n1. [A project in Chinese](https://github.com/Senkita/zLib-Web). I have no idea what it's about.\n2. [ZLibrary CLI](https://github.com/baroxyton/zlibrary-CLI).\n\nZLibrary CLI looks like the only solution these days? It accounts for personal domains, the new Z-Library setup.\n\nIs there anything else?\n\nThanks  \n", "author_fullname": "t2_w0ujml6e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bulk download non-fiction from Z-Library and search Z-Library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tfoud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682029007.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682025947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello peeps,&lt;/p&gt;\n\n&lt;p&gt;Until today, I thought  Z-Library was a mere mirror of LibGen, apparently this isn&amp;#39;t so :)&lt;/p&gt;\n\n&lt;p&gt;I have a nice setup with LibGen, it&amp;#39;s like this:&lt;/p&gt;\n\n&lt;p&gt;Interested in books about say Nutrition&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download non-fiction LibGen dump from any of the sites hosting them.&lt;/li&gt;\n&lt;li&gt;Initialize the DB locally.&lt;/li&gt;\n&lt;li&gt;Run queries to identify what I need, returning &lt;code&gt;updated&lt;/code&gt;.&lt;code&gt;IdentifierWODash&lt;/code&gt; which is the stripped down ISBN in the LibGen dump.&lt;/li&gt;\n&lt;li&gt;Download said books via Libgen Desktop, just because I haven&amp;#39;t found a better software for it.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Can I have something like that for Z-Library?&lt;/p&gt;\n\n&lt;p&gt;My use case is this: \u201cOMG, I should download every PDF book that ever existed on Psychology this weekend, that was published in the past 10 years\u201d.&lt;/p&gt;\n\n&lt;p&gt;Then I need a way to make that happen.&lt;/p&gt;\n\n&lt;p&gt;Does Z-Library have a DB dump like Libgen does? What&amp;#39;s your setup for getting files from it?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;Did a search on GitHub:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;zlibrary in:topic pushed:&amp;gt;2023-01-01 stars:&amp;gt;=10\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Means: repositories with zlibrary in their topics, where code was pushed this year and has at least 10 stars.&lt;/p&gt;\n\n&lt;p&gt;Resulted in&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Senkita/zLib-Web\"&gt;A project in Chinese&lt;/a&gt;. I have no idea what it&amp;#39;s about.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/baroxyton/zlibrary-CLI\"&gt;ZLibrary CLI&lt;/a&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;ZLibrary CLI looks like the only solution these days? It accounts for personal domains, the new Z-Library setup.&lt;/p&gt;\n\n&lt;p&gt;Is there anything else?&lt;/p&gt;\n\n&lt;p&gt;Thanks  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?auto=webp&amp;v=enabled&amp;s=00f73080ff2bf090f2dd5dd09bb7fdced4eda3f4", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3767cf5de977f41e6bd19b5891dc929dfd49a38c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fd88e7c5c2bee32f08b847306aa0c06cefb4982", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba9deed576c5c01fabf614f6d86b9808639e71b7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd8574ae90c27b380a18e94b71f50e4265674eaf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74362ce9fab54f851ec521a4b36cab471d5246e6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd461a5b0618ce06de92e1ce7698bf4aa16f0931", "width": 1080, "height": 540}], "variants": {}, "id": "kogm69pcGJGnWfA-SgY36JvhgpMJjd2m-mJYyI6cGak"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tfoud", "is_robot_indexable": true, "report_reasons": null, "author": "EUTIORti", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tfoud/how_to_bulk_download_nonfiction_from_zlibrary_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tfoud/how_to_bulk_download_nonfiction_from_zlibrary_and/", "subreddit_subscribers": 678877, "created_utc": 1682025947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm into searching how to put more storage disks into HP Z240 tower chassis. It has 2 sata conectors and one nvme on board itself. Only 2 SATA on motherboard was surprise since in tower there are plenty of room for internal drives. My guess is that I would need to go via PCIE expansion cards, but could not find consensus by searching the web on what is the best approach. Apparently this motherboard does not support bifurcation so for NVME is only one drive per one PCIE slot? SATA PCIE expansion card is also needed. Ideally I would like to run minimum of 3 SATA drives and 3 NVME...", "author_fullname": "t2_i2xgwndg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HP Z240 tower storage options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tqct7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682051528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m into searching how to put more storage disks into HP Z240 tower chassis. It has 2 sata conectors and one nvme on board itself. Only 2 SATA on motherboard was surprise since in tower there are plenty of room for internal drives. My guess is that I would need to go via PCIE expansion cards, but could not find consensus by searching the web on what is the best approach. Apparently this motherboard does not support bifurcation so for NVME is only one drive per one PCIE slot? SATA PCIE expansion card is also needed. Ideally I would like to run minimum of 3 SATA drives and 3 NVME...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tqct7", "is_robot_indexable": true, "report_reasons": null, "author": "No_Requirement_64OO", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tqct7/hp_z240_tower_storage_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tqct7/hp_z240_tower_storage_options/", "subreddit_subscribers": 678877, "created_utc": 1682051528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have started a project to help archive content that is in threat of being removed from the internet. I have created a git repo to track and receive contributions (just data no money) to help this move along.\n\nhttps://github.com/RCcola1987/Archival-Preservation", "author_fullname": "t2_ktl0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My project to help save content form deletion!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12txjj4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682073511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have started a project to help archive content that is in threat of being removed from the internet. I have created a git repo to track and receive contributions (just data no money) to help this move along.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/RCcola1987/Archival-Preservation\"&gt;https://github.com/RCcola1987/Archival-Preservation&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?auto=webp&amp;v=enabled&amp;s=7cd9a380febb7d8d78ab959927f72a6f610a4b57", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5dda02da8d55b0b78193d9fa666049dacae260c1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4492ddbd74392fcb8adc70869c8671efa55c2ad6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2bdf5b3f8fcdc9255a5e249dd434fc7b61685dc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37989bfbac705dc8432e526a91409ed3f5955121", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4e2dca63d7efc403a2f6a371cafb087e66ee0f0", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38cdee863a5df5ba2dbf403274e730c2273ccdf1", "width": 1080, "height": 540}], "variants": {}, "id": "wRArez8_fnbnmbuEG5fBf4RHPOGdtn2d1U1egag_iDw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1PB Formatted", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12txjj4", "is_robot_indexable": true, "report_reasons": null, "author": "RCcola1987", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12txjj4/my_project_to_help_save_content_form_deletion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12txjj4/my_project_to_help_save_content_form_deletion/", "subreddit_subscribers": 678877, "created_utc": 1682073511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since Imgur is purging its old archives, I thought it'd be a good idea to post about gallery-dl for those who haven't heard of it before\n\nFor those that have image galleries they want to save, I'd highly recommend the use of gallery-dl to save them to your hard drive.  You only need a little bit of knowledge with the command line.  (Grab the Standalone Executable for the easiest time, or use the pip installer command if you have Python)\n\n[https://github.com/mikf/gallery-dl](https://github.com/mikf/gallery-dl)\n\nIt supports Imgur, Pixiv, Deviantart, Tumblr, Reddit, and a host of other gallery and blog sites.\n\nYou can either feed a gallery URL straight to it\n\n    gallery-dl https://imgur.com/a/gC5fd\n\nor create a text file of URLs (let's say lotsofURLs.txt) with one URL per line. You can feed that text file in and it will download each line with a URL one by one.\n\n    gallery-dl -i lotsofURLs.txt\n\nSome sites (such as Pixiv) will require you to provide a username and password via a config file  in your user directory (ie on Windows if your account name is \"hoarderdude\" your user directory would be C:\\\\Users\\\\hoarderdude\n\nThe default Imgur gallery directory saving path does not use the gallery title AFAIK, so if you want a nicer directory structure editing a config file may also be useful.\n\nTo do this, create a text file named gallery-dl.txt in your user directory, fill it with the following (as an example):\n\n    {\n    \"extractor\":\n    {\n        \"base-directory\": \"./gallery-dl/\",\n        \"imgur\":\n    \t{\n    \t\t\"directory\": [\"imgur\", \"{album['id']} - {album['title']}\"]\n    \t}\n    }\n    }\n\nand then rename it from gallery-dl.txt to gallery-dl.conf\n\nThis will ensure directories are labelled with the Imgur gallery name if it exists.\n\nFor further configuration file examples, see:\n\n[https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl.conf](https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl.conf)\n\n[https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl-example.conf](https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl-example.conf)", "author_fullname": "t2_3ag4dgwo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "gallery-dl - Tool to download entire image galleries (and lists of galleries) from dozens of different sites. (Very relevant now due to Imgur purging its galleries, best download your favs before it's too late)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tvpay", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682068114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since Imgur is purging its old archives, I thought it&amp;#39;d be a good idea to post about gallery-dl for those who haven&amp;#39;t heard of it before&lt;/p&gt;\n\n&lt;p&gt;For those that have image galleries they want to save, I&amp;#39;d highly recommend the use of gallery-dl to save them to your hard drive.  You only need a little bit of knowledge with the command line.  (Grab the Standalone Executable for the easiest time, or use the pip installer command if you have Python)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl\"&gt;https://github.com/mikf/gallery-dl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It supports Imgur, Pixiv, Deviantart, Tumblr, Reddit, and a host of other gallery and blog sites.&lt;/p&gt;\n\n&lt;p&gt;You can either feed a gallery URL straight to it&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl https://imgur.com/a/gC5fd\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;or create a text file of URLs (let&amp;#39;s say lotsofURLs.txt) with one URL per line. You can feed that text file in and it will download each line with a URL one by one.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl -i lotsofURLs.txt\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Some sites (such as Pixiv) will require you to provide a username and password via a config file  in your user directory (ie on Windows if your account name is &amp;quot;hoarderdude&amp;quot; your user directory would be C:\\Users\\hoarderdude&lt;/p&gt;\n\n&lt;p&gt;The default Imgur gallery directory saving path does not use the gallery title AFAIK, so if you want a nicer directory structure editing a config file may also be useful.&lt;/p&gt;\n\n&lt;p&gt;To do this, create a text file named gallery-dl.txt in your user directory, fill it with the following (as an example):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n&amp;quot;extractor&amp;quot;:\n{\n    &amp;quot;base-directory&amp;quot;: &amp;quot;./gallery-dl/&amp;quot;,\n    &amp;quot;imgur&amp;quot;:\n    {\n        &amp;quot;directory&amp;quot;: [&amp;quot;imgur&amp;quot;, &amp;quot;{album[&amp;#39;id&amp;#39;]} - {album[&amp;#39;title&amp;#39;]}&amp;quot;]\n    }\n}\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and then rename it from gallery-dl.txt to gallery-dl.conf&lt;/p&gt;\n\n&lt;p&gt;This will ensure directories are labelled with the Imgur gallery name if it exists.&lt;/p&gt;\n\n&lt;p&gt;For further configuration file examples, see:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl.conf\"&gt;https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl.conf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl-example.conf\"&gt;https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl-example.conf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?auto=webp&amp;v=enabled&amp;s=78e243e74ab154280d2365bc3a61af0ef5bbfc88", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36beb6e6e384a17de98bd61bd1419834898d4e96", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2d33d492cc71cbd84229959427255d3968a8874", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce5e77d09375b7711e4a9603efaa1d0bb2770237", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ab3bb30c3d902b4e666fe27348dd06c79dca19c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79f108592d905d6f59ea93e6c817cd44e2bacd5c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b8ad76a800514dad7ba0eb8469e9f5dec589511", "width": 1080, "height": 540}], "variants": {}, "id": "b2NtGXcZyhx8Fn1TFGs7L3pJbMITjJPMk0h1XE30rSQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tvpay", "is_robot_indexable": true, "report_reasons": null, "author": "boastful_inaba", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tvpay/gallerydl_tool_to_download_entire_image_galleries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tvpay/gallerydl_tool_to_download_entire_image_galleries/", "subreddit_subscribers": 678877, "created_utc": 1682068114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Okay okay, I'm not trying to delete them (see: sub banner). But I've heard Twitter has an 800 bookmark view/API limit, so I'm wondering if I need to download things *past* that limit, would I need to remove the previous 800? If so, is there no better way other than just manually removing one at a time?\n\nI'm trying out tools like wfdownloader but its throwing me a 403 so I'm waiting in the meantime to try again later. Dropping in to ask if anyone's had experience with this though!\n\n\\[Bonus question, how the heck do people find out how many bookmarks they have. I know I have at least 800 but the exact number would be neat to know.\\]", "author_fullname": "t2_72ndd4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Solutions for mass-removing Twitter bookmarks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tvayd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682066742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Okay okay, I&amp;#39;m not trying to delete them (see: sub banner). But I&amp;#39;ve heard Twitter has an 800 bookmark view/API limit, so I&amp;#39;m wondering if I need to download things &lt;em&gt;past&lt;/em&gt; that limit, would I need to remove the previous 800? If so, is there no better way other than just manually removing one at a time?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying out tools like wfdownloader but its throwing me a 403 so I&amp;#39;m waiting in the meantime to try again later. Dropping in to ask if anyone&amp;#39;s had experience with this though!&lt;/p&gt;\n\n&lt;p&gt;[Bonus question, how the heck do people find out how many bookmarks they have. I know I have at least 800 but the exact number would be neat to know.]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tvayd", "is_robot_indexable": true, "report_reasons": null, "author": "Ikkyu9541", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tvayd/solutions_for_massremoving_twitter_bookmarks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tvayd/solutions_for_massremoving_twitter_bookmarks/", "subreddit_subscribers": 678877, "created_utc": 1682066742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Do you know any simple program that can backup folders into a \"database of files\", for deduplication, but without breaking files into chunks? Just simply to move the files into the database?\n\nIdeally the program will contain: database of files, identified by their hash + index for each backup cycle.\n\n&amp;#x200B;\n\nThe most important feature that I am looking for is: Deduplication.\n\nWhy I don't want any splitting/chunking? For simplicity, and so I will not be forced to \"trust\" the program for recovery. It should be easy to manually recover files, by checking the index &amp; find the files in the database, even without the program.\n\n&amp;#x200B;\n\nI will run this program manually.\n\nit also has to run on windows.\n\nBonus if it has some GUI.\n\nBonus if it can handle relative paths.\n\n&amp;#x200B;\n\nI checked duplicati, restic and borg and they all have a \"chunking\" feature that cannot be disabled. Their backup database is hard to read without the program.", "author_fullname": "t2_vq86wim9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a very simple deduplication program for backups (no file splitting/chunking, no encryption, no compression, no scheduling)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tr404", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682055185.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682053720.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you know any simple program that can backup folders into a &amp;quot;database of files&amp;quot;, for deduplication, but without breaking files into chunks? Just simply to move the files into the database?&lt;/p&gt;\n\n&lt;p&gt;Ideally the program will contain: database of files, identified by their hash + index for each backup cycle.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;The most important feature that I am looking for is: Deduplication.&lt;/p&gt;\n\n&lt;p&gt;Why I don&amp;#39;t want any splitting/chunking? For simplicity, and so I will not be forced to &amp;quot;trust&amp;quot; the program for recovery. It should be easy to manually recover files, by checking the index &amp;amp; find the files in the database, even without the program.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I will run this program manually.&lt;/p&gt;\n\n&lt;p&gt;it also has to run on windows.&lt;/p&gt;\n\n&lt;p&gt;Bonus if it has some GUI.&lt;/p&gt;\n\n&lt;p&gt;Bonus if it can handle relative paths.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I checked duplicati, restic and borg and they all have a &amp;quot;chunking&amp;quot; feature that cannot be disabled. Their backup database is hard to read without the program.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tr404", "is_robot_indexable": true, "report_reasons": null, "author": "algotrader944", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tr404/looking_for_a_very_simple_deduplication_program/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tr404/looking_for_a_very_simple_deduplication_program/", "subreddit_subscribers": 678877, "created_utc": 1682053720.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to save all the linked photos from a forum for posterity? With imgur removing photos that are not related to an account, it will completely destroy most of the content on a dreamwidth forum (it was originally hosted on LiveJournal) I go on. \n\nEvery day, a number of secrets are posted for people to comment on. They are posted in the main post as embedded images and then are linked directly below one by one for people to discuss. \n\nI don't want all those secrets just lost forever. But I also don't want to save every photo made in the replies.", "author_fullname": "t2_bamgj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to save every linked image from the main body of posts on a site (but not the comments)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tj4p6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682033681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to save all the linked photos from a forum for posterity? With imgur removing photos that are not related to an account, it will completely destroy most of the content on a dreamwidth forum (it was originally hosted on LiveJournal) I go on. &lt;/p&gt;\n\n&lt;p&gt;Every day, a number of secrets are posted for people to comment on. They are posted in the main post as embedded images and then are linked directly below one by one for people to discuss. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want all those secrets just lost forever. But I also don&amp;#39;t want to save every photo made in the replies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tj4p6", "is_robot_indexable": true, "report_reasons": null, "author": "Ashmeadow", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tj4p6/is_there_a_way_to_save_every_linked_image_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tj4p6/is_there_a_way_to_save_every_linked_image_from/", "subreddit_subscribers": 678877, "created_utc": 1682033681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a lot of HDD's used on eBay and the seller stated they were all operating as they should. All but one contains some sort of error but this one confused me. Should I be using this drive? Any help is appreciated. Thank you!\n\n    Complete error log:\n    \n    SMART Extended Comprehensive Error Log Version: 1 (6 sectors)\n    Device Error Count: 1\n    \tCR     = Command Register\n    \tFEATR  = Features Register\n    \tCOUNT  = Count (was: Sector Count) Register\n    \tLBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    \tLH     = LBA High (was: Cylinder High) Register    ]   LBA\n    \tLM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    \tLL     = LBA Low (was: Sector Number) Register     ]\n    \tDV     = Device (was: Device/Head) Register\n    \tDC     = Device Control Register\n    \tER     = Error register\n    \tST     = Status register\n    Powered_Up_Time is measured from power on, and printed as\n    DDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\n    SS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n    \n    Error 1 [0] occurred at disk power-on lifetime: 1321 hours (55 days + 1 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      04 -- 51 00 01 00 00 00 00 00 00 40 00  Error: ABRT\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      3f 00 00 00 01 00 00 00 00 00 e0 40 00     00:00:02.250  WRITE LOG EXT\n      ec 00 00 00 00 00 00 00 00 00 00 00 00     00:00:02.250  IDENTIFY DEVICE\n      b0 00 d0 00 00 00 00 00 c2 4f 00 00 00     00:00:02.246  SMART READ DATA\n      ec 00 00 00 00 00 00 00 00 00 00 00 00     00:00:02.246  IDENTIFY DEVICE\n      b0 00 d0 00 00 00 00 00 c2 4f 00 00 00     00:00:02.242  SMART READ DATA", "author_fullname": "t2_11naa8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the meaning of this error given by GSmartControl?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12szliz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681999137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a lot of HDD&amp;#39;s used on eBay and the seller stated they were all operating as they should. All but one contains some sort of error but this one confused me. Should I be using this drive? Any help is appreciated. Thank you!&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Complete error log:\n\nSMART Extended Comprehensive Error Log Version: 1 (6 sectors)\nDevice Error Count: 1\n    CR     = Command Register\n    FEATR  = Features Register\n    COUNT  = Count (was: Sector Count) Register\n    LBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    LH     = LBA High (was: Cylinder High) Register    ]   LBA\n    LM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    LL     = LBA Low (was: Sector Number) Register     ]\n    DV     = Device (was: Device/Head) Register\n    DC     = Device Control Register\n    ER     = Error register\n    ST     = Status register\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It &amp;quot;wraps&amp;quot; after 49.710 days.\n\nError 1 [0] occurred at disk power-on lifetime: 1321 hours (55 days + 1 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  04 -- 51 00 01 00 00 00 00 00 00 40 00  Error: ABRT\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  3f 00 00 00 01 00 00 00 00 00 e0 40 00     00:00:02.250  WRITE LOG EXT\n  ec 00 00 00 00 00 00 00 00 00 00 00 00     00:00:02.250  IDENTIFY DEVICE\n  b0 00 d0 00 00 00 00 00 c2 4f 00 00 00     00:00:02.246  SMART READ DATA\n  ec 00 00 00 00 00 00 00 00 00 00 00 00     00:00:02.246  IDENTIFY DEVICE\n  b0 00 d0 00 00 00 00 00 c2 4f 00 00 00     00:00:02.242  SMART READ DATA\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12szliz", "is_robot_indexable": true, "report_reasons": null, "author": "bravemenrun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12szliz/what_is_the_meaning_of_this_error_given_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12szliz/what_is_the_meaning_of_this_error_given_by/", "subreddit_subscribers": 678877, "created_utc": 1681999137.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}