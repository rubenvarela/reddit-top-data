{"kind": "Listing", "data": {"after": "t3_12tj764", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_6kz4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Office Depot offered me a free charcoal grill with my hard drive purchase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12tipvp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 631, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 631, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Wqhy-NagOiu23poPEIEIhnyH2qIBE-yveYILc2jSgMA.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682032730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/nRjYJXN.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?auto=webp&amp;v=enabled&amp;s=877a9a18b8b50d040663871def4a1421df6e6386", "width": 3264, "height": 2448}, "resolutions": [{"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73051bbc5204949741714b34cb4b3b04f7590406", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f2b9690cc37062aef73886911d0f1e42ace0489", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccca79086fb20fe136388167e21bf0d06de5a397", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e6b22c4eb4c8459362e65f071bffd24eceb87db0", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09732ba25ca2f59740344f35746feec4199a9026", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bed1c49de8987d7410fdc80006b433cd17bc39a8", "width": 1080, "height": 810}], "variants": {}, "id": "Jo_AoGZNLMYWGVFreZ_a7VwlLo7DvuTcr0I0Xi7tqb8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1/10 PB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tipvp", "is_robot_indexable": true, "report_reasons": null, "author": "mrtramplefoot", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12tipvp/office_depot_offered_me_a_free_charcoal_grill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/nRjYJXN.jpg", "subreddit_subscribers": 678668, "created_utc": 1682032730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Imgur is updating their TOS on May 15, 2023: All NSFW content to be banned, all content outside of a registered account (no-account uploads) to be removed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sq7ip", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 374, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_of987", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 374, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "apolloapp", "selftext": "", "author_fullname": "t2_5cymt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Imgur is updating their TOS on May 15, 2023: All NSFW content to be banned, all content outside of a registered account (no-account uploads) to be removed.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/apolloapp", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sjeo5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1178, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Announcement \ud83d\udce3", "can_mod_post": false, "score": 1178, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "nsfw", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681958916.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgurinc.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgurinc.com/rules", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "1ba6dd68-f254-11eb-b308-5621ae2f972b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_363lq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12sjeo5", "is_robot_indexable": true, "report_reasons": null, "author": "rekabis", "discussion_type": null, "num_comments": 277, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/apolloapp/comments/12sjeo5/imgur_is_updating_their_tos_on_may_15_2023_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgurinc.com/rules", "subreddit_subscribers": 767211, "created_utc": 1681958916.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1681976321.0, "link_flair_type": "text", "wls": 3, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgurinc.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgurinc.com/rules", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": true, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": true, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12sq7ip", "is_robot_indexable": true, "report_reasons": null, "author": "GubmintTroll", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "promo_adult_nsfw", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12sjeo5", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sq7ip/imgur_is_updating_their_tos_on_may_15_2023_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgurinc.com/rules", "subreddit_subscribers": 678668, "created_utc": 1681976321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_yaxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shutting down Legit Torrents after 17 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sy755", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 215, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 215, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681996304.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "legittorrents.info", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.legittorrents.info/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12sy755", "is_robot_indexable": true, "report_reasons": null, "author": "en3r0", "discussion_type": null, "num_comments": 31, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sy755/shutting_down_legit_torrents_after_17_years/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.legittorrents.info/", "subreddit_subscribers": 678668, "created_utc": 1681996304.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We have to prepare ourselves for the possibility that Reddit might try to pull a Tumblr soon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sqpn6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 143, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_udbds5yu", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 143, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Archiveteam", "selftext": "Reddit has been quietly putting restrictions on NSFW subs for a while now. You can't upload videos in then anymore. You can't use Reddit's native image host in them anymore. They won't show up on [r/all](https://www.reddit.com/r/all) anymore. And now, with the upcoming API change, you soon won't be able to access them through 3rd party tools anymore. \n\nAll of this sounds to me a lot like foreplay. I wouldn't be surprised if, soon, Reddit simply pulls the plug on NSFW content altogether. Just look at what they're doing to Imgur, which might I remind you is owned by many of the same people who own Reddit. \n\nReddit, as it stands, is a relic of an older age of the Internet. Far less sanitized than other social media sites. And far less marketable. For the upcoming IPO, they want Reddit to be clean, squeaky clean.", "author_fullname": "t2_udbds5yu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "We have to prepare ourselves for the possibility that Reddit might try to pull a Tumblr soon", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Archiveteam", "hidden": false, "pwls": 6, "link_flair_css_class": null, "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sqopw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 129, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 129, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681977696.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Archiveteam", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reddit has been quietly putting restrictions on NSFW subs for a while now. You can&amp;#39;t upload videos in then anymore. You can&amp;#39;t use Reddit&amp;#39;s native image host in them anymore. They won&amp;#39;t show up on &lt;a href=\"https://www.reddit.com/r/all\"&gt;r/all&lt;/a&gt; anymore. And now, with the upcoming API change, you soon won&amp;#39;t be able to access them through 3rd party tools anymore. &lt;/p&gt;\n\n&lt;p&gt;All of this sounds to me a lot like foreplay. I wouldn&amp;#39;t be surprised if, soon, Reddit simply pulls the plug on NSFW content altogether. Just look at what they&amp;#39;re doing to Imgur, which might I remind you is owned by many of the same people who own Reddit. &lt;/p&gt;\n\n&lt;p&gt;Reddit, as it stands, is a relic of an older age of the Internet. Far less sanitized than other social media sites. And far less marketable. For the upcoming IPO, they want Reddit to be clean, squeaky clean.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sug0", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12sqopw", "is_robot_indexable": true, "report_reasons": null, "author": "I_got_too_silly", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Archiveteam/comments/12sqopw/we_have_to_prepare_ourselves_for_the_possibility/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/Archiveteam/comments/12sqopw/we_have_to_prepare_ourselves_for_the_possibility/", "subreddit_subscribers": 11858, "created_utc": 1681977696.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1681977769.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.Archiveteam", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/Archiveteam/comments/12sqopw/we_have_to_prepare_ourselves_for_the_possibility/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12sqpn6", "is_robot_indexable": true, "report_reasons": null, "author": "I_got_too_silly", "discussion_type": null, "num_comments": 62, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12sqopw", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sqpn6/we_have_to_prepare_ourselves_for_the_possibility/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/Archiveteam/comments/12sqopw/we_have_to_prepare_ourselves_for_the_possibility/", "subreddit_subscribers": 678668, "created_utc": 1681977769.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_3qve30kb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "reddit-img-dl: Tool to download media and comments from subreddits or reddit users. Its a fork of the The-Eye-Team/reddit-dl with additional features. I worked on this afew years ago, but it might be useful to the people here with the recent reddit and imgur changes.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12t4ohz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 56, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 56, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682004665.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/IceWreck/reddit-img-dl", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12t4ohz", "is_robot_indexable": true, "report_reasons": null, "author": "HulkaBurninFudge", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12t4ohz/redditimgdl_tool_to_download_media_and_comments/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/IceWreck/reddit-img-dl", "subreddit_subscribers": 678668, "created_utc": 1682004665.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have over 100 URLs of bookmarks I\u2019ve collected over the years saved in a file. My question is if I wanted to permanently save the actual pages themselves (articles, essays, blog posts etc) what would be the best and easiest way to do it?\n\nIs making a PDF or an HTML file of the page in question sufficient?", "author_fullname": "t2_gfj7b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to save a webpage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12t6c63", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": 1682009102.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682006868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have over 100 URLs of bookmarks I\u2019ve collected over the years saved in a file. My question is if I wanted to permanently save the actual pages themselves (articles, essays, blog posts etc) what would be the best and easiest way to do it?&lt;/p&gt;\n\n&lt;p&gt;Is making a PDF or an HTML file of the page in question sufficient?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12t6c63", "is_robot_indexable": true, "report_reasons": null, "author": "Cmyers1980", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12t6c63/whats_the_best_way_to_save_a_webpage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12t6c63/whats_the_best_way_to_save_a_webpage/", "subreddit_subscribers": 678668, "created_utc": 1682006868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm merging dashcam files using bandicam on an HDD that has 400gb of free space. The output file is under 100gb - but I'm getting an error 70% of the way through saying \"not enough disk space on output folder\".\n\nWhat could be the reason for this?", "author_fullname": "t2_n7q75", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Not enough disk space, please change output folder\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12td88j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.69, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682020650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m merging dashcam files using bandicam on an HDD that has 400gb of free space. The output file is under 100gb - but I&amp;#39;m getting an error 70% of the way through saying &amp;quot;not enough disk space on output folder&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What could be the reason for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12td88j", "is_robot_indexable": true, "report_reasons": null, "author": "StarSurf", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12td88j/not_enough_disk_space_please_change_output_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12td88j/not_enough_disk_space_please_change_output_folder/", "subreddit_subscribers": 678668, "created_utc": 1682020650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a book scanner to digitize a large collection of books for a project. The scanner should handle various book sizes and types and scan books accurately without causing any damage. The scans need to be high-resolution with clear and sharp images for digital archiving and printing purposes. Open to considering options across different budgets. Please share any experience or recommendations for the best professional book scanner. Thank you!", "author_fullname": "t2_cv6pdkug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing Book Scanner? Need Recommendation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tb9z1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682016650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a book scanner to digitize a large collection of books for a project. The scanner should handle various book sizes and types and scan books accurately without causing any damage. The scans need to be high-resolution with clear and sharp images for digital archiving and printing purposes. Open to considering options across different budgets. Please share any experience or recommendations for the best professional book scanner. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tb9z1", "is_robot_indexable": true, "report_reasons": null, "author": "InterestingEmploy669", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tb9z1/choosing_book_scanner_need_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tb9z1/choosing_book_scanner_need_recommendation/", "subreddit_subscribers": 678668, "created_utc": 1682016650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of \"assets\" for things like video editing a graphic design that I want to not only organize well but navigate and preview content from efficiently. A common workflow for me is to be Photoshopping something, need an icon, and often it could be faster to google for some icon svg than it would be to locate it on my NAS.\n\n\u200b\n\nI also often want to add associated metadata that I could keep or possibly search later. Say I download a photo that requires artist attribution, I need to keep that license information with it.  \n\n\nI probably am looking for a Windows-based desktop utility since I am trying to use it with Photoshop and such, but I could potentially host something that better indexes/searches my content and download individual assets per-project.\n\nEdit: I am realizing that Adobe Bridge is actually looking pretty good", "author_fullname": "t2_1jfc9efx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use any \"Digital Asset Management\" tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlwwt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682049658.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682040211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of &amp;quot;assets&amp;quot; for things like video editing a graphic design that I want to not only organize well but navigate and preview content from efficiently. A common workflow for me is to be Photoshopping something, need an icon, and often it could be faster to google for some icon svg than it would be to locate it on my NAS.&lt;/p&gt;\n\n&lt;p&gt;\u200b&lt;/p&gt;\n\n&lt;p&gt;I also often want to add associated metadata that I could keep or possibly search later. Say I download a photo that requires artist attribution, I need to keep that license information with it.  &lt;/p&gt;\n\n&lt;p&gt;I probably am looking for a Windows-based desktop utility since I am trying to use it with Photoshop and such, but I could potentially host something that better indexes/searches my content and download individual assets per-project.&lt;/p&gt;\n\n&lt;p&gt;Edit: I am realizing that Adobe Bridge is actually looking pretty good&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tlwwt", "is_robot_indexable": true, "report_reasons": null, "author": "TechSquidTV", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tlwwt/do_you_use_any_digital_asset_management_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tlwwt/do_you_use_any_digital_asset_management_tools/", "subreddit_subscribers": 678668, "created_utc": 1682040211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have two large SAS/SATA expanders, a 16-bay and a 24-bay both made by RAID Machine (link to the 24-bay version below) They're great and they do the trick but the fans are like jet engines taking off, they're ridiculously loud. Would anyone happen to know if replacing the fans is a viable option? i dunno if they have temp sensors or something like that. If they are replaceable, would anyone have suggestions on quieter replacement fans? Or if the noise is just what it is, any suggestions on some kind of cabinet or something I could build out to try to mitigate it? Obviously airflow/ventilation would be a primary concern. They're currently rackmounted in a custom ATA style road case on wheels that slides under my desk. Anyway, I'm all ears if anyone has thoughts.\n\n&amp;#x200B;\n\n[https://www.pc-pitstop.com/24-bay-12g-expander-enclosure](https://www.pc-pitstop.com/24-bay-12g-expander-enclosure)", "author_fullname": "t2_aukaiie4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loud fans", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tkp6o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682037370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have two large SAS/SATA expanders, a 16-bay and a 24-bay both made by RAID Machine (link to the 24-bay version below) They&amp;#39;re great and they do the trick but the fans are like jet engines taking off, they&amp;#39;re ridiculously loud. Would anyone happen to know if replacing the fans is a viable option? i dunno if they have temp sensors or something like that. If they are replaceable, would anyone have suggestions on quieter replacement fans? Or if the noise is just what it is, any suggestions on some kind of cabinet or something I could build out to try to mitigate it? Obviously airflow/ventilation would be a primary concern. They&amp;#39;re currently rackmounted in a custom ATA style road case on wheels that slides under my desk. Anyway, I&amp;#39;m all ears if anyone has thoughts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pc-pitstop.com/24-bay-12g-expander-enclosure\"&gt;https://www.pc-pitstop.com/24-bay-12g-expander-enclosure&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?auto=webp&amp;v=enabled&amp;s=da26c84211db6bfe70d900ba26856f0bcf91505c", "width": 400, "height": 170}, "resolutions": [{"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f2f4ff1fca374eb83aa4760f5c7d3e9712af75c", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1730f57cf94ea15b684b80527e7242b25348ae9b", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45d4f4395be91ba11bf9c841d9371c77ae8631a3", "width": 320, "height": 136}], "variants": {}, "id": "Qh5tIF25DB92XS8vDre-aDXLS7_h3WcZhBsWj1sK2DQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tkp6o", "is_robot_indexable": true, "report_reasons": null, "author": "2Ksmooth", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tkp6o/loud_fans/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tkp6o/loud_fans/", "subreddit_subscribers": 678668, "created_utc": 1682037370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So i am currently running httrack for pretty large web backup projects (looking for newer alternatives tho)\n\n**What are your recommendations for deduplication of the downloaded files?**  \n(currently saving everything on a zfs pool with dedup/compression enabled)\n\n**Are there scripts for changing the local urls within the downloaded pages afterwards?**  \nLets say i download [example.com](https://example.com) and [forum.example.com](https://forum.example.com) separatly a couple weeks apart, since they are different projects within httrack all links that go to the other one are still refering to the original instead of the download (if an [forum.example.com](https://forum.example.com) site is linked within an [example.com](https://example.com) site it is still linking to the original [forum.example.com](https://forum.example.com) instad of the downloaded one) which makes sense since httrack didnt know it existed when it was downloaded but is there a way to change is later down the line? Basically a script that goes though all html files and replaces the external domains with internal ones?\n\n**What settings do you use? What to look out for to get the best performance and backup?**", "author_fullname": "t2_8jnr5wv6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HTTRack Discussion | deduplication, merging &amp; optimal settings", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12szkc3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681999070.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i am currently running httrack for pretty large web backup projects (looking for newer alternatives tho)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What are your recommendations for deduplication of the downloaded files?&lt;/strong&gt;&lt;br/&gt;\n(currently saving everything on a zfs pool with dedup/compression enabled)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Are there scripts for changing the local urls within the downloaded pages afterwards?&lt;/strong&gt;&lt;br/&gt;\nLets say i download &lt;a href=\"https://example.com\"&gt;example.com&lt;/a&gt; and &lt;a href=\"https://forum.example.com\"&gt;forum.example.com&lt;/a&gt; separatly a couple weeks apart, since they are different projects within httrack all links that go to the other one are still refering to the original instead of the download (if an &lt;a href=\"https://forum.example.com\"&gt;forum.example.com&lt;/a&gt; site is linked within an &lt;a href=\"https://example.com\"&gt;example.com&lt;/a&gt; site it is still linking to the original &lt;a href=\"https://forum.example.com\"&gt;forum.example.com&lt;/a&gt; instad of the downloaded one) which makes sense since httrack didnt know it existed when it was downloaded but is there a way to change is later down the line? Basically a script that goes though all html files and replaces the external domains with internal ones?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What settings do you use? What to look out for to get the best performance and backup?&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12szkc3", "is_robot_indexable": true, "report_reasons": null, "author": "Pommes254", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12szkc3/httrack_discussion_deduplication_merging_optimal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12szkc3/httrack_discussion_deduplication_merging_optimal/", "subreddit_subscribers": 678668, "created_utc": 1681999070.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "They used to be able to grab the .m3u8 file pretty easily and get the file. \nHowever, now they are possibly using DASH or maybe even .m3u8, but no sniffers are able to find it. They use (from what I have seen), two different video players THEOplayer and bitmovin player. They are also now using DRM protection.\n\nMost people I have spoken to are using OBS to just record it, however, unfortunately, my computer kind of sucks for doing stuff like that. Which results in choppy playback at some points.\n\nDoes anyone here (figured this may be one of the only places someone might) know how to still grab the file and download using FFMPEG?", "author_fullname": "t2_h5d1uatv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LivePhish has been updated", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12su2j8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681986853.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They used to be able to grab the .m3u8 file pretty easily and get the file. \nHowever, now they are possibly using DASH or maybe even .m3u8, but no sniffers are able to find it. They use (from what I have seen), two different video players THEOplayer and bitmovin player. They are also now using DRM protection.&lt;/p&gt;\n\n&lt;p&gt;Most people I have spoken to are using OBS to just record it, however, unfortunately, my computer kind of sucks for doing stuff like that. Which results in choppy playback at some points.&lt;/p&gt;\n\n&lt;p&gt;Does anyone here (figured this may be one of the only places someone might) know how to still grab the file and download using FFMPEG?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12su2j8", "is_robot_indexable": true, "report_reasons": null, "author": "GoldPhysical", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12su2j8/livephish_has_been_updated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12su2j8/livephish_has_been_updated/", "subreddit_subscribers": 678668, "created_utc": 1681986853.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve been collecting the audio files from podcasts and I want to cut out the ads from them. Are there any super simple audio editors that would work for this? All I need is the ability to cut out portions, I don\u2019t need any fancy editing tools. The main criteria would be efficiency, as I will need to do this for many files and it will probably have to be done manually.\n\nThank you for any recommendations!", "author_fullname": "t2_13f9bd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple Audio Editor for Removing Ads from Podcasts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlkzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682039447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been collecting the audio files from podcasts and I want to cut out the ads from them. Are there any super simple audio editors that would work for this? All I need is the ability to cut out portions, I don\u2019t need any fancy editing tools. The main criteria would be efficiency, as I will need to do this for many files and it will probably have to be done manually.&lt;/p&gt;\n\n&lt;p&gt;Thank you for any recommendations!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tlkzl", "is_robot_indexable": true, "report_reasons": null, "author": "rkusty23", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tlkzl/simple_audio_editor_for_removing_ads_from_podcasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tlkzl/simple_audio_editor_for_removing_ads_from_podcasts/", "subreddit_subscribers": 678668, "created_utc": 1682039447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to save all the linked photos from a forum for posterity? With imgur removing photos that are not related to an account, it will completely destroy most of the content on a dreamwidth forum (it was originally hosted on LiveJournal) I go on. \n\nEvery day, a number of secrets are posted for people to comment on. They are posted in the main post as embedded images and then are linked directly below one by one for people to discuss. \n\nI don't want all those secrets just lost forever. But I also don't want to save every photo made in the replies.", "author_fullname": "t2_bamgj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to save every linked image from the main body of posts on a site (but not the comments)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tj4p6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682033681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to save all the linked photos from a forum for posterity? With imgur removing photos that are not related to an account, it will completely destroy most of the content on a dreamwidth forum (it was originally hosted on LiveJournal) I go on. &lt;/p&gt;\n\n&lt;p&gt;Every day, a number of secrets are posted for people to comment on. They are posted in the main post as embedded images and then are linked directly below one by one for people to discuss. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want all those secrets just lost forever. But I also don&amp;#39;t want to save every photo made in the replies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tj4p6", "is_robot_indexable": true, "report_reasons": null, "author": "Ashmeadow", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tj4p6/is_there_a_way_to_save_every_linked_image_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tj4p6/is_there_a_way_to_save_every_linked_image_from/", "subreddit_subscribers": 678668, "created_utc": 1682033681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello peeps,\n\nUntil today, I thought  Z-Library was a mere mirror of LibGen, apparently this isn't so :)\n\nI have a nice setup with LibGen, it's like this:\n\nInterested in books about say Nutrition\n\n1. Download non-fiction LibGen dump from any of the sites hosting them.\n2. Initialize the DB locally.\n3. Run queries to identify what I need, returning `updated`.`IdentifierWODash` which is the stripped down ISBN in the LibGen dump.\n4. Download said books via Libgen Desktop, just because I haven't found a better software for it.\n\nCan I have something like that for Z-Library?\n\nMy use case is this: \u201cOMG, I should download every PDF book that ever existed on Psychology this weekend, that was published in the past 10 years\u201d.\n\nThen I need a way to make that happen.\n\nDoes Z-Library have a DB dump like Libgen does? What's your setup for getting files from it?\n\nEdit:\n\nDid a search on GitHub:\n\n    zlibrary in:topic pushed:&gt;2023-01-01 stars:&gt;=10\n\nMeans: repositories with zlibrary in their topics, where code was pushed this year and has at least 10 stars.\n\nResulted in\n\n1. [A project in Chinese](https://github.com/Senkita/zLib-Web). I have no idea what it's about.\n2. [ZLibrary CLI](https://github.com/baroxyton/zlibrary-CLI).\n\nZLibrary CLI looks like the only solution these days? It accounts for personal domains, the new Z-Library setup.\n\nIs there anything else?\n\nThanks  \n", "author_fullname": "t2_w0ujml6e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bulk download non-fiction from Z-Library and search Z-Library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tfoud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682029007.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682025947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello peeps,&lt;/p&gt;\n\n&lt;p&gt;Until today, I thought  Z-Library was a mere mirror of LibGen, apparently this isn&amp;#39;t so :)&lt;/p&gt;\n\n&lt;p&gt;I have a nice setup with LibGen, it&amp;#39;s like this:&lt;/p&gt;\n\n&lt;p&gt;Interested in books about say Nutrition&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download non-fiction LibGen dump from any of the sites hosting them.&lt;/li&gt;\n&lt;li&gt;Initialize the DB locally.&lt;/li&gt;\n&lt;li&gt;Run queries to identify what I need, returning &lt;code&gt;updated&lt;/code&gt;.&lt;code&gt;IdentifierWODash&lt;/code&gt; which is the stripped down ISBN in the LibGen dump.&lt;/li&gt;\n&lt;li&gt;Download said books via Libgen Desktop, just because I haven&amp;#39;t found a better software for it.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Can I have something like that for Z-Library?&lt;/p&gt;\n\n&lt;p&gt;My use case is this: \u201cOMG, I should download every PDF book that ever existed on Psychology this weekend, that was published in the past 10 years\u201d.&lt;/p&gt;\n\n&lt;p&gt;Then I need a way to make that happen.&lt;/p&gt;\n\n&lt;p&gt;Does Z-Library have a DB dump like Libgen does? What&amp;#39;s your setup for getting files from it?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;Did a search on GitHub:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;zlibrary in:topic pushed:&amp;gt;2023-01-01 stars:&amp;gt;=10\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Means: repositories with zlibrary in their topics, where code was pushed this year and has at least 10 stars.&lt;/p&gt;\n\n&lt;p&gt;Resulted in&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Senkita/zLib-Web\"&gt;A project in Chinese&lt;/a&gt;. I have no idea what it&amp;#39;s about.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/baroxyton/zlibrary-CLI\"&gt;ZLibrary CLI&lt;/a&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;ZLibrary CLI looks like the only solution these days? It accounts for personal domains, the new Z-Library setup.&lt;/p&gt;\n\n&lt;p&gt;Is there anything else?&lt;/p&gt;\n\n&lt;p&gt;Thanks  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?auto=webp&amp;v=enabled&amp;s=00f73080ff2bf090f2dd5dd09bb7fdced4eda3f4", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3767cf5de977f41e6bd19b5891dc929dfd49a38c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fd88e7c5c2bee32f08b847306aa0c06cefb4982", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba9deed576c5c01fabf614f6d86b9808639e71b7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd8574ae90c27b380a18e94b71f50e4265674eaf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74362ce9fab54f851ec521a4b36cab471d5246e6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd461a5b0618ce06de92e1ce7698bf4aa16f0931", "width": 1080, "height": 540}], "variants": {}, "id": "kogm69pcGJGnWfA-SgY36JvhgpMJjd2m-mJYyI6cGak"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tfoud", "is_robot_indexable": true, "report_reasons": null, "author": "EUTIORti", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tfoud/how_to_bulk_download_nonfiction_from_zlibrary_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tfoud/how_to_bulk_download_nonfiction_from_zlibrary_and/", "subreddit_subscribers": 678668, "created_utc": 1682025947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I relocate overseas for a few months each year, and I am looking for a portable solution.\n\nI am restricted by international travel weight restrictions, so would rather not have anything too heavy.\n\nAt home i run a 4 x 3tb Ironwolf softraid which currently backs up my 2 x 5tb seagate portable drives and macbook.\n\nI usually take the macbook and portable drives overseas with me. One of the 5tb for movies and the other for archived data that i may need. The macbook backups onto the data drive while I am away as a I only add a few gigs. This archive drive is nearly full and the movie drive tired so hence the need for expansion.\n\nMy current thought is either get a Synology DS220j with a single 16tb drive for now which I can expand later or the WD 16 TB Elements Desktop External USBC Hard Drive.   \nThis would allow me to add one of my 5tb portable drives to my softraid, and using the other as an offsite backup as it is getting old and isn't much use for anything else.\n\nDoes anyone have any other ideas?", "author_fullname": "t2_1gm1x1ds", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Semi-Portable NAS option? Currently looking at Synology DS220j but would a large external drive be better?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sv8lu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681989723.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I relocate overseas for a few months each year, and I am looking for a portable solution.&lt;/p&gt;\n\n&lt;p&gt;I am restricted by international travel weight restrictions, so would rather not have anything too heavy.&lt;/p&gt;\n\n&lt;p&gt;At home i run a 4 x 3tb Ironwolf softraid which currently backs up my 2 x 5tb seagate portable drives and macbook.&lt;/p&gt;\n\n&lt;p&gt;I usually take the macbook and portable drives overseas with me. One of the 5tb for movies and the other for archived data that i may need. The macbook backups onto the data drive while I am away as a I only add a few gigs. This archive drive is nearly full and the movie drive tired so hence the need for expansion.&lt;/p&gt;\n\n&lt;p&gt;My current thought is either get a Synology DS220j with a single 16tb drive for now which I can expand later or the WD 16 TB Elements Desktop External USBC Hard Drive.&lt;br/&gt;\nThis would allow me to add one of my 5tb portable drives to my softraid, and using the other as an offsite backup as it is getting old and isn&amp;#39;t much use for anything else.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any other ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12sv8lu", "is_robot_indexable": true, "report_reasons": null, "author": "matmah", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sv8lu/semiportable_nas_option_currently_looking_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12sv8lu/semiportable_nas_option_currently_looking_at/", "subreddit_subscribers": 678668, "created_utc": 1681989723.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So here\u2019s the problem. Despite my diligence with backing up, I noticed my backup drive has fewer folders than my primary drive and I\u2019m trying to figure out what\u2019s missing, but there are hundreds of folders in the main directory alone and manually comparing is a nightmare. I\u2019m tempted to just delete the backup and re-copy it all, but that\u2019s 3 terabytes of data, I don\u2019t have all day to wait around for it to copy over. \n\nWhat I\u2019m looking for is a program that will compare the directories, and if there\u2019s a whole folder or even just a single file missing from one, it\u2019ll make note of that and report it back. It would be even better if it could compare the checksum of each file and report if one is different. I believe the missing folders are due to my mistake rather than file corruption, but it would be a relief to know for sure.", "author_fullname": "t2_tggommtv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to automatically compare two folder trees and report back the differences between them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12tpfvr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682049023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So here\u2019s the problem. Despite my diligence with backing up, I noticed my backup drive has fewer folders than my primary drive and I\u2019m trying to figure out what\u2019s missing, but there are hundreds of folders in the main directory alone and manually comparing is a nightmare. I\u2019m tempted to just delete the backup and re-copy it all, but that\u2019s 3 terabytes of data, I don\u2019t have all day to wait around for it to copy over. &lt;/p&gt;\n\n&lt;p&gt;What I\u2019m looking for is a program that will compare the directories, and if there\u2019s a whole folder or even just a single file missing from one, it\u2019ll make note of that and report it back. It would be even better if it could compare the checksum of each file and report if one is different. I believe the missing folders are due to my mistake rather than file corruption, but it would be a relief to know for sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tpfvr", "is_robot_indexable": true, "report_reasons": null, "author": "bobisnotmyuncIe", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tpfvr/is_there_a_way_to_automatically_compare_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpfvr/is_there_a_way_to_automatically_compare_two/", "subreddit_subscribers": 678668, "created_utc": 1682049023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Link: [https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html](https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html)", "author_fullname": "t2_22bur9io", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BuzzFeed News, Which Dragged Media Into the Digital Age, Shuts Down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tno3h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682044442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Link: &lt;a href=\"https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html\"&gt;https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?auto=webp&amp;v=enabled&amp;s=23240c57aab8133784c52a6677408f9e5bbcae8e", "width": 1050, "height": 549}, "resolutions": [{"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c6302b02d8bf5d9e01ef89d1951c0544f454a0a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c49dbec0d1ad8f5ff6d557048585eae3d2a7c147", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fad551e310ce78029b4e1864946889af23b1d70", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a6297044c9341a30035e3f8337f912a0995302be", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d73ea86ac977f2982068f14de815ec3b4154ea57", "width": 960, "height": 501}], "variants": {}, "id": "b1Knl5I0c-FFco_fthSZJqci2gEa_iHaq3W3AkA04tE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tno3h", "is_robot_indexable": true, "report_reasons": null, "author": "classicalist", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tno3h/buzzfeed_news_which_dragged_media_into_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tno3h/buzzfeed_news_which_dragged_media_into_the/", "subreddit_subscribers": 678668, "created_utc": 1682044442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "my favourite vtuber channel is being deleted in 10 days. Ive archived 1.5TB of vids, shorts, and livestreams but was wondering what the best way to copy community content.  Are there any tools that can help automate this since there are too many memes/photos to manually save.", "author_fullname": "t2_i8ob7wwj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I copy community posts in a good way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12th5q2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682029218.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;my favourite vtuber channel is being deleted in 10 days. Ive archived 1.5TB of vids, shorts, and livestreams but was wondering what the best way to copy community content.  Are there any tools that can help automate this since there are too many memes/photos to manually save.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12th5q2", "is_robot_indexable": true, "report_reasons": null, "author": "CRTAutist1337", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12th5q2/how_do_i_copy_community_posts_in_a_good_way/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12th5q2/how_do_i_copy_community_posts_in_a_good_way/", "subreddit_subscribers": 678668, "created_utc": 1682029218.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I bought a lot of HDD's used on eBay and the seller stated they were all operating as they should. All but one contains some sort of error but this one confused me. Should I be using this drive? Any help is appreciated. Thank you!\n\n    Complete error log:\n    \n    SMART Extended Comprehensive Error Log Version: 1 (6 sectors)\n    Device Error Count: 1\n    \tCR     = Command Register\n    \tFEATR  = Features Register\n    \tCOUNT  = Count (was: Sector Count) Register\n    \tLBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    \tLH     = LBA High (was: Cylinder High) Register    ]   LBA\n    \tLM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    \tLL     = LBA Low (was: Sector Number) Register     ]\n    \tDV     = Device (was: Device/Head) Register\n    \tDC     = Device Control Register\n    \tER     = Error register\n    \tST     = Status register\n    Powered_Up_Time is measured from power on, and printed as\n    DDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\n    SS=sec, and sss=millisec. It \"wraps\" after 49.710 days.\n    \n    Error 1 [0] occurred at disk power-on lifetime: 1321 hours (55 days + 1 hours)\n      When the command that caused the error occurred, the device was active or idle.\n    \n      After command completion occurred, registers were:\n      ER -- ST COUNT  LBA_48  LH LM LL DV DC\n      -- -- -- == -- == == == -- -- -- -- --\n      04 -- 51 00 01 00 00 00 00 00 00 40 00  Error: ABRT\n    \n      Commands leading to the command that caused the error were:\n      CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n      -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n      3f 00 00 00 01 00 00 00 00 00 e0 40 00     00:00:02.250  WRITE LOG EXT\n      ec 00 00 00 00 00 00 00 00 00 00 00 00     00:00:02.250  IDENTIFY DEVICE\n      b0 00 d0 00 00 00 00 00 c2 4f 00 00 00     00:00:02.246  SMART READ DATA\n      ec 00 00 00 00 00 00 00 00 00 00 00 00     00:00:02.246  IDENTIFY DEVICE\n      b0 00 d0 00 00 00 00 00 c2 4f 00 00 00     00:00:02.242  SMART READ DATA", "author_fullname": "t2_11naa8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the meaning of this error given by GSmartControl?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12szliz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681999137.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I bought a lot of HDD&amp;#39;s used on eBay and the seller stated they were all operating as they should. All but one contains some sort of error but this one confused me. Should I be using this drive? Any help is appreciated. Thank you!&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Complete error log:\n\nSMART Extended Comprehensive Error Log Version: 1 (6 sectors)\nDevice Error Count: 1\n    CR     = Command Register\n    FEATR  = Features Register\n    COUNT  = Count (was: Sector Count) Register\n    LBA_48 = Upper bytes of LBA High/Mid/Low Registers ]  ATA-8\n    LH     = LBA High (was: Cylinder High) Register    ]   LBA\n    LM     = LBA Mid (was: Cylinder Low) Register      ] Register\n    LL     = LBA Low (was: Sector Number) Register     ]\n    DV     = Device (was: Device/Head) Register\n    DC     = Device Control Register\n    ER     = Error register\n    ST     = Status register\nPowered_Up_Time is measured from power on, and printed as\nDDd+hh:mm:SS.sss where DD=days, hh=hours, mm=minutes,\nSS=sec, and sss=millisec. It &amp;quot;wraps&amp;quot; after 49.710 days.\n\nError 1 [0] occurred at disk power-on lifetime: 1321 hours (55 days + 1 hours)\n  When the command that caused the error occurred, the device was active or idle.\n\n  After command completion occurred, registers were:\n  ER -- ST COUNT  LBA_48  LH LM LL DV DC\n  -- -- -- == -- == == == -- -- -- -- --\n  04 -- 51 00 01 00 00 00 00 00 00 40 00  Error: ABRT\n\n  Commands leading to the command that caused the error were:\n  CR FEATR COUNT  LBA_48  LH LM LL DV DC  Powered_Up_Time  Command/Feature_Name\n  -- == -- == -- == == == -- -- -- -- --  ---------------  --------------------\n  3f 00 00 00 01 00 00 00 00 00 e0 40 00     00:00:02.250  WRITE LOG EXT\n  ec 00 00 00 00 00 00 00 00 00 00 00 00     00:00:02.250  IDENTIFY DEVICE\n  b0 00 d0 00 00 00 00 00 c2 4f 00 00 00     00:00:02.246  SMART READ DATA\n  ec 00 00 00 00 00 00 00 00 00 00 00 00     00:00:02.246  IDENTIFY DEVICE\n  b0 00 d0 00 00 00 00 00 c2 4f 00 00 00     00:00:02.242  SMART READ DATA\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12szliz", "is_robot_indexable": true, "report_reasons": null, "author": "bravemenrun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12szliz/what_is_the_meaning_of_this_error_given_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12szliz/what_is_the_meaning_of_this_error_given_by/", "subreddit_subscribers": 678668, "created_utc": 1681999137.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I have a Synology with 2  identical disks (6tb each) but it seems one of them is dying. I got an  email saying it was failing with this on the SMART: Multi\\_zone\\_error\\_rate 1\n\nI tried running a fast SMART and all was ok, but the extended SMART won\u2019t go over 90%, so I am assuming the disk is not ok.\n\nThis a WD RED from 2019, model WDC-WD60EFRX-68L0BN1\n\nI have a few questions I would like your help with, but a bit of an insight about the situation first.\n\nThe NAS is at my parents house  and I am currently in another country. They will swap the disk for me,  but I am buying a new one where I am as it is way cheaper. A friend of  mine will be visiting me and taking the disk  with him, so I will have to wait 3 weeks until I can replace it.\n\n&amp;#x200B;\n\n1. Is  the disk done for, or it just one part of it that is not OK? Being the  later, I am assuming I could use it as a tertiary backup?\n2. If  the disk is done for, should I remove it from the NAS? This question is more on the fact the NAS as a mirror (SHR) and I am afraid the bad disk data  might affect the  good one?\n3. I  am swapping the disk wit a bigger one (from 6 to 8tb). I am aware I  will only have 6tb available and that is fine, but I prefer swapping the  other at a later date  and have different manufacture dates. How does that work? Can I swap  one disk and wait for it to rebuild? And at a later date do it again,  but there will be an extra step to increase the size?\n\n&amp;#x200B;\n\nThank you for your help", "author_fullname": "t2_f4cphpgz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replace disk on synology for bigger one", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sz4ad", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681998150.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a Synology with 2  identical disks (6tb each) but it seems one of them is dying. I got an  email saying it was failing with this on the SMART: Multi_zone_error_rate 1&lt;/p&gt;\n\n&lt;p&gt;I tried running a fast SMART and all was ok, but the extended SMART won\u2019t go over 90%, so I am assuming the disk is not ok.&lt;/p&gt;\n\n&lt;p&gt;This a WD RED from 2019, model WDC-WD60EFRX-68L0BN1&lt;/p&gt;\n\n&lt;p&gt;I have a few questions I would like your help with, but a bit of an insight about the situation first.&lt;/p&gt;\n\n&lt;p&gt;The NAS is at my parents house  and I am currently in another country. They will swap the disk for me,  but I am buying a new one where I am as it is way cheaper. A friend of  mine will be visiting me and taking the disk  with him, so I will have to wait 3 weeks until I can replace it.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is  the disk done for, or it just one part of it that is not OK? Being the  later, I am assuming I could use it as a tertiary backup?&lt;/li&gt;\n&lt;li&gt;If  the disk is done for, should I remove it from the NAS? This question is more on the fact the NAS as a mirror (SHR) and I am afraid the bad disk data  might affect the  good one?&lt;/li&gt;\n&lt;li&gt;I  am swapping the disk wit a bigger one (from 6 to 8tb). I am aware I  will only have 6tb available and that is fine, but I prefer swapping the  other at a later date  and have different manufacture dates. How does that work? Can I swap  one disk and wait for it to rebuild? And at a later date do it again,  but there will be an extra step to increase the size?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12sz4ad", "is_robot_indexable": true, "report_reasons": null, "author": "babs-jojo", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sz4ad/replace_disk_on_synology_for_bigger_one/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12sz4ad/replace_disk_on_synology_for_bigger_one/", "subreddit_subscribers": 678668, "created_utc": 1681998150.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Keeping external TB NVMe SSD's connected overnight", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sqhya", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_15nb5n1c", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "MacOS", "selftext": "I've got a couple of NVMe SSD's connected via Thunderbolt to my mac mini (running ventura). \n\nWhen using a couple of different apps for the first time in the morning - Ableton, Photos, Music so far - there's a definite delay in opening files / starting up, and with Ableton, it 'forgets' the location of plugins stored on that external drive.\n\nIs there any way to make these types of drive act/look more like the internal drive and remain connected at all times (obv without creating heat or performance issues). I know they can't spin down like HDD but any advice appreciated. Thanks", "author_fullname": "t2_15nb5n1c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Keeping external TB NVMe SSD's connected overnight", "link_flair_richtext": [{"e": "text", "t": "Help"}], "subreddit_name_prefixed": "r/MacOS", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12sqhhk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681977128.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MacOS", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a couple of NVMe SSD&amp;#39;s connected via Thunderbolt to my mac mini (running ventura). &lt;/p&gt;\n\n&lt;p&gt;When using a couple of different apps for the first time in the morning - Ableton, Photos, Music so far - there&amp;#39;s a definite delay in opening files / starting up, and with Ableton, it &amp;#39;forgets&amp;#39; the location of plugins stored on that external drive.&lt;/p&gt;\n\n&lt;p&gt;Is there any way to make these types of drive act/look more like the internal drive and remain connected at all times (obv without creating heat or performance issues). I know they can&amp;#39;t spin down like HDD but any advice appreciated. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "85adf2d4-c706-11ea-ab81-0e3be20b73c5", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2s2gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12sqhhk", "is_robot_indexable": true, "report_reasons": null, "author": "ohsomacho", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MacOS/comments/12sqhhk/keeping_external_tb_nvme_ssds_connected_overnight/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/MacOS/comments/12sqhhk/keeping_external_tb_nvme_ssds_connected_overnight/", "subreddit_subscribers": 259415, "created_utc": 1681977128.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1681977171.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MacOS", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/MacOS/comments/12sqhhk/keeping_external_tb_nvme_ssds_connected_overnight/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12sqhya", "is_robot_indexable": true, "report_reasons": null, "author": "ohsomacho", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12sqhhk", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12sqhya/keeping_external_tb_nvme_ssds_connected_overnight/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/MacOS/comments/12sqhhk/keeping_external_tb_nvme_ssds_connected_overnight/", "subreddit_subscribers": 678668, "created_utc": 1681977171.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I need to scrape the webpage but can't find anything that copies all the posts perfectly.", "author_fullname": "t2_i8ob7wwj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best tool to scrape community posts from Youtube?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tmawz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682041112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to scrape the webpage but can&amp;#39;t find anything that copies all the posts perfectly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tmawz", "is_robot_indexable": true, "report_reasons": null, "author": "CRTAutist1337", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tmawz/what_is_the_best_tool_to_scrape_community_posts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tmawz/what_is_the_best_tool_to_scrape_community_posts/", "subreddit_subscribers": 678668, "created_utc": 1682041112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD compromised, unable to process RMAs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tk70n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_q4cpr", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "homelab", "selftext": "Just a heads up, it seems that Western Digital has been compromised for some time, and it is not only affecting their cloud services as they previously said. They are unable to complete RMAs.\n\nI sent a failed 16tb WD Red Pro (failed with \\~2k power on hours) in at the end of March.\n\nTheir RMA status tool does not work, it's just stuck at \"Pending return\"\n\nTheir online chat cannot lookup RMAs, confirm if your drive was received, etc. They said they have not been able to log into their internal systems for weeks.\n\nThey have turned every phone number into an endless loop, you cannot get a person on the phone.\n\nTheir twitter says \"Need help? DM @ WesternDigiCare\" - but they have turned off DMs.\n\nHere are a couple of the links I've stumbled upon while trying to figure out what is going on:\n\n[https://www.darkreading.com/vulnerabilities-threats/hackers-hold-data-hostage-demanding-8-figure-ransom-payment](https://www.darkreading.com/vulnerabilities-threats/hackers-hold-data-hostage-demanding-8-figure-ransom-payment)\n\n[https://twitter.com/vxunderground/status/1648139240864096256](https://twitter.com/vxunderground/status/1648139240864096256)", "author_fullname": "t2_q4cpr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD compromised, unable to process RMAs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/homelab", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tk5oz", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 42, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 42, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682036308.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682036118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just a heads up, it seems that Western Digital has been compromised for some time, and it is not only affecting their cloud services as they previously said. They are unable to complete RMAs.&lt;/p&gt;\n\n&lt;p&gt;I sent a failed 16tb WD Red Pro (failed with ~2k power on hours) in at the end of March.&lt;/p&gt;\n\n&lt;p&gt;Their RMA status tool does not work, it&amp;#39;s just stuck at &amp;quot;Pending return&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Their online chat cannot lookup RMAs, confirm if your drive was received, etc. They said they have not been able to log into their internal systems for weeks.&lt;/p&gt;\n\n&lt;p&gt;They have turned every phone number into an endless loop, you cannot get a person on the phone.&lt;/p&gt;\n\n&lt;p&gt;Their twitter says &amp;quot;Need help? DM @ WesternDigiCare&amp;quot; - but they have turned off DMs.&lt;/p&gt;\n\n&lt;p&gt;Here are a couple of the links I&amp;#39;ve stumbled upon while trying to figure out what is going on:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.darkreading.com/vulnerabilities-threats/hackers-hold-data-hostage-demanding-8-figure-ransom-payment\"&gt;https://www.darkreading.com/vulnerabilities-threats/hackers-hold-data-hostage-demanding-8-figure-ransom-payment&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://twitter.com/vxunderground/status/1648139240864096256\"&gt;https://twitter.com/vxunderground/status/1648139240864096256&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?auto=webp&amp;v=enabled&amp;s=d012ac5a7a664524a0c875d6db4d23fb9072be49", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5a79af38467baa3c1a743b9421d60a3d20cbf68", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d90bd9629a61db4d7f0db40ff77264008ec18bf", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04b22a41b1a0c3f4dd21da1d10d845b4d8f3bba1", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c3b2a9bffa3688fda7f9be391dfb3d57962ec1a", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=743666a3eab18ee3546ae9dbefcb320cda47ab6c", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=933ca691254c3e014b9b9d7bf088bde0761ad20c", "width": 1080, "height": 720}], "variants": {}, "id": "MJ5NjK1i1_-LUQRUJB_I01Zp8Vg9X3GT4d44qOh-PPQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7793475a-322a-11e6-a5cc-0e74ee60e56b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2ubz7", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#6b238e", "id": "12tk5oz", "is_robot_indexable": true, "report_reasons": null, "author": "CaptainBlanton", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/homelab/comments/12tk5oz/wd_compromised_unable_to_process_rmas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/homelab/comments/12tk5oz/wd_compromised_unable_to_process_rmas/", "subreddit_subscribers": 566616, "created_utc": 1682036118.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1682036202.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.homelab", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "/r/homelab/comments/12tk5oz/wd_compromised_unable_to_process_rmas/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?auto=webp&amp;v=enabled&amp;s=d012ac5a7a664524a0c875d6db4d23fb9072be49", "width": 1200, "height": 800}, "resolutions": [{"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5a79af38467baa3c1a743b9421d60a3d20cbf68", "width": 108, "height": 72}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d90bd9629a61db4d7f0db40ff77264008ec18bf", "width": 216, "height": 144}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=04b22a41b1a0c3f4dd21da1d10d845b4d8f3bba1", "width": 320, "height": 213}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9c3b2a9bffa3688fda7f9be391dfb3d57962ec1a", "width": 640, "height": 426}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=743666a3eab18ee3546ae9dbefcb320cda47ab6c", "width": 960, "height": 640}, {"url": "https://external-preview.redd.it/gI1TYlp9xP5p2yoBERWRAUWtHrahR0Lv1DJmgyZcwPE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=933ca691254c3e014b9b9d7bf088bde0761ad20c", "width": 1080, "height": 720}], "variants": {}, "id": "MJ5NjK1i1_-LUQRUJB_I01Zp8Vg9X3GT4d44qOh-PPQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12tk70n", "is_robot_indexable": true, "report_reasons": null, "author": "CaptainBlanton", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12tk5oz", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tk70n/wd_compromised_unable_to_process_rmas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/homelab/comments/12tk5oz/wd_compromised_unable_to_process_rmas/", "subreddit_subscribers": 678668, "created_utc": 1682036202.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey there, like the title ask, which one would you pick. I know the question must have been answered plenty of times, but here's my use case (aswell as a 2nd question). \n\nI want to upgrade my PC mass storage so I can backup my personal files but also store media. (My current WD Black HDD is nearly 10yo with 60k hrs power-on). Said media is being accessed through a Jellyfin server, but is also shared through torrents (so lots of random reads happening) so which one is more reliable?\n\nHere comes my 2nd question: should I get 2x 4TB drives then split my media and files backup on each, or a single 8TB? I'm worried the constant random reads/occasional writes would kill the drive faster and potentially ruin my backups.\n\nTLDR: Which one is more reliable/best suited for random reads? Thanks", "author_fullname": "t2_2icrihpf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "WD Red Plus or IronWolf", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tj764", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682033843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, like the title ask, which one would you pick. I know the question must have been answered plenty of times, but here&amp;#39;s my use case (aswell as a 2nd question). &lt;/p&gt;\n\n&lt;p&gt;I want to upgrade my PC mass storage so I can backup my personal files but also store media. (My current WD Black HDD is nearly 10yo with 60k hrs power-on). Said media is being accessed through a Jellyfin server, but is also shared through torrents (so lots of random reads happening) so which one is more reliable?&lt;/p&gt;\n\n&lt;p&gt;Here comes my 2nd question: should I get 2x 4TB drives then split my media and files backup on each, or a single 8TB? I&amp;#39;m worried the constant random reads/occasional writes would kill the drive faster and potentially ruin my backups.&lt;/p&gt;\n\n&lt;p&gt;TLDR: Which one is more reliable/best suited for random reads? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tj764", "is_robot_indexable": true, "report_reasons": null, "author": "_Fantaz_", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tj764/wd_red_plus_or_ironwolf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tj764/wd_red_plus_or_ironwolf/", "subreddit_subscribers": 678668, "created_utc": 1682033843.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}