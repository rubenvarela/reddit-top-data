{"kind": "Listing", "data": {"after": "t3_12tu8ab", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I DESPISE live coding interviews. I\u2019m a good engineer and I can talk through skills and whiteboard and data model interview just fine. But seriously ask me a basic select statement in sql live and I barely remember how to do that. Panic sets in immediately and I barely make it through. I promise give me an hour to code something real and it will be done but just don\u2019t make me live code. I have almost 10 years experience and can barely write sql in a coding interview. It\u2019s just really rough.", "author_fullname": "t2_58wh4oyh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Live coding interview hatred", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12th15p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 94, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 94, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682028930.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I DESPISE live coding interviews. I\u2019m a good engineer and I can talk through skills and whiteboard and data model interview just fine. But seriously ask me a basic select statement in sql live and I barely remember how to do that. Panic sets in immediately and I barely make it through. I promise give me an hour to code something real and it will be done but just don\u2019t make me live code. I have almost 10 years experience and can barely write sql in a coding interview. It\u2019s just really rough.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12th15p", "is_robot_indexable": true, "report_reasons": null, "author": "k-dani-b", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12th15p/live_coding_interview_hatred/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12th15p/live_coding_interview_hatred/", "subreddit_subscribers": 101152, "created_utc": 1682028930.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone! I am thrilled to announce that as part of the [dbt technical writing mentorship program](https://www.getdbt.com/blog/technical-writing-mentorship-program/), I have just published a brand new developer blog article for all my fellow data enthusiasts out there! In this tutorial, I provide a step-by-step guide on how to build a Kimball dimensional model with dbt. \n\n* Blog article: [https://docs.getdbt.com/blog/kimball-dimensional-model](https://docs.getdbt.com/blog/kimball-dimensional-model) \n* Repository: [https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling](https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling) \n\nI had trouble finding clear explanations on this topic myself, which is why I decided to write one and share my knowledge with the community. Check out my latest article and let me know what you think!", "author_fullname": "t2_qhi3dexs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Step-by-step tutorial: Building a Kimball dimensional model with dbt", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12u2542", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682084519.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I am thrilled to announce that as part of the &lt;a href=\"https://www.getdbt.com/blog/technical-writing-mentorship-program/\"&gt;dbt technical writing mentorship program&lt;/a&gt;, I have just published a brand new developer blog article for all my fellow data enthusiasts out there! In this tutorial, I provide a step-by-step guide on how to build a Kimball dimensional model with dbt. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Blog article: &lt;a href=\"https://docs.getdbt.com/blog/kimball-dimensional-model\"&gt;https://docs.getdbt.com/blog/kimball-dimensional-model&lt;/a&gt; &lt;/li&gt;\n&lt;li&gt;Repository: &lt;a href=\"https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling\"&gt;https://github.com/Data-Engineer-Camp/dbt-dimensional-modelling&lt;/a&gt; &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I had trouble finding clear explanations on this topic myself, which is why I decided to write one and share my knowledge with the community. Check out my latest article and let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?auto=webp&amp;v=enabled&amp;s=d4169652ab7f36209622ee244f310aefe0cf8ce2", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd185c71f8fd75a33c0c79555ab7aa66ff94f024", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=10e2c53de9b2969366504c9ded514bbe1e2eb4d1", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7634b43f18facd6b7a0eaa901c2b135c2c9f47cf", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=487560d27ca62339efe41270d29a0644753c4362", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43637abd63f6c913d8d643f155a22cfee21ab4eb", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/PJCj8tgzdxpdMf_mbE9zWO1dQfyK3Jf3irVTvz8-RF0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7eff2686e5c425ddf4ad53b33e6ccec820db7c7", "width": 1080, "height": 567}], "variants": {}, "id": "2FMzESMf4JTwwEXCI7l6BQ_Y4vxSjUVpLJpZ0tVaorg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Founder of Data Engineer Camp, Data Engineer at Canva", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12u2542", "is_robot_indexable": true, "report_reasons": null, "author": "j__neo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12u2542/stepbystep_tutorial_building_a_kimball/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u2542/stepbystep_tutorial_building_a_kimball/", "subreddit_subscribers": 101152, "created_utc": 1682084519.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Applied for a few new opportunities, including some small or mid-size company. Some job *description* specifically asks for experience about data engineering 'on cloud', gcp/aws for example. But  we mainly make data pipeline in our own platform that are built by the department in charge of clusters and data storage, computation (Hadoop, Spark, Hive and other open-source componments mostly). When I told them such, I got rejected.\n\nI only worked in this one single company. Pertty big in the industry Mainly doing data warehouse, inner data products and analysis. Don't really know why 'On-Cloud' experience is such a must. Because from what I see it's just other kind of big data technology used instead of hadoop or spark. For data engineer, tasks are still the same: Making and optimizing data pipelines to transit, build or query using SQL or scripts. The logic beneath is still the same.", "author_fullname": "t2_oorup", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's so special about on-cloud data engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tockx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682046143.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Applied for a few new opportunities, including some small or mid-size company. Some job &lt;em&gt;description&lt;/em&gt; specifically asks for experience about data engineering &amp;#39;on cloud&amp;#39;, gcp/aws for example. But  we mainly make data pipeline in our own platform that are built by the department in charge of clusters and data storage, computation (Hadoop, Spark, Hive and other open-source componments mostly). When I told them such, I got rejected.&lt;/p&gt;\n\n&lt;p&gt;I only worked in this one single company. Pertty big in the industry Mainly doing data warehouse, inner data products and analysis. Don&amp;#39;t really know why &amp;#39;On-Cloud&amp;#39; experience is such a must. Because from what I see it&amp;#39;s just other kind of big data technology used instead of hadoop or spark. For data engineer, tasks are still the same: Making and optimizing data pipelines to transit, build or query using SQL or scripts. The logic beneath is still the same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12tockx", "is_robot_indexable": true, "report_reasons": null, "author": "GeForceKawaiiyo", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tockx/whats_so_special_about_oncloud_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tockx/whats_so_special_about_oncloud_data_engineering/", "subreddit_subscribers": 101152, "created_utc": 1682046143.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does anybody feel like a fraud? I joined a DE team 2 years ago as my first DE job. The team mostly uses low code tools and really only knows SQL. I have helped them with starting to employ software engineering best practices. \nI want to continue to learn more of the software engineering side of this role but it won\u2019t happen at my current company. \n\nI have been deciding to interview around and started to realize I still don\u2019t know a lot of stuff for programming. \n\nHas anyone been in this spot before?", "author_fullname": "t2_uww96dnc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anybody feel like a fraud?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tzhs7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682078478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anybody feel like a fraud? I joined a DE team 2 years ago as my first DE job. The team mostly uses low code tools and really only knows SQL. I have helped them with starting to employ software engineering best practices. \nI want to continue to learn more of the software engineering side of this role but it won\u2019t happen at my current company. &lt;/p&gt;\n\n&lt;p&gt;I have been deciding to interview around and started to realize I still don\u2019t know a lot of stuff for programming. &lt;/p&gt;\n\n&lt;p&gt;Has anyone been in this spot before?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12tzhs7", "is_robot_indexable": true, "report_reasons": null, "author": "anon_data_person", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tzhs7/anybody_feel_like_a_fraud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tzhs7/anybody_feel_like_a_fraud/", "subreddit_subscribers": 101152, "created_utc": 1682078478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "[https://www.archerapi.com/](https://www.archerapi.com/)\n\nI web scraped a lot of the data from an Archer Fandom site using Python.\n\nI then cleaned up the data using both Python and SQL and stored the data in a SQLite database.\n\nFlask/Python was used to build the API and I'm hosting it on Heroku. For the front-end web page I used a Bootstrap template. \n\nAs this is my first API I'm open to any and all feedback. Apologies if this post isn't suitable for the subreddit.\n\nSource code: [https://github.com/ben-n93/archer\\_api](https://github.com/ben-n93/archer_api)\n\nThanks!", "author_fullname": "t2_a7uw8rfx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "It's not a data engineering pipeline but I'm proud of an API I built that provides data about the animated sitcom Archer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tv82a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682066463.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.archerapi.com/\"&gt;https://www.archerapi.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I web scraped a lot of the data from an Archer Fandom site using Python.&lt;/p&gt;\n\n&lt;p&gt;I then cleaned up the data using both Python and SQL and stored the data in a SQLite database.&lt;/p&gt;\n\n&lt;p&gt;Flask/Python was used to build the API and I&amp;#39;m hosting it on Heroku. For the front-end web page I used a Bootstrap template. &lt;/p&gt;\n\n&lt;p&gt;As this is my first API I&amp;#39;m open to any and all feedback. Apologies if this post isn&amp;#39;t suitable for the subreddit.&lt;/p&gt;\n\n&lt;p&gt;Source code: &lt;a href=\"https://github.com/ben-n93/archer_api\"&gt;https://github.com/ben-n93/archer_api&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12tv82a", "is_robot_indexable": true, "report_reasons": null, "author": "ruthlesscattle", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tv82a/its_not_a_data_engineering_pipeline_but_im_proud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tv82a/its_not_a_data_engineering_pipeline_but_im_proud/", "subreddit_subscribers": 101152, "created_utc": 1682066463.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nQuick career question, I currently work at a fortune 500 as a azure data engineer and we also utilize databricks. \n\nI recently received 2 offers: \n\nOne from Accenture federal services as a azure data engineer (25,000 more than I make now)\n\nOne from a smaller company as a data engineer using palantir foundry (15,000$ more than I make now)\n\nAt my current company I will get to that 25,000 more salary within 3 years. \n\nIs there even a market for palantir foundry ? \n\nAnyone have any experience at Accenture or the WLB?\n\nWhat would you guys do?", "author_fullname": "t2_uf4ne7uq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tmyym", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682042730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Quick career question, I currently work at a fortune 500 as a azure data engineer and we also utilize databricks. &lt;/p&gt;\n\n&lt;p&gt;I recently received 2 offers: &lt;/p&gt;\n\n&lt;p&gt;One from Accenture federal services as a azure data engineer (25,000 more than I make now)&lt;/p&gt;\n\n&lt;p&gt;One from a smaller company as a data engineer using palantir foundry (15,000$ more than I make now)&lt;/p&gt;\n\n&lt;p&gt;At my current company I will get to that 25,000 more salary within 3 years. &lt;/p&gt;\n\n&lt;p&gt;Is there even a market for palantir foundry ? &lt;/p&gt;\n\n&lt;p&gt;Anyone have any experience at Accenture or the WLB?&lt;/p&gt;\n\n&lt;p&gt;What would you guys do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12tmyym", "is_robot_indexable": true, "report_reasons": null, "author": "NipsAhoy2", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tmyym/career_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tmyym/career_question/", "subreddit_subscribers": 101152, "created_utc": 1682042730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is a massive 11,000+ word blog put together by my colleagues here at StarTree re: these three open source real-time analytics (OLAP) databases. Would love to hear anyone's feedback.\n\nSee more: [https://startree.ai/blog/a-tale-of-three-real-time-olap-databases](https://startree.ai/blog/a-tale-of-three-real-time-olap-databases)", "author_fullname": "t2_jt32w", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Tale of Three Real-Time OLAP Databases: Apache Pinot, Apache Druid, and ClickHouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ta6lj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682014501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a massive 11,000+ word blog put together by my colleagues here at StarTree re: these three open source real-time analytics (OLAP) databases. Would love to hear anyone&amp;#39;s feedback.&lt;/p&gt;\n\n&lt;p&gt;See more: &lt;a href=\"https://startree.ai/blog/a-tale-of-three-real-time-olap-databases\"&gt;https://startree.ai/blog/a-tale-of-three-real-time-olap-databases&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?auto=webp&amp;v=enabled&amp;s=900d83cd5f59066e8a997ca05e5dd401b65aa563", "width": 2880, "height": 1620}, "resolutions": [{"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1fd7d9a8c0ef829fae0ad30f6452699455eb1b1e", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f1535da5d7d68a15334670c8cb48b82cde3cc61", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cf51b4f14df330739b3572a0dd0e79cfaa787317", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b3f795497aad444876dfde9daeddd7d7a656780", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e4aa07c438b37cde774b65c4a6d7445c4373b6e1", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/J8MqGe3QKvcsgh5tj2WYfKftzd-_Yx59tNSCPMaXORY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea90ea7cc15158a64cc12d0bd087278e7feed305", "width": 1080, "height": 607}], "variants": {}, "id": "wD1aiR5D-SqApZ0bRwMaKKr7L8tLMnpHn28caOSDhAk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ta6lj", "is_robot_indexable": true, "report_reasons": null, "author": "PeterCorless", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ta6lj/a_tale_of_three_realtime_olap_databases_apache/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ta6lj/a_tale_of_three_realtime_olap_databases_apache/", "subreddit_subscribers": 101152, "created_utc": 1682014501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, I work at a Fortune firm as a data science manager right now. I'm being tagged to take over management for a small data engineering team. It's not my background and I want to make sure I help these folks and don't screw everything up. :)\n\nI've got management experience and understand the basics of programming and productionilization of code (though my experience is all DS side with python &amp; R).\n\nMy question is what should I familiarize myself with to help set my new folks up for success? My experience with data engineering has largely been one of setting up ETL in automation for data processes but it's always been self service for my DS (and my DS team's) needs.\n\nI know a lot of this will have to do with:\n\n* Understanding, end to end, where the data needs for the team live and making sure we have the correct owners designated\n* Data pipelining is a huge part of the job, most of my knowledge here starts and ends with ETL in automation\n* Data quality is a huge consideration, though I've never had to worry about it in the past b/c I have always known exactly what I need (here I'll have to figure out what others will likely need)\n* Security, I believe, is largely owned by a different team\n\nI know there is a ton more that I'm missing. Besides learning new toolsets (react and node being the two primary ones, already familiar with Airflow &amp; DAGs), what else do I need to give myself a crash course in? What do I need to watch out for to protect my team from? What else can I do to help them?", "author_fullname": "t2_7jjayp3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tagged to lead a DE team, my background is data science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12u349o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682086543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, I work at a Fortune firm as a data science manager right now. I&amp;#39;m being tagged to take over management for a small data engineering team. It&amp;#39;s not my background and I want to make sure I help these folks and don&amp;#39;t screw everything up. :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got management experience and understand the basics of programming and productionilization of code (though my experience is all DS side with python &amp;amp; R).&lt;/p&gt;\n\n&lt;p&gt;My question is what should I familiarize myself with to help set my new folks up for success? My experience with data engineering has largely been one of setting up ETL in automation for data processes but it&amp;#39;s always been self service for my DS (and my DS team&amp;#39;s) needs.&lt;/p&gt;\n\n&lt;p&gt;I know a lot of this will have to do with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Understanding, end to end, where the data needs for the team live and making sure we have the correct owners designated&lt;/li&gt;\n&lt;li&gt;Data pipelining is a huge part of the job, most of my knowledge here starts and ends with ETL in automation&lt;/li&gt;\n&lt;li&gt;Data quality is a huge consideration, though I&amp;#39;ve never had to worry about it in the past b/c I have always known exactly what I need (here I&amp;#39;ll have to figure out what others will likely need)&lt;/li&gt;\n&lt;li&gt;Security, I believe, is largely owned by a different team&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I know there is a ton more that I&amp;#39;m missing. Besides learning new toolsets (react and node being the two primary ones, already familiar with Airflow &amp;amp; DAGs), what else do I need to give myself a crash course in? What do I need to watch out for to protect my team from? What else can I do to help them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12u349o", "is_robot_indexable": true, "report_reasons": null, "author": "quantpsychguy", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u349o/tagged_to_lead_a_de_team_my_background_is_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u349o/tagged_to_lead_a_de_team_my_background_is_data/", "subreddit_subscribers": 101152, "created_utc": 1682086543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Been trying to move files in an Azure Data Lake G2 container using airflow and I've faced so many issues, for one there's no operator for moving files in the same container or methods on hooks to move files in the container.\n\nOne way is to Download the files in that container filepath and then once its locally in the machine hosting airflow you upload it back to the target container filepath. \n\nIssue with this is if you're using Azure-Managed Airflow like I am, you can't actually access the filesystem airflow is hosted on, so you don't know what filepath to input to download the file to.\n\n&amp;#x200B;\n\n*So then how are people moving around files in an azure storage container using Azure-Managed Airflow or Google Cloud Composer?*\n\n&amp;#x200B;\n\nI tried the API approach using storage SDKs but it feels so complex rather than using an easy operator. On GCP for instance they have an airflow operator called \"gcs\\_to\\_gcs\", which lets you easily move files from one container to the next. \n\n&amp;#x200B;\n\n*Are people just not doing file movement in Azure Storage Containers with airflow?* \n\n&amp;#x200B;\n\n**TLDR: Trying to figure out how to move files in an azure container with only operators and not downloading files to the local filesystem of a cloud-hosted airflow instance since they don't give access to the file system and wondering if people have any ideas.**", "author_fullname": "t2_4akiu5cg1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How are people moving files in an Azure Storage Container using Airflow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12trdan", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682054485.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been trying to move files in an Azure Data Lake G2 container using airflow and I&amp;#39;ve faced so many issues, for one there&amp;#39;s no operator for moving files in the same container or methods on hooks to move files in the container.&lt;/p&gt;\n\n&lt;p&gt;One way is to Download the files in that container filepath and then once its locally in the machine hosting airflow you upload it back to the target container filepath. &lt;/p&gt;\n\n&lt;p&gt;Issue with this is if you&amp;#39;re using Azure-Managed Airflow like I am, you can&amp;#39;t actually access the filesystem airflow is hosted on, so you don&amp;#39;t know what filepath to input to download the file to.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;So then how are people moving around files in an azure storage container using Azure-Managed Airflow or Google Cloud Composer?&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I tried the API approach using storage SDKs but it feels so complex rather than using an easy operator. On GCP for instance they have an airflow operator called &amp;quot;gcs_to_gcs&amp;quot;, which lets you easily move files from one container to the next. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Are people just not doing file movement in Azure Storage Containers with airflow?&lt;/em&gt; &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR: Trying to figure out how to move files in an azure container with only operators and not downloading files to the local filesystem of a cloud-hosted airflow instance since they don&amp;#39;t give access to the file system and wondering if people have any ideas.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12trdan", "is_robot_indexable": true, "report_reasons": null, "author": "Sunya_ONE", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12trdan/how_are_people_moving_files_in_an_azure_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12trdan/how_are_people_moving_files_in_an_azure_storage/", "subreddit_subscribers": 101152, "created_utc": 1682054485.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "my current data engineering position mostly involves data and systems that aren\u2019t very exciting to me (e.g. financial data). but this spaceX launch has me thinking about how interesting it would be to work on the streaming pipelines for SpaceX rocket telemetry. What other unique and interesting data engineering roles like that exist?", "author_fullname": "t2_8e3c179e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "the most interesting data engineering positions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpa3u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682048591.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;my current data engineering position mostly involves data and systems that aren\u2019t very exciting to me (e.g. financial data). but this spaceX launch has me thinking about how interesting it would be to work on the streaming pipelines for SpaceX rocket telemetry. What other unique and interesting data engineering roles like that exist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12tpa3u", "is_robot_indexable": true, "report_reasons": null, "author": "jaredfromspacecamp", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tpa3u/the_most_interesting_data_engineering_positions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tpa3u/the_most_interesting_data_engineering_positions/", "subreddit_subscribers": 101152, "created_utc": 1682048591.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, I would appreciate any experienced feedback here.\n\nI am currently building a data lake using delta tables, but without Databricks. \n\nI have some Spark pipelines that periodically partitionally write, compact, `vacuum`, `z-order`, `analize` and this works quite fine for upserting purposes and I wouldn't change it.\n\nAlso, I have a single node client that reads data directly from the data lake and exposes it to some https endpoints with FastAPI. So far I needed Spark to leverage predicate pushdown and delta metadata (lots of partitions and hundreds of GBs, can't afford to read all the data at once).\n\nRead performances are okay-ish, but not perfect. Especially because I would like to provide pagination and no Databricks means no Delta caching.\n\nI have no experience with Polars whatsoever, how would it compare in a read-only scenario considering that it does support delta tables? \n\nWould it be reasonable to think it may overspeed pyspark for filter-only queries meant to retrieve page-sized chunk of data (i.e., 100-1000 rows at the time)?\n\nThe APIs only expose `SELECT * FROM &lt;&gt; WHERE &lt;&gt;`\nkind of queries, where the conditions uses either partition or z-ordered columns.\n\nWould appreciate any feedback here, thanks in advance.", "author_fullname": "t2_31fa566d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Read-only queries on delta tables. Polars or Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tebg8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682022965.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I would appreciate any experienced feedback here.&lt;/p&gt;\n\n&lt;p&gt;I am currently building a data lake using delta tables, but without Databricks. &lt;/p&gt;\n\n&lt;p&gt;I have some Spark pipelines that periodically partitionally write, compact, &lt;code&gt;vacuum&lt;/code&gt;, &lt;code&gt;z-order&lt;/code&gt;, &lt;code&gt;analize&lt;/code&gt; and this works quite fine for upserting purposes and I wouldn&amp;#39;t change it.&lt;/p&gt;\n\n&lt;p&gt;Also, I have a single node client that reads data directly from the data lake and exposes it to some https endpoints with FastAPI. So far I needed Spark to leverage predicate pushdown and delta metadata (lots of partitions and hundreds of GBs, can&amp;#39;t afford to read all the data at once).&lt;/p&gt;\n\n&lt;p&gt;Read performances are okay-ish, but not perfect. Especially because I would like to provide pagination and no Databricks means no Delta caching.&lt;/p&gt;\n\n&lt;p&gt;I have no experience with Polars whatsoever, how would it compare in a read-only scenario considering that it does support delta tables? &lt;/p&gt;\n\n&lt;p&gt;Would it be reasonable to think it may overspeed pyspark for filter-only queries meant to retrieve page-sized chunk of data (i.e., 100-1000 rows at the time)?&lt;/p&gt;\n\n&lt;p&gt;The APIs only expose &lt;code&gt;SELECT * FROM &amp;lt;&amp;gt; WHERE &amp;lt;&amp;gt;&lt;/code&gt;\nkind of queries, where the conditions uses either partition or z-ordered columns.&lt;/p&gt;\n\n&lt;p&gt;Would appreciate any feedback here, thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tebg8", "is_robot_indexable": true, "report_reasons": null, "author": "Perfecy", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tebg8/readonly_queries_on_delta_tables_polars_or_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tebg8/readonly_queries_on_delta_tables_polars_or_spark/", "subreddit_subscribers": 101152, "created_utc": 1682022965.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2bhtmk4t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Data Quality with Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_12thpih", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ma6jemfxkJhO7PBZlyCf9prP2PIAq3CX435QBjT8PIc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682030443.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ssmertin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ssmertin.com/articles/into-to-data-quality-with-apache-spark/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?auto=webp&amp;v=enabled&amp;s=d3f75f8fb2b7b466ac5387f5eb8e2138515fd21a", "width": 956, "height": 650}, "resolutions": [{"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=084a6b0fa0f0dce1526f171fc30a06be309563ff", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8be8319f0804dc32bd9f64fedcfd89c088e01d38", "width": 216, "height": 146}, {"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30daca78d1685d6c8fa9df505d49dd0f96e7fb28", "width": 320, "height": 217}, {"url": "https://external-preview.redd.it/fqkzaI4T2FudgP00zsOVI36AqllpjU5e0GQVtDI3MDk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53fa18a4763425e8da48dc6e1c3e40ffdd7dd5e0", "width": 640, "height": 435}], "variants": {}, "id": "L1Yb0PtWmCThbzzDwWEupJeJQaMgNhDypG04WFeckl4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12thpih", "is_robot_indexable": true, "report_reasons": null, "author": "nf_x", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12thpih/introduction_to_data_quality_with_apache_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ssmertin.com/articles/into-to-data-quality-with-apache-spark/", "subreddit_subscribers": 101152, "created_utc": 1682030443.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_14v3ms", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The False Promise of dbt Contracts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12t4hf0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682004488.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tobikodata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tobikodata.com/the-false-promise-of-dbt-contracts.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12t4hf0", "is_robot_indexable": true, "report_reasons": null, "author": "s0ck_r4w", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12t4hf0/the_false_promise_of_dbt_contracts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tobikodata.com/the-false-promise-of-dbt-contracts.html", "subreddit_subscribers": 101152, "created_utc": 1682004488.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm kind of an accidental DE. I'd worked various reporting/BI roles for years and then after a restructure I ended up primarily working on data integrations and migrations as my company moved from on-prem SQL Servers over to Cloud solutions. Our basic tech stack is AWS(S3), Informatica (CAI/CDI) and Snowflake.\n\n I actually reaslly enjoy my job, and would like to think that a future career as a DE would be possible (intentionally this time!), but my current team is fairly low code. Our primary pipeline is a shared process which handles most of the heavy lifting for any new ingestion. Most of my work involves bringing in new data sources, typically from APIs (but also from SQL, SFTP, etc). It's functional and it works, but I'm interested from a personal development perspective of how else we could do things.\n\nI'm reasonably confident with Python and we have access to Databricks in my company (as well as access to the free Databricks Academy, for which i've just enrolled in the Data Engineer plan). \n\nI guess what im trying to find out is where i could be/should be using Databricks in my workflow/data pipelines? I see a lot of posts on this subreddit around Databricks, and felt like i should use some periods of downtime at work to get some Databricks accreditations, but it would be good if there were some practical use cases.\n\nApologies if this is a bit of a non-question. More generally, feedback on how others are using Databricks in their data pipelines might help me understand its place a bit more!", "author_fullname": "t2_4smhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to use Databricks in my workflow (Novice DE)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12txryi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682074118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m kind of an accidental DE. I&amp;#39;d worked various reporting/BI roles for years and then after a restructure I ended up primarily working on data integrations and migrations as my company moved from on-prem SQL Servers over to Cloud solutions. Our basic tech stack is AWS(S3), Informatica (CAI/CDI) and Snowflake.&lt;/p&gt;\n\n&lt;p&gt;I actually reaslly enjoy my job, and would like to think that a future career as a DE would be possible (intentionally this time!), but my current team is fairly low code. Our primary pipeline is a shared process which handles most of the heavy lifting for any new ingestion. Most of my work involves bringing in new data sources, typically from APIs (but also from SQL, SFTP, etc). It&amp;#39;s functional and it works, but I&amp;#39;m interested from a personal development perspective of how else we could do things.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m reasonably confident with Python and we have access to Databricks in my company (as well as access to the free Databricks Academy, for which i&amp;#39;ve just enrolled in the Data Engineer plan). &lt;/p&gt;\n\n&lt;p&gt;I guess what im trying to find out is where i could be/should be using Databricks in my workflow/data pipelines? I see a lot of posts on this subreddit around Databricks, and felt like i should use some periods of downtime at work to get some Databricks accreditations, but it would be good if there were some practical use cases.&lt;/p&gt;\n\n&lt;p&gt;Apologies if this is a bit of a non-question. More generally, feedback on how others are using Databricks in their data pipelines might help me understand its place a bit more!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12txryi", "is_robot_indexable": true, "report_reasons": null, "author": "andyby2k26", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12txryi/where_to_use_databricks_in_my_workflow_novice_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12txryi/where_to_use_databricks_in_my_workflow_novice_de/", "subreddit_subscribers": 101152, "created_utc": 1682074118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi All. I'm planning to get Azure DP-203 DE Associate certification. I have started the preparations only this week.\n\nThough I have a doubt, I haven't got the DP-900 certification, and I'm going for DP-203 straight away. Is it recommended to take the exams in order?  \n\n\nI have experience with AWS from my previous job", "author_fullname": "t2_w4mta741", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Guidance for Azure certification", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tu6uq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682065299.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682062959.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi All. I&amp;#39;m planning to get Azure DP-203 DE Associate certification. I have started the preparations only this week.&lt;/p&gt;\n\n&lt;p&gt;Though I have a doubt, I haven&amp;#39;t got the DP-900 certification, and I&amp;#39;m going for DP-203 straight away. Is it recommended to take the exams in order?  &lt;/p&gt;\n\n&lt;p&gt;I have experience with AWS from my previous job&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tu6uq", "is_robot_indexable": true, "report_reasons": null, "author": "Ketonium10", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tu6uq/guidance_for_azure_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tu6uq/guidance_for_azure_certification/", "subreddit_subscribers": 101152, "created_utc": 1682062959.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Recently I have been hearing a lot about AI and the implementation of Generative AI tools in software engineering, some of my friends from non-tech backgrounds have been speaking about it(mostly ChatGpt).\n\n  \nI want to create this post to ask data engineers here to ask If you have used any such tools at work, and if so what are they and how they are being useful.\n\nAlso, what are the AI tools good to have in a data engineer's portfolio?", "author_fullname": "t2_nie4cn9s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AI Tools in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tqvgg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682053032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently I have been hearing a lot about AI and the implementation of Generative AI tools in software engineering, some of my friends from non-tech backgrounds have been speaking about it(mostly ChatGpt).&lt;/p&gt;\n\n&lt;p&gt;I want to create this post to ask data engineers here to ask If you have used any such tools at work, and if so what are they and how they are being useful.&lt;/p&gt;\n\n&lt;p&gt;Also, what are the AI tools good to have in a data engineer&amp;#39;s portfolio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12tqvgg", "is_robot_indexable": true, "report_reasons": null, "author": "sds66", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tqvgg/ai_tools_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tqvgg/ai_tools_in_data_engineering/", "subreddit_subscribers": 101152, "created_utc": 1682053032.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a use case to store (ingest) telemetry data that is formatted as json.  Each message is about 1K.  Data comes in over Kafka.  When the data is stored, I need to query the data (I can use SQL or NoSQL, I am flexible in the interface).\n\nMy architect is asking me to store this data into a mongo cluster.  From what I understand, Druid does the same thing as well.  What do you guys recommend?  What are the pros and cons?", "author_fullname": "t2_bluzq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Apache druid vs Mongo", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlwyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682040214.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a use case to store (ingest) telemetry data that is formatted as json.  Each message is about 1K.  Data comes in over Kafka.  When the data is stored, I need to query the data (I can use SQL or NoSQL, I am flexible in the interface).&lt;/p&gt;\n\n&lt;p&gt;My architect is asking me to store this data into a mongo cluster.  From what I understand, Druid does the same thing as well.  What do you guys recommend?  What are the pros and cons?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tlwyl", "is_robot_indexable": true, "report_reasons": null, "author": "Beertimeanytime", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tlwyl/apache_druid_vs_mongo/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tlwyl/apache_druid_vs_mongo/", "subreddit_subscribers": 101152, "created_utc": 1682040214.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to run a web scraping python script that generates json files on EC2 instance then connect the instance to Kinesis.\n\nI can't wrap my head on how to do this. \n\nI need some resources that could help me do this. Something like a guided project for example. \n\nI need some help please.", "author_fullname": "t2_1f15w5lz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "EC2 with Kinesis Data streams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlhsq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682039242.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to run a web scraping python script that generates json files on EC2 instance then connect the instance to Kinesis.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t wrap my head on how to do this. &lt;/p&gt;\n\n&lt;p&gt;I need some resources that could help me do this. Something like a guided project for example. &lt;/p&gt;\n\n&lt;p&gt;I need some help please.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tlhsq", "is_robot_indexable": true, "report_reasons": null, "author": "I-am_Not_Sure", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tlhsq/ec2_with_kinesis_data_streams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tlhsq/ec2_with_kinesis_data_streams/", "subreddit_subscribers": 101152, "created_utc": 1682039242.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone worked with the \"Operational Data Layer\" architecture? The idea is to have an offload layer of mainframe data in the cloud for transactional consumption via APIs, not for analytical.\n\nRef: https://www.mongodb.com/collateral/implementing-an-operational-data-layer", "author_fullname": "t2_z4ea7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Operational Data Layer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tj6ly", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682033804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone worked with the &amp;quot;Operational Data Layer&amp;quot; architecture? The idea is to have an offload layer of mainframe data in the cloud for transactional consumption via APIs, not for analytical.&lt;/p&gt;\n\n&lt;p&gt;Ref: &lt;a href=\"https://www.mongodb.com/collateral/implementing-an-operational-data-layer\"&gt;https://www.mongodb.com/collateral/implementing-an-operational-data-layer&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?auto=webp&amp;v=enabled&amp;s=4d0e5122de6d4ad87574f5c31c130c2f0816e5ea", "width": 1200, "height": 601}, "resolutions": [{"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e50bf9f8ae0b10c2550f9ee3eac6b6788e725d58", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22acd42800e34b60c780f8e454a502f68a5e1b80", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62ee96dc5c7ee55560cb8e18e03dc7954f7d9e3c", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78985a472e163338ceefd97e75a5fc3e2e6937bd", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73b969be08b7028fdd476f5e48e8cd8b0d712cc9", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/iIXCbGZ5mGkuRyTFnyoaeSmeTEQ-s_hXbSA2TI3W05w.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f989646a090537bac16869c3f68e36c658baa0b0", "width": 1080, "height": 540}], "variants": {}, "id": "9cMT5a0hMRy5J7cG3lurHYe3JYa222Qg02VNunMDa60"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12tj6ly", "is_robot_indexable": true, "report_reasons": null, "author": "aleebit", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tj6ly/operational_data_layer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tj6ly/operational_data_layer/", "subreddit_subscribers": 101152, "created_utc": 1682033804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone, this is my first post here and I'm a little bit excited.\n\nWhen I first started working with column-oriented DBMS ClickHouse, I struggled to find a tool (other than the one built into the [ClickHouse Cloud](https://clickhouse.cloud/) web-UI) that would create the necessary table with the required columns and data types based on a CSV file or Pandas dataframe. Neither clickhouse-connect nor clickhouse-driver had this functionality, such as the `to_sql` method in SQLAlchemy.\n\nI wanted to load my favorite dataset of open-wheel Formula 1 racing world championship results into ClickHouse, but manually creating 15 tables was too time-consuming.\n\nWhen I previously familiared with PySpark,  I noticed that many data professionals use Pandas to define the data  schema before loading CSV files into PySpark. And I thought, why not use Pandas to define the data types by columns?\n\nThis is how [this script](https://github.com/pvl-k/csv2clickhouse) was born, which I want to share. I hope it saves you some time, and it will give me the opportunity to receive a couple of feedbacks and ideas from you for improvement.\n\nI'm not sure about the complete compatibility of data types between Pandas and ClickHouse: quick research gave conflicting results, so please correct me if you find any discrepancies.\n\nAnd be careful with the `replace_flag` \\- when set to True, the script will recreate tables with the same name if they already exist, so you may lose existing data in your database. To avoid this, but also prevent data duplication, I recommend specifying a non-existent database name as the `database_name`. When set to False in the `replace_flag`, data from your CSV files will be added to existing tables with the same name (of course, the number of columns and their data types must match).", "author_fullname": "t2_hmsnetde", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fast way to upload several CSV files -&gt; ClickHouse with create a database and tables", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tfq8h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682026033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, this is my first post here and I&amp;#39;m a little bit excited.&lt;/p&gt;\n\n&lt;p&gt;When I first started working with column-oriented DBMS ClickHouse, I struggled to find a tool (other than the one built into the &lt;a href=\"https://clickhouse.cloud/\"&gt;ClickHouse Cloud&lt;/a&gt; web-UI) that would create the necessary table with the required columns and data types based on a CSV file or Pandas dataframe. Neither clickhouse-connect nor clickhouse-driver had this functionality, such as the &lt;code&gt;to_sql&lt;/code&gt; method in SQLAlchemy.&lt;/p&gt;\n\n&lt;p&gt;I wanted to load my favorite dataset of open-wheel Formula 1 racing world championship results into ClickHouse, but manually creating 15 tables was too time-consuming.&lt;/p&gt;\n\n&lt;p&gt;When I previously familiared with PySpark,  I noticed that many data professionals use Pandas to define the data  schema before loading CSV files into PySpark. And I thought, why not use Pandas to define the data types by columns?&lt;/p&gt;\n\n&lt;p&gt;This is how &lt;a href=\"https://github.com/pvl-k/csv2clickhouse\"&gt;this script&lt;/a&gt; was born, which I want to share. I hope it saves you some time, and it will give me the opportunity to receive a couple of feedbacks and ideas from you for improvement.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure about the complete compatibility of data types between Pandas and ClickHouse: quick research gave conflicting results, so please correct me if you find any discrepancies.&lt;/p&gt;\n\n&lt;p&gt;And be careful with the &lt;code&gt;replace_flag&lt;/code&gt; - when set to True, the script will recreate tables with the same name if they already exist, so you may lose existing data in your database. To avoid this, but also prevent data duplication, I recommend specifying a non-existent database name as the &lt;code&gt;database_name&lt;/code&gt;. When set to False in the &lt;code&gt;replace_flag&lt;/code&gt;, data from your CSV files will be added to existing tables with the same name (of course, the number of columns and their data types must match).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12tfq8h", "is_robot_indexable": true, "report_reasons": null, "author": "Pavel_K0", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tfq8h/fast_way_to_upload_several_csv_files_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tfq8h/fast_way_to_upload_several_csv_files_clickhouse/", "subreddit_subscribers": 101152, "created_utc": 1682026033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working with S3 and my team has just tasked me with figuring out how to track versioning of the objects. For example, say that one of the files has a monthly upload, but sometimes those monthly uploads are changed with a new version. So we want the latest version of the \"January\" upload. \n\nI'm looking at the Data Cataglog that comes with S3 and writing metadata when the object is uploaded but there doesn't seem to be any clear cut resources to implement the following: \n\n1) Create an entry in a data catalog with the month upload and version.   \n2) Pull the object  URI from the data catalog based a query of month and latest version  \n3) Bonus points if when a new version is uploaded it can alert downstream consumers (email?) that a new version has been uploaded\n\nApologize if this is a simple ask and/or it is stated poorly. I'm very new to AWS.", "author_fullname": "t2_1yn5o9p2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tracking Objects in a Data Lake - Help!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12te2as", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682022430.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working with S3 and my team has just tasked me with figuring out how to track versioning of the objects. For example, say that one of the files has a monthly upload, but sometimes those monthly uploads are changed with a new version. So we want the latest version of the &amp;quot;January&amp;quot; upload. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking at the Data Cataglog that comes with S3 and writing metadata when the object is uploaded but there doesn&amp;#39;t seem to be any clear cut resources to implement the following: &lt;/p&gt;\n\n&lt;p&gt;1) Create an entry in a data catalog with the month upload and version.&lt;br/&gt;\n2) Pull the object  URI from the data catalog based a query of month and latest version&lt;br/&gt;\n3) Bonus points if when a new version is uploaded it can alert downstream consumers (email?) that a new version has been uploaded&lt;/p&gt;\n\n&lt;p&gt;Apologize if this is a simple ask and/or it is stated poorly. I&amp;#39;m very new to AWS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12te2as", "is_robot_indexable": true, "report_reasons": null, "author": "10002Hours", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12te2as/tracking_objects_in_a_data_lake_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12te2as/tracking_objects_in_a_data_lake_help/", "subreddit_subscribers": 101152, "created_utc": 1682022430.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone -\n\n&amp;#x200B;\n\ni am running a pipeline that consumes data from continously scheduled api calls, returning json files. These json files are written as rawdata to a datalake, where they get picked up by a script that does some transformations and writes them out as parquet files. This happens on a per file basis, i.e.\n\ndata1001.json -&gt; data1001\\_00000.parquet\n\ndata1002.json -&gt; data1002\\_00000.parquet\n\netc.\n\nSince there are plenty of small files, this results in a bit of an IO mess, as doing it this way results in lots and lots of rather small parquet files. Therefore i have added a subsequent spark job here that reads all the files, repartions them (by a 'type' and 'product' column present in the data) and writes them back to the datalake, now with fewer files but larger file size. In order to keep these repartitioned files up to date, this spark job is run daily and processes the entire dataset.\n\nI was wondering if there was a more effective pipeline architecture that doesn't require re-loading the entire dataset into spark every day, repartitioning it and writing it back to the datalake, but rather performs some sort of upsert, to add new entries and update potentially changed rows", "author_fullname": "t2_11cqnxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "partitioned parquet files upserts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tc6mm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682018492.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone -&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;i am running a pipeline that consumes data from continously scheduled api calls, returning json files. These json files are written as rawdata to a datalake, where they get picked up by a script that does some transformations and writes them out as parquet files. This happens on a per file basis, i.e.&lt;/p&gt;\n\n&lt;p&gt;data1001.json -&amp;gt; data1001_00000.parquet&lt;/p&gt;\n\n&lt;p&gt;data1002.json -&amp;gt; data1002_00000.parquet&lt;/p&gt;\n\n&lt;p&gt;etc.&lt;/p&gt;\n\n&lt;p&gt;Since there are plenty of small files, this results in a bit of an IO mess, as doing it this way results in lots and lots of rather small parquet files. Therefore i have added a subsequent spark job here that reads all the files, repartions them (by a &amp;#39;type&amp;#39; and &amp;#39;product&amp;#39; column present in the data) and writes them back to the datalake, now with fewer files but larger file size. In order to keep these repartitioned files up to date, this spark job is run daily and processes the entire dataset.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if there was a more effective pipeline architecture that doesn&amp;#39;t require re-loading the entire dataset into spark every day, repartitioning it and writing it back to the datalake, but rather performs some sort of upsert, to add new entries and update potentially changed rows&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tc6mm", "is_robot_indexable": true, "report_reasons": null, "author": "nihi_", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tc6mm/partitioned_parquet_files_upserts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tc6mm/partitioned_parquet_files_upserts/", "subreddit_subscribers": 101152, "created_utc": 1682018492.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nFYI, my systems are hosted at AWS (our data is mostly stored in RDS MySQL). However, we are considered small-scaled in GB. I expect my database to grow to 500GB - 1TB (part of data migration).\n\nCurrently, running specific reporting queries on RDS is slow, despite optimisation done. Can't imagine after the data migration, probably worst.\n\nI heard data warehouse may be able to solve my issue.\n\nI need to work on \n\na. data analytics for visualisation (most likely will use quicksight)\n\nb. near real-time dashboard (transactions related e.g. total orders today incremental)\n\nc. reporting (wonder my reports can pull from data warehouse, instead of RDS)\n\nd. AI-related (this is more like a future)\n\nPriority is a, b and c. I do not want to hit my RDS and slow it down (though I think RDS replica may solve this, but slow query is unavoidable i think).\n\nnote: though most of my data resides in RDS, I am also exploring kinesis data stream and maybe some data files stored in S3. \n\nI noticed this article recommended not to use Redshift (and somewhere i read, not mistaken if my data is less than 5TB, no point I touch Redshift)\n\nhttps://www.reddit.com/r/dataengineering/comments/12rueol/what\\_is\\_a\\_good\\_resource\\_to\\_learn\\_how\\_to\\_set\\_up\\_a/\n\nI tried Athena. I think Athena is more like query csv or parquet files in S3. Not sure it can query RDS database directly or store in columnar format for fast retrieval. But I don't think Athena is data warehouse.\n\nI also heard good stuff about BigQuery. But I am not sure transferring data from AWS RDS to BigQuery, is a good idea. Bigquery omni doesn't work for me (which also quite expensive), as my data resides in AWS ap-southeast-1. I think some don't recommend multi cloud.\n\nIf I want to retain in AWS, which can help me to achieve a, b c and d, which AWS service is suitable for me? Or should I consider BigQuery for my data warehouse. \n\nNote: I am not interested in Snowflake or Databricks at this moment. \n\nAny help? Thanks.", "author_fullname": "t2_adysoxz3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "New to Data Warehouse (currently using AWS), which solution works best for small-scale setup?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u0hmw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682080914.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;FYI, my systems are hosted at AWS (our data is mostly stored in RDS MySQL). However, we are considered small-scaled in GB. I expect my database to grow to 500GB - 1TB (part of data migration).&lt;/p&gt;\n\n&lt;p&gt;Currently, running specific reporting queries on RDS is slow, despite optimisation done. Can&amp;#39;t imagine after the data migration, probably worst.&lt;/p&gt;\n\n&lt;p&gt;I heard data warehouse may be able to solve my issue.&lt;/p&gt;\n\n&lt;p&gt;I need to work on &lt;/p&gt;\n\n&lt;p&gt;a. data analytics for visualisation (most likely will use quicksight)&lt;/p&gt;\n\n&lt;p&gt;b. near real-time dashboard (transactions related e.g. total orders today incremental)&lt;/p&gt;\n\n&lt;p&gt;c. reporting (wonder my reports can pull from data warehouse, instead of RDS)&lt;/p&gt;\n\n&lt;p&gt;d. AI-related (this is more like a future)&lt;/p&gt;\n\n&lt;p&gt;Priority is a, b and c. I do not want to hit my RDS and slow it down (though I think RDS replica may solve this, but slow query is unavoidable i think).&lt;/p&gt;\n\n&lt;p&gt;note: though most of my data resides in RDS, I am also exploring kinesis data stream and maybe some data files stored in S3. &lt;/p&gt;\n\n&lt;p&gt;I noticed this article recommended not to use Redshift (and somewhere i read, not mistaken if my data is less than 5TB, no point I touch Redshift)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/12rueol/what%5C_is%5C_a%5C_good%5C_resource%5C_to%5C_learn%5C_how%5C_to%5C_set%5C_up%5C_a/\"&gt;https://www.reddit.com/r/dataengineering/comments/12rueol/what\\_is\\_a\\_good\\_resource\\_to\\_learn\\_how\\_to\\_set\\_up\\_a/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I tried Athena. I think Athena is more like query csv or parquet files in S3. Not sure it can query RDS database directly or store in columnar format for fast retrieval. But I don&amp;#39;t think Athena is data warehouse.&lt;/p&gt;\n\n&lt;p&gt;I also heard good stuff about BigQuery. But I am not sure transferring data from AWS RDS to BigQuery, is a good idea. Bigquery omni doesn&amp;#39;t work for me (which also quite expensive), as my data resides in AWS ap-southeast-1. I think some don&amp;#39;t recommend multi cloud.&lt;/p&gt;\n\n&lt;p&gt;If I want to retain in AWS, which can help me to achieve a, b c and d, which AWS service is suitable for me? Or should I consider BigQuery for my data warehouse. &lt;/p&gt;\n\n&lt;p&gt;Note: I am not interested in Snowflake or Databricks at this moment. &lt;/p&gt;\n\n&lt;p&gt;Any help? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12u0hmw", "is_robot_indexable": true, "report_reasons": null, "author": "ericchuawc", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u0hmw/new_to_data_warehouse_currently_using_aws_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u0hmw/new_to_data_warehouse_currently_using_aws_which/", "subreddit_subscribers": 101152, "created_utc": 1682080914.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Title. \n\nLooking for an alternative for SQL Developer for MacOS Ventura. Something not clunky, free, that has dark mode available, and supports multiple connections for Oracle DB\u2019s.", "author_fullname": "t2_4ehxr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Oracle SQL Developer alternative for MacOS?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u0f1d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682080744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title. &lt;/p&gt;\n\n&lt;p&gt;Looking for an alternative for SQL Developer for MacOS Ventura. Something not clunky, free, that has dark mode available, and supports multiple connections for Oracle DB\u2019s.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12u0f1d", "is_robot_indexable": true, "report_reasons": null, "author": "thenyx", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u0f1d/oracle_sql_developer_alternative_for_macos/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u0f1d/oracle_sql_developer_alternative_for_macos/", "subreddit_subscribers": 101152, "created_utc": 1682080744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been doing a fair bit of bootstrap resampling recently, where I have a large data hosted on HDFS that I'd like to create several bootstrap resamples. This involves sampling rows with replacement to create a new dataset, and I know that duckdb/polars/Spark all have some equivalent of `sample(withReplacement=True)`.\n\nSo far I've been writing simple Spark applications that do this process sequentially but I was just thinking, the creation of one resample should be independent of the creation of another, so really, this is a process that should be able to run in parallel.\n\nHas anyone attempted this kind of resampling in parallel using duckdb/polars/spark? Looking for a sample project online to understand how to approach this.", "author_fullname": "t2_3i0xn3gy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bootstrap resampling with Duckdb / Polars / Spark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tu8ab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682063095.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been doing a fair bit of bootstrap resampling recently, where I have a large data hosted on HDFS that I&amp;#39;d like to create several bootstrap resamples. This involves sampling rows with replacement to create a new dataset, and I know that duckdb/polars/Spark all have some equivalent of &lt;code&gt;sample(withReplacement=True)&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;So far I&amp;#39;ve been writing simple Spark applications that do this process sequentially but I was just thinking, the creation of one resample should be independent of the creation of another, so really, this is a process that should be able to run in parallel.&lt;/p&gt;\n\n&lt;p&gt;Has anyone attempted this kind of resampling in parallel using duckdb/polars/spark? Looking for a sample project online to understand how to approach this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12tu8ab", "is_robot_indexable": true, "report_reasons": null, "author": "ddanieltan", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12tu8ab/bootstrap_resampling_with_duckdb_polars_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12tu8ab/bootstrap_resampling_with_duckdb_polars_spark/", "subreddit_subscribers": 101152, "created_utc": 1682063095.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}