{"kind": "Listing", "data": {"after": "t3_12u2wgl", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_6kz4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Office Depot offered me a free charcoal grill with my hard drive purchase", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12tipvp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 2015, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2015, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Wqhy-NagOiu23poPEIEIhnyH2qIBE-yveYILc2jSgMA.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682032730.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "imgur.com", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://imgur.com/nRjYJXN.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?auto=webp&amp;v=enabled&amp;s=877a9a18b8b50d040663871def4a1421df6e6386", "width": 3264, "height": 2448}, "resolutions": [{"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73051bbc5204949741714b34cb4b3b04f7590406", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5f2b9690cc37062aef73886911d0f1e42ace0489", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ccca79086fb20fe136388167e21bf0d06de5a397", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e6b22c4eb4c8459362e65f071bffd24eceb87db0", "width": 640, "height": 480}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09732ba25ca2f59740344f35746feec4199a9026", "width": 960, "height": 720}, {"url": "https://external-preview.redd.it/QkVMugFi3w3-XxgR1LFOw4gQV_vZCUiuYbDAcuhsSpI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bed1c49de8987d7410fdc80006b433cd17bc39a8", "width": 1080, "height": 810}], "variants": {}, "id": "Jo_AoGZNLMYWGVFreZ_a7VwlLo7DvuTcr0I0Xi7tqb8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1/10 PB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tipvp", "is_robot_indexable": true, "report_reasons": null, "author": "mrtramplefoot", "discussion_type": null, "num_comments": 163, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12tipvp/office_depot_offered_me_a_free_charcoal_grill/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://imgur.com/nRjYJXN.jpg", "subreddit_subscribers": 679032, "created_utc": 1682032730.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey guys, I recently downloaded all the savegames on GameFAQs and uploaded them on [archive.org](https://archive.org). Here's the page [https://archive.org/details/gamefaqs\\_savegames](https://archive.org/details/gamefaqs_savegames). A short description:\n\nAll the savegames that can be downloaded on gamefaqs as of 21 April 2023. Each zip file contains a folder for each game with at least a savegame available and the game's folder contains a series of subfolders for every type of different gamesave available. For example, [here](https://gamefaqs.gamespot.com/snes/519824-super-mario-world/saves) there are save files from gba and wii from different regions. Every folder also contains a description.txt file with the description of the gamesaves.\n\nGames for each console:\n\n* dreamcast - 152\n* ds - 501\n* famicomds - 1\n* gamecube - 313\n* gba - 307\n* genesis - 3\n* n64 - 121\n* nes - 6\n* ps - 698\n* ps2 - 1853\n* ps3 - 548\n* psp - 1131\n* sms - 1\n* snes - 23\n* tg16 - 3\n* turbocd - 2\n* wii - 411\n* xbox - 452\n\nThe games were mostly deduplicated, but sometimes the scraping would fail due to the connection being reset by the server so the same game might be present twice under different names.", "author_fullname": "t2_24wtqt95", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GameFAQs savegames collection", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tv29a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682065890.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I recently downloaded all the savegames on GameFAQs and uploaded them on &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt;. Here&amp;#39;s the page &lt;a href=\"https://archive.org/details/gamefaqs_savegames\"&gt;https://archive.org/details/gamefaqs_savegames&lt;/a&gt;. A short description:&lt;/p&gt;\n\n&lt;p&gt;All the savegames that can be downloaded on gamefaqs as of 21 April 2023. Each zip file contains a folder for each game with at least a savegame available and the game&amp;#39;s folder contains a series of subfolders for every type of different gamesave available. For example, &lt;a href=\"https://gamefaqs.gamespot.com/snes/519824-super-mario-world/saves\"&gt;here&lt;/a&gt; there are save files from gba and wii from different regions. Every folder also contains a description.txt file with the description of the gamesaves.&lt;/p&gt;\n\n&lt;p&gt;Games for each console:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;dreamcast - 152&lt;/li&gt;\n&lt;li&gt;ds - 501&lt;/li&gt;\n&lt;li&gt;famicomds - 1&lt;/li&gt;\n&lt;li&gt;gamecube - 313&lt;/li&gt;\n&lt;li&gt;gba - 307&lt;/li&gt;\n&lt;li&gt;genesis - 3&lt;/li&gt;\n&lt;li&gt;n64 - 121&lt;/li&gt;\n&lt;li&gt;nes - 6&lt;/li&gt;\n&lt;li&gt;ps - 698&lt;/li&gt;\n&lt;li&gt;ps2 - 1853&lt;/li&gt;\n&lt;li&gt;ps3 - 548&lt;/li&gt;\n&lt;li&gt;psp - 1131&lt;/li&gt;\n&lt;li&gt;sms - 1&lt;/li&gt;\n&lt;li&gt;snes - 23&lt;/li&gt;\n&lt;li&gt;tg16 - 3&lt;/li&gt;\n&lt;li&gt;turbocd - 2&lt;/li&gt;\n&lt;li&gt;wii - 411&lt;/li&gt;\n&lt;li&gt;xbox - 452&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The games were mostly deduplicated, but sometimes the scraping would fail due to the connection being reset by the server so the same game might be present twice under different names.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tv29a", "is_robot_indexable": true, "report_reasons": null, "author": "youuuuuuuuuuuuuuuuu", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tv29a/gamefaqs_savegames_collection/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tv29a/gamefaqs_savegames_collection/", "subreddit_subscribers": 679032, "created_utc": 1682065890.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Ok, it took some finding but I found a working link for of RipMe. This is a slightly updated fork of the RipMe software. The link in the description was dead, but some light URL editing lead me to working JAR file.  It's not perfect, and has not been updated since October 2022, but it has an easy to understand GUI that most people can get started right now with. \n\nYes, the 1000 post API limit is present here.\n\nhttps://github.com/ripmeapp2/ripme/releases\n\nGood luck and happy archiving.\n\nEdit, I have it loaded and working, so far archiving both Reddit images and Imgur images from reddit subs.\n\n_________________________\n\n\nRipMe2 is really easy to setup for those without CLI experience or knowledge.  \n\nDownload the JAR to a folder, make a TXT file, in the TXT file list all the subs you want to copy.  RipMe2 is only grabbing the media, photos and videos, so text heavy subs this is not great for.  \n\nOnce your TXT file is made, load it inside the program, set your default download folder (so you don't save everything to your main SSD) and let it go.  I started a list of 500 subs last night and in 8 hours it did about 15GB, 11,139 items.", "author_fullname": "t2_afj5b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RipMe2, For ripping Imgur and Reddit images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tupuh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 36, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 36, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682088358.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682064702.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ok, it took some finding but I found a working link for of RipMe. This is a slightly updated fork of the RipMe software. The link in the description was dead, but some light URL editing lead me to working JAR file.  It&amp;#39;s not perfect, and has not been updated since October 2022, but it has an easy to understand GUI that most people can get started right now with. &lt;/p&gt;\n\n&lt;p&gt;Yes, the 1000 post API limit is present here.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ripmeapp2/ripme/releases\"&gt;https://github.com/ripmeapp2/ripme/releases&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Good luck and happy archiving.&lt;/p&gt;\n\n&lt;p&gt;Edit, I have it loaded and working, so far archiving both Reddit images and Imgur images from reddit subs.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;RipMe2 is really easy to setup for those without CLI experience or knowledge.  &lt;/p&gt;\n\n&lt;p&gt;Download the JAR to a folder, make a TXT file, in the TXT file list all the subs you want to copy.  RipMe2 is only grabbing the media, photos and videos, so text heavy subs this is not great for.  &lt;/p&gt;\n\n&lt;p&gt;Once your TXT file is made, load it inside the program, set your default download folder (so you don&amp;#39;t save everything to your main SSD) and let it go.  I started a list of 500 subs last night and in 8 hours it did about 15GB, 11,139 items.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?auto=webp&amp;v=enabled&amp;s=096eeb8dc19b502cc1490d2f012ab14a03d79c5e", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c05cf6d23623aaf07578fba58f2526f67bd7c104", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0a8ae83a4acb9914d3b2147eb6e97fde0f951c25", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e305d5ebfa072137476305ac12177637160b386", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20c24e5a8f0ef0664121a9d5d4d3a827a4fd4b2a", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ac16d9a992941d10b11b2c7b543c9b4de03485a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/YQOq-h4Gn17eBXm1FB7JrK7I9kjjS_sG6xKUPT-tzOY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a86bc54551172eea421e03965f895c2929e230e5", "width": 1080, "height": 540}], "variants": {}, "id": "kmYT9_2i9LCQNRX9rdnzmRppuqRVXBQapgR-p2FDmjk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1300+TB ZFS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tupuh", "is_robot_indexable": true, "report_reasons": null, "author": "EchoGecko795", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12tupuh/ripme2_for_ripping_imgur_and_reddit_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tupuh/ripme2_for_ripping_imgur_and_reddit_images/", "subreddit_subscribers": 679032, "created_utc": 1682064702.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have seen massive explosion in ML industry and now it has reached peak level of attention. Major powers have jumped in to regulate/control the technology (a cease on research has been called by major figures) and its not far fetched that a lot of content related to ML will be made illegal for general public. Governments and/or Big Tech will try to regulate and take this technology out of general public as they can't function if its accessible to everyone. They need consumers for economy to function and if people become content with what they can produce at home it is threat to their power and control. \n\nAlso a lot of people (who want to protect their jobs) are in favor of regulating the AI which may or may not be a good thing depending on who you ask but all ML related content that we can easily access today can be restricted or removed in future. Also not every country is going to ban this. So it will still be legal in some countries (i.e China, Russia and third world countries). \n\nThe content that is in most demand are models and datasets. HuggingFace is website that is currently providing free access to all tools while datasets can be downloaded from their providers i.e LAION, Facebook, Microsoft or research groups. Future regulations may restrict their access,\n\nI am pretty sure people here will be aware of this. As most open ML research is backed by archivists who are lending space to host datasets and models. I believe this industry is going to change a lot due to power struggle and it may be good idea to backup stuff.", "author_fullname": "t2_24zdsiec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Start Hoarding AI/ML content!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpm40", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682049504.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen massive explosion in ML industry and now it has reached peak level of attention. Major powers have jumped in to regulate/control the technology (a cease on research has been called by major figures) and its not far fetched that a lot of content related to ML will be made illegal for general public. Governments and/or Big Tech will try to regulate and take this technology out of general public as they can&amp;#39;t function if its accessible to everyone. They need consumers for economy to function and if people become content with what they can produce at home it is threat to their power and control. &lt;/p&gt;\n\n&lt;p&gt;Also a lot of people (who want to protect their jobs) are in favor of regulating the AI which may or may not be a good thing depending on who you ask but all ML related content that we can easily access today can be restricted or removed in future. Also not every country is going to ban this. So it will still be legal in some countries (i.e China, Russia and third world countries). &lt;/p&gt;\n\n&lt;p&gt;The content that is in most demand are models and datasets. HuggingFace is website that is currently providing free access to all tools while datasets can be downloaded from their providers i.e LAION, Facebook, Microsoft or research groups. Future regulations may restrict their access,&lt;/p&gt;\n\n&lt;p&gt;I am pretty sure people here will be aware of this. As most open ML research is backed by archivists who are lending space to host datasets and models. I believe this industry is going to change a lot due to power struggle and it may be good idea to backup stuff.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12tpm40", "is_robot_indexable": true, "report_reasons": null, "author": "_H_a_c_k_e_r_", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tpm40/start_hoarding_aiml_content/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpm40/start_hoarding_aiml_content/", "subreddit_subscribers": 679032, "created_utc": 1682049504.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since Imgur is purging its old archives, I thought it'd be a good idea to post about gallery-dl for those who haven't heard of it before\n\nFor those that have image galleries they want to save, I'd highly recommend the use of gallery-dl to save them to your hard drive.  You only need a little bit of knowledge with the command line.  (Grab the Standalone Executable for the easiest time, or use the pip installer command if you have Python)\n\n[https://github.com/mikf/gallery-dl](https://github.com/mikf/gallery-dl)\n\nIt supports Imgur, Pixiv, Deviantart, Tumblr, Reddit, and a host of other gallery and blog sites.\n\nYou can either feed a gallery URL straight to it\n\n    gallery-dl https://imgur.com/a/gC5fd\n\nor create a text file of URLs (let's say lotsofURLs.txt) with one URL per line. You can feed that text file in and it will download each line with a URL one by one.\n\n    gallery-dl -i lotsofURLs.txt\n\nSome sites (such as Pixiv) will require you to provide a username and password via a config file  in your user directory (ie on Windows if your account name is \"hoarderdude\" your user directory would be C:\\\\Users\\\\hoarderdude\n\nThe default Imgur gallery directory saving path does not use the gallery title AFAIK, so if you want a nicer directory structure editing a config file may also be useful.\n\nTo do this, create a text file named gallery-dl.txt in your user directory, fill it with the following (as an example):\n\n    {\n    \"extractor\":\n    {\n        \"base-directory\": \"./gallery-dl/\",\n        \"imgur\":\n    \t{\n    \t\t\"directory\": [\"imgur\", \"{album['id']} - {album['title']}\"]\n    \t}\n    }\n    }\n\nand then rename it from gallery-dl.txt to gallery-dl.conf\n\nThis will ensure directories are labelled with the Imgur gallery name if it exists.\n\nFor further configuration file examples, see:\n\n[https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl.conf](https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl.conf)\n\n[https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl-example.conf](https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl-example.conf)", "author_fullname": "t2_3ag4dgwo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "gallery-dl - Tool to download entire image galleries (and lists of galleries) from dozens of different sites. (Very relevant now due to Imgur purging its galleries, best download your favs before it's too late)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tvpay", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682068114.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since Imgur is purging its old archives, I thought it&amp;#39;d be a good idea to post about gallery-dl for those who haven&amp;#39;t heard of it before&lt;/p&gt;\n\n&lt;p&gt;For those that have image galleries they want to save, I&amp;#39;d highly recommend the use of gallery-dl to save them to your hard drive.  You only need a little bit of knowledge with the command line.  (Grab the Standalone Executable for the easiest time, or use the pip installer command if you have Python)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl\"&gt;https://github.com/mikf/gallery-dl&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It supports Imgur, Pixiv, Deviantart, Tumblr, Reddit, and a host of other gallery and blog sites.&lt;/p&gt;\n\n&lt;p&gt;You can either feed a gallery URL straight to it&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl https://imgur.com/a/gC5fd\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;or create a text file of URLs (let&amp;#39;s say lotsofURLs.txt) with one URL per line. You can feed that text file in and it will download each line with a URL one by one.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;gallery-dl -i lotsofURLs.txt\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Some sites (such as Pixiv) will require you to provide a username and password via a config file  in your user directory (ie on Windows if your account name is &amp;quot;hoarderdude&amp;quot; your user directory would be C:\\Users\\hoarderdude&lt;/p&gt;\n\n&lt;p&gt;The default Imgur gallery directory saving path does not use the gallery title AFAIK, so if you want a nicer directory structure editing a config file may also be useful.&lt;/p&gt;\n\n&lt;p&gt;To do this, create a text file named gallery-dl.txt in your user directory, fill it with the following (as an example):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{\n&amp;quot;extractor&amp;quot;:\n{\n    &amp;quot;base-directory&amp;quot;: &amp;quot;./gallery-dl/&amp;quot;,\n    &amp;quot;imgur&amp;quot;:\n    {\n        &amp;quot;directory&amp;quot;: [&amp;quot;imgur&amp;quot;, &amp;quot;{album[&amp;#39;id&amp;#39;]} - {album[&amp;#39;title&amp;#39;]}&amp;quot;]\n    }\n}\n}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and then rename it from gallery-dl.txt to gallery-dl.conf&lt;/p&gt;\n\n&lt;p&gt;This will ensure directories are labelled with the Imgur gallery name if it exists.&lt;/p&gt;\n\n&lt;p&gt;For further configuration file examples, see:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl.conf\"&gt;https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl.conf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl-example.conf\"&gt;https://github.com/mikf/gallery-dl/blob/master/docs/gallery-dl-example.conf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?auto=webp&amp;v=enabled&amp;s=78e243e74ab154280d2365bc3a61af0ef5bbfc88", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36beb6e6e384a17de98bd61bd1419834898d4e96", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2d33d492cc71cbd84229959427255d3968a8874", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce5e77d09375b7711e4a9603efaa1d0bb2770237", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ab3bb30c3d902b4e666fe27348dd06c79dca19c", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79f108592d905d6f59ea93e6c817cd44e2bacd5c", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/RHmbfo8RTu4xv0mn0fsAEjL4B_M6455J8iWiHnYmHnQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b8ad76a800514dad7ba0eb8469e9f5dec589511", "width": 1080, "height": 540}], "variants": {}, "id": "b2NtGXcZyhx8Fn1TFGs7L3pJbMITjJPMk0h1XE30rSQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tvpay", "is_robot_indexable": true, "report_reasons": null, "author": "boastful_inaba", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tvpay/gallerydl_tool_to_download_entire_image_galleries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tvpay/gallerydl_tool_to_download_entire_image_galleries/", "subreddit_subscribers": 679032, "created_utc": 1682068114.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have over 100 URLs of bookmarks I\u2019ve collected over the years saved in a file. My question is if I wanted to permanently save the actual pages themselves (articles, essays, blog posts etc) what would be the best and easiest way to do it?\n\nIs making a PDF or an HTML file of the page in question sufficient?", "author_fullname": "t2_gfj7b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What\u2019s the best way to save a webpage?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12t6c63", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682009102.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682006868.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have over 100 URLs of bookmarks I\u2019ve collected over the years saved in a file. My question is if I wanted to permanently save the actual pages themselves (articles, essays, blog posts etc) what would be the best and easiest way to do it?&lt;/p&gt;\n\n&lt;p&gt;Is making a PDF or an HTML file of the page in question sufficient?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12t6c63", "is_robot_indexable": true, "report_reasons": null, "author": "Cmyers1980", "discussion_type": null, "num_comments": 11, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12t6c63/whats_the_best_way_to_save_a_webpage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12t6c63/whats_the_best_way_to_save_a_webpage/", "subreddit_subscribers": 679032, "created_utc": 1682006868.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of \"assets\" for things like video editing a graphic design that I want to not only organize well but navigate and preview content from efficiently. A common workflow for me is to be Photoshopping something, need an icon, and often it could be faster to google for some icon svg than it would be to locate it on my NAS.\n\n\u200b\n\nI also often want to add associated metadata that I could keep or possibly search later. Say I download a photo that requires artist attribution, I need to keep that license information with it.  \n\n\nI probably am looking for a Windows-based desktop utility since I am trying to use it with Photoshop and such, but I could potentially host something that better indexes/searches my content and download individual assets per-project.\n\nEdit: I am realizing that Adobe Bridge is actually looking pretty good", "author_fullname": "t2_1jfc9efx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do you use any \"Digital Asset Management\" tools?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlwwt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682049658.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682040211.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of &amp;quot;assets&amp;quot; for things like video editing a graphic design that I want to not only organize well but navigate and preview content from efficiently. A common workflow for me is to be Photoshopping something, need an icon, and often it could be faster to google for some icon svg than it would be to locate it on my NAS.&lt;/p&gt;\n\n&lt;p&gt;\u200b&lt;/p&gt;\n\n&lt;p&gt;I also often want to add associated metadata that I could keep or possibly search later. Say I download a photo that requires artist attribution, I need to keep that license information with it.  &lt;/p&gt;\n\n&lt;p&gt;I probably am looking for a Windows-based desktop utility since I am trying to use it with Photoshop and such, but I could potentially host something that better indexes/searches my content and download individual assets per-project.&lt;/p&gt;\n\n&lt;p&gt;Edit: I am realizing that Adobe Bridge is actually looking pretty good&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tlwwt", "is_robot_indexable": true, "report_reasons": null, "author": "TechSquidTV", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tlwwt/do_you_use_any_digital_asset_management_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tlwwt/do_you_use_any_digital_asset_management_tools/", "subreddit_subscribers": 679032, "created_utc": 1682040211.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello everyone. I need help downloading multiple streams from a music festival starting tomorrow. There are 5 different streams all going on for the same time and last about 8-9 hours each. I tried testing some livestreams from other channels on YoutubeDL Material, but it gives me an error. Also, I want to know if there is a way to set the times and links to auto record at certain times. I need answers by tomorrow afternoon!! I have an unRAID server w/ a GTX 1660Ti and a Windows PC with an RTX 2060.\n\nEdit: Solved! Thank you all for your help!", "author_fullname": "t2_in3ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with Youtube Livestreams (concurrent)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tq2wl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682087551.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682050761.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I need help downloading multiple streams from a music festival starting tomorrow. There are 5 different streams all going on for the same time and last about 8-9 hours each. I tried testing some livestreams from other channels on YoutubeDL Material, but it gives me an error. Also, I want to know if there is a way to set the times and links to auto record at certain times. I need answers by tomorrow afternoon!! I have an unRAID server w/ a GTX 1660Ti and a Windows PC with an RTX 2060.&lt;/p&gt;\n\n&lt;p&gt;Edit: Solved! Thank you all for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tq2wl", "is_robot_indexable": true, "report_reasons": null, "author": "Kobeis2pac", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tq2wl/help_with_youtube_livestreams_concurrent/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tq2wl/help_with_youtube_livestreams_concurrent/", "subreddit_subscribers": 679032, "created_utc": 1682050761.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So here\u2019s the problem. Despite my diligence with backing up, I noticed my backup drive has fewer folders than my primary drive and I\u2019m trying to figure out what\u2019s missing, but there are hundreds of folders in the main directory alone and manually comparing is a nightmare. I\u2019m tempted to just delete the backup and re-copy it all, but that\u2019s 3 terabytes of data, I don\u2019t have all day to wait around for it to copy over. \n\nWhat I\u2019m looking for is a program that will compare the directories, and if there\u2019s a whole folder or even just a single file missing from one, it\u2019ll make note of that and report it back. It would be even better if it could compare the checksum of each file and report if one is different. I believe the missing folders are due to my mistake rather than file corruption, but it would be a relief to know for sure.", "author_fullname": "t2_tggommtv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to automatically compare two folder trees and report back the differences between them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpfvr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682049023.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So here\u2019s the problem. Despite my diligence with backing up, I noticed my backup drive has fewer folders than my primary drive and I\u2019m trying to figure out what\u2019s missing, but there are hundreds of folders in the main directory alone and manually comparing is a nightmare. I\u2019m tempted to just delete the backup and re-copy it all, but that\u2019s 3 terabytes of data, I don\u2019t have all day to wait around for it to copy over. &lt;/p&gt;\n\n&lt;p&gt;What I\u2019m looking for is a program that will compare the directories, and if there\u2019s a whole folder or even just a single file missing from one, it\u2019ll make note of that and report it back. It would be even better if it could compare the checksum of each file and report if one is different. I believe the missing folders are due to my mistake rather than file corruption, but it would be a relief to know for sure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tpfvr", "is_robot_indexable": true, "report_reasons": null, "author": "bobisnotmyuncIe", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tpfvr/is_there_a_way_to_automatically_compare_two/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpfvr/is_there_a_way_to_automatically_compare_two/", "subreddit_subscribers": 679032, "created_utc": 1682049023.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm merging dashcam files using bandicam on an HDD that has 400gb of free space. The output file is under 100gb - but I'm getting an error 70% of the way through saying \"not enough disk space on output folder\".\n\nWhat could be the reason for this?", "author_fullname": "t2_n7q75", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\"Not enough disk space, please change output folder\"", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12td88j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682020650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m merging dashcam files using bandicam on an HDD that has 400gb of free space. The output file is under 100gb - but I&amp;#39;m getting an error 70% of the way through saying &amp;quot;not enough disk space on output folder&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What could be the reason for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12td88j", "is_robot_indexable": true, "report_reasons": null, "author": "StarSurf", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12td88j/not_enough_disk_space_please_change_output_folder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12td88j/not_enough_disk_space_please_change_output_folder/", "subreddit_subscribers": 679032, "created_utc": 1682020650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm not sure if this is the best place to ask, but I'm starting my Jellyfin library and was wondering. I understand the purpose of remuxing a DVD since the original file structure is so messy, but what's the point of going to extra steps to remux a blu-ray when you can just copy the M2TS and rename it? The vast majority of players support M2TS and MKV, why bother with the extra steps? It's only in the edge cases where it seems to make sense, yet it's the standard so there must be a reason. Can someone please enlighten me? Thank you", "author_fullname": "t2_ebdgt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the point of remuxing a blu ray? Why not just copy and rename the M2TS file?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tpzaq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": "", "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682050478.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure if this is the best place to ask, but I&amp;#39;m starting my Jellyfin library and was wondering. I understand the purpose of remuxing a DVD since the original file structure is so messy, but what&amp;#39;s the point of going to extra steps to remux a blu-ray when you can just copy the M2TS and rename it? The vast majority of players support M2TS and MKV, why bother with the extra steps? It&amp;#39;s only in the edge cases where it seems to make sense, yet it&amp;#39;s the standard so there must be a reason. Can someone please enlighten me? Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tpzaq", "is_robot_indexable": true, "report_reasons": null, "author": "incarrion", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12tpzaq/whats_the_point_of_remuxing_a_blu_ray_why_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tpzaq/whats_the_point_of_remuxing_a_blu_ray_why_not/", "subreddit_subscribers": 679032, "created_utc": 1682050478.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Looking for a book scanner to digitize a large collection of books for a project. The scanner should handle various book sizes and types and scan books accurately without causing any damage. The scans need to be high-resolution with clear and sharp images for digital archiving and printing purposes. Open to considering options across different budgets. Please share any experience or recommendations for the best professional book scanner. Thank you!", "author_fullname": "t2_cv6pdkug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Choosing Book Scanner? Need Recommendation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tb9z1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682016650.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for a book scanner to digitize a large collection of books for a project. The scanner should handle various book sizes and types and scan books accurately without causing any damage. The scans need to be high-resolution with clear and sharp images for digital archiving and printing purposes. Open to considering options across different budgets. Please share any experience or recommendations for the best professional book scanner. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tb9z1", "is_robot_indexable": true, "report_reasons": null, "author": "InterestingEmploy669", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tb9z1/choosing_book_scanner_need_recommendation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tb9z1/choosing_book_scanner_need_recommendation/", "subreddit_subscribers": 679032, "created_utc": 1682016650.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019ve been collecting the audio files from podcasts and I want to cut out the ads from them. Are there any super simple audio editors that would work for this? All I need is the ability to cut out portions, I don\u2019t need any fancy editing tools. The main criteria would be efficiency, as I will need to do this for many files and it will probably have to be done manually.\n\nThank you for any recommendations!", "author_fullname": "t2_13f9bd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Simple Audio Editor for Removing Ads from Podcasts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tlkzl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682039447.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been collecting the audio files from podcasts and I want to cut out the ads from them. Are there any super simple audio editors that would work for this? All I need is the ability to cut out portions, I don\u2019t need any fancy editing tools. The main criteria would be efficiency, as I will need to do this for many files and it will probably have to be done manually.&lt;/p&gt;\n\n&lt;p&gt;Thank you for any recommendations!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tlkzl", "is_robot_indexable": true, "report_reasons": null, "author": "rkusty23", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tlkzl/simple_audio_editor_for_removing_ads_from_podcasts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tlkzl/simple_audio_editor_for_removing_ads_from_podcasts/", "subreddit_subscribers": 679032, "created_utc": 1682039447.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello peeps,\n\nUntil today, I thought  Z-Library was a mere mirror of LibGen, apparently this isn't so :)\n\nI have a nice setup with LibGen, it's like this:\n\nInterested in books about say Nutrition\n\n1. Download non-fiction LibGen dump from any of the sites hosting them.\n2. Initialize the DB locally.\n3. Run queries to identify what I need, returning `updated`.`IdentifierWODash` which is the stripped down ISBN in the LibGen dump.\n4. Download said books via Libgen Desktop, just because I haven't found a better software for it.\n\nCan I have something like that for Z-Library?\n\nMy use case is this: \u201cOMG, I should download every PDF book that ever existed on Psychology this weekend, that was published in the past 10 years\u201d.\n\nThen I need a way to make that happen.\n\nDoes Z-Library have a DB dump like Libgen does? What's your setup for getting files from it?\n\nEdit:\n\nDid a search on GitHub:\n\n    zlibrary in:topic pushed:&gt;2023-01-01 stars:&gt;=10\n\nMeans: repositories with zlibrary in their topics, where code was pushed this year and has at least 10 stars.\n\nResulted in\n\n1. [A project in Chinese](https://github.com/Senkita/zLib-Web). I have no idea what it's about.\n2. [ZLibrary CLI](https://github.com/baroxyton/zlibrary-CLI).\n\nZLibrary CLI looks like the only solution these days? It accounts for personal domains, the new Z-Library setup.\n\nIs there anything else?\n\nThanks  \n", "author_fullname": "t2_w0ujml6e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to bulk download non-fiction from Z-Library and search Z-Library", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tfoud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682029007.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682025947.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello peeps,&lt;/p&gt;\n\n&lt;p&gt;Until today, I thought  Z-Library was a mere mirror of LibGen, apparently this isn&amp;#39;t so :)&lt;/p&gt;\n\n&lt;p&gt;I have a nice setup with LibGen, it&amp;#39;s like this:&lt;/p&gt;\n\n&lt;p&gt;Interested in books about say Nutrition&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download non-fiction LibGen dump from any of the sites hosting them.&lt;/li&gt;\n&lt;li&gt;Initialize the DB locally.&lt;/li&gt;\n&lt;li&gt;Run queries to identify what I need, returning &lt;code&gt;updated&lt;/code&gt;.&lt;code&gt;IdentifierWODash&lt;/code&gt; which is the stripped down ISBN in the LibGen dump.&lt;/li&gt;\n&lt;li&gt;Download said books via Libgen Desktop, just because I haven&amp;#39;t found a better software for it.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Can I have something like that for Z-Library?&lt;/p&gt;\n\n&lt;p&gt;My use case is this: \u201cOMG, I should download every PDF book that ever existed on Psychology this weekend, that was published in the past 10 years\u201d.&lt;/p&gt;\n\n&lt;p&gt;Then I need a way to make that happen.&lt;/p&gt;\n\n&lt;p&gt;Does Z-Library have a DB dump like Libgen does? What&amp;#39;s your setup for getting files from it?&lt;/p&gt;\n\n&lt;p&gt;Edit:&lt;/p&gt;\n\n&lt;p&gt;Did a search on GitHub:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;zlibrary in:topic pushed:&amp;gt;2023-01-01 stars:&amp;gt;=10\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Means: repositories with zlibrary in their topics, where code was pushed this year and has at least 10 stars.&lt;/p&gt;\n\n&lt;p&gt;Resulted in&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/Senkita/zLib-Web\"&gt;A project in Chinese&lt;/a&gt;. I have no idea what it&amp;#39;s about.&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/baroxyton/zlibrary-CLI\"&gt;ZLibrary CLI&lt;/a&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;ZLibrary CLI looks like the only solution these days? It accounts for personal domains, the new Z-Library setup.&lt;/p&gt;\n\n&lt;p&gt;Is there anything else?&lt;/p&gt;\n\n&lt;p&gt;Thanks  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?auto=webp&amp;v=enabled&amp;s=00f73080ff2bf090f2dd5dd09bb7fdced4eda3f4", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3767cf5de977f41e6bd19b5891dc929dfd49a38c", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9fd88e7c5c2bee32f08b847306aa0c06cefb4982", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba9deed576c5c01fabf614f6d86b9808639e71b7", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dd8574ae90c27b380a18e94b71f50e4265674eaf", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74362ce9fab54f851ec521a4b36cab471d5246e6", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/QkuZP24TUnLLfItH9mvOFeQylYma80R3D7WdC1MoiZU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cd461a5b0618ce06de92e1ce7698bf4aa16f0931", "width": 1080, "height": 540}], "variants": {}, "id": "kogm69pcGJGnWfA-SgY36JvhgpMJjd2m-mJYyI6cGak"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tfoud", "is_robot_indexable": true, "report_reasons": null, "author": "EUTIORti", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tfoud/how_to_bulk_download_nonfiction_from_zlibrary_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tfoud/how_to_bulk_download_nonfiction_from_zlibrary_and/", "subreddit_subscribers": 679032, "created_utc": 1682025947.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone ever spoken to Denis Petrov who owns the [archive.is](https://archive.is) website? I've tried so many Emails and haven't received any reply concerning my DCMA request. I've been very polite, but this is getting ridiculous.  \n\n\nI did find a phone number and address when I put the website into ICANN and WhoIs. Has anyone managed to reach him using those?  \n\n\nI don't want to annoy him, just get the pictures of my daughters removed that were put on his site by a person who has been creeping on me. I deleted them off the account they originally were on when things started to happen, but then I found out the creep had archived the pages! I don't want this creep who has been very.... I can't go into details here but just they make their love of underaged things known... I don't want that kind of person to have access to these images that they clearly saved for a sick purpose!  \n\n\nI don't know what to do. If anyone knows anything please let me know!", "author_fullname": "t2_qboeung3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has Anyone Ever Actually Spoken to Denis Petrov of Archive.Is?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12trawt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682054293.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ever spoken to Denis Petrov who owns the &lt;a href=\"https://archive.is\"&gt;archive.is&lt;/a&gt; website? I&amp;#39;ve tried so many Emails and haven&amp;#39;t received any reply concerning my DCMA request. I&amp;#39;ve been very polite, but this is getting ridiculous.  &lt;/p&gt;\n\n&lt;p&gt;I did find a phone number and address when I put the website into ICANN and WhoIs. Has anyone managed to reach him using those?  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to annoy him, just get the pictures of my daughters removed that were put on his site by a person who has been creeping on me. I deleted them off the account they originally were on when things started to happen, but then I found out the creep had archived the pages! I don&amp;#39;t want this creep who has been very.... I can&amp;#39;t go into details here but just they make their love of underaged things known... I don&amp;#39;t want that kind of person to have access to these images that they clearly saved for a sick purpose!  &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know what to do. If anyone knows anything please let me know!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cMsDEjZcw1yJ2-Oox97YXjr_B80QoYA7KKKBUw8desk.jpg?auto=webp&amp;v=enabled&amp;s=70049ac5e4587eb732d86f2bf3c7f941a6314e91", "width": 144, "height": 144}, "resolutions": [{"url": "https://external-preview.redd.it/cMsDEjZcw1yJ2-Oox97YXjr_B80QoYA7KKKBUw8desk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e81d09b8bf08f6c2e49210b8e709ad2fba21fef5", "width": 108, "height": 108}], "variants": {}, "id": "5WAXcyxu5qzeFIANl2gYNgI5-HT3q1BW7sA5zqtskZE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12trawt", "is_robot_indexable": true, "report_reasons": null, "author": "TheVeganDragon_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12trawt/has_anyone_ever_actually_spoken_to_denis_petrov/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12trawt/has_anyone_ever_actually_spoken_to_denis_petrov/", "subreddit_subscribers": 679032, "created_utc": 1682054293.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm into searching how to put more storage disks into HP Z240 tower chassis. It has 2 sata conectors and one nvme on board itself. Only 2 SATA on motherboard was surprise since in tower there are plenty of room for internal drives. My guess is that I would need to go via PCIE expansion cards, but could not find consensus by searching the web on what is the best approach. Apparently this motherboard does not support bifurcation so for NVME is only one drive per one PCIE slot? SATA PCIE expansion card is also needed. Ideally I would like to run minimum of 3 SATA drives and 3 NVME...", "author_fullname": "t2_i2xgwndg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HP Z240 tower storage options?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tqct7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682051528.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m into searching how to put more storage disks into HP Z240 tower chassis. It has 2 sata conectors and one nvme on board itself. Only 2 SATA on motherboard was surprise since in tower there are plenty of room for internal drives. My guess is that I would need to go via PCIE expansion cards, but could not find consensus by searching the web on what is the best approach. Apparently this motherboard does not support bifurcation so for NVME is only one drive per one PCIE slot? SATA PCIE expansion card is also needed. Ideally I would like to run minimum of 3 SATA drives and 3 NVME...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tqct7", "is_robot_indexable": true, "report_reasons": null, "author": "No_Requirement_64OO", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tqct7/hp_z240_tower_storage_options/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tqct7/hp_z240_tower_storage_options/", "subreddit_subscribers": 679032, "created_utc": 1682051528.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So I have two large SAS/SATA expanders, a 16-bay and a 24-bay both made by RAID Machine (link to the 24-bay version below) They're great and they do the trick but the fans are like jet engines taking off, they're ridiculously loud. Would anyone happen to know if replacing the fans is a viable option? i dunno if they have temp sensors or something like that. If they are replaceable, would anyone have suggestions on quieter replacement fans? Or if the noise is just what it is, any suggestions on some kind of cabinet or something I could build out to try to mitigate it? Obviously airflow/ventilation would be a primary concern. They're currently rackmounted in a custom ATA style road case on wheels that slides under my desk. Anyway, I'm all ears if anyone has thoughts.\n\n&amp;#x200B;\n\n[https://www.pc-pitstop.com/24-bay-12g-expander-enclosure](https://www.pc-pitstop.com/24-bay-12g-expander-enclosure)", "author_fullname": "t2_aukaiie4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loud fans", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tkp6o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682037370.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have two large SAS/SATA expanders, a 16-bay and a 24-bay both made by RAID Machine (link to the 24-bay version below) They&amp;#39;re great and they do the trick but the fans are like jet engines taking off, they&amp;#39;re ridiculously loud. Would anyone happen to know if replacing the fans is a viable option? i dunno if they have temp sensors or something like that. If they are replaceable, would anyone have suggestions on quieter replacement fans? Or if the noise is just what it is, any suggestions on some kind of cabinet or something I could build out to try to mitigate it? Obviously airflow/ventilation would be a primary concern. They&amp;#39;re currently rackmounted in a custom ATA style road case on wheels that slides under my desk. Anyway, I&amp;#39;m all ears if anyone has thoughts.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.pc-pitstop.com/24-bay-12g-expander-enclosure\"&gt;https://www.pc-pitstop.com/24-bay-12g-expander-enclosure&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?auto=webp&amp;v=enabled&amp;s=da26c84211db6bfe70d900ba26856f0bcf91505c", "width": 400, "height": 170}, "resolutions": [{"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f2f4ff1fca374eb83aa4760f5c7d3e9712af75c", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1730f57cf94ea15b684b80527e7242b25348ae9b", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/5rIs6-5KYuZRFH_Ktv5PALALxLl5grtifhx660eM62Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45d4f4395be91ba11bf9c841d9371c77ae8631a3", "width": 320, "height": 136}], "variants": {}, "id": "Qh5tIF25DB92XS8vDre-aDXLS7_h3WcZhBsWj1sK2DQ"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tkp6o", "is_robot_indexable": true, "report_reasons": null, "author": "2Ksmooth", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tkp6o/loud_fans/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tkp6o/loud_fans/", "subreddit_subscribers": 679032, "created_utc": 1682037370.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Related, what software is recommended just to play the DVD?  Both from another region and even in my own region.", "author_fullname": "t2_difju", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have a DVD from region 4 (I am in region 1), is there software to extract the videos or otherwise make a playable copy in my region?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "guide", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12u4xop", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Guide/How-to", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682088567.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Related, what software is recommended just to play the DVD?  Both from another region and even in my own region.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "69c7c050-b94e-11eb-9c9c-0eb6f39ede5f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12u4xop", "is_robot_indexable": true, "report_reasons": null, "author": "in4real", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12u4xop/i_have_a_dvd_from_region_4_i_am_in_region_1_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12u4xop/i_have_a_dvd_from_region_4_i_am_in_region_1_is/", "subreddit_subscribers": 679032, "created_utc": 1682088567.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have started a project to help archive content that is in threat of being removed from the internet. I have created a git repo to track and receive contributions (just data no money) to help this move along.\n\nhttps://github.com/RCcola1987/Archival-Preservation", "author_fullname": "t2_ktl0q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My project to help save content form deletion!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12txjj4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682073511.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have started a project to help archive content that is in threat of being removed from the internet. I have created a git repo to track and receive contributions (just data no money) to help this move along.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/RCcola1987/Archival-Preservation\"&gt;https://github.com/RCcola1987/Archival-Preservation&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?auto=webp&amp;v=enabled&amp;s=7cd9a380febb7d8d78ab959927f72a6f610a4b57", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5dda02da8d55b0b78193d9fa666049dacae260c1", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4492ddbd74392fcb8adc70869c8671efa55c2ad6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e2bdf5b3f8fcdc9255a5e249dd434fc7b61685dc", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=37989bfbac705dc8432e526a91409ed3f5955121", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4e2dca63d7efc403a2f6a371cafb087e66ee0f0", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/zIn3kJWiFkWvtY6pA_oSnQJ-nYBYxPDnPKENvuJilX0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38cdee863a5df5ba2dbf403274e730c2273ccdf1", "width": 1080, "height": 540}], "variants": {}, "id": "wRArez8_fnbnmbuEG5fBf4RHPOGdtn2d1U1egag_iDw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1PB Formatted", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12txjj4", "is_robot_indexable": true, "report_reasons": null, "author": "RCcola1987", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12txjj4/my_project_to_help_save_content_form_deletion/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12txjj4/my_project_to_help_save_content_form_deletion/", "subreddit_subscribers": 679032, "created_utc": 1682073511.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Link: [https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html](https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html)", "author_fullname": "t2_22bur9io", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "BuzzFeed News, Which Dragged Media Into the Digital Age, Shuts Down", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tno3h", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682044442.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Link: &lt;a href=\"https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html\"&gt;https://www.nytimes.com/2023/04/20/business/buzzfeed-news-shut-down.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?auto=webp&amp;v=enabled&amp;s=23240c57aab8133784c52a6677408f9e5bbcae8e", "width": 1050, "height": 549}, "resolutions": [{"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2c6302b02d8bf5d9e01ef89d1951c0544f454a0a", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c49dbec0d1ad8f5ff6d557048585eae3d2a7c147", "width": 216, "height": 112}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4fad551e310ce78029b4e1864946889af23b1d70", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a6297044c9341a30035e3f8337f912a0995302be", "width": 640, "height": 334}, {"url": "https://external-preview.redd.it/GZFZu1LcirJA4gtmnsYokLDHyjfn1UTV68VfA_V3o3w.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d73ea86ac977f2982068f14de815ec3b4154ea57", "width": 960, "height": 501}], "variants": {}, "id": "b1Knl5I0c-FFco_fthSZJqci2gEa_iHaq3W3AkA04tE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tno3h", "is_robot_indexable": true, "report_reasons": null, "author": "classicalist", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tno3h/buzzfeed_news_which_dragged_media_into_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tno3h/buzzfeed_news_which_dragged_media_into_the/", "subreddit_subscribers": 679032, "created_utc": 1682044442.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a way to save all the linked photos from a forum for posterity? With imgur removing photos that are not related to an account, it will completely destroy most of the content on a dreamwidth forum (it was originally hosted on LiveJournal) I go on. \n\nEvery day, a number of secrets are posted for people to comment on. They are posted in the main post as embedded images and then are linked directly below one by one for people to discuss. \n\nI don't want all those secrets just lost forever. But I also don't want to save every photo made in the replies.", "author_fullname": "t2_bamgj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to save every linked image from the main body of posts on a site (but not the comments)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12tj4p6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682033681.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a way to save all the linked photos from a forum for posterity? With imgur removing photos that are not related to an account, it will completely destroy most of the content on a dreamwidth forum (it was originally hosted on LiveJournal) I go on. &lt;/p&gt;\n\n&lt;p&gt;Every day, a number of secrets are posted for people to comment on. They are posted in the main post as embedded images and then are linked directly below one by one for people to discuss. &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want all those secrets just lost forever. But I also don&amp;#39;t want to save every photo made in the replies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12tj4p6", "is_robot_indexable": true, "report_reasons": null, "author": "Ashmeadow", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12tj4p6/is_there_a_way_to_save_every_linked_image_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12tj4p6/is_there_a_way_to_save_every_linked_image_from/", "subreddit_subscribers": 679032, "created_utc": 1682033681.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Can i ask how people have set their hard drives up for media streaming at home for movies and music etc, such as PLEX. \n\nSo  i originally had some little Noontec N5 NAS enclosures, that had 4TB  drives in. I started with 4 of these and went up to 8 enclosures.\n\nSeeing  that I was going to need a lot more I decided to build a second  computer specifically for my media which currently has 4x 10TB drives,  2x 4TB drives, and then 4x 5TB USB drives attached.\n\nI can see that in the very near future this probably isnt going to be enough.\n\nWhat do you guys do, do I build a 2nd media computer or is there a better solution?\n\nSomething  else that may be important, I have backup drives for the above  (currently around 20x 3TB / 4TB drives), these are kept in a cupboard  and I only plug them into a USB enclosure for backup purposes. Its a  pain having to keep plugging them in when I need them, although its not  very often, so I probably wouldnt want them spinning up and down all the  time for months on end when they're not used, so probably best to maybe  keep them in the cupboard as they are? I dont have RAID of any kind, I  just backup manually (I just basically add anything new to my backup  drives every month or two).", "author_fullname": "t2_64bzq7hu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need some advice on whether I've got the right hard drive setup for PLEX.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12u8p12", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682092247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can i ask how people have set their hard drives up for media streaming at home for movies and music etc, such as PLEX. &lt;/p&gt;\n\n&lt;p&gt;So  i originally had some little Noontec N5 NAS enclosures, that had 4TB  drives in. I started with 4 of these and went up to 8 enclosures.&lt;/p&gt;\n\n&lt;p&gt;Seeing  that I was going to need a lot more I decided to build a second  computer specifically for my media which currently has 4x 10TB drives,  2x 4TB drives, and then 4x 5TB USB drives attached.&lt;/p&gt;\n\n&lt;p&gt;I can see that in the very near future this probably isnt going to be enough.&lt;/p&gt;\n\n&lt;p&gt;What do you guys do, do I build a 2nd media computer or is there a better solution?&lt;/p&gt;\n\n&lt;p&gt;Something  else that may be important, I have backup drives for the above  (currently around 20x 3TB / 4TB drives), these are kept in a cupboard  and I only plug them into a USB enclosure for backup purposes. Its a  pain having to keep plugging them in when I need them, although its not  very often, so I probably wouldnt want them spinning up and down all the  time for months on end when they&amp;#39;re not used, so probably best to maybe  keep them in the cupboard as they are? I dont have RAID of any kind, I  just backup manually (I just basically add anything new to my backup  drives every month or two).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12u8p12", "is_robot_indexable": true, "report_reasons": null, "author": "Sparky_S127", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12u8p12/need_some_advice_on_whether_ive_got_the_right/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12u8p12/need_some_advice_on_whether_ive_got_the_right/", "subreddit_subscribers": 679032, "created_utc": 1682092247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I own a business and I produce a lot of video content.  I produce television commercials, documentaries, wedding videos, convert people's VHS to MP4 files, work with 360 video and many other video applications.   \n\nI built a server and gave that server 20tb of storage over the last 5 years.  Literally all it is for is hosting these massive files.  I'm using about 17.5tb now and about to dump a few hours of 4K drone video to edit.  It won't use 2.5tb, but these kind of data dumps are regular occurrences and I don't want to throw anything away that I might need.   \n\nA good solution would be to keep adding storage to my server, but what can I use for back up?   I've been manually using Microsoft oneDrive so far but I'd like something sophisticated or that is built for video.", "author_fullname": "t2_3mk8hf90", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Storage Solution for Video Needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12u6ly5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682090032.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I own a business and I produce a lot of video content.  I produce television commercials, documentaries, wedding videos, convert people&amp;#39;s VHS to MP4 files, work with 360 video and many other video applications.   &lt;/p&gt;\n\n&lt;p&gt;I built a server and gave that server 20tb of storage over the last 5 years.  Literally all it is for is hosting these massive files.  I&amp;#39;m using about 17.5tb now and about to dump a few hours of 4K drone video to edit.  It won&amp;#39;t use 2.5tb, but these kind of data dumps are regular occurrences and I don&amp;#39;t want to throw anything away that I might need.   &lt;/p&gt;\n\n&lt;p&gt;A good solution would be to keep adding storage to my server, but what can I use for back up?   I&amp;#39;ve been manually using Microsoft oneDrive so far but I&amp;#39;d like something sophisticated or that is built for video.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12u6ly5", "is_robot_indexable": true, "report_reasons": null, "author": "adamwestland", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12u6ly5/storage_solution_for_video_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12u6ly5/storage_solution_for_video_needed/", "subreddit_subscribers": 679032, "created_utc": 1682090032.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi :3\n\nI'm about to upgrade my homeserver which runs a RAIDz1 with 4x 18TB drives at the moment and I'm quite happy with it and the performance. It mostly stores big files (usually &gt;1GB), and the Performance is decent for my usecase.  \nNow I have read some contradicting information about RAIDz3 Performance and just wanted to ask some people here that have more experience with ZFS. My simple question is:  \n\n\nIs the Performance of a RAIDz3 with 10 drives higher or equal to a RAIDz1 with 4 drives? Keep in mind that I use it 99% for big files. So mostly sequential read and writes.  \n\n\nFrom my understanding it should be higher or equal.  \n\n\nThanks for clearing that (hopefully) up ;)  \n\n\nCheers", "author_fullname": "t2_16zxb9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ZFS Performance comparison question: 4 Disks in RAIDz1 vs 10 Disks in RAIDz3", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12u61mx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682089542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi :3&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m about to upgrade my homeserver which runs a RAIDz1 with 4x 18TB drives at the moment and I&amp;#39;m quite happy with it and the performance. It mostly stores big files (usually &amp;gt;1GB), and the Performance is decent for my usecase.&lt;br/&gt;\nNow I have read some contradicting information about RAIDz3 Performance and just wanted to ask some people here that have more experience with ZFS. My simple question is:  &lt;/p&gt;\n\n&lt;p&gt;Is the Performance of a RAIDz3 with 10 drives higher or equal to a RAIDz1 with 4 drives? Keep in mind that I use it 99% for big files. So mostly sequential read and writes.  &lt;/p&gt;\n\n&lt;p&gt;From my understanding it should be higher or equal.  &lt;/p&gt;\n\n&lt;p&gt;Thanks for clearing that (hopefully) up ;)  &lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12u61mx", "is_robot_indexable": true, "report_reasons": null, "author": "CMDR_Kassandra", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12u61mx/zfs_performance_comparison_question_4_disks_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12u61mx/zfs_performance_comparison_question_4_disks_in/", "subreddit_subscribers": 679032, "created_utc": 1682089542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Pretty much the title. Wondering which image only subreddits that have most of their content on imgur are worth archiving.", "author_fullname": "t2_eu4yq6nr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "With imgur removing content, which subreddits are worth archiving that depend on it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u2wgl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682086089.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pretty much the title. Wondering which image only subreddits that have most of their content on imgur are worth archiving.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12u2wgl", "is_robot_indexable": true, "report_reasons": null, "author": "MajAstro", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12u2wgl/with_imgur_removing_content_which_subreddits_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12u2wgl/with_imgur_removing_content_which_subreddits_are/", "subreddit_subscribers": 679032, "created_utc": 1682086089.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}