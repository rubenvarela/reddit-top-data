{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone, let me preface this: **I understand this is a very general ask and therefore I can only expect very general responses.**\n\nI am most likely going to transition from my current data scientist role into a data analytics role because it aligns better with my interests and skillset. I know that generally speaking, the data scientist's salary is higher than that of the data analyst, but I also expect I will be able to leverage my experience to get a more senior role within data analytics that I am hoping will at least offset the difference.\n\nMy current role is an Associate-level Data Scientist, and my target role will be a Senior-level Data Analyst. Again, **I know there are a lot of variables at play here**, but in a very general sense, what do people think I should expect salary-wise? A slight decrease? Slight increase? Roughly the same?\n\nAny and all input is appreciated. Thanks all!", "author_fullname": "t2_ngcpuv32", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salary expectations moving from data science into data analytics?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q8oaq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 144, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 144, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681788406.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, let me preface this: &lt;strong&gt;I understand this is a very general ask and therefore I can only expect very general responses.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I am most likely going to transition from my current data scientist role into a data analytics role because it aligns better with my interests and skillset. I know that generally speaking, the data scientist&amp;#39;s salary is higher than that of the data analyst, but I also expect I will be able to leverage my experience to get a more senior role within data analytics that I am hoping will at least offset the difference.&lt;/p&gt;\n\n&lt;p&gt;My current role is an Associate-level Data Scientist, and my target role will be a Senior-level Data Analyst. Again, &lt;strong&gt;I know there are a lot of variables at play here&lt;/strong&gt;, but in a very general sense, what do people think I should expect salary-wise? A slight decrease? Slight increase? Roughly the same?&lt;/p&gt;\n\n&lt;p&gt;Any and all input is appreciated. Thanks all!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12q8oaq", "is_robot_indexable": true, "report_reasons": null, "author": "abnormal_oats", "discussion_type": null, "num_comments": 91, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12q8oaq/salary_expectations_moving_from_data_science_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12q8oaq/salary_expectations_moving_from_data_science_into/", "subreddit_subscribers": 875958, "created_utc": 1681788406.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "This particular project is for client-facing stakeholders. My team lead and I are tasked with automating several of their data-driven slides on Tableau that they currently manually produce not sure how or where.\n\nOne particular slide is a pie chart (yeah, I know) that splits the data into ~10 different segments or so, each with its % of market share.\n\nWe did so, and they complained that the numbers percentage points add up to 98%.\n\nWe explained that it's because of rounding, and if we included the decimal it would add up to 100%.\n\nThey started going on about how they present this to CFOs and they'll ask why it doesn't add up to 100% and it has to be perfect and etc.\n\nSo we offered to show the decimal, but nope, can't do that because it's \"hard to read.\"\n\nRemember how they produce those manually at the moment? They said, and I quote, \"sometimes I change a 3% to a 4% to make it work, because what's 1% more?\"\n\nI can kind of understand changing 20% to 21%, because that's only a 5% difference. But really, 3% to 4%? A whopping 33% difference?\n\nAnyway, I'm not about to tell them how to do their job, since I can barely do mine. Lord knows I have no idea how to automate this arbitrary number-fudging on Tableau, so I'll have to figure that one out (it has to be automated so that it adds up to 100% no matter what data ranges the user chooses). \n\nBut I just wonder, how hard is it to tell a CFO \"yeah, it doesn't add up to 100% because of rounding, but if we included the decimals it would\"?", "author_fullname": "t2_9g0ib", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I was just asked to fudge the numbers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qzs1k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 119, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 119, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681843364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This particular project is for client-facing stakeholders. My team lead and I are tasked with automating several of their data-driven slides on Tableau that they currently manually produce not sure how or where.&lt;/p&gt;\n\n&lt;p&gt;One particular slide is a pie chart (yeah, I know) that splits the data into ~10 different segments or so, each with its % of market share.&lt;/p&gt;\n\n&lt;p&gt;We did so, and they complained that the numbers percentage points add up to 98%.&lt;/p&gt;\n\n&lt;p&gt;We explained that it&amp;#39;s because of rounding, and if we included the decimal it would add up to 100%.&lt;/p&gt;\n\n&lt;p&gt;They started going on about how they present this to CFOs and they&amp;#39;ll ask why it doesn&amp;#39;t add up to 100% and it has to be perfect and etc.&lt;/p&gt;\n\n&lt;p&gt;So we offered to show the decimal, but nope, can&amp;#39;t do that because it&amp;#39;s &amp;quot;hard to read.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Remember how they produce those manually at the moment? They said, and I quote, &amp;quot;sometimes I change a 3% to a 4% to make it work, because what&amp;#39;s 1% more?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I can kind of understand changing 20% to 21%, because that&amp;#39;s only a 5% difference. But really, 3% to 4%? A whopping 33% difference?&lt;/p&gt;\n\n&lt;p&gt;Anyway, I&amp;#39;m not about to tell them how to do their job, since I can barely do mine. Lord knows I have no idea how to automate this arbitrary number-fudging on Tableau, so I&amp;#39;ll have to figure that one out (it has to be automated so that it adds up to 100% no matter what data ranges the user chooses). &lt;/p&gt;\n\n&lt;p&gt;But I just wonder, how hard is it to tell a CFO &amp;quot;yeah, it doesn&amp;#39;t add up to 100% because of rounding, but if we included the decimals it would&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qzs1k", "is_robot_indexable": true, "report_reasons": null, "author": "Malarazz", "discussion_type": null, "num_comments": 82, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qzs1k/i_was_just_asked_to_fudge_the_numbers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qzs1k/i_was_just_asked_to_fudge_the_numbers/", "subreddit_subscribers": 875958, "created_utc": 1681843364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m fortunate to be with a company, and on a project, that does not go on hiring sprees, and due to its market niche rarely lays off data science folks. I\u2019m newish here, and thankfully have a straightforward role doing exploratory analysis and some people management, but I want to make myself less disposable. I meet deadlines, and get good feedback on my work, but I want to develop skills that will make me more valuable to management.\n\nWhat are you all doing to make your position more secure?", "author_fullname": "t2_7fs1p26vi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What skills make you less dispensable?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qs1ib", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681832074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m fortunate to be with a company, and on a project, that does not go on hiring sprees, and due to its market niche rarely lays off data science folks. I\u2019m newish here, and thankfully have a straightforward role doing exploratory analysis and some people management, but I want to make myself less disposable. I meet deadlines, and get good feedback on my work, but I want to develop skills that will make me more valuable to management.&lt;/p&gt;\n\n&lt;p&gt;What are you all doing to make your position more secure?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qs1ib", "is_robot_indexable": true, "report_reasons": null, "author": "Cannoli_Emma", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qs1ib/what_skills_make_you_less_dispensable/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qs1ib/what_skills_make_you_less_dispensable/", "subreddit_subscribers": 875958, "created_utc": 1681832074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I am having a business problem, where we used to send some metrics Numbers each day, by putting them into excel sheet,  and send them to each stakeholders \n\nI have used Python to automate the numbers, and send the stakeholders daily mails using smtp lib.\n\nBut that doesn't  looks nice to present them to stakeholders like, CEO ,\n\nSo, I just wanna know, can we replicate this view using CSS/HTML or any other method,\n\nif yes, which method would work? I am data analyst So dont have any idea in css html or any other ,\n\n&amp;#x200B;\n\nhttps://preview.redd.it/yh9wlmk0xmua1.png?width=1073&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=245e3cb63690ed90500f7907803a01857d141144", "author_fullname": "t2_agvtvokn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Replicating a sheet", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"yh9wlmk0xmua1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 177, "x": 108, "u": "https://preview.redd.it/yh9wlmk0xmua1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ff7756410f3eff1a130b849b9733885d6cbbe71"}, {"y": 354, "x": 216, "u": "https://preview.redd.it/yh9wlmk0xmua1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a330aca0093670629bdfb7c2d8eea74925eb44e8"}, {"y": 525, "x": 320, "u": "https://preview.redd.it/yh9wlmk0xmua1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a9cedccd7673911d2cb53e763715c91d52ef7a5b"}, {"y": 1050, "x": 640, "u": "https://preview.redd.it/yh9wlmk0xmua1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5278ca7cab7a0e72f6fce20abe0491ca5182860b"}, {"y": 1575, "x": 960, "u": "https://preview.redd.it/yh9wlmk0xmua1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd92b7bf8f9bf0b60daf7879c2010376c3209941"}], "s": {"y": 1761, "x": 1073, "u": "https://preview.redd.it/yh9wlmk0xmua1.png?width=1073&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=245e3cb63690ed90500f7907803a01857d141144"}, "id": "yh9wlmk0xmua1"}}, "name": "t3_12ql37a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/CIVfGZm5nguI7lUIpCWOZ9xw4WgLmAzBMvYI162AuBM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681820093.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am having a business problem, where we used to send some metrics Numbers each day, by putting them into excel sheet,  and send them to each stakeholders &lt;/p&gt;\n\n&lt;p&gt;I have used Python to automate the numbers, and send the stakeholders daily mails using smtp lib.&lt;/p&gt;\n\n&lt;p&gt;But that doesn&amp;#39;t  looks nice to present them to stakeholders like, CEO ,&lt;/p&gt;\n\n&lt;p&gt;So, I just wanna know, can we replicate this view using CSS/HTML or any other method,&lt;/p&gt;\n\n&lt;p&gt;if yes, which method would work? I am data analyst So dont have any idea in css html or any other ,&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yh9wlmk0xmua1.png?width=1073&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=245e3cb63690ed90500f7907803a01857d141144\"&gt;https://preview.redd.it/yh9wlmk0xmua1.png?width=1073&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=245e3cb63690ed90500f7907803a01857d141144&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ql37a", "is_robot_indexable": true, "report_reasons": null, "author": "chilly_tomato", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ql37a/replicating_a_sheet/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ql37a/replicating_a_sheet/", "subreddit_subscribers": 875958, "created_utc": 1681820093.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How do you guys deal with date time features ( like transaction time stamp) when training classifiers ? I extract hour, minute, weekday number, month and year as new features. Should I drop the date time column as I am not sure how a logistic regression model would treat it for example.", "author_fullname": "t2_9atv3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Date time features in tree based models", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qtd9o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681833611.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you guys deal with date time features ( like transaction time stamp) when training classifiers ? I extract hour, minute, weekday number, month and year as new features. Should I drop the date time column as I am not sure how a logistic regression model would treat it for example.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qtd9o", "is_robot_indexable": true, "report_reasons": null, "author": "longgamma", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qtd9o/date_time_features_in_tree_based_models/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qtd9o/date_time_features_in_tree_based_models/", "subreddit_subscribers": 875958, "created_utc": 1681833611.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently majoring in MIS, and I want to do a master's degree in Data Science after completing my undergraduate. Are there any good, affordable online programs for a master's degree in Data Science? Since I want to become a data scientist, I would believe that a master's degree in Data Science would be the best option for me, especially since I don't have a computer science background. Do you have any other recommendations for what I could do?", "author_fullname": "t2_vunakdjh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Master Programs for Data Science", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12rcvv5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681869789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently majoring in MIS, and I want to do a master&amp;#39;s degree in Data Science after completing my undergraduate. Are there any good, affordable online programs for a master&amp;#39;s degree in Data Science? Since I want to become a data scientist, I would believe that a master&amp;#39;s degree in Data Science would be the best option for me, especially since I don&amp;#39;t have a computer science background. Do you have any other recommendations for what I could do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12rcvv5", "is_robot_indexable": true, "report_reasons": null, "author": "VoidType0", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12rcvv5/master_programs_for_data_science/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12rcvv5/master_programs_for_data_science/", "subreddit_subscribers": 875958, "created_utc": 1681869789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is it normal that companies are not serious about hiring, take very long time and getting an offer is just not common\n\nI\u2019m not talking about job boards. Is it normal that you find a lot of positions that have not been posted yet, through perhaps word of mouth or recommendations, those companies constantly say you\u2019re perfect for this, will send you an offer shortly, then just don\u2019t? \n\nI have not quite experienced this in other fields. Maybe it takes too much to be in this field, and it\u2019s better to pursue something else? I never heard of so much playing around, or not knowing what they want?  ( career level: experienced)\n\nThis happened 3 times in the last month. Basically you\u2019re the chosen candidate, there are  no other candidates, let me just draft up the papers, then nothing or rejection. Also no questions in interviews or tests? They just reiterate your resume to you and how they know you\u2019d be a perfect fit", "author_fullname": "t2_6h5h5k7v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hiring practices particular to this field?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qp3p8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681828525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it normal that companies are not serious about hiring, take very long time and getting an offer is just not common&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not talking about job boards. Is it normal that you find a lot of positions that have not been posted yet, through perhaps word of mouth or recommendations, those companies constantly say you\u2019re perfect for this, will send you an offer shortly, then just don\u2019t? &lt;/p&gt;\n\n&lt;p&gt;I have not quite experienced this in other fields. Maybe it takes too much to be in this field, and it\u2019s better to pursue something else? I never heard of so much playing around, or not knowing what they want?  ( career level: experienced)&lt;/p&gt;\n\n&lt;p&gt;This happened 3 times in the last month. Basically you\u2019re the chosen candidate, there are  no other candidates, let me just draft up the papers, then nothing or rejection. Also no questions in interviews or tests? They just reiterate your resume to you and how they know you\u2019d be a perfect fit&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qp3p8", "is_robot_indexable": true, "report_reasons": null, "author": "DerpyOwlofParadise", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qp3p8/hiring_practices_particular_to_this_field/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qp3p8/hiring_practices_particular_to_this_field/", "subreddit_subscribers": 875958, "created_utc": 1681828525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Maybe my doubt is dumb, but how would you show on a table the variation between 0 and non-zero number? \n\nFor example: on 2021 I sold 0 cars and on 2022 I sold 4. To calculate the % change from on year to the other I would have to do (4/0)-1, wich is impossible to calculate. Should I just write something like \"increase\"? Or should I just put an * and explain the impossibility?", "author_fullname": "t2_7e27w7hg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to display the result of a division by 0?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r95u8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681861600.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe my doubt is dumb, but how would you show on a table the variation between 0 and non-zero number? &lt;/p&gt;\n\n&lt;p&gt;For example: on 2021 I sold 0 cars and on 2022 I sold 4. To calculate the % change from on year to the other I would have to do (4/0)-1, wich is impossible to calculate. Should I just write something like &amp;quot;increase&amp;quot;? Or should I just put an * and explain the impossibility?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12r95u8", "is_robot_indexable": true, "report_reasons": null, "author": "Accomplished-Wave356", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12r95u8/how_to_display_the_result_of_a_division_by_0/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12r95u8/how_to_display_the_result_of_a_division_by_0/", "subreddit_subscribers": 875958, "created_utc": 1681861600.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hi r/datascience\n\nI'm trying to design a method to evaluate the price of an asset given certain features. I have lots of data to work with, so the # of observations is not a real constraint.\n\nBased on my conceptual knowledge of the features, I expect most of them to have a linear/semi-linear relationship with the predicted value except for 2. For these 2 features, I expect the predicted value to have more of a clustering/radial relationship.\n\nI can understand how to model each of the two feature-types and their relationship to the predicted variable separately, but how could I ensure that the interaction between them is captured as well?", "author_fullname": "t2_v406kvhe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Applying Different Statistical Methods to Certain Areas of The Feature Space", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r79j4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681857610.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to design a method to evaluate the price of an asset given certain features. I have lots of data to work with, so the # of observations is not a real constraint.&lt;/p&gt;\n\n&lt;p&gt;Based on my conceptual knowledge of the features, I expect most of them to have a linear/semi-linear relationship with the predicted value except for 2. For these 2 features, I expect the predicted value to have more of a clustering/radial relationship.&lt;/p&gt;\n\n&lt;p&gt;I can understand how to model each of the two feature-types and their relationship to the predicted variable separately, but how could I ensure that the interaction between them is captured as well?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12r79j4", "is_robot_indexable": true, "report_reasons": null, "author": "ryan_s007", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12r79j4/applying_different_statistical_methods_to_certain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12r79j4/applying_different_statistical_methods_to_certain/", "subreddit_subscribers": 875958, "created_utc": 1681857610.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "From my understanding, ML algorithms can work best if seasonality or trends are removed to make the time series data stationary.\n\nIf I have multivariate time series data, I should ensure that its stationary before feeding the data into a model such as random forest. One question I have that is unanswered, when should I make the time series stationary? Before splitting into train and test? Additionally, if I wanted to resample hourly observations in my time series to get a daily average, would I perform the resampling after I split the data set into train and test, and after I make it stationary? \n\nI want to find what variables from my time series have the largest predictability or highest significance for predicting a value.", "author_fullname": "t2_atmge", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time Series Data Preparation - Random Forest Help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r773z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681857474.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;From my understanding, ML algorithms can work best if seasonality or trends are removed to make the time series data stationary.&lt;/p&gt;\n\n&lt;p&gt;If I have multivariate time series data, I should ensure that its stationary before feeding the data into a model such as random forest. One question I have that is unanswered, when should I make the time series stationary? Before splitting into train and test? Additionally, if I wanted to resample hourly observations in my time series to get a daily average, would I perform the resampling after I split the data set into train and test, and after I make it stationary? &lt;/p&gt;\n\n&lt;p&gt;I want to find what variables from my time series have the largest predictability or highest significance for predicting a value.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12r773z", "is_robot_indexable": true, "report_reasons": null, "author": "cdub4200", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12r773z/time_series_data_preparation_random_forest_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12r773z/time_series_data_preparation_random_forest_help/", "subreddit_subscribers": 875958, "created_utc": 1681857474.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "[Map of Data Science](https://preview.redd.it/x57wqnbj7rua1.jpg?width=3267&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3a300f096713fa263cc1c280b3b84ee2bccbdfed)\n\nHey Data Science People! I've been playing around with creating an educational \"*Map of Data Science*\" for a while now &amp; would love any + all feedback. Thanks!", "author_fullname": "t2_7ok5hnjp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83d\uddfa\ufe0f Map of Data Science [feedback appreciated]", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 108, "top_awarded_type": null, "hide_score": true, "media_metadata": {"x57wqnbj7rua1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 83, "x": 108, "u": "https://preview.redd.it/x57wqnbj7rua1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=333ecfd388b8182079b0a2a2ae0c9eb6378a0f15"}, {"y": 166, "x": 216, "u": "https://preview.redd.it/x57wqnbj7rua1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0388cd11adb8f1d8daf622b10091b040b04a1189"}, {"y": 247, "x": 320, "u": "https://preview.redd.it/x57wqnbj7rua1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2038bc2f22a48d28ad8a5d56c50d28a67ed76d6"}, {"y": 494, "x": 640, "u": "https://preview.redd.it/x57wqnbj7rua1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8fb33e337c61df35adc8965de8de66459fe6649"}, {"y": 741, "x": 960, "u": "https://preview.redd.it/x57wqnbj7rua1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3289db89dc002a8369ad224d10a9fadb091f26e7"}, {"y": 834, "x": 1080, "u": "https://preview.redd.it/x57wqnbj7rua1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a88e9f0f19b3a0f519006ea15531f3cf810e9c8c"}], "s": {"y": 2523, "x": 3267, "u": "https://preview.redd.it/x57wqnbj7rua1.jpg?width=3267&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=3a300f096713fa263cc1c280b3b84ee2bccbdfed"}, "id": "x57wqnbj7rua1"}}, "name": "t3_12rdz8i", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/V24P3_Hv0oUK8z2sk3HeX1BcB_FNbBtfM_Ley9xtPh0.jpg", "edited": 1681875208.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681872129.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/x57wqnbj7rua1.jpg?width=3267&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3a300f096713fa263cc1c280b3b84ee2bccbdfed\"&gt;Map of Data Science&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey Data Science People! I&amp;#39;ve been playing around with creating an educational &amp;quot;&lt;em&gt;Map of Data Science&lt;/em&gt;&amp;quot; for a while now &amp;amp; would love any + all feedback. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12rdz8i", "is_robot_indexable": true, "report_reasons": null, "author": "Britney-Ramona", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12rdz8i/map_of_data_science_feedback_appreciated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12rdz8i/map_of_data_science_feedback_appreciated/", "subreddit_subscribers": 875958, "created_utc": 1681872129.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am trying to build a classifier that will classify a movie into a main genre (About 8 different categories) based on two given features. Namely, subgenre and movie name. While working on this, I encountered a few challenges and would like some feedback and direction about how to approach this. First, is the issue regarding encoding the sub and main genres. I thought of using OneHotEncoder but I would have a really wide dataset. Given that I would have to also vectorize the movie names. A potential workaround would be to group some subgenres together and reduce the 100+ categories. Second, would LabelEncoding the main genre category be the correct approach? Additionally, Given that many movie names have almost very little meaningful information would it still be a good idea to use them regardless? For example, \"Finding Hank\" when preprocessed would just be \"Hank\".\n\nThe approach thus far in Python:\n\n1). Preprocessed movie names ( Removed stop words and Stemmed)\n\n2). Vectorized Movie names\n\n3). Encoded Sub genre with OneHotEncoder\n\n4). Encoded Main genre with LabelEncoder\n\n5). Tried to fix model = Disaster.\n\nWhat is your recommended approach?\n\nThanks.", "author_fullname": "t2_d57av2g4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Multiclass classification Beginner problem.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12rcp9f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681870509.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681869401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to build a classifier that will classify a movie into a main genre (About 8 different categories) based on two given features. Namely, subgenre and movie name. While working on this, I encountered a few challenges and would like some feedback and direction about how to approach this. First, is the issue regarding encoding the sub and main genres. I thought of using OneHotEncoder but I would have a really wide dataset. Given that I would have to also vectorize the movie names. A potential workaround would be to group some subgenres together and reduce the 100+ categories. Second, would LabelEncoding the main genre category be the correct approach? Additionally, Given that many movie names have almost very little meaningful information would it still be a good idea to use them regardless? For example, &amp;quot;Finding Hank&amp;quot; when preprocessed would just be &amp;quot;Hank&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;The approach thus far in Python:&lt;/p&gt;\n\n&lt;p&gt;1). Preprocessed movie names ( Removed stop words and Stemmed)&lt;/p&gt;\n\n&lt;p&gt;2). Vectorized Movie names&lt;/p&gt;\n\n&lt;p&gt;3). Encoded Sub genre with OneHotEncoder&lt;/p&gt;\n\n&lt;p&gt;4). Encoded Main genre with LabelEncoder&lt;/p&gt;\n\n&lt;p&gt;5). Tried to fix model = Disaster.&lt;/p&gt;\n\n&lt;p&gt;What is your recommended approach?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12rcp9f", "is_robot_indexable": true, "report_reasons": null, "author": "Gloomy_Ad_333", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12rcp9f/multiclass_classification_beginner_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12rcp9f/multiclass_classification_beginner_problem/", "subreddit_subscribers": 875958, "created_utc": 1681869401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm an experienced(4yrs) data scientist looking to transition  to machine learning engineering. I'm making top dollar in my role level and area. I had the chance to do some machine learning engineering projects in my previous company(series D) and I loved it. Especially learning how to optimize code to make it use lesser resources or to make it faster, cloud resources to use, choosing the right deployment tool, etc. I got laid off from that company and currently working in a new one. It's a startup with 2016 data infrastructure. It's a good opportunity to shake things up a bit here but management is tight on budget right now. I received an offer from a company with a mature data stack. However, my pay will drop by 18-35% depending on how negotiations go. I am aware that I am earning a lot more than average based on skill and level in my role and area. Considering that, if I had the average pay, I'd have a drop of 9%-18% in annual salary. Compared to my previous company annual salary it's a bump of 12%.\n\n I find the offer as a decent opportunity to start my machine learning engineer career since machine learning jobs in my area are scarce. The number of local tech companies doing it are less than 10. Others are already offshore consultancy based. However, I don't know if switching jobs is going to be worth it. I specialize in developing NLP and information retrieval systems. The new job will be a mix of fraud and machine learning engineering. I've had 1.5 years of experience on SWE and SysAd before I went to DS.\n\nCareer-wise, I'm hoping to increase my chances to be hired abroad, and move to another country in the future(i.e EU or Australia/NZ).\n\nWhat do you think of moving from data science to machine learning engineering? Is there a future or is it better to stay in my DS role?", "author_fullname": "t2_896476q8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching jobs from data science to machine learning engineering?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rc3vc", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681868046.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m an experienced(4yrs) data scientist looking to transition  to machine learning engineering. I&amp;#39;m making top dollar in my role level and area. I had the chance to do some machine learning engineering projects in my previous company(series D) and I loved it. Especially learning how to optimize code to make it use lesser resources or to make it faster, cloud resources to use, choosing the right deployment tool, etc. I got laid off from that company and currently working in a new one. It&amp;#39;s a startup with 2016 data infrastructure. It&amp;#39;s a good opportunity to shake things up a bit here but management is tight on budget right now. I received an offer from a company with a mature data stack. However, my pay will drop by 18-35% depending on how negotiations go. I am aware that I am earning a lot more than average based on skill and level in my role and area. Considering that, if I had the average pay, I&amp;#39;d have a drop of 9%-18% in annual salary. Compared to my previous company annual salary it&amp;#39;s a bump of 12%.&lt;/p&gt;\n\n&lt;p&gt;I find the offer as a decent opportunity to start my machine learning engineer career since machine learning jobs in my area are scarce. The number of local tech companies doing it are less than 10. Others are already offshore consultancy based. However, I don&amp;#39;t know if switching jobs is going to be worth it. I specialize in developing NLP and information retrieval systems. The new job will be a mix of fraud and machine learning engineering. I&amp;#39;ve had 1.5 years of experience on SWE and SysAd before I went to DS.&lt;/p&gt;\n\n&lt;p&gt;Career-wise, I&amp;#39;m hoping to increase my chances to be hired abroad, and move to another country in the future(i.e EU or Australia/NZ).&lt;/p&gt;\n\n&lt;p&gt;What do you think of moving from data science to machine learning engineering? Is there a future or is it better to stay in my DS role?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12rc3vc", "is_robot_indexable": true, "report_reasons": null, "author": "ramenmaster2021", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12rc3vc/switching_jobs_from_data_science_to_machine/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12rc3vc/switching_jobs_from_data_science_to_machine/", "subreddit_subscribers": 875958, "created_utc": 1681868046.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a classification problem with a tabular dataset consisting of 5,000 rows and 1,024 unnamed features. These features are extracted from images, but I don't have access to the images themselves. I'm concerned that with such a large number of features, my model may easily overfit the data. I tried using PCA, but it didn't work well. I'm considering using denoising autoencoders, but I'm not sure if they will be helpful. I've heard that neural networks don't work well with tabular data, especially with a limited number of samples. Any insights or suggestions would be greatly appreciated.", "author_fullname": "t2_7uwwf65z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "high-dimensional tabular dataset extracted from images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12re1df", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681872268.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a classification problem with a tabular dataset consisting of 5,000 rows and 1,024 unnamed features. These features are extracted from images, but I don&amp;#39;t have access to the images themselves. I&amp;#39;m concerned that with such a large number of features, my model may easily overfit the data. I tried using PCA, but it didn&amp;#39;t work well. I&amp;#39;m considering using denoising autoencoders, but I&amp;#39;m not sure if they will be helpful. I&amp;#39;ve heard that neural networks don&amp;#39;t work well with tabular data, especially with a limited number of samples. Any insights or suggestions would be greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12re1df", "is_robot_indexable": true, "report_reasons": null, "author": "Hamdi_bks", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12re1df/highdimensional_tabular_dataset_extracted_from/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12re1df/highdimensional_tabular_dataset_extracted_from/", "subreddit_subscribers": 875958, "created_utc": 1681872268.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Mainly info like latitude, longitude Or address", "author_fullname": "t2_gosxm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are the best ways to deal with PII data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rc0l7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681867847.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mainly info like latitude, longitude Or address&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12rc0l7", "is_robot_indexable": true, "report_reasons": null, "author": "ggopinathan1", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12rc0l7/what_are_the_best_ways_to_deal_with_pii_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12rc0l7/what_are_the_best_ways_to_deal_with_pii_data/", "subreddit_subscribers": 875958, "created_utc": 1681867847.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "How are customers with good credit scores and lenders who offer credit to those with lower scores. Let us say the lender uses a combination of risk based pricing and the use of different data in it\u2019s credit scoring model.", "author_fullname": "t2_jm8x8jsx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Credit Risk Analysis Question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ra25m", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681863561.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How are customers with good credit scores and lenders who offer credit to those with lower scores. Let us say the lender uses a combination of risk based pricing and the use of different data in it\u2019s credit scoring model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ra25m", "is_robot_indexable": true, "report_reasons": null, "author": "sydneysweeney69", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ra25m/credit_risk_analysis_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12ra25m/credit_risk_analysis_question/", "subreddit_subscribers": 875958, "created_utc": 1681863561.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m a masters student I just thought this would be a cool fun side project, and help the community at the same time. \n\nThe council have just put up bollards cutting the town I live in into two halves (increasing drive time in traffic from one half to the other from ~5 to ~30 minutes. \n\nDriving away people from using the towns high street and increasing emissions on school drops offs etc since you know have to drive around the town \n\nI wanted to make a heatmap showing the increase in drive time between a POI (the high street/ the school) and the surrounding area before and after the bollards. \n\nI looked in to using Google\u2019s distance matrix API but I can\u2019t get previous data from before the bollards were put up? \n\nAny thoughts or ways I can better show this increase in non essential travel.\n\n\n\n(I fully understand the scheme what they are trying to do to reduce emissions and grow cyclists/walkers but they haven\u2019t done it efficiently at all I want to show this. )\n\nAny helps much appreciated xD\n\nPreferably in R but I\u2019m comfortable in Python and visualisation systems like power BI", "author_fullname": "t2_9woatez4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drive time heatmap", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qwiea", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681837729.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681837510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a masters student I just thought this would be a cool fun side project, and help the community at the same time. &lt;/p&gt;\n\n&lt;p&gt;The council have just put up bollards cutting the town I live in into two halves (increasing drive time in traffic from one half to the other from ~5 to ~30 minutes. &lt;/p&gt;\n\n&lt;p&gt;Driving away people from using the towns high street and increasing emissions on school drops offs etc since you know have to drive around the town &lt;/p&gt;\n\n&lt;p&gt;I wanted to make a heatmap showing the increase in drive time between a POI (the high street/ the school) and the surrounding area before and after the bollards. &lt;/p&gt;\n\n&lt;p&gt;I looked in to using Google\u2019s distance matrix API but I can\u2019t get previous data from before the bollards were put up? &lt;/p&gt;\n\n&lt;p&gt;Any thoughts or ways I can better show this increase in non essential travel.&lt;/p&gt;\n\n&lt;p&gt;(I fully understand the scheme what they are trying to do to reduce emissions and grow cyclists/walkers but they haven\u2019t done it efficiently at all I want to show this. )&lt;/p&gt;\n\n&lt;p&gt;Any helps much appreciated xD&lt;/p&gt;\n\n&lt;p&gt;Preferably in R but I\u2019m comfortable in Python and visualisation systems like power BI&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qwiea", "is_robot_indexable": true, "report_reasons": null, "author": "Ok_Order6450", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qwiea/drive_time_heatmap/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qwiea/drive_time_heatmap/", "subreddit_subscribers": 875958, "created_utc": 1681837510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I\u2019m trying to back track from survival function down to hazard function which upon dividing by the partial hazard should give the same baseline hazard for any 2 example subjects. Unfortunately predict_hazard function in CoxPHFitter is throwing an error (doesn\u2019t have the attribute apparently, which is v weird). And if I manually get to a hazard function from the cumulative hazard function (not super sure of the calculation), the step after dividing by partial hazard is not the same i.e., 2 different baseline hazards. Any help appreciated.", "author_fullname": "t2_u3vhf5lz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Facing inconsistencies while trying to reconstruct Cox PH predictions by lifelines package", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qltap", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681821775.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m trying to back track from survival function down to hazard function which upon dividing by the partial hazard should give the same baseline hazard for any 2 example subjects. Unfortunately predict_hazard function in CoxPHFitter is throwing an error (doesn\u2019t have the attribute apparently, which is v weird). And if I manually get to a hazard function from the cumulative hazard function (not super sure of the calculation), the step after dividing by partial hazard is not the same i.e., 2 different baseline hazards. Any help appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qltap", "is_robot_indexable": true, "report_reasons": null, "author": "thesaintyouneed", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qltap/facing_inconsistencies_while_trying_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qltap/facing_inconsistencies_while_trying_to/", "subreddit_subscribers": 875958, "created_utc": 1681821775.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello! I am currently writing a series of articles about the pre-processing steps for time series data.  \n\nIn the first article, I suggest the following order: \n\n1. Handle missing values \n2. Remove trend \n3. Remove seasonality \n4. Check for stationarity and make it stationary if necessary \n5. Normalize the data \n6. Remove outliers \n7. Smooth the data \n\nHowever, I know this order is not universal and can be changed depending on our data. Also, not all the steps are always required.  \n\nMy question is, which would be the \"standard\" order that you would suggest?  \n\nI leave the first part of these articles [here](https://mlpills.dev/time-series/clean-your-time-series-data-i/) and the second one [here](https://mlpills.dev/time-series/clean-your-time-series-data-ii/). The last two parts are written but not published yet :( \n\nI'd love to hear some feedback. :)\n\n Thanks!", "author_fullname": "t2_gvvf9r1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pre-processing order for time series data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qi9b4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681812524.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I am currently writing a series of articles about the pre-processing steps for time series data.  &lt;/p&gt;\n\n&lt;p&gt;In the first article, I suggest the following order: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Handle missing values &lt;/li&gt;\n&lt;li&gt;Remove trend &lt;/li&gt;\n&lt;li&gt;Remove seasonality &lt;/li&gt;\n&lt;li&gt;Check for stationarity and make it stationary if necessary &lt;/li&gt;\n&lt;li&gt;Normalize the data &lt;/li&gt;\n&lt;li&gt;Remove outliers &lt;/li&gt;\n&lt;li&gt;Smooth the data &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;However, I know this order is not universal and can be changed depending on our data. Also, not all the steps are always required.  &lt;/p&gt;\n\n&lt;p&gt;My question is, which would be the &amp;quot;standard&amp;quot; order that you would suggest?  &lt;/p&gt;\n\n&lt;p&gt;I leave the first part of these articles &lt;a href=\"https://mlpills.dev/time-series/clean-your-time-series-data-i/\"&gt;here&lt;/a&gt; and the second one &lt;a href=\"https://mlpills.dev/time-series/clean-your-time-series-data-ii/\"&gt;here&lt;/a&gt;. The last two parts are written but not published yet :( &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to hear some feedback. :)&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cwrfoLVTPp530kQiNj_W-0Uhsl2AIrHhEZpn7VP7An8.jpg?auto=webp&amp;v=enabled&amp;s=2e987cc53a9606585b9baea3b9c3056362d77e3f", "width": 2560, "height": 1440}, "resolutions": [{"url": "https://external-preview.redd.it/cwrfoLVTPp530kQiNj_W-0Uhsl2AIrHhEZpn7VP7An8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b73276f3c7bed3898752484fca308aad5f66cc0", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/cwrfoLVTPp530kQiNj_W-0Uhsl2AIrHhEZpn7VP7An8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e2fa795d34a622e420f1f36e8388f25a2ac8789", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/cwrfoLVTPp530kQiNj_W-0Uhsl2AIrHhEZpn7VP7An8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=358d8be8f2dd2edc85977a8aa927b07835df2b53", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/cwrfoLVTPp530kQiNj_W-0Uhsl2AIrHhEZpn7VP7An8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b01d8de571f37cfff87ee9b58097cff988258d1", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/cwrfoLVTPp530kQiNj_W-0Uhsl2AIrHhEZpn7VP7An8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0d9065265650f0e4234b339bc7f47411a7c790b", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/cwrfoLVTPp530kQiNj_W-0Uhsl2AIrHhEZpn7VP7An8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8de1b636fa63a6391629a2acd3898bee1f5f405", "width": 1080, "height": 607}], "variants": {}, "id": "z2mUwpw_935DsBDU4V1KJzNU_rF0PEhxMVMa8oVerhk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qi9b4", "is_robot_indexable": true, "report_reasons": null, "author": "daansan-ml", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qi9b4/preprocessing_order_for_time_series_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qi9b4/preprocessing_order_for_time_series_data/", "subreddit_subscribers": 875958, "created_utc": 1681812524.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hey r/datascience. \n\nI come from a business background and over the years have slowly made my way to data analysis and now moving to data science. However, one thing I noticed from other professionals that come from a computer science background is their more intricate knowledge of the tools and their interactions. Mid 2000's. after getting tired of Excel + VBA I started working with SQL. When programming languages became a necessity for my work, I learned Python (also some JavaScript, but that's not here nor there). after getting to the point I needed more number crunching and statistical approaches, I learned R. I don't consider myself an expert, but I at least understand the tools.... separately, at least. Now that I'm formally doing my masters, more than once I have been jumping from software to software, not to mention when doing collaborations (all my work so far was done by me and for me, only the insights and reports needed to be shared, so no one else ever worked on my codes), I have no idea on how to setup everything.   \n\nSo, I came here to ask: What is a good workflow and best practices to start with? Currently, most of my work is done either on PyCharm (for more complex coding), Jupiter (for EDA) and RStudio (for R). Regarding git and GitHub, I only ever used it through RStudio (other than the bash shell, of course). However, is there go-to approach where I can use the same tool for everything (or for most part of it)? I know how to do what I need, just not an efficient way to do it specially if I have to work with others.", "author_fullname": "t2_fna4qb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with workflow and best practices", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12q9wxe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681791112.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/datascience\"&gt;r/datascience&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;I come from a business background and over the years have slowly made my way to data analysis and now moving to data science. However, one thing I noticed from other professionals that come from a computer science background is their more intricate knowledge of the tools and their interactions. Mid 2000&amp;#39;s. after getting tired of Excel + VBA I started working with SQL. When programming languages became a necessity for my work, I learned Python (also some JavaScript, but that&amp;#39;s not here nor there). after getting to the point I needed more number crunching and statistical approaches, I learned R. I don&amp;#39;t consider myself an expert, but I at least understand the tools.... separately, at least. Now that I&amp;#39;m formally doing my masters, more than once I have been jumping from software to software, not to mention when doing collaborations (all my work so far was done by me and for me, only the insights and reports needed to be shared, so no one else ever worked on my codes), I have no idea on how to setup everything.   &lt;/p&gt;\n\n&lt;p&gt;So, I came here to ask: What is a good workflow and best practices to start with? Currently, most of my work is done either on PyCharm (for more complex coding), Jupiter (for EDA) and RStudio (for R). Regarding git and GitHub, I only ever used it through RStudio (other than the bash shell, of course). However, is there go-to approach where I can use the same tool for everything (or for most part of it)? I know how to do what I need, just not an efficient way to do it specially if I have to work with others.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12q9wxe", "is_robot_indexable": true, "report_reasons": null, "author": "rpcsanches", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12q9wxe/help_with_workflow_and_best_practices/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12q9wxe/help_with_workflow_and_best_practices/", "subreddit_subscribers": 875958, "created_utc": 1681791112.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "ClickHouse is one of the fastest SQL engines in the world. But installing and maintaining a Clickhouse server is not easy.\n\nNow, you can run complex SQL blazing fast with chDB which is an embedded Clickhouse engine in Python.\n\n# features\n\n* In-process SQL OLAP Engine, powered by ClickHouse\n* No need to install ClickHouse\n* Minimized data copy from C++ to Python with [python memoryview](https://docs.python.org/3/c-api/memoryview.html)\n* Input&amp;Output support Parquet, CSV, JSON, Arrow, ORC and [more](https://clickhouse.com/docs/en/interfaces/formats)\n\n# install\n```bash\npip install chdb\n```\n\n# examples\nRun any SQL in just one line\n```python\nimport chdb\nres = chdb.query('select * from file(\"data.parquet\", Parquet)', 'Dataframe')\nprint res\n```\n\n- PyPi: https://pypi.org/project/chdb/\n- GitHub: https://github.com/auxten/chdb\n\n&gt; Due to the size of the engine, the first SQL could be slow.", "author_fullname": "t2_5x6me", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Run SQL blazing fast with the embedded Clickhouse engine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qopcr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681827774.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ClickHouse is one of the fastest SQL engines in the world. But installing and maintaining a Clickhouse server is not easy.&lt;/p&gt;\n\n&lt;p&gt;Now, you can run complex SQL blazing fast with chDB which is an embedded Clickhouse engine in Python.&lt;/p&gt;\n\n&lt;h1&gt;features&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;In-process SQL OLAP Engine, powered by ClickHouse&lt;/li&gt;\n&lt;li&gt;No need to install ClickHouse&lt;/li&gt;\n&lt;li&gt;Minimized data copy from C++ to Python with &lt;a href=\"https://docs.python.org/3/c-api/memoryview.html\"&gt;python memoryview&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Input&amp;amp;Output support Parquet, CSV, JSON, Arrow, ORC and &lt;a href=\"https://clickhouse.com/docs/en/interfaces/formats\"&gt;more&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;install&lt;/h1&gt;\n\n&lt;p&gt;&lt;code&gt;bash\npip install chdb\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;h1&gt;examples&lt;/h1&gt;\n\n&lt;p&gt;Run any SQL in just one line\n&lt;code&gt;python\nimport chdb\nres = chdb.query(&amp;#39;select * from file(&amp;quot;data.parquet&amp;quot;, Parquet)&amp;#39;, &amp;#39;Dataframe&amp;#39;)\nprint res\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;PyPi: &lt;a href=\"https://pypi.org/project/chdb/\"&gt;https://pypi.org/project/chdb/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;GitHub: &lt;a href=\"https://github.com/auxten/chdb\"&gt;https://github.com/auxten/chdb&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Due to the size of the engine, the first SQL could be slow.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uQnwG5vj0W2B8UmXvnpbBF8bitWWN9ApOpjGDNFekLw.jpg?auto=webp&amp;v=enabled&amp;s=8cffb70177a255a5d273f49b07a192aae79dd5d1", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/uQnwG5vj0W2B8UmXvnpbBF8bitWWN9ApOpjGDNFekLw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=00b81de315991e3643052ea46585a6ef9bd8c2d5", "width": 108, "height": 108}], "variants": {}, "id": "qsyApzksUdlxqY7z2ubJCNmhQilhXIOl25dSHcQQLB4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qopcr", "is_robot_indexable": true, "report_reasons": null, "author": "auxten", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qopcr/run_sql_blazing_fast_with_the_embedded_clickhouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qopcr/run_sql_blazing_fast_with_the_embedded_clickhouse/", "subreddit_subscribers": 875958, "created_utc": 1681827774.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have an HR screening interview for a data analyst role and wonder what questions will be asked. I know some common questions like tell me about yourself, what are you interested in this role? Why this company?Biggest accomplishment? Do you have any thoughts on other types of questions?", "author_fullname": "t2_8lzodaki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "HR Screening Interview for half an hour", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qevdw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681803127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an HR screening interview for a data analyst role and wonder what questions will be asked. I know some common questions like tell me about yourself, what are you interested in this role? Why this company?Biggest accomplishment? Do you have any thoughts on other types of questions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qevdw", "is_robot_indexable": true, "report_reasons": null, "author": "Hidimbaa", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qevdw/hr_screening_interview_for_half_an_hour/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qevdw/hr_screening_interview_for_half_an_hour/", "subreddit_subscribers": 875958, "created_utc": 1681803127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "A data set including 700,000 pictures of industrial items with defects is given to you. The only thing you are aware of is that every image contains at least one obvious flaw. But you don't know where the flaw is or what kind of problem it is. Build a classifier that can distinguish between pieces that are OK and those that are defective. What would be your strategy?\n\n**Edit** : \nWhat would you say - if I would try to use an algorithm to try to match each image with one another, and transform an image via homography from one to another and try to find the difference between the images. Build an image which has all common features and then augment 80% of the images to remove all defects, but still have all variations of image perspectives. Would this be possible?", "author_fullname": "t2_30m20xwn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Datascience Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qij3w", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.31, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681820860.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681813262.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A data set including 700,000 pictures of industrial items with defects is given to you. The only thing you are aware of is that every image contains at least one obvious flaw. But you don&amp;#39;t know where the flaw is or what kind of problem it is. Build a classifier that can distinguish between pieces that are OK and those that are defective. What would be your strategy?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt; : \nWhat would you say - if I would try to use an algorithm to try to match each image with one another, and transform an image via homography from one to another and try to find the difference between the images. Build an image which has all common features and then augment 80% of the images to remove all defects, but still have all variations of image perspectives. Would this be possible?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qij3w", "is_robot_indexable": true, "report_reasons": null, "author": "_synaps_", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12qij3w/datascience_challenge/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12qij3w/datascience_challenge/", "subreddit_subscribers": 875958, "created_utc": 1681813262.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}