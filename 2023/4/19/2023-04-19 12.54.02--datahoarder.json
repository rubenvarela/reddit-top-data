{"kind": "Listing", "data": {"after": "t3_12rnh2p", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_v2a9t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The Internet Archive's troubles are bad news for book lovers - by Robin Ashenden", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 103, "top_awarded_type": null, "hide_score": false, "name": "t3_12r6b6z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 590, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 590, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5X5HbwXSFdn7XcsN1yTtFkSMfQNmunJEsqISvT7l8DM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681855692.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "spectator.co.uk", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.spectator.co.uk/article/the-internet-archives-troubles-are-bad-news-for-book-lovers/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/teLWmGr8uYeJBg4KP63ohMnZ60UIGg4WvWQufTV2fIg.jpg?auto=webp&amp;v=enabled&amp;s=d350acbb74f74598e6941b84982fbfa0f67e1a7a", "width": 730, "height": 539}, "resolutions": [{"url": "https://external-preview.redd.it/teLWmGr8uYeJBg4KP63ohMnZ60UIGg4WvWQufTV2fIg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a4f069f99f16dc0646446c2de734f022316eb9d2", "width": 108, "height": 79}, {"url": "https://external-preview.redd.it/teLWmGr8uYeJBg4KP63ohMnZ60UIGg4WvWQufTV2fIg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab6e8357f6944d38897b60d793c63e87acb94f7d", "width": 216, "height": 159}, {"url": "https://external-preview.redd.it/teLWmGr8uYeJBg4KP63ohMnZ60UIGg4WvWQufTV2fIg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d885a62dad3ec78af0be08f8858a6c27dc4ac060", "width": 320, "height": 236}, {"url": "https://external-preview.redd.it/teLWmGr8uYeJBg4KP63ohMnZ60UIGg4WvWQufTV2fIg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=43e12e839e9d77b8b21b7282a6e5cb04910ba9d9", "width": 640, "height": 472}], "variants": {}, "id": "YchY_hbqbq_QUNUdCea5wV5_WuEI8QEBQcvScPqEruw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12r6b6z", "is_robot_indexable": true, "report_reasons": null, "author": "Crazy-Red-Fox", "discussion_type": null, "num_comments": 46, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12r6b6z/the_internet_archives_troubles_are_bad_news_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.spectator.co.uk/article/the-internet-archives-troubles-are-bad-news-for-book-lovers/", "subreddit_subscribers": 678300, "created_utc": 1681855692.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I thought I'd share this 'little' project of mine I've been working on since I got my Steam Deck last year.\n\nThe Steam Deck, being really nothing but a handheld PC, is super flexible in what you can do.  While most gamers focused on adding MicroSD storage to hold games, I went a different way to exclusively use the MicroSD card for media storage.  Kodi in a FlatPak is installed and it uses the MicroSD card for media files.  The Steam Deck features hardware accelerated AV1 decoding so it's even pretty future proof for media playback.\n\nThe 'big' thing here however has been to make good use of the MicroSD card; To aggressively compress media files to 720p at 'passable low bitrates'.  Initially at HEVC and later in AV1 once HandBrake had AV1 encoding implimented.  I mostly use Handbrake in a docker on my UnRAID servers to used unused CPU resources to do the encoding.  The end result after a year of encoding is about a large, highly portable hoard of media files, TV, cartoons, anime and movies.\n\n350GB... But also 2173 hours of run time.  90 ***days*** of media on a 512gb MicroSD card the size if your pinky nail.  It also took a year to encode this much.  This is not peak quality media but it's 'perfectly watchable' especially on a 7\" screen.  Honestly, also pretty much fine on a TV from across the room too.  Only looks 'bad' when watched on a PC monitor 45cm from my face.\n\nBefore someone brings up 'Plex' and 'Steaming' and all that, the idea is to keep the Steam Deck as an offline media device.  Kodi, the files, the metadata library are all stored locally in the device.  I can be trapped in an airport at Christmas with a blizzard outside and no functional wifi but it still works.  It'll work in an airplane at 30 0000 feet.  It'll work on a train or car crossing the country far from the nearest cell tower.  You follow?  It 'just works' with no external dependencies other than an outlet to recharge it from.  The Steam Deck can also be docked to a TV in a hotel with other peripherals attached, effectively converting it from 'Handheld PC' to 'MiniPC' for gaming and media in that scenario.  A 'Travel Entertainment Survival Kit' if you will.\n\nBefore someone suggests hardware encoding with the likes of NVENC?  No, hardware encoding is crazy fast, but it's inefficient for storage.  In terms of 'Quality Per Megabyte', hardware encoding is trash, it just makes up for it by being lightning fast.  Software encoding offers far better compression ratios even if it takes far more time and energy to get the job done.\n\nWhy stop at 350GB?  ...Well, it's spring and it's getting warmer.  In the Winter having two Xeon E5-2697v2's cooking away is just another source of heat to keep my home warm.  In the summer it's excess heat for the air conditioning to deal with.  So the encoding is parked until October or so.  But I have 3 months of media on a Steam Deck so I think I'm set for my flight to LTX this summer. :)", "author_fullname": "t2_76pgn19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A Large, Tiny, Portable Hoard", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qpgvv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 158, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 158, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681829198.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I thought I&amp;#39;d share this &amp;#39;little&amp;#39; project of mine I&amp;#39;ve been working on since I got my Steam Deck last year.&lt;/p&gt;\n\n&lt;p&gt;The Steam Deck, being really nothing but a handheld PC, is super flexible in what you can do.  While most gamers focused on adding MicroSD storage to hold games, I went a different way to exclusively use the MicroSD card for media storage.  Kodi in a FlatPak is installed and it uses the MicroSD card for media files.  The Steam Deck features hardware accelerated AV1 decoding so it&amp;#39;s even pretty future proof for media playback.&lt;/p&gt;\n\n&lt;p&gt;The &amp;#39;big&amp;#39; thing here however has been to make good use of the MicroSD card; To aggressively compress media files to 720p at &amp;#39;passable low bitrates&amp;#39;.  Initially at HEVC and later in AV1 once HandBrake had AV1 encoding implimented.  I mostly use Handbrake in a docker on my UnRAID servers to used unused CPU resources to do the encoding.  The end result after a year of encoding is about a large, highly portable hoard of media files, TV, cartoons, anime and movies.&lt;/p&gt;\n\n&lt;p&gt;350GB... But also 2173 hours of run time.  90 &lt;strong&gt;&lt;em&gt;days&lt;/em&gt;&lt;/strong&gt; of media on a 512gb MicroSD card the size if your pinky nail.  It also took a year to encode this much.  This is not peak quality media but it&amp;#39;s &amp;#39;perfectly watchable&amp;#39; especially on a 7&amp;quot; screen.  Honestly, also pretty much fine on a TV from across the room too.  Only looks &amp;#39;bad&amp;#39; when watched on a PC monitor 45cm from my face.&lt;/p&gt;\n\n&lt;p&gt;Before someone brings up &amp;#39;Plex&amp;#39; and &amp;#39;Steaming&amp;#39; and all that, the idea is to keep the Steam Deck as an offline media device.  Kodi, the files, the metadata library are all stored locally in the device.  I can be trapped in an airport at Christmas with a blizzard outside and no functional wifi but it still works.  It&amp;#39;ll work in an airplane at 30 0000 feet.  It&amp;#39;ll work on a train or car crossing the country far from the nearest cell tower.  You follow?  It &amp;#39;just works&amp;#39; with no external dependencies other than an outlet to recharge it from.  The Steam Deck can also be docked to a TV in a hotel with other peripherals attached, effectively converting it from &amp;#39;Handheld PC&amp;#39; to &amp;#39;MiniPC&amp;#39; for gaming and media in that scenario.  A &amp;#39;Travel Entertainment Survival Kit&amp;#39; if you will.&lt;/p&gt;\n\n&lt;p&gt;Before someone suggests hardware encoding with the likes of NVENC?  No, hardware encoding is crazy fast, but it&amp;#39;s inefficient for storage.  In terms of &amp;#39;Quality Per Megabyte&amp;#39;, hardware encoding is trash, it just makes up for it by being lightning fast.  Software encoding offers far better compression ratios even if it takes far more time and energy to get the job done.&lt;/p&gt;\n\n&lt;p&gt;Why stop at 350GB?  ...Well, it&amp;#39;s spring and it&amp;#39;s getting warmer.  In the Winter having two Xeon E5-2697v2&amp;#39;s cooking away is just another source of heat to keep my home warm.  In the summer it&amp;#39;s excess heat for the air conditioning to deal with.  So the encoding is parked until October or so.  But I have 3 months of media on a Steam Deck so I think I&amp;#39;m set for my flight to LTX this summer. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12qpgvv", "is_robot_indexable": true, "report_reasons": null, "author": "AshleyUncia", "discussion_type": null, "num_comments": 29, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12qpgvv/a_large_tiny_portable_hoard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12qpgvv/a_large_tiny_portable_hoard/", "subreddit_subscribers": 678300, "created_utc": 1681829198.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "With Reddit's new \"[Update Regarding Reddit\u2019s API](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/)\", removed content databases like [pushshift](https://www.reddit.com/r/pushshift/) will no longer be able to scrape Reddit. I feel that this is a lead up into removing all third party apps like Apollo and RIF. This is unacceptable to me.\n\n&amp;#x200B;\n\n[This guy](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/) already downloaded **\\~ 1.7 billion comments @ 250 GB compressed** (and then founded pushshift) so, I think it would be reasonable to download all post data and comments from non NSFW Subreddits, and store it in a few terabytes, right?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nAnd Ideas? What is the best strategy for downloading the entirety of Reddit, and then using it offline?\n\n&amp;#x200B;\n\nedit 1: wrote my first python downloading script with praw, it's kinda cool\n\n&amp;#x200B;\n\nedit 2: [paid API is confirmed](https://reddit.com/r/apolloapp/comments/12ram0f/had_a_few_calls_with_reddit_today_about_the/). Fuck. I bet their also going to remove old.reddit, fuck them.\n\n&amp;#x200B;\n\nedit 3: torrent magnet with 2tb of reddit data, mostly 100% of text posts/comments (base64 bWFnbmV0Oj94dD11cm46YnRpaDo3YzA2NDVjOTQzMjEzMTFiYjA1YmQ4NzlkZGVlNGQwZWJhMDhhYWVlJnRyPWh0dHBzJTNBJTJGJTJGYWNhZGVtaWN0b3JyZW50cy5jb20lMkZhbm5vdW5jZS5waHAmdHI9dWRwJTNBJTJGJTJGdHJhY2tlci5jb3BwZXJzdXJmZXIudGslM0E2OTY5JnRyPXVkcCUzQSUyRiUyRnRyYWNrZXIub3BlbnRyYWNrci5vcmclM0ExMzM3JTJGYW5ub3VuY2U= )", "author_fullname": "t2_cbhs4xvy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'll fucking download the entirety of Reddit before I use the official first party app. What's the best way?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rb7n8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 59, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 59, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681889203.0, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681866084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With Reddit&amp;#39;s new &amp;quot;&lt;a href=\"https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/\"&gt;Update Regarding Reddit\u2019s API&lt;/a&gt;&amp;quot;, removed content databases like &lt;a href=\"https://www.reddit.com/r/pushshift/\"&gt;pushshift&lt;/a&gt; will no longer be able to scrape Reddit. I feel that this is a lead up into removing all third party apps like Apollo and RIF. This is unacceptable to me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/\"&gt;This guy&lt;/a&gt; already downloaded &lt;strong&gt;~ 1.7 billion comments @ 250 GB compressed&lt;/strong&gt; (and then founded pushshift) so, I think it would be reasonable to download all post data and comments from non NSFW Subreddits, and store it in a few terabytes, right?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;And Ideas? What is the best strategy for downloading the entirety of Reddit, and then using it offline?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;edit 1: wrote my first python downloading script with praw, it&amp;#39;s kinda cool&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;edit 2: &lt;a href=\"https://reddit.com/r/apolloapp/comments/12ram0f/had_a_few_calls_with_reddit_today_about_the/\"&gt;paid API is confirmed&lt;/a&gt;. Fuck. I bet their also going to remove old.reddit, fuck them.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;edit 3: torrent magnet with 2tb of reddit data, mostly 100% of text posts/comments (base64 bWFnbmV0Oj94dD11cm46YnRpaDo3YzA2NDVjOTQzMjEzMTFiYjA1YmQ4NzlkZGVlNGQwZWJhMDhhYWVlJnRyPWh0dHBzJTNBJTJGJTJGYWNhZGVtaWN0b3JyZW50cy5jb20lMkZhbm5vdW5jZS5waHAmdHI9dWRwJTNBJTJGJTJGdHJhY2tlci5jb3BwZXJzdXJmZXIudGslM0E2OTY5JnRyPXVkcCUzQSUyRiUyRnRyYWNrZXIub3BlbnRyYWNrci5vcmclM0ExMzM3JTJGYW5ub3VuY2U= )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "RIP enterprisegoogledriveunlimited", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12rb7n8", "is_robot_indexable": true, "report_reasons": null, "author": "GoryRamsy", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12rb7n8/ill_fucking_download_the_entirety_of_reddit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12rb7n8/ill_fucking_download_the_entirety_of_reddit/", "subreddit_subscribers": 678300, "created_utc": 1681866084.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "[Backside of case with all 8 drives pre-wired](https://preview.redd.it/cmccamy7loua1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c02dcc1a0ee61f13a9e2f3e91a9fa9da724cea5e)\n\nI previously posted this project several months ago, but it was incomplete, as it only supported the top 5 drives, but I recently took the time to model the 3 drive version. It uses short sata extensions that are available on amazon. Inventor part files are also posted for anyone wishing to modify or otherwise adjust the fit. I'm still working on a version of my mounting ears that support WD drives that don't have the middle holes, but using the rubber grommets for some added compliance. the existing mounting ears I have made clip into the sled and rigidly screw to the drive.\n\nIt is available here: [https://www.printables.com/model/235313-fractal-define-r5-sata-backplane](https://www.printables.com/model/235313-fractal-define-r5-sata-backplane)\n\nI also finally got myself an LSI HBA, which contributes to the extremely satisfying wiring.\n\n&amp;#x200B;\n\n[only 3 drives installed so far](https://preview.redd.it/mbd0w2mfloua1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e33e8353d03203af34de00ff5e121cc60edc1ed3)", "author_fullname": "t2_cuo9p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Updated 3d printed Fractal Define R5 backplane - all 8 drives supported now.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"cmccamy7loua1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/cmccamy7loua1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af0d883b0066872c3b5e30367c4a7817a9f9200f"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/cmccamy7loua1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7c1e1ddb655c0209fb458f232bf63a4f8cf24bcf"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/cmccamy7loua1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ec16885f280117e81d319565fc0d3d440f0fc9b"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/cmccamy7loua1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9bb896bfb885f44a42d6f1e87b1f8f93214f5f9a"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/cmccamy7loua1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ebb0f35df6dc3c72258d70bc899e17db2437a26"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/cmccamy7loua1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1ad678de3b6459995a65be4d479e40801443ce5c"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/cmccamy7loua1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c02dcc1a0ee61f13a9e2f3e91a9fa9da724cea5e"}, "id": "cmccamy7loua1"}, "mbd0w2mfloua1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/mbd0w2mfloua1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d6e408674657e520c3e6727d34c119315a6300b"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/mbd0w2mfloua1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50a23a03d6f54dedf7cf72ac1b915e19b727f484"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/mbd0w2mfloua1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=868de09380f5d5db1b098e76b4dc963b418b1e4d"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/mbd0w2mfloua1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66e839ac6da14905644cf5eceee2edeecb4fe75c"}, {"y": 720, "x": 960, "u": "https://preview.redd.it/mbd0w2mfloua1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12565050de07c29b61ece48a096a70b7f4908cb1"}, {"y": 810, "x": 1080, "u": "https://preview.redd.it/mbd0w2mfloua1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1a5f472e6297c672611f7994cb01501bfb0345f"}], "s": {"y": 3024, "x": 4032, "u": "https://preview.redd.it/mbd0w2mfloua1.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e33e8353d03203af34de00ff5e121cc60edc1ed3"}, "id": "mbd0w2mfloua1"}}, "name": "t3_12qyb1c", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/ZJZK8iYegnMEBai5kdeOcneItaotC0yIKICaNL2Tl2M.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681840682.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/cmccamy7loua1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=c02dcc1a0ee61f13a9e2f3e91a9fa9da724cea5e\"&gt;Backside of case with all 8 drives pre-wired&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I previously posted this project several months ago, but it was incomplete, as it only supported the top 5 drives, but I recently took the time to model the 3 drive version. It uses short sata extensions that are available on amazon. Inventor part files are also posted for anyone wishing to modify or otherwise adjust the fit. I&amp;#39;m still working on a version of my mounting ears that support WD drives that don&amp;#39;t have the middle holes, but using the rubber grommets for some added compliance. the existing mounting ears I have made clip into the sled and rigidly screw to the drive.&lt;/p&gt;\n\n&lt;p&gt;It is available here: &lt;a href=\"https://www.printables.com/model/235313-fractal-define-r5-sata-backplane\"&gt;https://www.printables.com/model/235313-fractal-define-r5-sata-backplane&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I also finally got myself an LSI HBA, which contributes to the extremely satisfying wiring.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mbd0w2mfloua1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e33e8353d03203af34de00ff5e121cc60edc1ed3\"&gt;only 3 drives installed so far&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "30TB ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12qyb1c", "is_robot_indexable": true, "report_reasons": null, "author": "Pjtruslow", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12qyb1c/updated_3d_printed_fractal_define_r5_backplane/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12qyb1c/updated_3d_printed_fractal_define_r5_backplane/", "subreddit_subscribers": 678300, "created_utc": 1681840682.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_ayza6h03", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Netflix is shutting down its original DVD business after 25 years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r2ut6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 50, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 50, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681849033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "theverge.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.theverge.com/2023/4/18/23688543/netflix-dvd-com-business-25-years", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12r2ut6", "is_robot_indexable": true, "report_reasons": null, "author": "ufs2", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12r2ut6/netflix_is_shutting_down_its_original_dvd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.theverge.com/2023/4/18/23688543/netflix-dvd-com-business-25-years", "subreddit_subscribers": 678300, "created_utc": 1681849033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": " I'm  trying to archive all of them and I don't have much left to find. If anyone archived them years ago it could help me out a lot. I have around 3450 Indie games so far. Thanks!\n\n[https://www.reddit.com/r/xbox360/comments/12qzo7a/im\\_trying\\_to\\_archive\\_all\\_the\\_xbox\\_360\\_indie\\_games/?utm\\_source=share&amp;utm\\_medium=web2x&amp;context=3](https://www.reddit.com/r/xbox360/comments/12qzo7a/im_trying_to_archive_all_the_xbox_360_indie_games/?utm_source=share&amp;utm_medium=web2x&amp;context=3)", "author_fullname": "t2_584b5xa4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Did anyone download and preserve all the Xbox 360 Indie Games or Free Demos as they came out from 2005 to 2017?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r7n5r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681858374.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m  trying to archive all of them and I don&amp;#39;t have much left to find. If anyone archived them years ago it could help me out a lot. I have around 3450 Indie games so far. Thanks!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/xbox360/comments/12qzo7a/im_trying_to_archive_all_the_xbox_360_indie_games/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3\"&gt;https://www.reddit.com/r/xbox360/comments/12qzo7a/im_trying_to_archive_all_the_xbox_360_indie_games/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12r7n5r", "is_robot_indexable": true, "report_reasons": null, "author": "KJxbox", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12r7n5r/did_anyone_download_and_preserve_all_the_xbox_360/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12r7n5r/did_anyone_download_and_preserve_all_the_xbox_360/", "subreddit_subscribers": 678300, "created_utc": 1681858374.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Was doing some cleaning and found an old iOmega backup drive (Model LDHD320-U) and looks like the files (mainly photos) were backed up in their proprietary software format (iab). Unfortunately, I don't have the software and CD anymore so I'm unable to restore the files nor can I find another software that can open them. None of the uploads on [archive.org](https://archive.org) works for this drive. It looks like the software name is iOmega Automatic Backup. By any small chance would anyone here know where to find it?", "author_fullname": "t2_51lzu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Old iOmega Automatic backup software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qrelq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.77, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681831372.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Was doing some cleaning and found an old iOmega backup drive (Model LDHD320-U) and looks like the files (mainly photos) were backed up in their proprietary software format (iab). Unfortunately, I don&amp;#39;t have the software and CD anymore so I&amp;#39;m unable to restore the files nor can I find another software that can open them. None of the uploads on &lt;a href=\"https://archive.org\"&gt;archive.org&lt;/a&gt; works for this drive. It looks like the software name is iOmega Automatic Backup. By any small chance would anyone here know where to find it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12qrelq", "is_robot_indexable": true, "report_reasons": null, "author": "Vampir1c", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12qrelq/old_iomega_automatic_backup_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12qrelq/old_iomega_automatic_backup_software/", "subreddit_subscribers": 678300, "created_utc": 1681831372.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MRI Brain Images Just Got 64 Million Times Sharper. From 2 mm resolution to 5 microns. I ponder how much data is needed for this.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12r0umo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.58, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_fwo7huu", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/xJlBaDk22S-tcX5en1-Qw4hZr0Zs637NaBOiwFkWO8I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "Futurology", "selftext": "", "author_fullname": "t2_s4ya9", "saved": false, "mod_reason_title": null, "gilded": 1, "clicked": false, "title": "MRI Brain Images Just Got 64 Million Times Sharper. From 2 mm resolution to 5 microns", "link_flair_richtext": [], "subreddit_name_prefixed": "r/Futurology", "hidden": false, "pwls": 6, "link_flair_css_class": "medicine", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12qcw0x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 18306, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Medicine", "can_mod_post": false, "score": 18306, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/xJlBaDk22S-tcX5en1-Qw4hZr0Zs637NaBOiwFkWO8I.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {"gid_2": 1}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681798061.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "today.duke.edu", "allow_live_comments": true, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://today.duke.edu/2023/04/brain-images-just-got-64-million-times-sharper", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?auto=webp&amp;v=enabled&amp;s=e292d1c48f128b4832ff5a3d82a593017bee925a", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5072a00e02950e46dde5b85412499c7bcb80f2a6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d341bac0894c09b44c9f6dd9b521d0fb0e71a19", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=802f06480df6c54c3b735321f68ac5a3a17660d7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a75ef8a2d85252516a42f4da213603aa4304a5d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=137a2bd2bd564853e104e130eff6ac740c2ed6b3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bd459635ca0d4d20e34688828fd879c891aa767", "width": 1080, "height": 567}], "variants": {}, "id": "Q5HzIc41979w_ncnk_dTlE1hUv-prlrDpZzfzP3clYc"}], "enabled": false}, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 500, "id": "gid_2", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 100, "icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png", "days_of_premium": 7, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_width": 512, "static_icon_width": 512, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "Gives 100 Reddit Coins and a week of r/lounge access and ad-free browsing.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 512, "name": "Gold", "resized_static_icons": [{"url": "https://www.redditstatic.com/gold/awards/icon/gold_16.png", "width": 16, "height": 16}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_32.png", "width": 32, "height": 32}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_48.png", "width": 48, "height": 48}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_64.png", "width": 64, "height": 64}, {"url": "https://www.redditstatic.com/gold/awards/icon/gold_128.png", "width": 128, "height": 128}], "icon_format": null, "icon_height": 512, "penny_price": null, "award_type": "global", "static_icon_url": "https://www.redditstatic.com/gold/awards/icon/gold_512.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "3f00c588-5407-11ed-a0b1-62dd6d66ff59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2t7no", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12qcw0x", "is_robot_indexable": true, "report_reasons": null, "author": "Andune88", "discussion_type": null, "num_comments": 608, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/Futurology/comments/12qcw0x/mri_brain_images_just_got_64_million_times/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://today.duke.edu/2023/04/brain-images-just-got-64-million-times-sharper", "subreddit_subscribers": 18515449, "created_utc": 1681798061.0, "num_crossposts": 13, "media": null, "is_video": false}], "created": 1681845384.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "today.duke.edu", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://today.duke.edu/2023/04/brain-images-just-got-64-million-times-sharper", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?auto=webp&amp;v=enabled&amp;s=e292d1c48f128b4832ff5a3d82a593017bee925a", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5072a00e02950e46dde5b85412499c7bcb80f2a6", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d341bac0894c09b44c9f6dd9b521d0fb0e71a19", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=802f06480df6c54c3b735321f68ac5a3a17660d7", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a75ef8a2d85252516a42f4da213603aa4304a5d", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=137a2bd2bd564853e104e130eff6ac740c2ed6b3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/8bEbgZFezsYuvvkYpheMXRfYHWE1EVnvzud86q13oS8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5bd459635ca0d4d20e34688828fd879c891aa767", "width": 1080, "height": 567}], "variants": {}, "id": "Q5HzIc41979w_ncnk_dTlE1hUv-prlrDpZzfzP3clYc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12r0umo", "is_robot_indexable": true, "report_reasons": null, "author": "det1rac", "discussion_type": null, "num_comments": 9, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12qcw0x", "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12r0umo/mri_brain_images_just_got_64_million_times/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://today.duke.edu/2023/04/brain-images-just-got-64-million-times-sharper", "subreddit_subscribers": 678300, "created_utc": 1681845384.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I am a noobie and have just begun my data hoarding adventure last night. I have spent hours going through the yt-dlp GitHub page and I am too retarded to use it. I only know how to do \"yt-dlp.exe\" and then copy paste the URL of the youtube video I want to download. I have downloaded hundreds of videos in my youtube playlist already but I noticed the video quality is not the highest quality.\n\nI would like to create a config file that contains the following:\n\nThe best video and audio quality\n\nThe name of the video and the name of the channel that uploaded it\n\nAnd the date the video was uploaded.\n\nMay somebody please show me what commands to string together to do this or send me a link to a tutorial on how to do it? Thanks guys.", "author_fullname": "t2_95mwcdcp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Noobie needs help making a config file for yt-dlp", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qxtwh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681839778.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am a noobie and have just begun my data hoarding adventure last night. I have spent hours going through the yt-dlp GitHub page and I am too retarded to use it. I only know how to do &amp;quot;yt-dlp.exe&amp;quot; and then copy paste the URL of the youtube video I want to download. I have downloaded hundreds of videos in my youtube playlist already but I noticed the video quality is not the highest quality.&lt;/p&gt;\n\n&lt;p&gt;I would like to create a config file that contains the following:&lt;/p&gt;\n\n&lt;p&gt;The best video and audio quality&lt;/p&gt;\n\n&lt;p&gt;The name of the video and the name of the channel that uploaded it&lt;/p&gt;\n\n&lt;p&gt;And the date the video was uploaded.&lt;/p&gt;\n\n&lt;p&gt;May somebody please show me what commands to string together to do this or send me a link to a tutorial on how to do it? Thanks guys.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12qxtwh", "is_robot_indexable": true, "report_reasons": null, "author": "MewingIntrovert", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12qxtwh/noobie_needs_help_making_a_config_file_for_ytdlp/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12qxtwh/noobie_needs_help_making_a_config_file_for_ytdlp/", "subreddit_subscribers": 678300, "created_utc": 1681839778.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Just as the title indicates.   I would like to see if theres a way inside windows with either a command or a third party piece of software that will quickly tell me how much free space I have spread across multiple drives (Sata and USB really) \n\nI know I can go into each drive manually and look at each one, but id like to just see like XTB freespace combined.\n\n&amp;#x200B;\n\nThanks :)", "author_fullname": "t2_12dgn1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone know a way in Windows to see all free space from combined drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r6xrp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681856954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just as the title indicates.   I would like to see if theres a way inside windows with either a command or a third party piece of software that will quickly tell me how much free space I have spread across multiple drives (Sata and USB really) &lt;/p&gt;\n\n&lt;p&gt;I know I can go into each drive manually and look at each one, but id like to just see like XTB freespace combined.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12r6xrp", "is_robot_indexable": true, "report_reasons": null, "author": "JohnnyNintendo", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12r6xrp/does_anyone_know_a_way_in_windows_to_see_all_free/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12r6xrp/does_anyone_know_a_way_in_windows_to_see_all_free/", "subreddit_subscribers": 678300, "created_utc": 1681856954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Books are data right? lol\n\nI'm looking for a book inventory software. Something that I can access via mobile, an app or webpage. I was thinking about snipeIT. But wondering if there is something else.\n\n&amp;#x200B;\n\nEdit: This was for physical books.", "author_fullname": "t2_3ez9v5po", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Book Inventory Software", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qz0bj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681845586.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681841893.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Books are data right? lol&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for a book inventory software. Something that I can access via mobile, an app or webpage. I was thinking about snipeIT. But wondering if there is something else.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Edit: This was for physical books.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12qz0bj", "is_robot_indexable": true, "report_reasons": null, "author": "rokar83", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12qz0bj/book_inventory_software/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12qz0bj/book_inventory_software/", "subreddit_subscribers": 678300, "created_utc": 1681841893.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "OK, I have a 5TB WD Elements. The pins looked messed up and it was not being recognized.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI shucked it and used a controller board I had to buy 3 years ago to fix the exact same thing on another drive\n\n&amp;#x200B;\n\nWell, now it is not recognized this way. I had to fine the WD SES drivers and install that and it shows things working right but still not recognized\n\n&amp;#x200B;\n\nDo I need a different model number for the controller board? The other drive is I use this one for is a WD green EZRX just like this one I am trying to fix but it is 4 TB v.s. 5 TB", "author_fullname": "t2_viuwzrr1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help fixing my external HD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qukre", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681835042.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;OK, I have a 5TB WD Elements. The pins looked messed up and it was not being recognized.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I shucked it and used a controller board I had to buy 3 years ago to fix the exact same thing on another drive&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Well, now it is not recognized this way. I had to fine the WD SES drivers and install that and it shows things working right but still not recognized&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Do I need a different model number for the controller board? The other drive is I use this one for is a WD green EZRX just like this one I am trying to fix but it is 4 TB v.s. 5 TB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12qukre", "is_robot_indexable": true, "report_reasons": null, "author": "Rotisseriejedi", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12qukre/need_help_fixing_my_external_hd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12qukre/need_help_fixing_my_external_hd/", "subreddit_subscribers": 678300, "created_utc": 1681835042.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi!\n\n&amp;nbsp;\n\nSo I have a lot of spare healthy 1TB and 2TB disks which I'd like to use to backup my 11TB of data on server.\n\n&amp;nbsp;\n\nIt's sorted kinda weird, so I was thinking what would be the best way to archive, that is backup all of this data to many small disks?\n\n&amp;nbsp;\n\nPerhaps a filesystem that simulates files as if they're there but they're actually not, so that I can just run rsync or something.\n\n&amp;nbsp;\n\nI could manually sort out data into 1TB / 2TB etc but that would kill the structure of media, since like.. only the raw footage is 3.5TB itself, and now separating it it would be basically spreading multiple projects across multiple disks and it would be an organizational nightmare.\n\n&amp;nbsp;\n\nSo I'm trying to figure out a way to organize it all now.\n\n&amp;nbsp;\n\nThanks :)", "author_fullname": "t2_fiawk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "MergerFS or some other way to manually cycle disks to backup? (11TB of data backup to multiple 1TB and 2TB disks)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ro5pz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681898975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;So I have a lot of spare healthy 1TB and 2TB disks which I&amp;#39;d like to use to backup my 11TB of data on server.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s sorted kinda weird, so I was thinking what would be the best way to archive, that is backup all of this data to many small disks?&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Perhaps a filesystem that simulates files as if they&amp;#39;re there but they&amp;#39;re actually not, so that I can just run rsync or something.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;I could manually sort out data into 1TB / 2TB etc but that would kill the structure of media, since like.. only the raw footage is 3.5TB itself, and now separating it it would be basically spreading multiple projects across multiple disks and it would be an organizational nightmare.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m trying to figure out a way to organize it all now.&lt;/p&gt;\n\n&lt;p&gt;&amp;nbsp;&lt;/p&gt;\n\n&lt;p&gt;Thanks :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12ro5pz", "is_robot_indexable": true, "report_reasons": null, "author": "Xillenn", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12ro5pz/mergerfs_or_some_other_way_to_manually_cycle/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12ro5pz/mergerfs_or_some_other_way_to_manually_cycle/", "subreddit_subscribers": 678300, "created_utc": 1681898975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I was trying to setup a NAS but was getting poor performance (&lt;100MB/s) on 10Gbps.\nTo try to diagnose I build a minimal version. Here is my simplified setup:\n\n- Ubuntu 20.04 LTS\n\n- 500 GB Sata SSD, no HDD to eliminate storage bottleneck\n\n- ext4\n\n- Connected via smb://127.0.0.1 to eliminate network bottleneck\n\n\nCopies without samba are ~1100MB/s.\nCopies using smb://127.0.0.1 are 60-100 MB/s\n\nI have plenty of ram and have tried increasing the buffer sizes+done the normal config adjustments recommended for performance as well.\n\nI haven't setup a NAS before is performance this poor expected?", "author_fullname": "t2_n3wmr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Poor Samba NAS Performance on Ubuntu", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rhvr4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681887678.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681881355.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was trying to setup a NAS but was getting poor performance (&amp;lt;100MB/s) on 10Gbps.\nTo try to diagnose I build a minimal version. Here is my simplified setup:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Ubuntu 20.04 LTS&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;500 GB Sata SSD, no HDD to eliminate storage bottleneck&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;ext4&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Connected via smb://127.0.0.1 to eliminate network bottleneck&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Copies without samba are ~1100MB/s.\nCopies using smb://127.0.0.1 are 60-100 MB/s&lt;/p&gt;\n\n&lt;p&gt;I have plenty of ram and have tried increasing the buffer sizes+done the normal config adjustments recommended for performance as well.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t setup a NAS before is performance this poor expected?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12rhvr4", "is_robot_indexable": true, "report_reasons": null, "author": "clifdiver", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12rhvr4/poor_samba_nas_performance_on_ubuntu/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12rhvr4/poor_samba_nas_performance_on_ubuntu/", "subreddit_subscribers": 678300, "created_utc": 1681881355.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone done the hard work of compiling a crap ton of \"essential\" data to have on a hard drive for a potential long term loss of internet?", "author_fullname": "t2_5tn6005g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ultimate Prepper Download??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "setups", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rahu3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Hoarder-Setups", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681864502.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone done the hard work of compiling a crap ton of &amp;quot;essential&amp;quot; data to have on a hard drive for a potential long term loss of internet?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "d14a812c-b94e-11eb-b72a-0ee8a79b8cab", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12rahu3", "is_robot_indexable": true, "report_reasons": null, "author": "SuperDangerBro", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12rahu3/ultimate_prepper_download/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12rahu3/ultimate_prepper_download/", "subreddit_subscribers": 678300, "created_utc": 1681864502.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I want to archive an Apache directory, without having to go inside every folder and manually saving each file, since the number of files are in excess of 200,000, and there are so many levels of folders. I tried Sitesucker, but it's so slow, and it seems to be archiving the entire website, which I don't need. I only want it to archive the specific directory and all the files/folders inside it.\n\nAny tips are much appreciated.", "author_fullname": "t2_96cy3go", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How can I archive a public Apache directory?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r3iv3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681850322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to archive an Apache directory, without having to go inside every folder and manually saving each file, since the number of files are in excess of 200,000, and there are so many levels of folders. I tried Sitesucker, but it&amp;#39;s so slow, and it seems to be archiving the entire website, which I don&amp;#39;t need. I only want it to archive the specific directory and all the files/folders inside it.&lt;/p&gt;\n\n&lt;p&gt;Any tips are much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12r3iv3", "is_robot_indexable": true, "report_reasons": null, "author": "TheGreatScorpio", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12r3iv3/how_can_i_archive_a_public_apache_directory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12r3iv3/how_can_i_archive_a_public_apache_directory/", "subreddit_subscribers": 678300, "created_utc": 1681850322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I just discovered this subreddit while doing a google search on what to do with all my hoarded data.\n\nI have 9 or 10 hard drive from computers I no longer own, and a slew of portable usb hard drives including 3 - 10 terabit drives,  and the third one is almost full.\n\nI have pictures videos, books, documents etc. you name it I got it\n\nI'm sure there is a slew of doubles and triples across the drives.\n\nDoes anyone have any idea how to get a handle on sorting the good from the bad and a method of keeping track of it in the future?\n\nAlso does anyone know of a way to have my phone (android) dump its photos wirelessly and in some kind of orderly manner, I couldn't find a specific photo or video in a timely manner if my life depended on it\n\nI was thinking about getting another 10 terabit drive and going thru them one file at a time but I don't think I have that kind of time left\n\nThe answer may lie in this sub, but I just found reddit and barely know how to use it\n\nAny suggestions, Thanks in advance", "author_fullname": "t2_12m29n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I beyond help?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qv8yu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681835882.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just discovered this subreddit while doing a google search on what to do with all my hoarded data.&lt;/p&gt;\n\n&lt;p&gt;I have 9 or 10 hard drive from computers I no longer own, and a slew of portable usb hard drives including 3 - 10 terabit drives,  and the third one is almost full.&lt;/p&gt;\n\n&lt;p&gt;I have pictures videos, books, documents etc. you name it I got it&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure there is a slew of doubles and triples across the drives.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have any idea how to get a handle on sorting the good from the bad and a method of keeping track of it in the future?&lt;/p&gt;\n\n&lt;p&gt;Also does anyone know of a way to have my phone (android) dump its photos wirelessly and in some kind of orderly manner, I couldn&amp;#39;t find a specific photo or video in a timely manner if my life depended on it&lt;/p&gt;\n\n&lt;p&gt;I was thinking about getting another 10 terabit drive and going thru them one file at a time but I don&amp;#39;t think I have that kind of time left&lt;/p&gt;\n\n&lt;p&gt;The answer may lie in this sub, but I just found reddit and barely know how to use it&lt;/p&gt;\n\n&lt;p&gt;Any suggestions, Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12qv8yu", "is_robot_indexable": true, "report_reasons": null, "author": "jonbo1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12qv8yu/am_i_beyond_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12qv8yu/am_i_beyond_help/", "subreddit_subscribers": 678300, "created_utc": 1681835882.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to rip some DVD videos I bought that were made by a smaller company, so they have a commercially printed label and packaging but the burned them on DVD-R as opposed to pressing it. The videos play fine in any DVD player but if I put them in a computer for ripping, it claims they are blank discs. Tried this in Windows and Mac OS with two different drives, and several discs from the same company.\n\nI am wondering if they are trying some tricks as a homemade copy protection solution to prevent the discs mounting on a computer? Has anyone heard of something similar and might know a software solution?\n\n&amp;#x200B;\n\nThank you!", "author_fullname": "t2_13kgwm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Potential DVD Homemade Copy Protection Issues", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rp27j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681901302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to rip some DVD videos I bought that were made by a smaller company, so they have a commercially printed label and packaging but the burned them on DVD-R as opposed to pressing it. The videos play fine in any DVD player but if I put them in a computer for ripping, it claims they are blank discs. Tried this in Windows and Mac OS with two different drives, and several discs from the same company.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if they are trying some tricks as a homemade copy protection solution to prevent the discs mounting on a computer? Has anyone heard of something similar and might know a software solution?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12rp27j", "is_robot_indexable": true, "report_reasons": null, "author": "chessiesystem", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12rp27j/potential_dvd_homemade_copy_protection_issues/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12rp27j/potential_dvd_homemade_copy_protection_issues/", "subreddit_subscribers": 678300, "created_utc": 1681901302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "A few years ago, the sports entertainment conglomerate Mpora filed for bankruptcy. Part of the Mpora network was an international skateboarding publication named \"Sidewalk Magazine\". Sidewalk also had an online forum, which had over twenty years of community history. In the subsequent auction of assets, the forum was lost. This was despite a popular petition started and submitted by myself, to both the auctioneers (in the hope that they would pass on the request) and to the new owners. Despite several requests, a few years later the forum is nowhere to be found.\n\nIf anyone has any pages at all archived from https://forums.sidewalkmag.com/, I'm trying to piece together as much of it as I can in an attempt to preserve and continue the community. It would be great if you could get in touch!", "author_fullname": "t2_3wofxauk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sidewalk forum", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rnzrv", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Request", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681898558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A few years ago, the sports entertainment conglomerate Mpora filed for bankruptcy. Part of the Mpora network was an international skateboarding publication named &amp;quot;Sidewalk Magazine&amp;quot;. Sidewalk also had an online forum, which had over twenty years of community history. In the subsequent auction of assets, the forum was lost. This was despite a popular petition started and submitted by myself, to both the auctioneers (in the hope that they would pass on the request) and to the new owners. Despite several requests, a few years later the forum is nowhere to be found.&lt;/p&gt;\n\n&lt;p&gt;If anyone has any pages at all archived from &lt;a href=\"https://forums.sidewalkmag.com/\"&gt;https://forums.sidewalkmag.com/&lt;/a&gt;, I&amp;#39;m trying to piece together as much of it as I can in an attempt to preserve and continue the community. It would be great if you could get in touch!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a9b81a26-bb69-11eb-8bca-0ea4446fb5bf", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "12rnzrv", "is_robot_indexable": true, "report_reasons": null, "author": "NorthernScrub", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12rnzrv/sidewalk_forum/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12rnzrv/sidewalk_forum/", "subreddit_subscribers": 678300, "created_utc": 1681898558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been converting home videos on MiniDV tapes using the following setup:\n\nCamcorder --&gt; Firewire --&gt; USB C --&gt; VLC --&gt; DV File\n\nWith VLC, I just dump the raw input into a .dv file.\n\nNow, I would like to either overlay the timestamp or move it to a separate subtitle file. Now, I'm not sure with my current setup whether or not the timestamp metadata is stored in the .dv file as well. I've tried DVDate, but unfortunately, it only works with .avi files. Is there anyway to easily view if the timestamp data is in the .dv file and extract it?", "author_fullname": "t2_jd3m7yt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Extracting timestamps from MiniDV converted to .dv file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rjeth", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681885583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been converting home videos on MiniDV tapes using the following setup:&lt;/p&gt;\n\n&lt;p&gt;Camcorder --&amp;gt; Firewire --&amp;gt; USB C --&amp;gt; VLC --&amp;gt; DV File&lt;/p&gt;\n\n&lt;p&gt;With VLC, I just dump the raw input into a .dv file.&lt;/p&gt;\n\n&lt;p&gt;Now, I would like to either overlay the timestamp or move it to a separate subtitle file. Now, I&amp;#39;m not sure with my current setup whether or not the timestamp metadata is stored in the .dv file as well. I&amp;#39;ve tried DVDate, but unfortunately, it only works with .avi files. Is there anyway to easily view if the timestamp data is in the .dv file and extract it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12rjeth", "is_robot_indexable": true, "report_reasons": null, "author": "roadbiking19", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12rjeth/extracting_timestamps_from_minidv_converted_to_dv/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12rjeth/extracting_timestamps_from_minidv_converted_to_dv/", "subreddit_subscribers": 678300, "created_utc": 1681885583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have tons of videos with x264 encoding. I want to know if I can use H.265 from AMD GPU to reduce their sizes. I will be using Handbrake in Linux, is this sufficient, what settings should I use in H.265 to retain the same quality?", "author_fullname": "t2_5oj3oulo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Encoding to save space, what settings to use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r6yaz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681856986.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have tons of videos with x264 encoding. I want to know if I can use H.265 from AMD GPU to reduce their sizes. I will be using Handbrake in Linux, is this sufficient, what settings should I use in H.265 to retain the same quality?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12r6yaz", "is_robot_indexable": true, "report_reasons": null, "author": "securityconcerned", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12r6yaz/encoding_to_save_space_what_settings_to_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12r6yaz/encoding_to_save_space_what_settings_to_use/", "subreddit_subscribers": 678300, "created_utc": 1681856986.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi, I'm quite new to NAS and just bought my first DS923 2 months ago. So I have 3 hard drives in my NAS right now, 1x 20tb hdd and 2x 14tb hdd. \n\nI was wondering am I able to allocate 5tb for very important files which I want to back up to both the 14tb hard drives, then use the rest of the storage for non-important stuff like movies/videos which I don't care about data loss.\n\nSo in the end, I will up with 15tb, 9tb, 9tb storage for movies/videos and the rest will be used to triple back up important 5tb worth of files, just in case 2 drives decides to die on me at the same time for whatever reason.\n\nIs this possible, if so could someone tell me the method? Thanks!", "author_fullname": "t2_3r5pltsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a way to only backup a portion of the drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r3wxa", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681851067.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m quite new to NAS and just bought my first DS923 2 months ago. So I have 3 hard drives in my NAS right now, 1x 20tb hdd and 2x 14tb hdd. &lt;/p&gt;\n\n&lt;p&gt;I was wondering am I able to allocate 5tb for very important files which I want to back up to both the 14tb hard drives, then use the rest of the storage for non-important stuff like movies/videos which I don&amp;#39;t care about data loss.&lt;/p&gt;\n\n&lt;p&gt;So in the end, I will up with 15tb, 9tb, 9tb storage for movies/videos and the rest will be used to triple back up important 5tb worth of files, just in case 2 drives decides to die on me at the same time for whatever reason.&lt;/p&gt;\n\n&lt;p&gt;Is this possible, if so could someone tell me the method? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12r3wxa", "is_robot_indexable": true, "report_reasons": null, "author": "ExpiredNutJuice", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12r3wxa/is_there_a_way_to_only_backup_a_portion_of_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12r3wxa/is_there_a_way_to_only_backup_a_portion_of_the/", "subreddit_subscribers": 678300, "created_utc": 1681851067.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_gh87r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "14TB Seagate Expansion Desktop External Hard Drive - $189.99 - $13.5/TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "sale", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qy6ru", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.47, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Sale", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681840468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "newegg.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.newegg.com/seagate-expansion-14tb-black/p/N82E16822184958", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ef8232de-b94e-11eb-ba29-0ed106a6f983", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12qy6ru", "is_robot_indexable": true, "report_reasons": null, "author": "Viknee", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12qy6ru/14tb_seagate_expansion_desktop_external_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.newegg.com/seagate-expansion-14tb-black/p/N82E16822184958", "subreddit_subscribers": 678300, "created_utc": 1681840468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Not sure if this is the right subreddit for my question. I'm looking for software like Acronis for backups, cloning, etc. I like Acronis, but a yearly subscription is a no go for me. I am fine paying, even if it's expensive; so long as it's got great functionality and no issue backups/cloning.", "author_fullname": "t2_88kwqyl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free or One Time Paid Alternatives to Acronis?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12qvh4x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681836168.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure if this is the right subreddit for my question. I&amp;#39;m looking for software like Acronis for backups, cloning, etc. I like Acronis, but a yearly subscription is a no go for me. I am fine paying, even if it&amp;#39;s expensive; so long as it&amp;#39;s got great functionality and no issue backups/cloning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12qvh4x", "is_robot_indexable": true, "report_reasons": null, "author": "techtimee", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12qvh4x/free_or_one_time_paid_alternatives_to_acronis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12qvh4x/free_or_one_time_paid_alternatives_to_acronis/", "subreddit_subscribers": 678300, "created_utc": 1681836168.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I\u2019m looking for any way to be able to play Makeover Madness again. It was part of the 2019 purge of Java games on pogo, and I can\u2019t seem to find anyone who was able to save it. It also doesn\u2019t help that it was a club pogo (membership) exclusive. Any help is appreciated, thanks in advance.", "author_fullname": "t2_rha4jiog", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Was anyone able to save the Java games deleted from pogo.com?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rnh2p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681897175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m looking for any way to be able to play Makeover Madness again. It was part of the 2019 purge of Java games on pogo, and I can\u2019t seem to find anyone who was able to save it. It also doesn\u2019t help that it was a club pogo (membership) exclusive. Any help is appreciated, thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12rnh2p", "is_robot_indexable": true, "report_reasons": null, "author": "thesoundofgender", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12rnh2p/was_anyone_able_to_save_the_java_games_deleted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12rnh2p/was_anyone_able_to_save_the_java_games_deleted/", "subreddit_subscribers": 678300, "created_utc": 1681897175.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}