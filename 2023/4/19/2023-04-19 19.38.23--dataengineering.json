{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_56wplwbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zillacode Premium finally done, Leetcode for PySpark, Spark and Pandas at Zillacode.com", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "name": "t3_12r8x5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 112, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 112, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5GgSDcLT9bjG0kduOq9T3QHbNjaWdQE6153wBY49OPY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681861078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/64htp9vsaqua1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?auto=webp&amp;v=enabled&amp;s=b88bf5a671b0024424f3dcd8d9823e4889dabada", "width": 1797, "height": 937}, "resolutions": [{"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=923ab6f291c815ca616d00ebd156d83c7a2c1c4a", "width": 108, "height": 56}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e6b0f2875131323d5d8633f53e9d6253508a3cf", "width": 216, "height": 112}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c34378bd9dc47f7a194c04d9103b7e54502ff8ae", "width": 320, "height": 166}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0823b0fb72fe90df9f44688f808203be5559aa7", "width": 640, "height": 333}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=815573b2f8dc9e0a65d62a9b5bc47a6e92f6a7a6", "width": 960, "height": 500}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b53615dfbfca94a9fca1d332436197248dfaade", "width": 1080, "height": 563}], "variants": {}, "id": "qQFgHmHUcppe72_LgmE5jf0imJJkH6p5Fhx13osQ5GM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12r8x5c", "is_robot_indexable": true, "report_reasons": null, "author": "dmage5000", "discussion_type": null, "num_comments": 20, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r8x5c/zillacode_premium_finally_done_leetcode_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/64htp9vsaqua1.jpg", "subreddit_subscribers": 100573, "created_utc": 1681861078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "* Joe Reiss and Matthew Housley chatting to Felix GV about Venice and Designing Massive Distributed Systems at LinkedIn [https://overcast.fm/+wcMoWgOXw](https://overcast.fm/+wcMoWgOXw)\n* Benn Stancil's talk from Coalesce 2021 about the Modern Data Experience: [https://overcast.fm/+w94XLSJUU](https://overcast.fm/+w94XLSJUU)\n* Joe Reiss and Matthew Housley talk to Neelesh Salian about building data infrastructure [https://overcast.fm/+wcMp5SpsI](https://overcast.fm/+wcMp5SpsI)\n* Jean-Georges Perrin on DataEngPodcast talking about DataMesh in real life (not theoretical!) at PayPal [https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364](https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364)\n* Maggie Hays and Pete Soderling on DataEngPodcast talking about DataCouncil and building an intentional data culture [https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365](https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365)\n* Mark Rittman talking on DrilltoDetail with Stewart Bryson, Keenan Rice, and Jake Stein about the Modern Data Stack past, present &amp; future  [https://overcast.fm/+Iexdb0bH0](https://overcast.fm/+Iexdb0bH0)\n* Tristan Handy and Julia Schottenstein talking to Justin Borgman about Starburst, TrinoDB, and data access patterns and querying [https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data](https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data)\n\nWhat are your favourite episodes that you'd like to share with others?", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83c\udfa7 Random grab-bag of podcast episodes that I've enjoyed recently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rockb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 39, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 39, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681899465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;Joe Reiss and Matthew Housley chatting to Felix GV about Venice and Designing Massive Distributed Systems at LinkedIn &lt;a href=\"https://overcast.fm/+wcMoWgOXw\"&gt;https://overcast.fm/+wcMoWgOXw&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Benn Stancil&amp;#39;s talk from Coalesce 2021 about the Modern Data Experience: &lt;a href=\"https://overcast.fm/+w94XLSJUU\"&gt;https://overcast.fm/+w94XLSJUU&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Joe Reiss and Matthew Housley talk to Neelesh Salian about building data infrastructure &lt;a href=\"https://overcast.fm/+wcMp5SpsI\"&gt;https://overcast.fm/+wcMp5SpsI&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Jean-Georges Perrin on DataEngPodcast talking about DataMesh in real life (not theoretical!) at PayPal &lt;a href=\"https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364\"&gt;https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Maggie Hays and Pete Soderling on DataEngPodcast talking about DataCouncil and building an intentional data culture &lt;a href=\"https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365\"&gt;https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Mark Rittman talking on DrilltoDetail with Stewart Bryson, Keenan Rice, and Jake Stein about the Modern Data Stack past, present &amp;amp; future  &lt;a href=\"https://overcast.fm/+Iexdb0bH0\"&gt;https://overcast.fm/+Iexdb0bH0&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Tristan Handy and Julia Schottenstein talking to Justin Borgman about Starburst, TrinoDB, and data access patterns and querying &lt;a href=\"https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data\"&gt;https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What are your favourite episodes that you&amp;#39;d like to share with others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12rockb", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rockb/random_grabbag_of_podcast_episodes_that_ive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rockb/random_grabbag_of_podcast_episodes_that_ive/", "subreddit_subscribers": 100573, "created_utc": 1681899465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Looking for something thought provoking/educational...lately this sub is filled with either a) posts from people trying to break into DE, or b) people who have googled/heard something about DE and want to sound relevant. \n\nOther than Software/Tool specific subs, what other subreddits are you all subscribed to that keep you interested?", "author_fullname": "t2_714yyvua", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Other Subs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r38jq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 29, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 29, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681849787.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for something thought provoking/educational...lately this sub is filled with either a) posts from people trying to break into DE, or b) people who have googled/heard something about DE and want to sound relevant. &lt;/p&gt;\n\n&lt;p&gt;Other than Software/Tool specific subs, what other subreddits are you all subscribed to that keep you interested?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12r38jq", "is_robot_indexable": true, "report_reasons": null, "author": "amtobin33", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r38jq/other_subs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12r38jq/other_subs/", "subreddit_subscribers": 100573, "created_utc": 1681849787.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a small startup and we\u2019re reaching the point where our database is getting hammered by the amount of custom client reports we send. I\u2019m a junior dev, but am extremely interested in how to help develop a sustainable solution. We currently use an AWS hosted Postgres db, and run a ton of reports out of Looker. \n\nAre there any good resources to learn about standing something like this up? Thanks in advance!", "author_fullname": "t2_5am908px", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a good resource to learn how to set up a Redshift warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rueol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681913133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a small startup and we\u2019re reaching the point where our database is getting hammered by the amount of custom client reports we send. I\u2019m a junior dev, but am extremely interested in how to help develop a sustainable solution. We currently use an AWS hosted Postgres db, and run a ton of reports out of Looker. &lt;/p&gt;\n\n&lt;p&gt;Are there any good resources to learn about standing something like this up? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12rueol", "is_robot_indexable": true, "report_reasons": null, "author": "iambatmanman", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rueol/what_is_a_good_resource_to_learn_how_to_set_up_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rueol/what_is_a_good_resource_to_learn_how_to_set_up_a/", "subreddit_subscribers": 100573, "created_utc": 1681913133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We've sidestepped tradition and built our peer agendas based on what's important to those who provide feedback on our summits. Data Stack Summit is virtual and kicks off tomorrow, 4/19 at 8:00 AM PST. Registration is free at [datastacksummit.com](https://datastacksummit.com) \n\nSessions will be available on-demand for registrants post-event and we've enabled attendance certificates for those who need them for their employers.\n\nHere's a breakdown of the full agenda:\n\n8:05 AM PST - Keynote Panel: Peer-to-peer panel about managing cloud costs right now with Mike Fuller (FinOps Foundation), Joseph Machado (LinkedIn), Vikas Ranjan (T-Mobile), and Carlos Costa (Adidas)\n\n8:40 AM PST - Keynote: Modernizing the data stack - keeping it real with Mark Mullins (United Community Bank) and Raj Joseph (DQ Labs)\n\n9:15 AM PST - Breakout Session: From complex to simplicity: Our DataOps journey with Jennifer Romero-Higgins (American Airlines) \n\n9:15 AM PST - Breakout Session: An ever-increasing need for data quality with Dr. Rajkumar Bhojan (Fidelity)\n\n9:50 AM PST - Breakout Session: Turning your data lake into an asset with Bill Inmon (Forest Rim Technology)\n\n9:50 AM PST - Breakout Session: Maintaining price and performance SLAs across engineering teams with Mark Kidwell (Autodesk)\n\n10:25 AM PST - Breakout Session: A look at Walmart\u2019s self-service metadata-driven data loader framework with Manimuthu Aayyannan and Subramanya Mulgund (Walmart)\n\n10:25 AM PST - Breakout Session: YARN to Kubernetes: Modernizing big data workloads on a massive scale with Vikas Ranjan (T-Mobile)\n\n11:00 AM PST - Peer-to-Peer Panel: Enabling the analytics end user with Nicole Radziwill (Ultranauts), Sangeeta Krishnan (Bayer), Jess Ramos (Crunchbase), and Sunny Zhu (Indeed)\n\n11:35 AM PST - Breakout Session: Building a business-critical data platform to process over \u00a334bn in card transactions with Sandeep Mehta (Dojo)\n\n11:35 AM PST - Breakout Session: NLP &amp; ML data-driven decision making: Taming the curriculum beast with Joel Hernandez (eLumen) and Carlos Rodriguez (MentorMate)\n\n12:10 PM PST - Breakout Session: Is synthetic data useful for data engineers? with Dr. Alexander Mikhalev (Applied Knowledge Systems) and Matthew Norton (Nationwide)\n\n12:10 PM PST - Breakout Session: The great debate of data quality vs. data observability with Olga Maydanchik (Voya Financial) and Raj Joseph (DQ Labs)\n\n12:45 PM PST - Breakout Session: DataOps teams: Stop sprinting! With Monica Kay Royal (Nerd Nourishment)\n\n12:45 PM PST - Breakout Session: Designing a modern customer data center of excellence with Ted Sfikas (Tealium)", "author_fullname": "t2_ff7f8okm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hope you're able to support our peer-built agenda tomorrow at Data Stack Summit (virtual data community conference)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r93d3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681861456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve sidestepped tradition and built our peer agendas based on what&amp;#39;s important to those who provide feedback on our summits. Data Stack Summit is virtual and kicks off tomorrow, 4/19 at 8:00 AM PST. Registration is free at &lt;a href=\"https://datastacksummit.com\"&gt;datastacksummit.com&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Sessions will be available on-demand for registrants post-event and we&amp;#39;ve enabled attendance certificates for those who need them for their employers.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a breakdown of the full agenda:&lt;/p&gt;\n\n&lt;p&gt;8:05 AM PST - Keynote Panel: Peer-to-peer panel about managing cloud costs right now with Mike Fuller (FinOps Foundation), Joseph Machado (LinkedIn), Vikas Ranjan (T-Mobile), and Carlos Costa (Adidas)&lt;/p&gt;\n\n&lt;p&gt;8:40 AM PST - Keynote: Modernizing the data stack - keeping it real with Mark Mullins (United Community Bank) and Raj Joseph (DQ Labs)&lt;/p&gt;\n\n&lt;p&gt;9:15 AM PST - Breakout Session: From complex to simplicity: Our DataOps journey with Jennifer Romero-Higgins (American Airlines) &lt;/p&gt;\n\n&lt;p&gt;9:15 AM PST - Breakout Session: An ever-increasing need for data quality with Dr. Rajkumar Bhojan (Fidelity)&lt;/p&gt;\n\n&lt;p&gt;9:50 AM PST - Breakout Session: Turning your data lake into an asset with Bill Inmon (Forest Rim Technology)&lt;/p&gt;\n\n&lt;p&gt;9:50 AM PST - Breakout Session: Maintaining price and performance SLAs across engineering teams with Mark Kidwell (Autodesk)&lt;/p&gt;\n\n&lt;p&gt;10:25 AM PST - Breakout Session: A look at Walmart\u2019s self-service metadata-driven data loader framework with Manimuthu Aayyannan and Subramanya Mulgund (Walmart)&lt;/p&gt;\n\n&lt;p&gt;10:25 AM PST - Breakout Session: YARN to Kubernetes: Modernizing big data workloads on a massive scale with Vikas Ranjan (T-Mobile)&lt;/p&gt;\n\n&lt;p&gt;11:00 AM PST - Peer-to-Peer Panel: Enabling the analytics end user with Nicole Radziwill (Ultranauts), Sangeeta Krishnan (Bayer), Jess Ramos (Crunchbase), and Sunny Zhu (Indeed)&lt;/p&gt;\n\n&lt;p&gt;11:35 AM PST - Breakout Session: Building a business-critical data platform to process over \u00a334bn in card transactions with Sandeep Mehta (Dojo)&lt;/p&gt;\n\n&lt;p&gt;11:35 AM PST - Breakout Session: NLP &amp;amp; ML data-driven decision making: Taming the curriculum beast with Joel Hernandez (eLumen) and Carlos Rodriguez (MentorMate)&lt;/p&gt;\n\n&lt;p&gt;12:10 PM PST - Breakout Session: Is synthetic data useful for data engineers? with Dr. Alexander Mikhalev (Applied Knowledge Systems) and Matthew Norton (Nationwide)&lt;/p&gt;\n\n&lt;p&gt;12:10 PM PST - Breakout Session: The great debate of data quality vs. data observability with Olga Maydanchik (Voya Financial) and Raj Joseph (DQ Labs)&lt;/p&gt;\n\n&lt;p&gt;12:45 PM PST - Breakout Session: DataOps teams: Stop sprinting! With Monica Kay Royal (Nerd Nourishment)&lt;/p&gt;\n\n&lt;p&gt;12:45 PM PST - Breakout Session: Designing a modern customer data center of excellence with Ted Sfikas (Tealium)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?auto=webp&amp;v=enabled&amp;s=bad6e740602f1662394b1fe7dabd8981683aa1fb", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=161039fc3821f7cd1dfa39f0585f770c15bc946a", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73294167473cedd820bcad18a90c58650e4e224d", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76fdcab5605a3d9b860f5c97868c51c45dad182d", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af3928fee515c51d4526f0278a87666374ae09b1", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=275fa4eb875cbf9ca0394906f24d080551eabb32", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58ace0884f5f5330ea81fc7e4ec2429ab9c22d64", "width": 1080, "height": 1080}], "variants": {}, "id": "bgiUBgCfRhBtLAYa6NU8eNVDv2Bm8-xV4rWU9l-rDkI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12r93d3", "is_robot_indexable": true, "report_reasons": null, "author": "hesanastronaut", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r93d3/hope_youre_able_to_support_our_peerbuilt_agenda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12r93d3/hope_youre_able_to_support_our_peerbuilt_agenda/", "subreddit_subscribers": 100573, "created_utc": 1681861456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you run dbt core on Airflow or are considering doing so, I'm sure you'll find something of interest in tomorrow's workshop with [Charlie Summers](https://www.linkedin.com/in/charliesummers/) (Staff SE, [Merit](https://www.linkedin.com/company/earnmerits/)) @ 11am\n\nCharlie will share what he's learned after running dbt core on Airflow in production for 2 years.\n\nTopics he'll be covering include:\n\n\\- Isolating dbt core executions and avoiding python dependency hell with Kubernetes Pod Operators  \n\\- Making dbt DAGs DRY-er with re-usable Airflow templates  \n\\- Cutting \\~30 seconds off every DAG execution with pre-compilation (when I realized we could do this, I literally :man-facepalming: for not realizing it sooner - so much wasted compute!)\n\nSign up [here](https://www.operationalanalytics.club/events/running-dbt-core-on-airflow-in-production-learnings-from-2-years-of-battle-scars)!   \n\n\nPS: \n\n\\- If you want to attend but cant, no worries! I'll share recording w/ all signups  \n\\- No SaaS is being sold during this event. It's strictly educational for ppl interested in running dbt core on airflow.", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running dbt core on airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rehq9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681873281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you run dbt core on Airflow or are considering doing so, I&amp;#39;m sure you&amp;#39;ll find something of interest in tomorrow&amp;#39;s workshop with &lt;a href=\"https://www.linkedin.com/in/charliesummers/\"&gt;Charlie Summers&lt;/a&gt; (Staff SE, &lt;a href=\"https://www.linkedin.com/company/earnmerits/\"&gt;Merit&lt;/a&gt;) @ 11am&lt;/p&gt;\n\n&lt;p&gt;Charlie will share what he&amp;#39;s learned after running dbt core on Airflow in production for 2 years.&lt;/p&gt;\n\n&lt;p&gt;Topics he&amp;#39;ll be covering include:&lt;/p&gt;\n\n&lt;p&gt;- Isolating dbt core executions and avoiding python dependency hell with Kubernetes Pod Operators&lt;br/&gt;\n- Making dbt DAGs DRY-er with re-usable Airflow templates&lt;br/&gt;\n- Cutting ~30 seconds off every DAG execution with pre-compilation (when I realized we could do this, I literally :man-facepalming: for not realizing it sooner - so much wasted compute!)&lt;/p&gt;\n\n&lt;p&gt;Sign up &lt;a href=\"https://www.operationalanalytics.club/events/running-dbt-core-on-airflow-in-production-learnings-from-2-years-of-battle-scars\"&gt;here&lt;/a&gt;!   &lt;/p&gt;\n\n&lt;p&gt;PS: &lt;/p&gt;\n\n&lt;p&gt;- If you want to attend but cant, no worries! I&amp;#39;ll share recording w/ all signups&lt;br/&gt;\n- No SaaS is being sold during this event. It&amp;#39;s strictly educational for ppl interested in running dbt core on airflow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12rehq9", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rehq9/running_dbt_core_on_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rehq9/running_dbt_core_on_airflow/", "subreddit_subscribers": 100573, "created_utc": 1681873281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nI  recently got a new job as a data analyst (I have 2 YOE), however due to  personal reasons and moving. I've had to take a paycut and accept this  position. This new role, pay is not good ($71K + $8K bonus CAD).\n\nI'm  looking for some advice to transition into Data Scientist/Data Engineering positions. My goal is to stick with this position at least  1.5+ years and maybe try to progress within the company, but  realistically idk how much compensation would improve.\n\nI  have a (bachelors in ECON) &amp; Masters in stats where I learned the  math behind ML and DS procedures, however my previous role was more  focused on Azure data factory, pipelines and Power BI, so I am familiar with data engineering procedures as well (by no means a pro).\n\nAt  my current role, I use Python, SQL (Big Query), Looker and Excel,  however idk how much I can grow with just these skills as I am not doing  anything big on Python besides some data analysis. Rarely using ML models, TensorFlow, Pyspark etc.\n\nI  have a friend who is also a senior data analyst in Toronto and he's  making $94K CAD. He suggested I try to take the CFA exam and try my luck  with becoming Business Analyst within Finance for better compensation.\n\nAt this point I am not sure how to proceed with career growth, hence I am  asking here.Since you guys are in this field, I am hoping to hear your opinions  on what skills I should be accumulating to be able to apply to Data Engineering roles in about a year or so. Would  like to hear some opinions on realistic compensation for people moving into DE role from Data analyst.\n\nAt  this point I am thinking of learning more GCP (as my company uses this)  and the give the GCP associate and professional exam. Basically do more  with GCP, since I have the opportunity with the company. Learn more of  Machine Learning with GCP etc.\n\nAny advice is appreciated.\n\nThanks.\n\n(29M, in Montreal, Canada if that is relevant)", "author_fullname": "t2_44swbg2m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analyst career progression question", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r5f2e", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681853953.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;I  recently got a new job as a data analyst (I have 2 YOE), however due to  personal reasons and moving. I&amp;#39;ve had to take a paycut and accept this  position. This new role, pay is not good ($71K + $8K bonus CAD).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m  looking for some advice to transition into Data Scientist/Data Engineering positions. My goal is to stick with this position at least  1.5+ years and maybe try to progress within the company, but  realistically idk how much compensation would improve.&lt;/p&gt;\n\n&lt;p&gt;I  have a (bachelors in ECON) &amp;amp; Masters in stats where I learned the  math behind ML and DS procedures, however my previous role was more  focused on Azure data factory, pipelines and Power BI, so I am familiar with data engineering procedures as well (by no means a pro).&lt;/p&gt;\n\n&lt;p&gt;At  my current role, I use Python, SQL (Big Query), Looker and Excel,  however idk how much I can grow with just these skills as I am not doing  anything big on Python besides some data analysis. Rarely using ML models, TensorFlow, Pyspark etc.&lt;/p&gt;\n\n&lt;p&gt;I  have a friend who is also a senior data analyst in Toronto and he&amp;#39;s  making $94K CAD. He suggested I try to take the CFA exam and try my luck  with becoming Business Analyst within Finance for better compensation.&lt;/p&gt;\n\n&lt;p&gt;At this point I am not sure how to proceed with career growth, hence I am  asking here.Since you guys are in this field, I am hoping to hear your opinions  on what skills I should be accumulating to be able to apply to Data Engineering roles in about a year or so. Would  like to hear some opinions on realistic compensation for people moving into DE role from Data analyst.&lt;/p&gt;\n\n&lt;p&gt;At  this point I am thinking of learning more GCP (as my company uses this)  and the give the GCP associate and professional exam. Basically do more  with GCP, since I have the opportunity with the company. Learn more of  Machine Learning with GCP etc.&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n\n&lt;p&gt;(29M, in Montreal, Canada if that is relevant)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12r5f2e", "is_robot_indexable": true, "report_reasons": null, "author": "spicychilliwack", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r5f2e/data_analyst_career_progression_question/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12r5f2e/data_analyst_career_progression_question/", "subreddit_subscribers": 100573, "created_utc": 1681853953.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have several external sources of data where CDC, or any type of date/change monitoring is difficult. Right now, I take daily snapshots of the data (as intraday changes are irrelevant in the long run). \n\nMuch of the data is slow changing, so many of the rows are pure duplicates and rapidly growing the size of tables. Is there any reason to periodically prune and condense duplicate, historical rows or is it best to just run with the idea that storage is cheap and let it be? (data volume for me is relatively low compared to many businesses)", "author_fullname": "t2_ao7u40a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Raw data pruning: Should you do it or not?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rvewc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681915134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several external sources of data where CDC, or any type of date/change monitoring is difficult. Right now, I take daily snapshots of the data (as intraday changes are irrelevant in the long run). &lt;/p&gt;\n\n&lt;p&gt;Much of the data is slow changing, so many of the rows are pure duplicates and rapidly growing the size of tables. Is there any reason to periodically prune and condense duplicate, historical rows or is it best to just run with the idea that storage is cheap and let it be? (data volume for me is relatively low compared to many businesses)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12rvewc", "is_robot_indexable": true, "report_reasons": null, "author": "minormisgnomer", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rvewc/raw_data_pruning_should_you_do_it_or_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rvewc/raw_data_pruning_should_you_do_it_or_not/", "subreddit_subscribers": 100573, "created_utc": 1681915134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody,\n\nSo I stumbled about a curious problem I have evaded/solved, although the underlying issue still persists.\n\nFor a client we set up a small azure SQL instance and wanted to create a DWH in said instance.\nFirst of i started to stage the tables (from an sap source) and for some tables, the staging process dropped to a speed of only 100ish lines/sec while some tables were fine ( 5k lines/sec).\n\n\nThis \"slow\" behavior only seemed to happen with a few tables and only if I inserted specific columns (decimals). Sometimes this was solved by reducing the decimals to 18,2; sometimes even then the issue persisted. Even upgrading the tier did not give any Performance, I think it must be an underlying SQL topic? Exporting the data and saving, it to an XLSX.and importing it again speed up the transformation to normal speeds.\n\nDoes anybody have a clue? Cheers\n\n\nEdit 1:\n I forgot to add a crucial part, this \"behaviour\" only happens when staging into azure SQL db not in a local SQL server.\n\nEdit2:\nI checked I/O performance and we are not limited on this end as the insert runs perfectly fine with more data formatted correctly.", "author_fullname": "t2_yobj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slow inserts in azure SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rjrr7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681914105.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681886581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,&lt;/p&gt;\n\n&lt;p&gt;So I stumbled about a curious problem I have evaded/solved, although the underlying issue still persists.&lt;/p&gt;\n\n&lt;p&gt;For a client we set up a small azure SQL instance and wanted to create a DWH in said instance.\nFirst of i started to stage the tables (from an sap source) and for some tables, the staging process dropped to a speed of only 100ish lines/sec while some tables were fine ( 5k lines/sec).&lt;/p&gt;\n\n&lt;p&gt;This &amp;quot;slow&amp;quot; behavior only seemed to happen with a few tables and only if I inserted specific columns (decimals). Sometimes this was solved by reducing the decimals to 18,2; sometimes even then the issue persisted. Even upgrading the tier did not give any Performance, I think it must be an underlying SQL topic? Exporting the data and saving, it to an XLSX.and importing it again speed up the transformation to normal speeds.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have a clue? Cheers&lt;/p&gt;\n\n&lt;p&gt;Edit 1:\n I forgot to add a crucial part, this &amp;quot;behaviour&amp;quot; only happens when staging into azure SQL db not in a local SQL server.&lt;/p&gt;\n\n&lt;p&gt;Edit2:\nI checked I/O performance and we are not limited on this end as the insert runs perfectly fine with more data formatted correctly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12rjrr7", "is_robot_indexable": true, "report_reasons": null, "author": "lschozar", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rjrr7/slow_inserts_in_azure_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rjrr7/slow_inserts_in_azure_sql/", "subreddit_subscribers": 100573, "created_utc": 1681886581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a DE in an industry where we deal with large volumes of non-standardized data from many sources. One task involves classifying this data based on relevant attributes. The current process is trained on our data but is less than optimal and has a fair amount of manual work needing to be done once the process does a lot of the heavy lifting for us.\n\nI am in search of a solution that can help further automate this process. Ideally it could be trained on our data and continuously improve and refine classification accuracy as more data is ingested and processed. Even better if it could be integrated with Azure Data Factory.\n\nWith such a rapidly changing AI-tool industry emerging, it wouldn't surprise me if there was already a good solution out there. Maybe even the GPT-API could be leveraged? Thank you in advance.", "author_fullname": "t2_u1gwf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any AI tools available to leverage for classification of non-standardized data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r1v7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681847254.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a DE in an industry where we deal with large volumes of non-standardized data from many sources. One task involves classifying this data based on relevant attributes. The current process is trained on our data but is less than optimal and has a fair amount of manual work needing to be done once the process does a lot of the heavy lifting for us.&lt;/p&gt;\n\n&lt;p&gt;I am in search of a solution that can help further automate this process. Ideally it could be trained on our data and continuously improve and refine classification accuracy as more data is ingested and processed. Even better if it could be integrated with Azure Data Factory.&lt;/p&gt;\n\n&lt;p&gt;With such a rapidly changing AI-tool industry emerging, it wouldn&amp;#39;t surprise me if there was already a good solution out there. Maybe even the GPT-API could be leveraged? Thank you in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12r1v7y", "is_robot_indexable": true, "report_reasons": null, "author": "avibomb", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r1v7y/any_ai_tools_available_to_leverage_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12r1v7y/any_ai_tools_available_to_leverage_for/", "subreddit_subscribers": 100573, "created_utc": 1681847254.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forreal though", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 112, "top_awarded_type": null, "hide_score": true, "name": "t3_12s61tg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/hhW7P67ojFayqH8ZoHBF4Hs0VSbio8Gggz1u-5kDLhM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681929938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/485lt7l8hxua1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/485lt7l8hxua1.jpg?auto=webp&amp;v=enabled&amp;s=3d4d6027980f1beb46c7aed23d9d968a8bdcca1f", "width": 620, "height": 497}, "resolutions": [{"url": "https://preview.redd.it/485lt7l8hxua1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9a5a0b374b489cf700f31b975d2d6cc926791b5", "width": 108, "height": 86}, {"url": "https://preview.redd.it/485lt7l8hxua1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1189813beb8f94bfbc53a3192d32d9a596366eda", "width": 216, "height": 173}, {"url": "https://preview.redd.it/485lt7l8hxua1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c85db3b558372b06e69b10cbfa853ede801e7eb9", "width": 320, "height": 256}], "variants": {}, "id": "Smc0smfAG_wmqmlsr5jzBbONBfm_lWj8YRWvXJZJ0PU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12s61tg", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s61tg/forreal_though/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/485lt7l8hxua1.jpg", "subreddit_subscribers": 100573, "created_utc": 1681929938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Apologies if this is against the rules (it doesn't seem to be) - I have posted this question in r/learnpython, but I think perhaps it is a little too niche/nonsensical to fit the purview of the sub.\n\nI have been using pyArrow's `parquet.write_to_dataset()` function to produce a partitioned parquet dataset - the initial plan was to use Spark, but I consistently ended up with Java heap memory errors even when running on DataBricks, so switched to Arrow and it does the job just fine for my needs. However, I'm now looking to encrypt some columns, and it looks like I've hit a brick wall.\n\nI can encrypt columns well enough with `pyArrow.parquet.ParquetWriter()`, but this only allows writing a single table with `write_table` \\- no partitioned datasets here. Is there something I'm missing? From [the docs](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html) it looks like there are no encryption parameters available for `parquet.write_to_dataset()` or `dataset.write_dataset(),` but presumably these functions are just wrappers around the ParquetWriter's functionality, so is there a work around? I'm reaching the conclusion that I will have to roll my own partitioning for the encrypted data, but I'm concerned that will mess with querying via frameworks like DuckDB.\n\nAny help or suggestion is appreciated!\n\nFor a little more context, the code I am using to write out to dataset is along these lines:\n\n    def writeToParquet_arrow(pandas_frame, blob_folder='parquet_store_arrow'):\n        import pandas as pd\n        import pyarrow as pa\n        import pyarrow.parquet as pq\n    \n        adf = pa.Table.from_pandas(pandas_frame)\n        pq.write_to_dataset(adf, path, partition_cols=['c'], compression='gzip', compression_level=9)\n    \n    pandas_table = pd.DataFrame([{'a': 'hello', 'b': 'goodbye', 'c': 'womp'}])\n    writeToParquet_arrow(pandas_frame=pandas_table)\n\nFairly straight forward stuff. The code that I've been playing around with to test write\\_table encryption is somewhat more involved, this is set up as per [the docs](https://arrow.apache.org/docs/python/parquet.html#parquet-modular-encryption-columnar-encryption) and [this git example.](https://github.com/apache/arrow/blob/45918a90a6ca1cf3fd67c256a7d6a240249e555a/python/pyarrow/tests/parquet/test_encryption.py#L68-L98)\n\n    # Trying to setup encryption on Arrow without a key vault.\n    from datetime import timedelta\n    import pyarrow as pa\n    import pyarrow.parquet as pq\n    import pyarrow.parquet.encryption as pe\n    import pandas as pd\n    from pyarrow.tests.parquet.encryption import InMemoryKmsClient\n    \n    # Keys and key names set up to ape output from a vault.\n    PARQUET_NAME = 'encrypted_table.parquet'\n    FOOTER_KEY = b\"0123456789112345\"\n    FOOTER_KEY_NAME = \"footer_key\"\n    COL_KEY = b\"1234567890123450\"\n    COL_KEY_NAME = \"col_key\"\n    \n    table = pa.Table.from_pydict([{'a': 'hello', 'b': 'goodbye', 'c': 'womp'}])\n    \n    # Encrypt the footer with the footer key,\n    # encrypt column `a` and column `b` with another key,\n    # keep `c` plaintext\n    encryption_config = pe.EncryptionConfiguration(\n        footer_key=FOOTER_KEY_NAME,\n        column_keys={\n            COL_KEY_NAME: [\"a\", \"b\"],\n        },\n        encryption_algorithm=\"AES_GCM_V1\",\n        cache_lifetime=timedelta(minutes=5.0),\n        data_key_length_bits=256)\n    \n    kms_connection_config = pe.KmsConnectionConfig(\n        custom_kms_conf={\n            FOOTER_KEY_NAME: FOOTER_KEY.decode(\"UTF-8\"),\n            COL_KEY_NAME: COL_KEY.decode(\"UTF-8\"),\n        }\n    )\n    \n    def kms_factory(kms_connection_configuration):\n        return InMemoryKmsClient(kms_connection_configuration)\n    \n    crypto_factory = pe.CryptoFactory(kms_factory)\n    \n    file_encryption_properties = crypto_factory.file_encryption_properties(\n        kms_connection_config, encryption_config)\n    \n    with pq.ParquetWriter(path, table.schema, encryption_properties=file_encryption_properties) as writer:\n        writer.write_table(table)", "author_fullname": "t2_qid87sfh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Encrypting Arrow Datasets without Losing Functionality", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rt0ls", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681910311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies if this is against the rules (it doesn&amp;#39;t seem to be) - I have posted this question in &lt;a href=\"/r/learnpython\"&gt;r/learnpython&lt;/a&gt;, but I think perhaps it is a little too niche/nonsensical to fit the purview of the sub.&lt;/p&gt;\n\n&lt;p&gt;I have been using pyArrow&amp;#39;s &lt;code&gt;parquet.write_to_dataset()&lt;/code&gt; function to produce a partitioned parquet dataset - the initial plan was to use Spark, but I consistently ended up with Java heap memory errors even when running on DataBricks, so switched to Arrow and it does the job just fine for my needs. However, I&amp;#39;m now looking to encrypt some columns, and it looks like I&amp;#39;ve hit a brick wall.&lt;/p&gt;\n\n&lt;p&gt;I can encrypt columns well enough with &lt;code&gt;pyArrow.parquet.ParquetWriter()&lt;/code&gt;, but this only allows writing a single table with &lt;code&gt;write_table&lt;/code&gt; - no partitioned datasets here. Is there something I&amp;#39;m missing? From &lt;a href=\"https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html\"&gt;the docs&lt;/a&gt; it looks like there are no encryption parameters available for &lt;code&gt;parquet.write_to_dataset()&lt;/code&gt; or &lt;code&gt;dataset.write_dataset(),&lt;/code&gt; but presumably these functions are just wrappers around the ParquetWriter&amp;#39;s functionality, so is there a work around? I&amp;#39;m reaching the conclusion that I will have to roll my own partitioning for the encrypted data, but I&amp;#39;m concerned that will mess with querying via frameworks like DuckDB.&lt;/p&gt;\n\n&lt;p&gt;Any help or suggestion is appreciated!&lt;/p&gt;\n\n&lt;p&gt;For a little more context, the code I am using to write out to dataset is along these lines:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def writeToParquet_arrow(pandas_frame, blob_folder=&amp;#39;parquet_store_arrow&amp;#39;):\n    import pandas as pd\n    import pyarrow as pa\n    import pyarrow.parquet as pq\n\n    adf = pa.Table.from_pandas(pandas_frame)\n    pq.write_to_dataset(adf, path, partition_cols=[&amp;#39;c&amp;#39;], compression=&amp;#39;gzip&amp;#39;, compression_level=9)\n\npandas_table = pd.DataFrame([{&amp;#39;a&amp;#39;: &amp;#39;hello&amp;#39;, &amp;#39;b&amp;#39;: &amp;#39;goodbye&amp;#39;, &amp;#39;c&amp;#39;: &amp;#39;womp&amp;#39;}])\nwriteToParquet_arrow(pandas_frame=pandas_table)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Fairly straight forward stuff. The code that I&amp;#39;ve been playing around with to test write_table encryption is somewhat more involved, this is set up as per &lt;a href=\"https://arrow.apache.org/docs/python/parquet.html#parquet-modular-encryption-columnar-encryption\"&gt;the docs&lt;/a&gt; and &lt;a href=\"https://github.com/apache/arrow/blob/45918a90a6ca1cf3fd67c256a7d6a240249e555a/python/pyarrow/tests/parquet/test_encryption.py#L68-L98\"&gt;this git example.&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Trying to setup encryption on Arrow without a key vault.\nfrom datetime import timedelta\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport pyarrow.parquet.encryption as pe\nimport pandas as pd\nfrom pyarrow.tests.parquet.encryption import InMemoryKmsClient\n\n# Keys and key names set up to ape output from a vault.\nPARQUET_NAME = &amp;#39;encrypted_table.parquet&amp;#39;\nFOOTER_KEY = b&amp;quot;0123456789112345&amp;quot;\nFOOTER_KEY_NAME = &amp;quot;footer_key&amp;quot;\nCOL_KEY = b&amp;quot;1234567890123450&amp;quot;\nCOL_KEY_NAME = &amp;quot;col_key&amp;quot;\n\ntable = pa.Table.from_pydict([{&amp;#39;a&amp;#39;: &amp;#39;hello&amp;#39;, &amp;#39;b&amp;#39;: &amp;#39;goodbye&amp;#39;, &amp;#39;c&amp;#39;: &amp;#39;womp&amp;#39;}])\n\n# Encrypt the footer with the footer key,\n# encrypt column `a` and column `b` with another key,\n# keep `c` plaintext\nencryption_config = pe.EncryptionConfiguration(\n    footer_key=FOOTER_KEY_NAME,\n    column_keys={\n        COL_KEY_NAME: [&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;],\n    },\n    encryption_algorithm=&amp;quot;AES_GCM_V1&amp;quot;,\n    cache_lifetime=timedelta(minutes=5.0),\n    data_key_length_bits=256)\n\nkms_connection_config = pe.KmsConnectionConfig(\n    custom_kms_conf={\n        FOOTER_KEY_NAME: FOOTER_KEY.decode(&amp;quot;UTF-8&amp;quot;),\n        COL_KEY_NAME: COL_KEY.decode(&amp;quot;UTF-8&amp;quot;),\n    }\n)\n\ndef kms_factory(kms_connection_configuration):\n    return InMemoryKmsClient(kms_connection_configuration)\n\ncrypto_factory = pe.CryptoFactory(kms_factory)\n\nfile_encryption_properties = crypto_factory.file_encryption_properties(\n    kms_connection_config, encryption_config)\n\nwith pq.ParquetWriter(path, table.schema, encryption_properties=file_encryption_properties) as writer:\n    writer.write_table(table)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12rt0ls", "is_robot_indexable": true, "report_reasons": null, "author": "Alwaysragestillplay", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rt0ls/encrypting_arrow_datasets_without_losing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rt0ls/encrypting_arrow_datasets_without_losing/", "subreddit_subscribers": 100573, "created_utc": 1681910311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Transform data in your pipelines or in the database? The debate has been going on for decades. This Thursday 20th April, join Altinity and Rudderstack as we discuss the strengths of each approach, using real-time loading to ClickHouse as an example. Your best bet is to combine both to transform data most efficiently. Check this link to learn more!  [https://hubs.la/Q01J91nD0](https://hubs.la/Q01J91nD0)", "author_fullname": "t2_s3zu6zpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[WEBINAR] ETL vs ELT Cage Fight: Combining RudderStack and ClickHouse to Build Real-Time Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r8muj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681860468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Transform data in your pipelines or in the database? The debate has been going on for decades. This Thursday 20th April, join Altinity and Rudderstack as we discuss the strengths of each approach, using real-time loading to ClickHouse as an example. Your best bet is to combine both to transform data most efficiently. Check this link to learn more!  &lt;a href=\"https://hubs.la/Q01J91nD0\"&gt;https://hubs.la/Q01J91nD0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?auto=webp&amp;v=enabled&amp;s=2aaafe86918fe6f01e38bb8c371c4500d14c9215", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b329cc1fb821a4a2f31b8d86e33036ef33322768", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c62de98036de4ecaffc53931e5a510436884c97", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=234476154d69fd90eaca9958ad131e739cef4ab3", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8012513902430fec2a54b2b1e4a2210741fb2896", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af044e773e0424067adc364371f3dacf9a172cac", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b062b69f20ea40d01ef247d262755655c4f5950b", "width": 1080, "height": 607}], "variants": {}, "id": "B4-A4qYuf1HlA0IuVcTe3n2oeyZ6_zaXRft1KoWtMAk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12r8muj", "is_robot_indexable": true, "report_reasons": null, "author": "RyhanSunny_Altinity", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r8muj/webinar_etl_vs_elt_cage_fight_combining/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12r8muj/webinar_etl_vs_elt_cage_fight_combining/", "subreddit_subscribers": 100573, "created_utc": 1681860468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Virtual Data Environments: Develop data pipelines more accurately and efficiently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r32t0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681849470.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tobikodata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tobikodata.com/virtual-data-environments.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12r32t0", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r32t0/virtual_data_environments_develop_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tobikodata.com/virtual-data-environments.html", "subreddit_subscribers": 100573, "created_utc": 1681849470.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Obviously depends on your query patterns but I feel that simple things like date (creation/ingestion), unique IDs, etc are almost always useful and its best to have em than to need to build a new index once the DB is 500gb.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What indexes do you (almost) always set?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12s441p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681926052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously depends on your query patterns but I feel that simple things like date (creation/ingestion), unique IDs, etc are almost always useful and its best to have em than to need to build a new index once the DB is 500gb.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12s441p", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s441p/what_indexes_do_you_almost_always_set/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12s441p/what_indexes_do_you_almost_always_set/", "subreddit_subscribers": 100573, "created_utc": 1681926052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some of your best practices or fastest ways to get familiar with a database architecture when joining a new team/project? \n\nDo you start querying and playing around with different connections or are there any faster methods", "author_fullname": "t2_5x9e117l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get familiar with database architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12s0pvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681920744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some of your best practices or fastest ways to get familiar with a database architecture when joining a new team/project? &lt;/p&gt;\n\n&lt;p&gt;Do you start querying and playing around with different connections or are there any faster methods&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12s0pvx", "is_robot_indexable": true, "report_reasons": null, "author": "readoyniando", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s0pvx/get_familiar_with_database_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12s0pvx/get_familiar_with_database_architecture/", "subreddit_subscribers": 100573, "created_utc": 1681920744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Very new DE here. I have been asked to update a table in BQ based on changes to a gsheet and vice versa. Is there already the functionality to do this in GCP or will it have to be coded. \n\nThank you in advance!", "author_fullname": "t2_3fxv004y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to link a BigQuery table to a Google Sheet containing the same table and have them bi-directionally update?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rzqey", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681919865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Very new DE here. I have been asked to update a table in BQ based on changes to a gsheet and vice versa. Is there already the functionality to do this in GCP or will it have to be coded. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12rzqey", "is_robot_indexable": true, "report_reasons": null, "author": "J1010H", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rzqey/is_it_possible_to_link_a_bigquery_table_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rzqey/is_it_possible_to_link_a_bigquery_table_to_a/", "subreddit_subscribers": 100573, "created_utc": 1681919865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for reading materials and python libraries that can help me with ingesting and making sense of large amount of log files coming from various devices. What would you recommend? \n\nI have the unfriendly gut feeling that i cant avoid regex, but is there any good libraries for this? So parsing, etc.", "author_fullname": "t2_hp7r8vez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting, parsing and making sense of device log data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rpsyx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681903163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for reading materials and python libraries that can help me with ingesting and making sense of large amount of log files coming from various devices. What would you recommend? &lt;/p&gt;\n\n&lt;p&gt;I have the unfriendly gut feeling that i cant avoid regex, but is there any good libraries for this? So parsing, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12rpsyx", "is_robot_indexable": true, "report_reasons": null, "author": "Labanc_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12rpsyx/ingesting_parsing_and_making_sense_of_device_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rpsyx/ingesting_parsing_and_making_sense_of_device_log/", "subreddit_subscribers": 100573, "created_utc": 1681903163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm pretty fed up with airbyte cli right now\n\nusing dagster (python scripts + dbt + airbyte integration)\n\nhow can i replace airbyte in my stack quickly (i dont mind making the connections again from scratch) and what tool should I use. Ideally still open-source. Some wants: postgres connector, bigquery connector, google analytics (both gua &amp; ga4) connectors\n\nairflow?\n\n&amp;#x200B;\n\ndeployed w/ docker on digital ocean", "author_fullname": "t2_vmsi7zgn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "quickly replace a small airbyte instance in my stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r6r06", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681856578.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m pretty fed up with airbyte cli right now&lt;/p&gt;\n\n&lt;p&gt;using dagster (python scripts + dbt + airbyte integration)&lt;/p&gt;\n\n&lt;p&gt;how can i replace airbyte in my stack quickly (i dont mind making the connections again from scratch) and what tool should I use. Ideally still open-source. Some wants: postgres connector, bigquery connector, google analytics (both gua &amp;amp; ga4) connectors&lt;/p&gt;\n\n&lt;p&gt;airflow?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;deployed w/ docker on digital ocean&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12r6r06", "is_robot_indexable": true, "report_reasons": null, "author": "Derpschitzenstein", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r6r06/quickly_replace_a_small_airbyte_instance_in_my/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12r6r06/quickly_replace_a_small_airbyte_instance_in_my/", "subreddit_subscribers": 100573, "created_utc": 1681856578.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello All - \n\nJust seeking some advice for a project i have and the best way to accomplish said project as far as the architectural standpoint.\n\n\nMy Platform is Azure, I have incoming Data from various sources in a very specific format ( For my Sql Tables). I'm looking to Have these files (Daily,Weekly,Monthly) sent to me Via FTP which will connect to my Azure Storage to be ingested.\n\nNow i need to Ingest these Files, Transform the contents ( Maybe i dont because i have a specific format) and get them into my DB to which my tables are used for the contents of  a Web App.\n\nI hope this makes sense, So far i have been looking into  Azure Data Factor, Databricks, airbyte. Any help would be apprciated.", "author_fullname": "t2_9icde7jgx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seeking advice on Data Infrastructure", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r4ine", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681852207.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello All - &lt;/p&gt;\n\n&lt;p&gt;Just seeking some advice for a project i have and the best way to accomplish said project as far as the architectural standpoint.&lt;/p&gt;\n\n&lt;p&gt;My Platform is Azure, I have incoming Data from various sources in a very specific format ( For my Sql Tables). I&amp;#39;m looking to Have these files (Daily,Weekly,Monthly) sent to me Via FTP which will connect to my Azure Storage to be ingested.&lt;/p&gt;\n\n&lt;p&gt;Now i need to Ingest these Files, Transform the contents ( Maybe i dont because i have a specific format) and get them into my DB to which my tables are used for the contents of  a Web App.&lt;/p&gt;\n\n&lt;p&gt;I hope this makes sense, So far i have been looking into  Azure Data Factor, Databricks, airbyte. Any help would be apprciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12r4ine", "is_robot_indexable": true, "report_reasons": null, "author": "iConceptz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r4ine/seeking_advice_on_data_infrastructure/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12r4ine/seeking_advice_on_data_infrastructure/", "subreddit_subscribers": 100573, "created_utc": 1681852207.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_oy8bkrnq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building real-time solutions with Snowflake at a fraction of the cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12ru22b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yHjxOiD1-HzypQ4XXoZwiZxV9-izJXzklGzliKklbFU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681912440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tinybird.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tinybird.co/blog-posts/real-time-solutions-with-snowflake", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?auto=webp&amp;v=enabled&amp;s=c7981342e41935be9fe03de0c2cf0ac7626cce2d", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e3d615e334496c4f9af1eb713bc44873aba3abd", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bde4509df996abb62e7c37edd96f2e5bddd79892", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=261028432fd390795adafc54036cea768f22a6d0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b03275d14ea07785a1ac2db454871036e34fecd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5b8d93caca2b1dbd4fcffc0f334bab3c0d5714f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b5eee295d2f59751e5124ee897a3d909a660b9a", "width": 1080, "height": 567}], "variants": {}, "id": "ZN6KBbwbsvWox2-xxEY_L7jyiNubj-a3WNaKG9G0RoE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ru22b", "is_robot_indexable": true, "report_reasons": null, "author": "tinybirdco", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ru22b/building_realtime_solutions_with_snowflake_at_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tinybird.co/blog-posts/real-time-solutions-with-snowflake", "subreddit_subscribers": 100573, "created_utc": 1681912440.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}