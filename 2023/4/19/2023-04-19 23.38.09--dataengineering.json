{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_56wplwbg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Zillacode Premium finally done, Leetcode for PySpark, Spark and Pandas at Zillacode.com", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 72, "top_awarded_type": null, "hide_score": false, "name": "t3_12r8x5c", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "ups": 122, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 122, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/5GgSDcLT9bjG0kduOq9T3QHbNjaWdQE6153wBY49OPY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681861078.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/64htp9vsaqua1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?auto=webp&amp;v=enabled&amp;s=b88bf5a671b0024424f3dcd8d9823e4889dabada", "width": 1797, "height": 937}, "resolutions": [{"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=923ab6f291c815ca616d00ebd156d83c7a2c1c4a", "width": 108, "height": 56}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e6b0f2875131323d5d8633f53e9d6253508a3cf", "width": 216, "height": 112}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c34378bd9dc47f7a194c04d9103b7e54502ff8ae", "width": 320, "height": 166}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0823b0fb72fe90df9f44688f808203be5559aa7", "width": 640, "height": 333}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=815573b2f8dc9e0a65d62a9b5bc47a6e92f6a7a6", "width": 960, "height": 500}, {"url": "https://preview.redd.it/64htp9vsaqua1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5b53615dfbfca94a9fca1d332436197248dfaade", "width": 1080, "height": 563}], "variants": {}, "id": "qQFgHmHUcppe72_LgmE5jf0imJJkH6p5Fhx13osQ5GM"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12r8x5c", "is_robot_indexable": true, "report_reasons": null, "author": "dmage5000", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r8x5c/zillacode_premium_finally_done_leetcode_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/64htp9vsaqua1.jpg", "subreddit_subscribers": 100630, "created_utc": 1681861078.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_r8dyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Forreal though", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 112, "top_awarded_type": null, "hide_score": false, "name": "t3_12s61tg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "https://b.thumbs.redditmedia.com/hhW7P67ojFayqH8ZoHBF4Hs0VSbio8Gggz1u-5kDLhM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681929938.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/485lt7l8hxua1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/485lt7l8hxua1.jpg?auto=webp&amp;v=enabled&amp;s=3d4d6027980f1beb46c7aed23d9d968a8bdcca1f", "width": 620, "height": 497}, "resolutions": [{"url": "https://preview.redd.it/485lt7l8hxua1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b9a5a0b374b489cf700f31b975d2d6cc926791b5", "width": 108, "height": 86}, {"url": "https://preview.redd.it/485lt7l8hxua1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1189813beb8f94bfbc53a3192d32d9a596366eda", "width": 216, "height": 173}, {"url": "https://preview.redd.it/485lt7l8hxua1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c85db3b558372b06e69b10cbfa853ede801e7eb9", "width": 320, "height": 256}], "variants": {}, "id": "Smc0smfAG_wmqmlsr5jzBbONBfm_lWj8YRWvXJZJ0PU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12s61tg", "is_robot_indexable": true, "report_reasons": null, "author": "BoiElroy", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s61tg/forreal_though/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/485lt7l8hxua1.jpg", "subreddit_subscribers": 100630, "created_utc": 1681929938.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "* Joe Reiss and Matthew Housley chatting to Felix GV about Venice and Designing Massive Distributed Systems at LinkedIn [https://overcast.fm/+wcMoWgOXw](https://overcast.fm/+wcMoWgOXw)\n* Benn Stancil's talk from Coalesce 2021 about the Modern Data Experience: [https://overcast.fm/+w94XLSJUU](https://overcast.fm/+w94XLSJUU)\n* Joe Reiss and Matthew Housley talk to Neelesh Salian about building data infrastructure [https://overcast.fm/+wcMp5SpsI](https://overcast.fm/+wcMp5SpsI)\n* Jean-Georges Perrin on DataEngPodcast talking about DataMesh in real life (not theoretical!) at PayPal [https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364](https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364)\n* Maggie Hays and Pete Soderling on DataEngPodcast talking about DataCouncil and building an intentional data culture [https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365](https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365)\n* Mark Rittman talking on DrilltoDetail with Stewart Bryson, Keenan Rice, and Jake Stein about the Modern Data Stack past, present &amp; future  [https://overcast.fm/+Iexdb0bH0](https://overcast.fm/+Iexdb0bH0)\n* Tristan Handy and Julia Schottenstein talking to Justin Borgman about Starburst, TrinoDB, and data access patterns and querying [https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data](https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data)\n\nWhat are your favourite episodes that you'd like to share with others?", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "\ud83c\udfa7 Random grab-bag of podcast episodes that I've enjoyed recently", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rockb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681899465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;Joe Reiss and Matthew Housley chatting to Felix GV about Venice and Designing Massive Distributed Systems at LinkedIn &lt;a href=\"https://overcast.fm/+wcMoWgOXw\"&gt;https://overcast.fm/+wcMoWgOXw&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Benn Stancil&amp;#39;s talk from Coalesce 2021 about the Modern Data Experience: &lt;a href=\"https://overcast.fm/+w94XLSJUU\"&gt;https://overcast.fm/+w94XLSJUU&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Joe Reiss and Matthew Housley talk to Neelesh Salian about building data infrastructure &lt;a href=\"https://overcast.fm/+wcMp5SpsI\"&gt;https://overcast.fm/+wcMp5SpsI&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Jean-Georges Perrin on DataEngPodcast talking about DataMesh in real life (not theoretical!) at PayPal &lt;a href=\"https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364\"&gt;https://www.dataengineeringpodcast.com/building-a-data-mesh-at-paypal-episode-364&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Maggie Hays and Pete Soderling on DataEngPodcast talking about DataCouncil and building an intentional data culture &lt;a href=\"https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365\"&gt;https://www.dataengineeringpodcast.com/data-council-data-culture-track-episode-365&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Mark Rittman talking on DrilltoDetail with Stewart Bryson, Keenan Rice, and Jake Stein about the Modern Data Stack past, present &amp;amp; future  &lt;a href=\"https://overcast.fm/+Iexdb0bH0\"&gt;https://overcast.fm/+Iexdb0bH0&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Tristan Handy and Julia Schottenstein talking to Justin Borgman about Starburst, TrinoDB, and data access patterns and querying &lt;a href=\"https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data\"&gt;https://roundup.getdbt.com/p/ep-27-to-move-or-not-to-move-data&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What are your favourite episodes that you&amp;#39;d like to share with others?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12rockb", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rockb/random_grabbag_of_podcast_episodes_that_ive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rockb/random_grabbag_of_podcast_episodes_that_ive/", "subreddit_subscribers": 100630, "created_utc": 1681899465.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a small startup and we\u2019re reaching the point where our database is getting hammered by the amount of custom client reports we send. I\u2019m a junior dev, but am extremely interested in how to help develop a sustainable solution. We currently use an AWS hosted Postgres db, and run a ton of reports out of Looker. \n\nAre there any good resources to learn about standing something like this up? Thanks in advance!", "author_fullname": "t2_5am908px", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is a good resource to learn how to set up a Redshift warehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rueol", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681913133.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a small startup and we\u2019re reaching the point where our database is getting hammered by the amount of custom client reports we send. I\u2019m a junior dev, but am extremely interested in how to help develop a sustainable solution. We currently use an AWS hosted Postgres db, and run a ton of reports out of Looker. &lt;/p&gt;\n\n&lt;p&gt;Are there any good resources to learn about standing something like this up? Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12rueol", "is_robot_indexable": true, "report_reasons": null, "author": "iambatmanman", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rueol/what_is_a_good_resource_to_learn_how_to_set_up_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rueol/what_is_a_good_resource_to_learn_how_to_set_up_a/", "subreddit_subscribers": 100630, "created_utc": 1681913133.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We've sidestepped tradition and built our peer agendas based on what's important to those who provide feedback on our summits. Data Stack Summit is virtual and kicks off tomorrow, 4/19 at 8:00 AM PST. Registration is free at [datastacksummit.com](https://datastacksummit.com) \n\nSessions will be available on-demand for registrants post-event and we've enabled attendance certificates for those who need them for their employers.\n\nHere's a breakdown of the full agenda:\n\n8:05 AM PST - Keynote Panel: Peer-to-peer panel about managing cloud costs right now with Mike Fuller (FinOps Foundation), Joseph Machado (LinkedIn), Vikas Ranjan (T-Mobile), and Carlos Costa (Adidas)\n\n8:40 AM PST - Keynote: Modernizing the data stack - keeping it real with Mark Mullins (United Community Bank) and Raj Joseph (DQ Labs)\n\n9:15 AM PST - Breakout Session: From complex to simplicity: Our DataOps journey with Jennifer Romero-Higgins (American Airlines) \n\n9:15 AM PST - Breakout Session: An ever-increasing need for data quality with Dr. Rajkumar Bhojan (Fidelity)\n\n9:50 AM PST - Breakout Session: Turning your data lake into an asset with Bill Inmon (Forest Rim Technology)\n\n9:50 AM PST - Breakout Session: Maintaining price and performance SLAs across engineering teams with Mark Kidwell (Autodesk)\n\n10:25 AM PST - Breakout Session: A look at Walmart\u2019s self-service metadata-driven data loader framework with Manimuthu Aayyannan and Subramanya Mulgund (Walmart)\n\n10:25 AM PST - Breakout Session: YARN to Kubernetes: Modernizing big data workloads on a massive scale with Vikas Ranjan (T-Mobile)\n\n11:00 AM PST - Peer-to-Peer Panel: Enabling the analytics end user with Nicole Radziwill (Ultranauts), Sangeeta Krishnan (Bayer), Jess Ramos (Crunchbase), and Sunny Zhu (Indeed)\n\n11:35 AM PST - Breakout Session: Building a business-critical data platform to process over \u00a334bn in card transactions with Sandeep Mehta (Dojo)\n\n11:35 AM PST - Breakout Session: NLP &amp; ML data-driven decision making: Taming the curriculum beast with Joel Hernandez (eLumen) and Carlos Rodriguez (MentorMate)\n\n12:10 PM PST - Breakout Session: Is synthetic data useful for data engineers? with Dr. Alexander Mikhalev (Applied Knowledge Systems) and Matthew Norton (Nationwide)\n\n12:10 PM PST - Breakout Session: The great debate of data quality vs. data observability with Olga Maydanchik (Voya Financial) and Raj Joseph (DQ Labs)\n\n12:45 PM PST - Breakout Session: DataOps teams: Stop sprinting! With Monica Kay Royal (Nerd Nourishment)\n\n12:45 PM PST - Breakout Session: Designing a modern customer data center of excellence with Ted Sfikas (Tealium)", "author_fullname": "t2_ff7f8okm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hope you're able to support our peer-built agenda tomorrow at Data Stack Summit (virtual data community conference)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r93d3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681861456.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve sidestepped tradition and built our peer agendas based on what&amp;#39;s important to those who provide feedback on our summits. Data Stack Summit is virtual and kicks off tomorrow, 4/19 at 8:00 AM PST. Registration is free at &lt;a href=\"https://datastacksummit.com\"&gt;datastacksummit.com&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;Sessions will be available on-demand for registrants post-event and we&amp;#39;ve enabled attendance certificates for those who need them for their employers.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a breakdown of the full agenda:&lt;/p&gt;\n\n&lt;p&gt;8:05 AM PST - Keynote Panel: Peer-to-peer panel about managing cloud costs right now with Mike Fuller (FinOps Foundation), Joseph Machado (LinkedIn), Vikas Ranjan (T-Mobile), and Carlos Costa (Adidas)&lt;/p&gt;\n\n&lt;p&gt;8:40 AM PST - Keynote: Modernizing the data stack - keeping it real with Mark Mullins (United Community Bank) and Raj Joseph (DQ Labs)&lt;/p&gt;\n\n&lt;p&gt;9:15 AM PST - Breakout Session: From complex to simplicity: Our DataOps journey with Jennifer Romero-Higgins (American Airlines) &lt;/p&gt;\n\n&lt;p&gt;9:15 AM PST - Breakout Session: An ever-increasing need for data quality with Dr. Rajkumar Bhojan (Fidelity)&lt;/p&gt;\n\n&lt;p&gt;9:50 AM PST - Breakout Session: Turning your data lake into an asset with Bill Inmon (Forest Rim Technology)&lt;/p&gt;\n\n&lt;p&gt;9:50 AM PST - Breakout Session: Maintaining price and performance SLAs across engineering teams with Mark Kidwell (Autodesk)&lt;/p&gt;\n\n&lt;p&gt;10:25 AM PST - Breakout Session: A look at Walmart\u2019s self-service metadata-driven data loader framework with Manimuthu Aayyannan and Subramanya Mulgund (Walmart)&lt;/p&gt;\n\n&lt;p&gt;10:25 AM PST - Breakout Session: YARN to Kubernetes: Modernizing big data workloads on a massive scale with Vikas Ranjan (T-Mobile)&lt;/p&gt;\n\n&lt;p&gt;11:00 AM PST - Peer-to-Peer Panel: Enabling the analytics end user with Nicole Radziwill (Ultranauts), Sangeeta Krishnan (Bayer), Jess Ramos (Crunchbase), and Sunny Zhu (Indeed)&lt;/p&gt;\n\n&lt;p&gt;11:35 AM PST - Breakout Session: Building a business-critical data platform to process over \u00a334bn in card transactions with Sandeep Mehta (Dojo)&lt;/p&gt;\n\n&lt;p&gt;11:35 AM PST - Breakout Session: NLP &amp;amp; ML data-driven decision making: Taming the curriculum beast with Joel Hernandez (eLumen) and Carlos Rodriguez (MentorMate)&lt;/p&gt;\n\n&lt;p&gt;12:10 PM PST - Breakout Session: Is synthetic data useful for data engineers? with Dr. Alexander Mikhalev (Applied Knowledge Systems) and Matthew Norton (Nationwide)&lt;/p&gt;\n\n&lt;p&gt;12:10 PM PST - Breakout Session: The great debate of data quality vs. data observability with Olga Maydanchik (Voya Financial) and Raj Joseph (DQ Labs)&lt;/p&gt;\n\n&lt;p&gt;12:45 PM PST - Breakout Session: DataOps teams: Stop sprinting! With Monica Kay Royal (Nerd Nourishment)&lt;/p&gt;\n\n&lt;p&gt;12:45 PM PST - Breakout Session: Designing a modern customer data center of excellence with Ted Sfikas (Tealium)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?auto=webp&amp;v=enabled&amp;s=bad6e740602f1662394b1fe7dabd8981683aa1fb", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=161039fc3821f7cd1dfa39f0585f770c15bc946a", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73294167473cedd820bcad18a90c58650e4e224d", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=76fdcab5605a3d9b860f5c97868c51c45dad182d", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af3928fee515c51d4526f0278a87666374ae09b1", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=275fa4eb875cbf9ca0394906f24d080551eabb32", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/aQTIs2jcL52LGhuK1NQvonLRIGpjn8QPNEeMwwjWZHQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58ace0884f5f5330ea81fc7e4ec2429ab9c22d64", "width": 1080, "height": 1080}], "variants": {}, "id": "bgiUBgCfRhBtLAYa6NU8eNVDv2Bm8-xV4rWU9l-rDkI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12r93d3", "is_robot_indexable": true, "report_reasons": null, "author": "hesanastronaut", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r93d3/hope_youre_able_to_support_our_peerbuilt_agenda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12r93d3/hope_youre_able_to_support_our_peerbuilt_agenda/", "subreddit_subscribers": 100630, "created_utc": 1681861456.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you run dbt core on Airflow or are considering doing so, I'm sure you'll find something of interest in tomorrow's workshop with [Charlie Summers](https://www.linkedin.com/in/charliesummers/) (Staff SE, [Merit](https://www.linkedin.com/company/earnmerits/)) @ 11am\n\nCharlie will share what he's learned after running dbt core on Airflow in production for 2 years.\n\nTopics he'll be covering include:\n\n\\- Isolating dbt core executions and avoiding python dependency hell with Kubernetes Pod Operators  \n\\- Making dbt DAGs DRY-er with re-usable Airflow templates  \n\\- Cutting \\~30 seconds off every DAG execution with pre-compilation (when I realized we could do this, I literally :man-facepalming: for not realizing it sooner - so much wasted compute!)\n\nSign up [here](https://www.operationalanalytics.club/events/running-dbt-core-on-airflow-in-production-learnings-from-2-years-of-battle-scars)!   \n\n\nPS: \n\n\\- If you want to attend but cant, no worries! I'll share recording w/ all signups  \n\\- No SaaS is being sold during this event. It's strictly educational for ppl interested in running dbt core on airflow.", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Running dbt core on airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rehq9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681873281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you run dbt core on Airflow or are considering doing so, I&amp;#39;m sure you&amp;#39;ll find something of interest in tomorrow&amp;#39;s workshop with &lt;a href=\"https://www.linkedin.com/in/charliesummers/\"&gt;Charlie Summers&lt;/a&gt; (Staff SE, &lt;a href=\"https://www.linkedin.com/company/earnmerits/\"&gt;Merit&lt;/a&gt;) @ 11am&lt;/p&gt;\n\n&lt;p&gt;Charlie will share what he&amp;#39;s learned after running dbt core on Airflow in production for 2 years.&lt;/p&gt;\n\n&lt;p&gt;Topics he&amp;#39;ll be covering include:&lt;/p&gt;\n\n&lt;p&gt;- Isolating dbt core executions and avoiding python dependency hell with Kubernetes Pod Operators&lt;br/&gt;\n- Making dbt DAGs DRY-er with re-usable Airflow templates&lt;br/&gt;\n- Cutting ~30 seconds off every DAG execution with pre-compilation (when I realized we could do this, I literally :man-facepalming: for not realizing it sooner - so much wasted compute!)&lt;/p&gt;\n\n&lt;p&gt;Sign up &lt;a href=\"https://www.operationalanalytics.club/events/running-dbt-core-on-airflow-in-production-learnings-from-2-years-of-battle-scars\"&gt;here&lt;/a&gt;!   &lt;/p&gt;\n\n&lt;p&gt;PS: &lt;/p&gt;\n\n&lt;p&gt;- If you want to attend but cant, no worries! I&amp;#39;ll share recording w/ all signups&lt;br/&gt;\n- No SaaS is being sold during this event. It&amp;#39;s strictly educational for ppl interested in running dbt core on airflow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12rehq9", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rehq9/running_dbt_core_on_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rehq9/running_dbt_core_on_airflow/", "subreddit_subscribers": 100630, "created_utc": 1681873281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "\nEnglish isn't my first language, so I'll just apologise for any mistakes in advance. \n\nI've been working as a assistant at a Uni lab doing data analysis on Social Science data for one year now. Usually we work with government public data and - most of the time - this requires downloading files from a server of whatever agency makes the data available. Recently, I've sugestted starting creating a data warehouse using AWS and query the data using Athena with SQL instead of redownloading it all the time. So I've started to  write  some ETLs with data we were working on to move the files from the public servers straight to S3 Buckets. I've showed how this works for my academic advisor, and been told they have been doing the \"same thing\" using Google Drive to upload data: they download the data from the server, convert and upload to a shared folder there. He pointed me that there would be costs with this approach and someone would be needed to maintain all of this working - and that is true. Also, most students use SPSS or Excel instead of tools like R or Python. \n\nI think creating those scripts and understanding how those cloud services works is relevant to future jobs, and honestly felt like the dude really haven't bothered considering a tools he ins't familiar with. While I agree with some of his concerns, I'm thinking it might just be a waste of time to keep working on tools that aren't really that relevant outside of academia or maybe any job is relevant and this might count as something interesting in the future. I'm just looking for advice to how to handle this.", "author_fullname": "t2_nw7xfgb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Academic advisor insist on storing flat files on GDrive instead of using SQL data warehouse solution to manage data. Am I taking the wrong approach to the problem?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12s8mpa", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681935302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;English isn&amp;#39;t my first language, so I&amp;#39;ll just apologise for any mistakes in advance. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been working as a assistant at a Uni lab doing data analysis on Social Science data for one year now. Usually we work with government public data and - most of the time - this requires downloading files from a server of whatever agency makes the data available. Recently, I&amp;#39;ve sugestted starting creating a data warehouse using AWS and query the data using Athena with SQL instead of redownloading it all the time. So I&amp;#39;ve started to  write  some ETLs with data we were working on to move the files from the public servers straight to S3 Buckets. I&amp;#39;ve showed how this works for my academic advisor, and been told they have been doing the &amp;quot;same thing&amp;quot; using Google Drive to upload data: they download the data from the server, convert and upload to a shared folder there. He pointed me that there would be costs with this approach and someone would be needed to maintain all of this working - and that is true. Also, most students use SPSS or Excel instead of tools like R or Python. &lt;/p&gt;\n\n&lt;p&gt;I think creating those scripts and understanding how those cloud services works is relevant to future jobs, and honestly felt like the dude really haven&amp;#39;t bothered considering a tools he ins&amp;#39;t familiar with. While I agree with some of his concerns, I&amp;#39;m thinking it might just be a waste of time to keep working on tools that aren&amp;#39;t really that relevant outside of academia or maybe any job is relevant and this might count as something interesting in the future. I&amp;#39;m just looking for advice to how to handle this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12s8mpa", "is_robot_indexable": true, "report_reasons": null, "author": "MykeNogueira", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s8mpa/academic_advisor_insist_on_storing_flat_files_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12s8mpa/academic_advisor_insist_on_storing_flat_files_on/", "subreddit_subscribers": 100630, "created_utc": 1681935302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have several external sources of data where CDC, or any type of date/change monitoring is difficult. Right now, I take daily snapshots of the data (as intraday changes are irrelevant in the long run). \n\nMuch of the data is slow changing, so many of the rows are pure duplicates and rapidly growing the size of tables. Is there any reason to periodically prune and condense duplicate, historical rows or is it best to just run with the idea that storage is cheap and let it be? (data volume for me is relatively low compared to many businesses)", "author_fullname": "t2_ao7u40a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Raw data pruning: Should you do it or not?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rvewc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681915134.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have several external sources of data where CDC, or any type of date/change monitoring is difficult. Right now, I take daily snapshots of the data (as intraday changes are irrelevant in the long run). &lt;/p&gt;\n\n&lt;p&gt;Much of the data is slow changing, so many of the rows are pure duplicates and rapidly growing the size of tables. Is there any reason to periodically prune and condense duplicate, historical rows or is it best to just run with the idea that storage is cheap and let it be? (data volume for me is relatively low compared to many businesses)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12rvewc", "is_robot_indexable": true, "report_reasons": null, "author": "minormisgnomer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rvewc/raw_data_pruning_should_you_do_it_or_not/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rvewc/raw_data_pruning_should_you_do_it_or_not/", "subreddit_subscribers": 100630, "created_utc": 1681915134.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everybody,\n\nSo I stumbled about a curious problem I have evaded/solved, although the underlying issue still persists.\n\nFor a client we set up a small azure SQL instance and wanted to create a DWH in said instance.\nFirst of i started to stage the tables (from an sap source) and for some tables, the staging process dropped to a speed of only 100ish lines/sec while some tables were fine ( 5k lines/sec).\n\n\nThis \"slow\" behavior only seemed to happen with a few tables and only if I inserted specific columns (decimals). Sometimes this was solved by reducing the decimals to 18,2; sometimes even then the issue persisted. Even upgrading the tier did not give any Performance, I think it must be an underlying SQL topic? Exporting the data and saving, it to an XLSX.and importing it again speed up the transformation to normal speeds.\n\nDoes anybody have a clue? Cheers\n\n\nEdit 1:\n I forgot to add a crucial part, this \"behaviour\" only happens when staging into azure SQL db not in a local SQL server.\n\nEdit2:\nI checked I/O performance and we are not limited on this end as the insert runs perfectly fine with more data formatted correctly.", "author_fullname": "t2_yobj1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Slow inserts in azure SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rjrr7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681914105.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681886581.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everybody,&lt;/p&gt;\n\n&lt;p&gt;So I stumbled about a curious problem I have evaded/solved, although the underlying issue still persists.&lt;/p&gt;\n\n&lt;p&gt;For a client we set up a small azure SQL instance and wanted to create a DWH in said instance.\nFirst of i started to stage the tables (from an sap source) and for some tables, the staging process dropped to a speed of only 100ish lines/sec while some tables were fine ( 5k lines/sec).&lt;/p&gt;\n\n&lt;p&gt;This &amp;quot;slow&amp;quot; behavior only seemed to happen with a few tables and only if I inserted specific columns (decimals). Sometimes this was solved by reducing the decimals to 18,2; sometimes even then the issue persisted. Even upgrading the tier did not give any Performance, I think it must be an underlying SQL topic? Exporting the data and saving, it to an XLSX.and importing it again speed up the transformation to normal speeds.&lt;/p&gt;\n\n&lt;p&gt;Does anybody have a clue? Cheers&lt;/p&gt;\n\n&lt;p&gt;Edit 1:\n I forgot to add a crucial part, this &amp;quot;behaviour&amp;quot; only happens when staging into azure SQL db not in a local SQL server.&lt;/p&gt;\n\n&lt;p&gt;Edit2:\nI checked I/O performance and we are not limited on this end as the insert runs perfectly fine with more data formatted correctly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12rjrr7", "is_robot_indexable": true, "report_reasons": null, "author": "lschozar", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rjrr7/slow_inserts_in_azure_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rjrr7/slow_inserts_in_azure_sql/", "subreddit_subscribers": 100630, "created_utc": 1681886581.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I do like the model versioning I have some use cases that could benefit!", "author_fullname": "t2_1n3qfa0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Thoughts on the DBT Blog Today?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 69, "top_awarded_type": null, "hide_score": false, "name": "t3_12s9x0w", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IvrChOyKloWlEIn4mNaRl5w5KYM8-A6Tjalo4Z42FOc.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681937926.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "getdbt.com", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I do like the model versioning I have some use cases that could benefit!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.getdbt.com/blog/analytics-engineering-next-step-forwards/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/WejOqYqz2xIAGiw_6FdFfCzfplpSDhof3hBVpLAiw44.jpg?auto=webp&amp;v=enabled&amp;s=cd483434766f857f7651a95b97b87bda414fd11e", "width": 2178, "height": 1076}, "resolutions": [{"url": "https://external-preview.redd.it/WejOqYqz2xIAGiw_6FdFfCzfplpSDhof3hBVpLAiw44.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bd22d741741f11809954e71fcf7a5d64dcb2333c", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/WejOqYqz2xIAGiw_6FdFfCzfplpSDhof3hBVpLAiw44.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f2e9ba90ab8bb309b8aca69a36c737825b2479c", "width": 216, "height": 106}, {"url": "https://external-preview.redd.it/WejOqYqz2xIAGiw_6FdFfCzfplpSDhof3hBVpLAiw44.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=347e57a853c866afab651d17bdd1e1a00b702b3b", "width": 320, "height": 158}, {"url": "https://external-preview.redd.it/WejOqYqz2xIAGiw_6FdFfCzfplpSDhof3hBVpLAiw44.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2c1aa87c27930ad8d8e5d9bbf9afbad8ffa98f6", "width": 640, "height": 316}, {"url": "https://external-preview.redd.it/WejOqYqz2xIAGiw_6FdFfCzfplpSDhof3hBVpLAiw44.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1caa0c47e89759f44442b090a0f9eb672754364c", "width": 960, "height": 474}, {"url": "https://external-preview.redd.it/WejOqYqz2xIAGiw_6FdFfCzfplpSDhof3hBVpLAiw44.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f50d4b50969f51d65a798c11b7778627d441610", "width": 1080, "height": 533}], "variants": {}, "id": "Fm4kaJWViUw3WsmXNNK8vMdeM7N1SNRQMD5tnqKKPr0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12s9x0w", "is_robot_indexable": true, "report_reasons": null, "author": "Culpgrant21", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s9x0w/thoughts_on_the_dbt_blog_today/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.getdbt.com/blog/analytics-engineering-next-step-forwards/", "subreddit_subscribers": 100630, "created_utc": 1681937926.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are working on a POC where we have to use Azure CosmosDB with the gremlin API as we are dealing with graph data. As I am new to this topic I was wondering how we can create visuals on the data directly or after ruining some algorithms like centrality on top of it. The client really wants to go with PowerBI but it does not come with a lot of support for Graph Visuals natively. Any suggestion on how I can do this with PowerBI or with some open source tool/framework. If you have done this in your org, how do you approach graph visualization for analysis.", "author_fullname": "t2_s33vjakh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualizing graph data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12s91h8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681936144.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are working on a POC where we have to use Azure CosmosDB with the gremlin API as we are dealing with graph data. As I am new to this topic I was wondering how we can create visuals on the data directly or after ruining some algorithms like centrality on top of it. The client really wants to go with PowerBI but it does not come with a lot of support for Graph Visuals natively. Any suggestion on how I can do this with PowerBI or with some open source tool/framework. If you have done this in your org, how do you approach graph visualization for analysis.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12s91h8", "is_robot_indexable": true, "report_reasons": null, "author": "Puzzleheaded_Box7963", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s91h8/visualizing_graph_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12s91h8/visualizing_graph_data/", "subreddit_subscribers": 100630, "created_utc": 1681936144.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some of your best practices or fastest ways to get familiar with a database architecture when joining a new team/project? \n\nDo you start querying and playing around with different connections or are there any faster methods", "author_fullname": "t2_5x9e117l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Get familiar with database architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12s0pvx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681920744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some of your best practices or fastest ways to get familiar with a database architecture when joining a new team/project? &lt;/p&gt;\n\n&lt;p&gt;Do you start querying and playing around with different connections or are there any faster methods&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12s0pvx", "is_robot_indexable": true, "report_reasons": null, "author": "readoyniando", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s0pvx/get_familiar_with_database_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12s0pvx/get_familiar_with_database_architecture/", "subreddit_subscribers": 100630, "created_utc": 1681920744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Transform data in your pipelines or in the database? The debate has been going on for decades. This Thursday 20th April, join Altinity and Rudderstack as we discuss the strengths of each approach, using real-time loading to ClickHouse as an example. Your best bet is to combine both to transform data most efficiently. Check this link to learn more!  [https://hubs.la/Q01J91nD0](https://hubs.la/Q01J91nD0)", "author_fullname": "t2_s3zu6zpl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[WEBINAR] ETL vs ELT Cage Fight: Combining RudderStack and ClickHouse to Build Real-Time Data Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12r8muj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681860468.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Transform data in your pipelines or in the database? The debate has been going on for decades. This Thursday 20th April, join Altinity and Rudderstack as we discuss the strengths of each approach, using real-time loading to ClickHouse as an example. Your best bet is to combine both to transform data most efficiently. Check this link to learn more!  &lt;a href=\"https://hubs.la/Q01J91nD0\"&gt;https://hubs.la/Q01J91nD0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?auto=webp&amp;v=enabled&amp;s=2aaafe86918fe6f01e38bb8c371c4500d14c9215", "width": 1920, "height": 1080}, "resolutions": [{"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b329cc1fb821a4a2f31b8d86e33036ef33322768", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0c62de98036de4ecaffc53931e5a510436884c97", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=234476154d69fd90eaca9958ad131e739cef4ab3", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8012513902430fec2a54b2b1e4a2210741fb2896", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af044e773e0424067adc364371f3dacf9a172cac", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/8Uo9CznaujC4hy1tGsihV0LMk9ODRosR-zjl9nKhh50.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b062b69f20ea40d01ef247d262755655c4f5950b", "width": 1080, "height": 607}], "variants": {}, "id": "B4-A4qYuf1HlA0IuVcTe3n2oeyZ6_zaXRft1KoWtMAk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12r8muj", "is_robot_indexable": true, "report_reasons": null, "author": "RyhanSunny_Altinity", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12r8muj/webinar_etl_vs_elt_cage_fight_combining/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12r8muj/webinar_etl_vs_elt_cage_fight_combining/", "subreddit_subscribers": 100630, "created_utc": 1681860468.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company currently uses Redshift for our data warehouse. It has event data being pushed in real-time via Rudderstack. \nIt also has MSSQL data being sync in every 8hrs via airbyte. \nAnd AWS Postgres sync every 24hrs via AWS glue. \nBI visualisation using Metabase connected to the Redshift instance. \n\nIt\u2019s fine for now. But I\u2019d like to move to real time analytics. \n\nI don\u2019t mind moving away from AWS. \n\nMost of our core customer product is in Azure and we will stop with the AWS Postgres soon (we have decided to decommission our product offering that uses aws Postgres)\n\nWhat would be a good reference architecture to look into implementing to enable real time analytics across our Azure MSSQL + Event data from rudderstack that still connects to Metabase? \n\nI know other BI tools exist. But I did a lot of work helping non tech people get comfortable utilising Metabase. \n\nHelp is greatly appreciated.", "author_fullname": "t2_xhvhw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reference architecture for real time data analytics", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12sdfz8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681945068.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company currently uses Redshift for our data warehouse. It has event data being pushed in real-time via Rudderstack. \nIt also has MSSQL data being sync in every 8hrs via airbyte. \nAnd AWS Postgres sync every 24hrs via AWS glue. \nBI visualisation using Metabase connected to the Redshift instance. &lt;/p&gt;\n\n&lt;p&gt;It\u2019s fine for now. But I\u2019d like to move to real time analytics. &lt;/p&gt;\n\n&lt;p&gt;I don\u2019t mind moving away from AWS. &lt;/p&gt;\n\n&lt;p&gt;Most of our core customer product is in Azure and we will stop with the AWS Postgres soon (we have decided to decommission our product offering that uses aws Postgres)&lt;/p&gt;\n\n&lt;p&gt;What would be a good reference architecture to look into implementing to enable real time analytics across our Azure MSSQL + Event data from rudderstack that still connects to Metabase? &lt;/p&gt;\n\n&lt;p&gt;I know other BI tools exist. But I did a lot of work helping non tech people get comfortable utilising Metabase. &lt;/p&gt;\n\n&lt;p&gt;Help is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12sdfz8", "is_robot_indexable": true, "report_reasons": null, "author": "jinsy1", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12sdfz8/reference_architecture_for_real_time_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12sdfz8/reference_architecture_for_real_time_data/", "subreddit_subscribers": 100630, "created_utc": 1681945068.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "# Observability in distributed systems, from a data and site reliability engineering perspective.\n\n&amp;#x200B;\n\n[https://medium.com/swlh/introduction-to-instrumentation-and-observability-in-distributed-systems-part1-f66b3066e2c0](https://medium.com/swlh/introduction-to-instrumentation-and-observability-in-distributed-systems-part1-f66b3066e2c0)", "author_fullname": "t2_a0h885jg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Introduction to Instrumentation and Observability in Distributed Systems", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12s81mq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681934316.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681934123.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;Observability in distributed systems, from a data and site reliability engineering perspective.&lt;/h1&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/swlh/introduction-to-instrumentation-and-observability-in-distributed-systems-part1-f66b3066e2c0\"&gt;https://medium.com/swlh/introduction-to-instrumentation-and-observability-in-distributed-systems-part1-f66b3066e2c0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UCKG8WGYIKtHGBsKhh-k5WFkHN5wQGDAcynC0DWB7Ao.jpg?auto=webp&amp;v=enabled&amp;s=7a3495430d0c119931e9cc4c0d2209e70511bfa0", "width": 1011, "height": 484}, "resolutions": [{"url": "https://external-preview.redd.it/UCKG8WGYIKtHGBsKhh-k5WFkHN5wQGDAcynC0DWB7Ao.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0cb6ed229e49934ebcfb600fa72bad1a15bc358d", "width": 108, "height": 51}, {"url": "https://external-preview.redd.it/UCKG8WGYIKtHGBsKhh-k5WFkHN5wQGDAcynC0DWB7Ao.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=019a7235f49c903777afa03ef0d2b36a8a2d1d8c", "width": 216, "height": 103}, {"url": "https://external-preview.redd.it/UCKG8WGYIKtHGBsKhh-k5WFkHN5wQGDAcynC0DWB7Ao.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebcfdf73223d16a634ed28bf53aec2c1cbb4ceb2", "width": 320, "height": 153}, {"url": "https://external-preview.redd.it/UCKG8WGYIKtHGBsKhh-k5WFkHN5wQGDAcynC0DWB7Ao.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=398407d8d364064c0d2c3ed33244380438fcf81a", "width": 640, "height": 306}, {"url": "https://external-preview.redd.it/UCKG8WGYIKtHGBsKhh-k5WFkHN5wQGDAcynC0DWB7Ao.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=602ea712e8f370f9be60fbf3268f906176cbe23f", "width": 960, "height": 459}], "variants": {}, "id": "qG5PiHwHlBzR2chRsOlJfCC6LGC5nD6xpm9vf8_SQUU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12s81mq", "is_robot_indexable": true, "report_reasons": null, "author": "GoodCryptographer893", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s81mq/introduction_to_instrumentation_and_observability/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12s81mq/introduction_to_instrumentation_and_observability/", "subreddit_subscribers": 100630, "created_utc": 1681934123.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Very new DE here. I have been asked to update a table in BQ based on changes to a gsheet and vice versa. Is there already the functionality to do this in GCP or will it have to be coded. \n\nThank you in advance!", "author_fullname": "t2_3fxv004y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible to link a BigQuery table to a Google Sheet containing the same table and have them bi-directionally update?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rzqey", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681919865.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Very new DE here. I have been asked to update a table in BQ based on changes to a gsheet and vice versa. Is there already the functionality to do this in GCP or will it have to be coded. &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12rzqey", "is_robot_indexable": true, "report_reasons": null, "author": "J1010H", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12rzqey/is_it_possible_to_link_a_bigquery_table_to_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rzqey/is_it_possible_to_link_a_bigquery_table_to_a/", "subreddit_subscribers": 100630, "created_utc": 1681919865.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am looking for reading materials and python libraries that can help me with ingesting and making sense of large amount of log files coming from various devices. What would you recommend? \n\nI have the unfriendly gut feeling that i cant avoid regex, but is there any good libraries for this? So parsing, etc.", "author_fullname": "t2_hp7r8vez", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ingesting, parsing and making sense of device log data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12rpsyx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681903163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for reading materials and python libraries that can help me with ingesting and making sense of large amount of log files coming from various devices. What would you recommend? &lt;/p&gt;\n\n&lt;p&gt;I have the unfriendly gut feeling that i cant avoid regex, but is there any good libraries for this? So parsing, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12rpsyx", "is_robot_indexable": true, "report_reasons": null, "author": "Labanc_", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12rpsyx/ingesting_parsing_and_making_sense_of_device_log/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12rpsyx/ingesting_parsing_and_making_sense_of_device_log/", "subreddit_subscribers": 100630, "created_utc": 1681903163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used DataOps.live for Snowflake? Curious to hear from those that used it, how you like it.", "author_fullname": "t2_a0qsnkph", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataOps.live for Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12saeha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681938877.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used DataOps.live for Snowflake? Curious to hear from those that used it, how you like it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12saeha", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Membership-8", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12saeha/dataopslive_for_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12saeha/dataopslive_for_snowflake/", "subreddit_subscribers": 100630, "created_utc": 1681938877.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi!\n\nIm trying to help my company integrate hubspot with their native applications data in a mysql database. Im not very experienced with this type of work and would love to get some thoughts on best practices. Is it appropriate to rely on the hubspot generated ids or should say, a customer ID be created manually with more user friendly info (ex companyname-1234). How should the data be joined in if mysql doesnt have transaction ids? Could a mapping table be created and maintained to make mappings at the customer level and then rely on dates between systems? \n\nI just want to be sure im not doing anything egregious lol\n\nThanks!", "author_fullname": "t2_dkfbs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Could use some advice on crm platform (hubspot) integration with a native application.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12s7s63", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681933542.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;Im trying to help my company integrate hubspot with their native applications data in a mysql database. Im not very experienced with this type of work and would love to get some thoughts on best practices. Is it appropriate to rely on the hubspot generated ids or should say, a customer ID be created manually with more user friendly info (ex companyname-1234). How should the data be joined in if mysql doesnt have transaction ids? Could a mapping table be created and maintained to make mappings at the customer level and then rely on dates between systems? &lt;/p&gt;\n\n&lt;p&gt;I just want to be sure im not doing anything egregious lol&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12s7s63", "is_robot_indexable": true, "report_reasons": null, "author": "biga410", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s7s63/could_use_some_advice_on_crm_platform_hubspot/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12s7s63/could_use_some_advice_on_crm_platform_hubspot/", "subreddit_subscribers": 100630, "created_utc": 1681933542.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Obviously depends on your query patterns but I feel that simple things like date (creation/ingestion), unique IDs, etc are almost always useful and its best to have em than to need to build a new index once the DB is 500gb.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What indexes do you (almost) always set?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12s441p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681926052.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Obviously depends on your query patterns but I feel that simple things like date (creation/ingestion), unique IDs, etc are almost always useful and its best to have em than to need to build a new index once the DB is 500gb.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12s441p", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12s441p/what_indexes_do_you_almost_always_set/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12s441p/what_indexes_do_you_almost_always_set/", "subreddit_subscribers": 100630, "created_utc": 1681926052.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_oy8bkrnq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building real-time solutions with Snowflake at a fraction of the cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12ru22b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/yHjxOiD1-HzypQ4XXoZwiZxV9-izJXzklGzliKklbFU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681912440.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tinybird.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tinybird.co/blog-posts/real-time-solutions-with-snowflake", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?auto=webp&amp;v=enabled&amp;s=c7981342e41935be9fe03de0c2cf0ac7626cce2d", "width": 2400, "height": 1260}, "resolutions": [{"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e3d615e334496c4f9af1eb713bc44873aba3abd", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bde4509df996abb62e7c37edd96f2e5bddd79892", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=261028432fd390795adafc54036cea768f22a6d0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2b03275d14ea07785a1ac2db454871036e34fecd", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5b8d93caca2b1dbd4fcffc0f334bab3c0d5714f", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/IrqWJqHqPrPZFbc1OhotEcQGSWBLCeliFBQMLdJafMM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b5eee295d2f59751e5124ee897a3d909a660b9a", "width": 1080, "height": 567}], "variants": {}, "id": "ZN6KBbwbsvWox2-xxEY_L7jyiNubj-a3WNaKG9G0RoE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ru22b", "is_robot_indexable": true, "report_reasons": null, "author": "tinybirdco", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ru22b/building_realtime_solutions_with_snowflake_at_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tinybird.co/blog-posts/real-time-solutions-with-snowflake", "subreddit_subscribers": 100630, "created_utc": 1681912440.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}