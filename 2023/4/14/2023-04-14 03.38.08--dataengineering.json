{"kind": "Listing", "data": {"after": "t3_12kipj4", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who owns data quality?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 128, "top_awarded_type": null, "hide_score": false, "name": "t3_12l9mzx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "ups": 179, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 179, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pEGgrT6AA0Ee7mh3QIRZ3wvGng2GrkEtE8Vu2pZZaYs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681425657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8ma1yb67cqta1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8ma1yb67cqta1.jpg?auto=webp&amp;v=enabled&amp;s=c76f3ae14fda40cd714446ff0014957170862a87", "width": 544, "height": 500}, "resolutions": [{"url": "https://preview.redd.it/8ma1yb67cqta1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f5357d6ad8fd22807c299e69c1cea0f02eef2a7", "width": 108, "height": 99}, {"url": "https://preview.redd.it/8ma1yb67cqta1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=733fddc2fdf613c203bb9c2be183514abad5d0ee", "width": 216, "height": 198}, {"url": "https://preview.redd.it/8ma1yb67cqta1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b959130ca293342df5aaf140d27d94fe653fe379", "width": 320, "height": 294}], "variants": {}, "id": "_zipJWJt_VoFsn3ElGALbQLgo-DKOM9i4P2OFhKqSN8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12l9mzx", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12l9mzx/who_owns_data_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8ma1yb67cqta1.jpg", "subreddit_subscribers": 98717, "created_utc": 1681425657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As the title says. \n\nI'm based in the UK and interviewing for a well known company.\n\nI've been provided an outline of the interview and in it they want me to prepare a diagram of my current company's data architecture and spend 10-20 mins explaining it.\n\nI don't know if it's an odd request or not - I understand wanting to test my knowledge around architecture but it still feels odd.", "author_fullname": "t2_ske3s9us", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interviewer wants me to go into detail about current company's architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12knal1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 71, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 71, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681388629.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m based in the UK and interviewing for a well known company.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been provided an outline of the interview and in it they want me to prepare a diagram of my current company&amp;#39;s data architecture and spend 10-20 mins explaining it.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s an odd request or not - I understand wanting to test my knowledge around architecture but it still feels odd.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12knal1", "is_robot_indexable": true, "report_reasons": null, "author": "iamanoob38", "discussion_type": null, "num_comments": 63, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12knal1/interviewer_wants_me_to_go_into_detail_about/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12knal1/interviewer_wants_me_to_go_into_detail_about/", "subreddit_subscribers": 98717, "created_utc": 1681388629.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Worked as a full stack dev (front end) for four years and pivoted to DA. I enjoy exploring data but found it difficult to produce useful analyses. After being in the role for 4 years, I realized my communication skills do not get better, and I kinda hated sitting in a lot of meetings. A lot of my analyses weren't used, and I want to produce something useful and tangible. I enjoyed the first three years because I mostly built dashboards, wrote SQL and didn't have to attend too many meetings. It's definitely less stressful that SWE.\n\nMy last role as a product analyst in a startup and I hated it. I'm getting good at coming up with metrics but so many meetings and I am expected to work as consultant (i.e. provide recommendations about the product that I didn't know much in the first place. I interviewed around, and it seems really hard to get a role that pays &gt; 150k as DA (unless maybe in big tech). The analysis that took me weeks to produce is also not used. And, there are endless questions to explore. Started to think that I'm not good at this role. \n\nBecause I hate dealing with people, I don't think I'd be a good candidate to be an analytic manager either.  What do you do if you enjoy exploring the data but you are not good at it nor good at presenting the results? I am suck at presentation btw. I don't enjoy SWE as much as DA, but observing how fast I am troubleshooting bugs/researching something, I feel like I'm more natural as SWE. I love the investigation part of data analysis, but don't feel like I'm good at it. I don't find building as enjoyable as exploring, although I don't hate it. \n\nWith someone in my background, is it easier to be front-end dev again, even though it was four/five years ago, or is it easier to pivot to Data engineer? And will the hiring manager think it's strange that I want to be SWE again?", "author_fullname": "t2_a9ij7ckc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SWE --&gt; Data Analyst --&gt; SWE again", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kd66n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681363410.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Worked as a full stack dev (front end) for four years and pivoted to DA. I enjoy exploring data but found it difficult to produce useful analyses. After being in the role for 4 years, I realized my communication skills do not get better, and I kinda hated sitting in a lot of meetings. A lot of my analyses weren&amp;#39;t used, and I want to produce something useful and tangible. I enjoyed the first three years because I mostly built dashboards, wrote SQL and didn&amp;#39;t have to attend too many meetings. It&amp;#39;s definitely less stressful that SWE.&lt;/p&gt;\n\n&lt;p&gt;My last role as a product analyst in a startup and I hated it. I&amp;#39;m getting good at coming up with metrics but so many meetings and I am expected to work as consultant (i.e. provide recommendations about the product that I didn&amp;#39;t know much in the first place. I interviewed around, and it seems really hard to get a role that pays &amp;gt; 150k as DA (unless maybe in big tech). The analysis that took me weeks to produce is also not used. And, there are endless questions to explore. Started to think that I&amp;#39;m not good at this role. &lt;/p&gt;\n\n&lt;p&gt;Because I hate dealing with people, I don&amp;#39;t think I&amp;#39;d be a good candidate to be an analytic manager either.  What do you do if you enjoy exploring the data but you are not good at it nor good at presenting the results? I am suck at presentation btw. I don&amp;#39;t enjoy SWE as much as DA, but observing how fast I am troubleshooting bugs/researching something, I feel like I&amp;#39;m more natural as SWE. I love the investigation part of data analysis, but don&amp;#39;t feel like I&amp;#39;m good at it. I don&amp;#39;t find building as enjoyable as exploring, although I don&amp;#39;t hate it. &lt;/p&gt;\n\n&lt;p&gt;With someone in my background, is it easier to be front-end dev again, even though it was four/five years ago, or is it easier to pivot to Data engineer? And will the hiring manager think it&amp;#39;s strange that I want to be SWE again?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12kd66n", "is_robot_indexable": true, "report_reasons": null, "author": "thriftyberry", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kd66n/swe_data_analyst_swe_again/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kd66n/swe_data_analyst_swe_again/", "subreddit_subscribers": 98717, "created_utc": 1681363410.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "While the \u201cmodern data stack\u201d marketing has made the higher execs believe that the ETL/ELT tools solve for all data ingestion problems, but in reality all the platforms offer only handful of connectors that they maintain themselves - rest is outsourced to community which might/might not be very active, depending on the data source. In such scenarios, how do you handle data pipelines?", "author_fullname": "t2_14gits", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Pipelines - how do you build data pipelines for sources not available in today\u2019s ELT tools (Fivetran, Talend, Airbyte)? Old fashioned scripts and YOLO?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kppyf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 1, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681393467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While the \u201cmodern data stack\u201d marketing has made the higher execs believe that the ETL/ELT tools solve for all data ingestion problems, but in reality all the platforms offer only handful of connectors that they maintain themselves - rest is outsourced to community which might/might not be very active, depending on the data source. In such scenarios, how do you handle data pipelines?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 50, "id": "award_02d9ab2c-162e-4c01-8438-317a016ed3d9", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=32add54efce28cc8ce035c5e2bc89a27286a815e", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=dfb00ece05340570177df7cfa1af6d2737c0910b", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e8b0b87b868f6cd6313e2c90975dac636e4a0412", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=2a3ad7ec2ccc57b6c65b17e2b57647a81f335039", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=d4a8ca64b391e8b057408067d77f503752c29b7e", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "I'm in this with you.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 1, "static_icon_height": 2048, "name": "Take My Energy", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=4efb20a46b5cee58042da74830ee914d1547236c", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=83e8bea70baef2140842017e967f163a9f530a9d", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=14fb29ce140b35a21a7cc7ee1c4d212ce0b1179d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=533b05085677b48f15004bd7f9ff19ec5b29099f", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=6f767b3c289e5cb2a733b24da5f4c46d9c079bc7", "width": 128, "height": 128}], "icon_format": "PNG", "icon_height": 2048, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12kppyf", "is_robot_indexable": true, "report_reasons": null, "author": "nonheuristic", "discussion_type": null, "num_comments": 55, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kppyf/data_pipelines_how_do_you_build_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kppyf/data_pipelines_how_do_you_build_data_pipelines/", "subreddit_subscribers": 98717, "created_utc": 1681393467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If yes, which modeling pattern do you see winning out?\n\nprimarily based on Zones or Stages: Bronze -&gt; Silver -&gt; Gold?  \nprimarily based on Star schemas or Snowflake schemas?", "author_fullname": "t2_bv368at0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which data modeling pattern does your data team use today? Is it used much at all? Or have modern data warehouses made it easy for modeling logic to just be bundled into larger or more complex SQL queries?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12l2n5y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681417718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If yes, which modeling pattern do you see winning out?&lt;/p&gt;\n\n&lt;p&gt;primarily based on Zones or Stages: Bronze -&amp;gt; Silver -&amp;gt; Gold?&lt;br/&gt;\nprimarily based on Star schemas or Snowflake schemas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12l2n5y", "is_robot_indexable": true, "report_reasons": null, "author": "InterestingsBed", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12l2n5y/which_data_modeling_pattern_does_your_data_team/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12l2n5y/which_data_modeling_pattern_does_your_data_team/", "subreddit_subscribers": 98717, "created_utc": 1681417718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently joined a digital bank's data engineering team(it has been close to 4 months) and i dont even have read access to the production data. Only a couple of people in my team have access to it. We are using big query as a data lake. \n\nThis has led to me being left out of any business use case requirements, any production issues and has decreased confidence in myself, playing second tier and catch up. \n\nI have discussed this with my manager and he told that since there are regulatory requirements only a handful of people can have access to prod data. \n\n\nMy question is there anything that we can do technically to improve data security to get around this requirement and if anyone is also facing the same issue as me and how to handle it", "author_fullname": "t2_esibz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No access to prod data as DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kcs0u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681362411.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently joined a digital bank&amp;#39;s data engineering team(it has been close to 4 months) and i dont even have read access to the production data. Only a couple of people in my team have access to it. We are using big query as a data lake. &lt;/p&gt;\n\n&lt;p&gt;This has led to me being left out of any business use case requirements, any production issues and has decreased confidence in myself, playing second tier and catch up. &lt;/p&gt;\n\n&lt;p&gt;I have discussed this with my manager and he told that since there are regulatory requirements only a handful of people can have access to prod data. &lt;/p&gt;\n\n&lt;p&gt;My question is there anything that we can do technically to improve data security to get around this requirement and if anyone is also facing the same issue as me and how to handle it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12kcs0u", "is_robot_indexable": true, "report_reasons": null, "author": "ithellam_oru_pollapu", "discussion_type": null, "num_comments": 34, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kcs0u/no_access_to_prod_data_as_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kcs0u/no_access_to_prod_data_as_de/", "subreddit_subscribers": 98717, "created_utc": 1681362411.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm the sole data engineer at a small company (70-ish employees) and have been tasked with setting up a data warehouse. Currently all our infrastructure is on-prem, but we'd like to start with our DWH in SQL Server on Azure and using analysis services for compute for Power BI. \n\nI'm struggling a lot to come up with reasonable estimates for our expected costs. We have recently started a separate project, done by an external consultant, calls an API using Azure Functions to grab a few hundred rows (a few very straight-forward GET requests through Python), store it in a container, then uses Azure Data Factory to call a few stored procedures on the SQL Server to update the database. So far, the costs seem to be about USD 5.00 per function call (+ about USD 5.00 per day in other costs, totaling in about USD 300 per month), which - to me - seems like a lot for something so simple.\n\nThe consultant mentions the high function costs is due to the use of Azure Function Premium, which is needed as the alternative (consumption plan) doesn't support VNET access. \n\nI'm now concerned how much the costs will be if we want to move our ERP data from our on-prem database to Azure SQL Server using the same methods. \n\nAt this point I'm wondering if it isn't worth it to just use a virtual machine and run dagster/airflow/prefect to replace the Azure Functions &amp; Azure Data Factory. In that case, I'd only have to scale out the virtual machine if the process is taking too long, but at least my company isn't suddenly surprised by hundreds or thousands of dollars in extra costs due to (seemingly) expensive functions. I guess I'm also a bit worried of running into a [Troy Hunt-like situation](https://www.troyhunt.com/how-i-got-pwned-by-my-cloud-costs/) where a misconfiguration suddenly drives up the costs without me noticing in time. \n\nI feel like the only way to get a rough idea about the costs is to just make a proof of concept and try it, which would require quite the time investment (which could be spend on the many other projects I have lined up), plus the costs of the proof of concept may not scale linearly with the real-world data we'd eventually be using. \n\nI believe this problem isn't exclusive to Azure (it just happens to be what we're using) but could be AWS or GCP just as easily. \n\nHow have others dealt with small company budgets and estimating their costs?", "author_fullname": "t2_ko2ybcf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding and estimating cloud costs", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kgajr", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681371483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m the sole data engineer at a small company (70-ish employees) and have been tasked with setting up a data warehouse. Currently all our infrastructure is on-prem, but we&amp;#39;d like to start with our DWH in SQL Server on Azure and using analysis services for compute for Power BI. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m struggling a lot to come up with reasonable estimates for our expected costs. We have recently started a separate project, done by an external consultant, calls an API using Azure Functions to grab a few hundred rows (a few very straight-forward GET requests through Python), store it in a container, then uses Azure Data Factory to call a few stored procedures on the SQL Server to update the database. So far, the costs seem to be about USD 5.00 per function call (+ about USD 5.00 per day in other costs, totaling in about USD 300 per month), which - to me - seems like a lot for something so simple.&lt;/p&gt;\n\n&lt;p&gt;The consultant mentions the high function costs is due to the use of Azure Function Premium, which is needed as the alternative (consumption plan) doesn&amp;#39;t support VNET access. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m now concerned how much the costs will be if we want to move our ERP data from our on-prem database to Azure SQL Server using the same methods. &lt;/p&gt;\n\n&lt;p&gt;At this point I&amp;#39;m wondering if it isn&amp;#39;t worth it to just use a virtual machine and run dagster/airflow/prefect to replace the Azure Functions &amp;amp; Azure Data Factory. In that case, I&amp;#39;d only have to scale out the virtual machine if the process is taking too long, but at least my company isn&amp;#39;t suddenly surprised by hundreds or thousands of dollars in extra costs due to (seemingly) expensive functions. I guess I&amp;#39;m also a bit worried of running into a &lt;a href=\"https://www.troyhunt.com/how-i-got-pwned-by-my-cloud-costs/\"&gt;Troy Hunt-like situation&lt;/a&gt; where a misconfiguration suddenly drives up the costs without me noticing in time. &lt;/p&gt;\n\n&lt;p&gt;I feel like the only way to get a rough idea about the costs is to just make a proof of concept and try it, which would require quite the time investment (which could be spend on the many other projects I have lined up), plus the costs of the proof of concept may not scale linearly with the real-world data we&amp;#39;d eventually be using. &lt;/p&gt;\n\n&lt;p&gt;I believe this problem isn&amp;#39;t exclusive to Azure (it just happens to be what we&amp;#39;re using) but could be AWS or GCP just as easily. &lt;/p&gt;\n\n&lt;p&gt;How have others dealt with small company budgets and estimating their costs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lPeDJ248ZZFvNTYcuSeaEt0vOZFlzdWp-rN0_LK8rSE.jpg?auto=webp&amp;v=enabled&amp;s=6c5b54e1cbe021264a7db007976eecc17e974733", "width": 1337, "height": 757}, "resolutions": [{"url": "https://external-preview.redd.it/lPeDJ248ZZFvNTYcuSeaEt0vOZFlzdWp-rN0_LK8rSE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9609aac2705508a8fe89456107bc76f863db9ad6", "width": 108, "height": 61}, {"url": "https://external-preview.redd.it/lPeDJ248ZZFvNTYcuSeaEt0vOZFlzdWp-rN0_LK8rSE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb5c66e5c7b247bd993169b65485460302aedc85", "width": 216, "height": 122}, {"url": "https://external-preview.redd.it/lPeDJ248ZZFvNTYcuSeaEt0vOZFlzdWp-rN0_LK8rSE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c3cb2b47e5b9b87fff08fdbb2892318c5113d685", "width": 320, "height": 181}, {"url": "https://external-preview.redd.it/lPeDJ248ZZFvNTYcuSeaEt0vOZFlzdWp-rN0_LK8rSE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdd573ea39858e84b8b02e46c2fef549fad5ce0d", "width": 640, "height": 362}, {"url": "https://external-preview.redd.it/lPeDJ248ZZFvNTYcuSeaEt0vOZFlzdWp-rN0_LK8rSE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3323cce877ff4f089ddcea63ec7427d30a3799de", "width": 960, "height": 543}, {"url": "https://external-preview.redd.it/lPeDJ248ZZFvNTYcuSeaEt0vOZFlzdWp-rN0_LK8rSE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=96692c752313aba9b74361425bbfd766bb519fa2", "width": 1080, "height": 611}], "variants": {}, "id": "yyHzldgdQM02tfKU160oThzjjeuuF5lm9yQnCmU1Mfk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12kgajr", "is_robot_indexable": true, "report_reasons": null, "author": "nl_dhh", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kgajr/understanding_and_estimating_cloud_costs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kgajr/understanding_and_estimating_cloud_costs/", "subreddit_subscribers": 98717, "created_utc": 1681371483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, my department is moving to databricks, to be more specific it is been used already in another departments and our department will be adapting it soon. The other couple of departments swears by it, but I was wondering what are your opinions on it especially the main drawbacks. Thanks", "author_fullname": "t2_8b001z5d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hi all, my department is moving to databricks and I am trying to get familiar with all the merits and most importantly the demerits", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kue6t", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681402617.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, my department is moving to databricks, to be more specific it is been used already in another departments and our department will be adapting it soon. The other couple of departments swears by it, but I was wondering what are your opinions on it especially the main drawbacks. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12kue6t", "is_robot_indexable": true, "report_reasons": null, "author": "bagsofmysteries", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kue6t/hi_all_my_department_is_moving_to_databricks_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kue6t/hi_all_my_department_is_moving_to_databricks_and/", "subreddit_subscribers": 98717, "created_utc": 1681402617.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently working on implementing a data orchestration tool. To begin with it will be a single node instance.\nI\u2019m confused on how Dagster stores assets. Are the assets stored in the local filesystem? If that\u2019s the case, won\u2019t it blow up with time.\nIs there a project available for a simple \nETL pipeline that runs on a schedule. Thanks!", "author_fullname": "t2_bcs9s7nw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dagster Resources", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12klkq3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.68, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681384984.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently working on implementing a data orchestration tool. To begin with it will be a single node instance.\nI\u2019m confused on how Dagster stores assets. Are the assets stored in the local filesystem? If that\u2019s the case, won\u2019t it blow up with time.\nIs there a project available for a simple \nETL pipeline that runs on a schedule. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12klkq3", "is_robot_indexable": true, "report_reasons": null, "author": "Mundane-Compote-2157", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12klkq3/dagster_resources/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12klkq3/dagster_resources/", "subreddit_subscribers": 98717, "created_utc": 1681384984.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_xf2t5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Understanding AWS Regions and Availability Zones: A Guide for Beginners", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12kcsl5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IBESTC58YPkLBVQYqOZjCipKmvwU1r35uBK0K5Rl2qs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681362451.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "luminousmen.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://luminousmen.com/post/understanding-aws-regions-and-availability-zones-a-guide-for-beginners", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Er_NQSh5ay3ijtMEO8kCcMP1m3VxwFolKRAe8OWQwxE.jpg?auto=webp&amp;v=enabled&amp;s=ecb9e0c85657f7d483e8c30594eb558fd5881d78", "width": 800, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/Er_NQSh5ay3ijtMEO8kCcMP1m3VxwFolKRAe8OWQwxE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3decc591fa73981368a746092e57d6ad9e274875", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/Er_NQSh5ay3ijtMEO8kCcMP1m3VxwFolKRAe8OWQwxE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9544e57e6dc5e74a63c7041d9260257fb8364ea2", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/Er_NQSh5ay3ijtMEO8kCcMP1m3VxwFolKRAe8OWQwxE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8271e3290637949d07b205c401b2c36d8c512c59", "width": 320, "height": 240}, {"url": "https://external-preview.redd.it/Er_NQSh5ay3ijtMEO8kCcMP1m3VxwFolKRAe8OWQwxE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2358a2f7225322213c2115c1136a2c4a2f089987", "width": 640, "height": 480}], "variants": {}, "id": "2-Tuv6RObVUGGib5vgYAIRrVVPwilSKhzbqf_NQskOk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12kcsl5", "is_robot_indexable": true, "report_reasons": null, "author": "luminoumen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kcsl5/understanding_aws_regions_and_availability_zones/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://luminousmen.com/post/understanding-aws-regions-and-availability-zones-a-guide-for-beginners", "subreddit_subscribers": 98717, "created_utc": 1681362451.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How to use Chat GPT or suggest any other online free tool for query tuning.\n\nI have data analysts writing a bunch of queries that uses same set tables of tables over and over again in left joins and CTEs with full table scans \ud83d\ude35\u200d\ud83d\udcab\ud83e\udd74", "author_fullname": "t2_f2obp4el", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "T-Sql Query Tuning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12l8ryw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681424647.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How to use Chat GPT or suggest any other online free tool for query tuning.&lt;/p&gt;\n\n&lt;p&gt;I have data analysts writing a bunch of queries that uses same set tables of tables over and over again in left joins and CTEs with full table scans \ud83d\ude35\u200d\ud83d\udcab\ud83e\udd74&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12l8ryw", "is_robot_indexable": true, "report_reasons": null, "author": "InterestingDot8089", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12l8ryw/tsql_query_tuning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12l8ryw/tsql_query_tuning/", "subreddit_subscribers": 98717, "created_utc": 1681424647.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "One hassle is that if I change the DAG name, it recognizes it as a new DAG. I'd prefer to maybe have a unique DAG id that never changes but a DAG name that is mutable. Is it possible in Airflow 2?\n\nThought about a second one: In Airflow UI, AFAIK, you can only view code, not the sql files that those code call for, unless they are embedded in code. Yeah I can view them in \"rendered template\" if they already ran once, but what if they are new?\n\n&amp;#x200B;", "author_fullname": "t2_ldvtxo0c", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One hassle of Airflow that I do not know how to solve", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12l1upw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681416422.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681416179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;One hassle is that if I change the DAG name, it recognizes it as a new DAG. I&amp;#39;d prefer to maybe have a unique DAG id that never changes but a DAG name that is mutable. Is it possible in Airflow 2?&lt;/p&gt;\n\n&lt;p&gt;Thought about a second one: In Airflow UI, AFAIK, you can only view code, not the sql files that those code call for, unless they are embedded in code. Yeah I can view them in &amp;quot;rendered template&amp;quot; if they already ran once, but what if they are new?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12l1upw", "is_robot_indexable": true, "report_reasons": null, "author": "throwaway20220231", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12l1upw/one_hassle_of_airflow_that_i_do_not_know_how_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12l1upw/one_hassle_of_airflow_that_i_do_not_know_how_to/", "subreddit_subscribers": 98717, "created_utc": 1681416179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data Stack Summit 2023 (live virtual) is next Wednesday 4/19. Passes are free and attendance certificates are issued if you need them for your employer. \n\nA couple of the interesting sessions: \n\n* 8:05 AM PST - Peer-to-Peer Panel: Managing cloud costs right now w/ Joseph Machado, Carlos Costa, Vikas Ranjan, Mike Fuller, and Mike Mooney\n* 10:25 AM PST - Walmart's self-service metadata-driven data loader framework with Manimuthu Aayyannan and Subramanya Mulgund\n* 12:10 PM PST - Is synthetic data useful for data engineers? with Alexander Mikhalev and Matthew Norton\n\nLive sessions will be available to registrants on-demand post-event, helpful for those in different time zones. Appreciate the support everything we do is community-built.", "author_fullname": "t2_ff7f8okm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Peer sessions at Data Stack Summit + attendance certificate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kxvfs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681409343.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data Stack Summit 2023 (live virtual) is next Wednesday 4/19. Passes are free and attendance certificates are issued if you need them for your employer. &lt;/p&gt;\n\n&lt;p&gt;A couple of the interesting sessions: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;8:05 AM PST - Peer-to-Peer Panel: Managing cloud costs right now w/ Joseph Machado, Carlos Costa, Vikas Ranjan, Mike Fuller, and Mike Mooney&lt;/li&gt;\n&lt;li&gt;10:25 AM PST - Walmart&amp;#39;s self-service metadata-driven data loader framework with Manimuthu Aayyannan and Subramanya Mulgund&lt;/li&gt;\n&lt;li&gt;12:10 PM PST - Is synthetic data useful for data engineers? with Alexander Mikhalev and Matthew Norton&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Live sessions will be available to registrants on-demand post-event, helpful for those in different time zones. Appreciate the support everything we do is community-built.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12kxvfs", "is_robot_indexable": true, "report_reasons": null, "author": "hesanastronaut", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kxvfs/peer_sessions_at_data_stack_summit_attendance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kxvfs/peer_sessions_at_data_stack_summit_attendance/", "subreddit_subscribers": 98717, "created_utc": 1681409343.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work as a data lead in a rather large company, but the BI/data infrastructure here is rather outdated. We don't have a dedicated data engineer, and most data-related solutions are coming from my team. In regards to our data infrastructure we don't use any popular cloud solutions (Azure/AWS etc). Also by no means I'm experienced in data engineering.\n\nAt the moment we have developed variety of data related tools for business end-users, related to BI, ETLs, couple ML tools, and other various process automations.\n\n~50 tools across departments (~100 business users). Technologies mostly used are: SAP (data export using VBS), Power Query, Excel &amp; VBA, Power Bi, DAX, Power Automate, Python. \nMajority of these tools are run locally by data analysts, in development environment.  \n\nMy objective is to implement Production environment, and some web-like platform, where end-users would be able to use these tools.\n\nWhat are some practices/options I should start investigating? \nConsidering we are using MS products, first thing that came to my mind is to use MS power apps &amp; pages.", "author_fullname": "t2_13lmug", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Production environment &amp; platform for data tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12km4nl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681386184.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work as a data lead in a rather large company, but the BI/data infrastructure here is rather outdated. We don&amp;#39;t have a dedicated data engineer, and most data-related solutions are coming from my team. In regards to our data infrastructure we don&amp;#39;t use any popular cloud solutions (Azure/AWS etc). Also by no means I&amp;#39;m experienced in data engineering.&lt;/p&gt;\n\n&lt;p&gt;At the moment we have developed variety of data related tools for business end-users, related to BI, ETLs, couple ML tools, and other various process automations.&lt;/p&gt;\n\n&lt;p&gt;~50 tools across departments (~100 business users). Technologies mostly used are: SAP (data export using VBS), Power Query, Excel &amp;amp; VBA, Power Bi, DAX, Power Automate, Python. \nMajority of these tools are run locally by data analysts, in development environment.  &lt;/p&gt;\n\n&lt;p&gt;My objective is to implement Production environment, and some web-like platform, where end-users would be able to use these tools.&lt;/p&gt;\n\n&lt;p&gt;What are some practices/options I should start investigating? \nConsidering we are using MS products, first thing that came to my mind is to use MS power apps &amp;amp; pages.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12km4nl", "is_robot_indexable": true, "report_reasons": null, "author": "artemyp", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12km4nl/production_environment_platform_for_data_tools/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12km4nl/production_environment_platform_for_data_tools/", "subreddit_subscribers": 98717, "created_utc": 1681386184.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am using Cloud Run and Scheduler to spin up a container containing DBT and am running \u2018dbt run\u2019 on start up via ENTRYPOINT. However, DBT cannot authenticate to BigQuery because it can\u2019t access the default auth credentials from the network of the VM that it is hosted on. \n\nDoes anyone have any ideas on how I can allow the container access to the Cloud Run VM network to get the credentials? \n\nThank you in advance for any suggestions!", "author_fullname": "t2_3fxv004y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to get a Docker Container started using a Google Cloud Run job to get default auth creds from the VM?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ki3af", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681376385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using Cloud Run and Scheduler to spin up a container containing DBT and am running \u2018dbt run\u2019 on start up via ENTRYPOINT. However, DBT cannot authenticate to BigQuery because it can\u2019t access the default auth credentials from the network of the VM that it is hosted on. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have any ideas on how I can allow the container access to the Cloud Run VM network to get the credentials? &lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for any suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ki3af", "is_robot_indexable": true, "report_reasons": null, "author": "J1010H", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ki3af/how_to_get_a_docker_container_started_using_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ki3af/how_to_get_a_docker_container_started_using_a/", "subreddit_subscribers": 98717, "created_utc": 1681376385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have explained the steps involved in preparing and scheduling the \"databricks certified associate developer for apache spark\" certification in this blog\n\n[https://blogs.sibyabin.tech/certification/databricks/associate-developer-spark/how-i-passed-databricks-certified-associate-developer-for-apachespark-certification/](https://blogs.sibyabin.tech/certification/databricks/associate-developer-spark/how-i-passed-databricks-certified-associate-developer-for-apachespark-certification/) \n\n[https:\\/\\/blogs.sibyabin.tech](https://preview.redd.it/urgin921mlta1.png?width=571&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7ddb8432edc8947537d861583093a7d204b47e5c)\n\n\\#spark #databricks #certification #dataengineering #apachespark", "author_fullname": "t2_c4myu4hh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How I passed the Databricks Certified Associate Developer for Apache Spark 3.0", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"urgin921mlta1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 149, "x": 108, "u": "https://preview.redd.it/urgin921mlta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=73814fb2cc39b18975e0718674392651ac80889e"}, {"y": 299, "x": 216, "u": "https://preview.redd.it/urgin921mlta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=362f1611a51fdaa5a0377e082abf10d2196ec75f"}, {"y": 443, "x": 320, "u": "https://preview.redd.it/urgin921mlta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09b18dd97b04c383ce545a93ff93084f41a04b3d"}], "s": {"y": 792, "x": 571, "u": "https://preview.redd.it/urgin921mlta1.png?width=571&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7ddb8432edc8947537d861583093a7d204b47e5c"}, "id": "urgin921mlta1"}}, "name": "t3_12kf5vq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.57, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/nFTHArZDcdZvPSQpEqrS0HevM8sMau5-jt0PSdZmN9g.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681368510.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have explained the steps involved in preparing and scheduling the &amp;quot;databricks certified associate developer for apache spark&amp;quot; certification in this blog&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://blogs.sibyabin.tech/certification/databricks/associate-developer-spark/how-i-passed-databricks-certified-associate-developer-for-apachespark-certification/\"&gt;https://blogs.sibyabin.tech/certification/databricks/associate-developer-spark/how-i-passed-databricks-certified-associate-developer-for-apachespark-certification/&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/urgin921mlta1.png?width=571&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7ddb8432edc8947537d861583093a7d204b47e5c\"&gt;https://blogs.sibyabin.tech&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;#spark #databricks #certification #dataengineering #apachespark&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12kf5vq", "is_robot_indexable": true, "report_reasons": null, "author": "Satm2021", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12kf5vq/how_i_passed_the_databricks_certified_associate/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kf5vq/how_i_passed_the_databricks_certified_associate/", "subreddit_subscribers": 98717, "created_utc": 1681368510.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently in the US Army and plan on becoming a data engineer after I get out. My plan is to take the Data Science: Analytics course on Codecademy, then start applying for part time jobs (I'm assuming I'd still be in the army).\n\n After the codecademy course, I plan on taking the new Google Data Analytics course to strengthen my data science skills. \n\nI'm assuming I'll get my foot in the door as an entry level data analyst/data scientist and then work my way up to data engineer.\n\nI'm also going to be working on the data anlytics/data engineering cloud certs from the big three (AWS, Azure, and GCP)\n\nWhat do ya'll think of my plan?", "author_fullname": "t2_7s8rukjx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My plan to become a data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ldrgk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681433634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently in the US Army and plan on becoming a data engineer after I get out. My plan is to take the Data Science: Analytics course on Codecademy, then start applying for part time jobs (I&amp;#39;m assuming I&amp;#39;d still be in the army).&lt;/p&gt;\n\n&lt;p&gt;After the codecademy course, I plan on taking the new Google Data Analytics course to strengthen my data science skills. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m assuming I&amp;#39;ll get my foot in the door as an entry level data analyst/data scientist and then work my way up to data engineer.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also going to be working on the data anlytics/data engineering cloud certs from the big three (AWS, Azure, and GCP)&lt;/p&gt;\n\n&lt;p&gt;What do ya&amp;#39;ll think of my plan?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ldrgk", "is_robot_indexable": true, "report_reasons": null, "author": "african_kid_1", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ldrgk/my_plan_to_become_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ldrgk/my_plan_to_become_a_data_engineer/", "subreddit_subscribers": 98717, "created_utc": 1681433634.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm deciding on the best tool to grab Jira data. I was originally going to use the Jira api and clean up the data myself but I read that airbyte Jira connector is very useful. \n\nUnfortunately I have been unable to connect it, I keep getting authentication error. I saw some comments on other forums that mentioned the connector only works for Jira cloud not self hosted. Is this true? \n\nIf so is it better to make a custom connector in airbyte or just go with my original plan of grabbing the data via Jira api?", "author_fullname": "t2_cyr5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is airbyte Jira connector only for SaaS version of jira?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ld8fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681432553.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m deciding on the best tool to grab Jira data. I was originally going to use the Jira api and clean up the data myself but I read that airbyte Jira connector is very useful. &lt;/p&gt;\n\n&lt;p&gt;Unfortunately I have been unable to connect it, I keep getting authentication error. I saw some comments on other forums that mentioned the connector only works for Jira cloud not self hosted. Is this true? &lt;/p&gt;\n\n&lt;p&gt;If so is it better to make a custom connector in airbyte or just go with my original plan of grabbing the data via Jira api?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ld8fw", "is_robot_indexable": true, "report_reasons": null, "author": "bigYman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ld8fw/is_airbyte_jira_connector_only_for_saas_version/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ld8fw/is_airbyte_jira_connector_only_for_saas_version/", "subreddit_subscribers": 98717, "created_utc": 1681432553.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Has anyone used https://metaphor.io/ ?\n\nI\u2019m looking for feedback against open source metadata tools like Datahub, Open Metadata etc", "author_fullname": "t2_p0rsp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone used Metaphor?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kys2r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681410862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone used &lt;a href=\"https://metaphor.io/\"&gt;https://metaphor.io/&lt;/a&gt; ?&lt;/p&gt;\n\n&lt;p&gt;I\u2019m looking for feedback against open source metadata tools like Datahub, Open Metadata etc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12kys2r", "is_robot_indexable": true, "report_reasons": null, "author": "darrenhaken", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kys2r/has_anyone_used_metaphor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kys2r/has_anyone_used_metaphor/", "subreddit_subscribers": 98717, "created_utc": 1681410862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey Reddit! Last week we made the codebase for product 100% open source. This week we shipped a dashboard to manage connectors, as well as integrations with Google Drive, Zendesk, Notion and Confluence. This means Sidekick is now the fastest way to sync data from these tools to a vector database.\n\nWhy is this important? For developers building LLM apps, data integrations are often the least interesting and most time consuming part of the process. For those that don\u2019t want to roll their own ETL, Sidekick is an opinionated tool that lets them get an API endpoint to run semantic searches or generative Q&amp;A over their own data in 5 minutes. In a future release, Sidekick will also handle data synchronization via polling/webhooks.\n\nWe use Weaviate\u2019s vector database for the cloud version but plan to be vector database agonistic.\n\nYou can try it here: [https://app.getsidekick.ai/sign-in](https://app.getsidekick.ai/sign-in)\n\nIf you don\u2019t want to share your email, you can use these test credentials: [founders@getsidekick.ai](mailto:founders@getsidekick.ai) / sidekickisawesome\n\nHere's a demo video showing how it works with Zendesk: [https://youtu.be/hH09kWi6Si0](https://youtu.be/hH09kWi6Si0)\n\nGithub link: [https://github.com/ai-sidekick/sidekick](https://github.com/ai-sidekick/sidekick)", "author_fullname": "t2_xle6lsj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sync data from SaaS tools to a vector database automatically", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kwame", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681406302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Reddit! Last week we made the codebase for product 100% open source. This week we shipped a dashboard to manage connectors, as well as integrations with Google Drive, Zendesk, Notion and Confluence. This means Sidekick is now the fastest way to sync data from these tools to a vector database.&lt;/p&gt;\n\n&lt;p&gt;Why is this important? For developers building LLM apps, data integrations are often the least interesting and most time consuming part of the process. For those that don\u2019t want to roll their own ETL, Sidekick is an opinionated tool that lets them get an API endpoint to run semantic searches or generative Q&amp;amp;A over their own data in 5 minutes. In a future release, Sidekick will also handle data synchronization via polling/webhooks.&lt;/p&gt;\n\n&lt;p&gt;We use Weaviate\u2019s vector database for the cloud version but plan to be vector database agonistic.&lt;/p&gt;\n\n&lt;p&gt;You can try it here: &lt;a href=\"https://app.getsidekick.ai/sign-in\"&gt;https://app.getsidekick.ai/sign-in&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you don\u2019t want to share your email, you can use these test credentials: [&lt;a href=\"mailto:founders@getsidekick.ai\"&gt;founders@getsidekick.ai&lt;/a&gt;](mailto:&lt;a href=\"mailto:founders@getsidekick.ai\"&gt;founders@getsidekick.ai&lt;/a&gt;) / sidekickisawesome&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a demo video showing how it works with Zendesk: &lt;a href=\"https://youtu.be/hH09kWi6Si0\"&gt;https://youtu.be/hH09kWi6Si0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Github link: &lt;a href=\"https://github.com/ai-sidekick/sidekick\"&gt;https://github.com/ai-sidekick/sidekick&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12kwame", "is_robot_indexable": true, "report_reasons": null, "author": "Single_Tomato_6233", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kwame/sync_data_from_saas_tools_to_a_vector_database/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kwame/sync_data_from_saas_tools_to_a_vector_database/", "subreddit_subscribers": 98717, "created_utc": 1681406302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, I just got my position as DataOps from previous job as DataEngineer.   \n\n\nI need some ideas from you about what are daily tasks, responsibilities of DataOps vs DataEngineer and where is the clear cut between the 2 ?  \n\n\nCheers", "author_fullname": "t2_93s65yqu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DataEngineering --&gt; DataOps", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kpq9i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681393483.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I just got my position as DataOps from previous job as DataEngineer.   &lt;/p&gt;\n\n&lt;p&gt;I need some ideas from you about what are daily tasks, responsibilities of DataOps vs DataEngineer and where is the clear cut between the 2 ?  &lt;/p&gt;\n\n&lt;p&gt;Cheers&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12kpq9i", "is_robot_indexable": true, "report_reasons": null, "author": "Striking_Athlete5685", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kpq9i/dataengineering_dataops/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kpq9i/dataengineering_dataops/", "subreddit_subscribers": 98717, "created_utc": 1681393483.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am looking for guidance on best practices to create cicd pipeline using databrick to run unit and integration test. I am inclined to use databrick connect however devops team is asking me not to use PAT token which will eliminate the option to use db connect however I am not still convinced with that suggestion as it makes us more dependent on databrick. Can someone help me or guide me here", "author_fullname": "t2_54dd8kh6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CICD pipeline using databrick connect", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kom45", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681391288.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am looking for guidance on best practices to create cicd pipeline using databrick to run unit and integration test. I am inclined to use databrick connect however devops team is asking me not to use PAT token which will eliminate the option to use db connect however I am not still convinced with that suggestion as it makes us more dependent on databrick. Can someone help me or guide me here&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12kom45", "is_robot_indexable": true, "report_reasons": null, "author": "Aromatic_Afternoon31", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kom45/cicd_pipeline_using_databrick_connect/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kom45/cicd_pipeline_using_databrick_connect/", "subreddit_subscribers": 98717, "created_utc": 1681391288.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Based on [this](https://docs.databricks.com/structured-streaming/triggers.html#configuring-incremental-batch-processing), Databricks Runtime &gt;= 10.2 supports the \"availableNow\" trigger that can be used in order to perform batch processing in smaller distinct microbatches, whose size can be configured either via total number of files (maxFilesPerTrigger) or total size in bytes (maxBytesPerTrigger). For my purposes, I am currently using both with the following values:\n\n    maxFilesPerTrigger = 10000\n    maxBytesPerTrigger = \"10gb\"\n\nMy daily batch includes a number of files in the range of 15,000 to 20,000. Having set a pretty high \"maxBytesPerTrigger\" limit, at least in relation to my data, I'd expect that during each batch process, there would form two microbatches, the first one containing 10,000 files, and the second containing the rest. However, it is always the case that three microbatches are formed, withe the first one containing a pretty small number of files (about 500 or so), the second one containing 10,000 files, and the third one containing the rest, usually 5,000 to 10,000 files.\n\nDoes anyone have an idea as to why there are three microbatches instead of two, with the first one always containing a small number of files?", "author_fullname": "t2_q7l1xoqo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Databricks Autoloader, Trigger.AvailableNow and batch size", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kmnha", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681387302.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Based on &lt;a href=\"https://docs.databricks.com/structured-streaming/triggers.html#configuring-incremental-batch-processing\"&gt;this&lt;/a&gt;, Databricks Runtime &amp;gt;= 10.2 supports the &amp;quot;availableNow&amp;quot; trigger that can be used in order to perform batch processing in smaller distinct microbatches, whose size can be configured either via total number of files (maxFilesPerTrigger) or total size in bytes (maxBytesPerTrigger). For my purposes, I am currently using both with the following values:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;maxFilesPerTrigger = 10000\nmaxBytesPerTrigger = &amp;quot;10gb&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;My daily batch includes a number of files in the range of 15,000 to 20,000. Having set a pretty high &amp;quot;maxBytesPerTrigger&amp;quot; limit, at least in relation to my data, I&amp;#39;d expect that during each batch process, there would form two microbatches, the first one containing 10,000 files, and the second containing the rest. However, it is always the case that three microbatches are formed, withe the first one containing a pretty small number of files (about 500 or so), the second one containing 10,000 files, and the third one containing the rest, usually 5,000 to 10,000 files.&lt;/p&gt;\n\n&lt;p&gt;Does anyone have an idea as to why there are three microbatches instead of two, with the first one always containing a small number of files?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12kmnha", "is_robot_indexable": true, "report_reasons": null, "author": "WerdenWissen", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kmnha/databricks_autoloader_triggeravailablenow_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kmnha/databricks_autoloader_triggeravailablenow_and/", "subreddit_subscribers": 98717, "created_utc": 1681387302.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is your setup for loading and maintaining relational data in S3 data lake? How do you handle incremental updates? How do you extract the most recent data? Do you use versioning or custom ETL?", "author_fullname": "t2_w1tdhbrp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Loading and maintaining relational data in S3 lake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kkxcl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681383530.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is your setup for loading and maintaining relational data in S3 data lake? How do you handle incremental updates? How do you extract the most recent data? Do you use versioning or custom ETL?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12kkxcl", "is_robot_indexable": true, "report_reasons": null, "author": "data_pie3", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kkxcl/loading_and_maintaining_relational_data_in_s3_lake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kkxcl/loading_and_maintaining_relational_data_in_s3_lake/", "subreddit_subscribers": 98717, "created_utc": 1681383530.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have multiple API's with different structure.  I have dumped them in raw JSON form. The schema has key, created and last updated date. I want to create a incremental ingestion. have you used control table to achieve this ? I am seeking help just to understand the anti patterns and understand the optimal solution.\n\nMy approach will be load the JSON in raw layer, then do a intial load to the dimension table, create a temp table with source as base and dimension as right table, to populate surrogate key with 0 \\[ Which means this is a insert scenario for non match records)\n\nFor Update if the surrogate key is not null, i will have to check each attribute.\n\nI am sure this will not be a efficient solution as it will involve multiple matching scenario and will definately add lot of overhead to the pipeline. Hence requesting for suggestions", "author_fullname": "t2_qinvsb2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental API ingestion with ADF", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kipj4", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681377996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have multiple API&amp;#39;s with different structure.  I have dumped them in raw JSON form. The schema has key, created and last updated date. I want to create a incremental ingestion. have you used control table to achieve this ? I am seeking help just to understand the anti patterns and understand the optimal solution.&lt;/p&gt;\n\n&lt;p&gt;My approach will be load the JSON in raw layer, then do a intial load to the dimension table, create a temp table with source as base and dimension as right table, to populate surrogate key with 0 [ Which means this is a insert scenario for non match records)&lt;/p&gt;\n\n&lt;p&gt;For Update if the surrogate key is not null, i will have to check each attribute.&lt;/p&gt;\n\n&lt;p&gt;I am sure this will not be a efficient solution as it will involve multiple matching scenario and will definately add lot of overhead to the pipeline. Hence requesting for suggestions&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12kipj4", "is_robot_indexable": true, "report_reasons": null, "author": "cida1205", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12kipj4/incremental_api_ingestion_with_adf/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12kipj4/incremental_api_ingestion_with_adf/", "subreddit_subscribers": 98717, "created_utc": 1681377996.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}