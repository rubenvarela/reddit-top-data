{"kind": "Listing", "data": {"after": null, "dist": 23, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_a49okn69", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who owns data quality?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 128, "top_awarded_type": null, "hide_score": false, "name": "t3_12l9mzx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "ups": 617, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 617, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/pEGgrT6AA0Ee7mh3QIRZ3wvGng2GrkEtE8Vu2pZZaYs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681425657.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/8ma1yb67cqta1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/8ma1yb67cqta1.jpg?auto=webp&amp;v=enabled&amp;s=c76f3ae14fda40cd714446ff0014957170862a87", "width": 544, "height": 500}, "resolutions": [{"url": "https://preview.redd.it/8ma1yb67cqta1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0f5357d6ad8fd22807c299e69c1cea0f02eef2a7", "width": 108, "height": 99}, {"url": "https://preview.redd.it/8ma1yb67cqta1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=733fddc2fdf613c203bb9c2be183514abad5d0ee", "width": 216, "height": 198}, {"url": "https://preview.redd.it/8ma1yb67cqta1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b959130ca293342df5aaf140d27d94fe653fe379", "width": 320, "height": 294}], "variants": {}, "id": "_zipJWJt_VoFsn3ElGALbQLgo-DKOM9i4P2OFhKqSN8"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12l9mzx", "is_robot_indexable": true, "report_reasons": null, "author": "Top-Substance2185", "discussion_type": null, "num_comments": 78, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12l9mzx/who_owns_data_quality/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/8ma1yb67cqta1.jpg", "subreddit_subscribers": 98990, "created_utc": 1681425657.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_16q5j0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exporting to excel is always a people pleaser...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12m8ml7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "ups": 180, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 180, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vEaDT3jzNOo03b2pQKsXWm6Ub5wKYYAGzWEdyltISTY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681498344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/6vtglxi2cwta1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/6vtglxi2cwta1.png?auto=webp&amp;v=enabled&amp;s=9cf19b47ab4a682995c2dc993bbab60faf5e6678", "width": 1182, "height": 1280}, "resolutions": [{"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74e103d9a117ab2ea94bb361002ddf3cca4c34c9", "width": 108, "height": 116}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e62c682e16c902679a397e81e186cb076821ea6", "width": 216, "height": 233}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61d45e345641564675e100b9818ec876e9286225", "width": 320, "height": 346}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e04a031c8f71e72bd1a6cb58439184f1045ed49", "width": 640, "height": 693}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f1685f7d00c688a18cbc5efa2096749d6948205", "width": 960, "height": 1039}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32cd0519cfc66341a05a9ee9fb848816a3bbcb2c", "width": 1080, "height": 1169}], "variants": {}, "id": "4T_doMPlm1O7dNGlmOIpToj64res7j58NG4Xkccej7A"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12m8ml7", "is_robot_indexable": true, "report_reasons": null, "author": "audiologician", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12m8ml7/exporting_to_excel_is_always_a_people_pleaser/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/6vtglxi2cwta1.png", "subreddit_subscribers": 98990, "created_utc": 1681498344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In every interview for a Data Engineer role, Spark Architecture seems be the only concept the recruiters are interested. \n\nI have 1 year experience as a Data Engineer. I work with Databricks on a day to day basis in Azure, without having to learn what's happening in the background (Spark Architecture). But this does seem to be enough to get a new job as Data Engineer.\n\nI tried searching online for a Spark course, but couldn't find the one that has all the important concepts and good for beginners.\n\nExperts of Spark, how did you learn Spark ? I'd really appreciate if you suggest some good resources/courses to learn Spark Architecture, so that I can clear interviews to get a job.\n\n TIA.", "author_fullname": "t2_tme0hylh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not clearing interviews due to Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lu3wk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 66, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 66, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681471782.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In every interview for a Data Engineer role, Spark Architecture seems be the only concept the recruiters are interested. &lt;/p&gt;\n\n&lt;p&gt;I have 1 year experience as a Data Engineer. I work with Databricks on a day to day basis in Azure, without having to learn what&amp;#39;s happening in the background (Spark Architecture). But this does seem to be enough to get a new job as Data Engineer.&lt;/p&gt;\n\n&lt;p&gt;I tried searching online for a Spark course, but couldn&amp;#39;t find the one that has all the important concepts and good for beginners.&lt;/p&gt;\n\n&lt;p&gt;Experts of Spark, how did you learn Spark ? I&amp;#39;d really appreciate if you suggest some good resources/courses to learn Spark Architecture, so that I can clear interviews to get a job.&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12lu3wk", "is_robot_indexable": true, "report_reasons": null, "author": "fightinmee", "discussion_type": null, "num_comments": 41, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12lu3wk/not_clearing_interviews_due_to_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lu3wk/not_clearing_interviews_due_to_spark/", "subreddit_subscribers": 98990, "created_utc": 1681471782.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For anyone that has any interest, I've updated the backend of my Premier League Visualization (Football Data Pipeline) project with the following:\n\n* Implemented code formatting with [Black](https://github.com/psf/black) and linting with [Pylint](https://github.com/pylint-dev/pylint) in my CI pipeline.\n   * Here is my updated GitHub Actions Workflow file: [ci.yml](https://github.com/digitalghost-dev/premier-league/blob/main/.github/workflows/ci.yml) \n* Split up the data endpoints into their own Docker images to achieve more of a \"micro-services\" architecture. Previously, I had one Docker image for all endpoints and made troubleshooting a bit tougher.\n   * The files are under the `/data` folder in my [repo](https://github.com/digitalghost-dev/premier-league/tree/main/data).\n   * I run the containers twice a day now. I'm thinking of upgrading my subscription to allow more calls for more frequent updates.\n   * I also plan to bring in a \"fixtures\" tab to show game scores and history.\n* I also updated the [Streamlit dashboard](https://premierleague.streamlit.app) to include the rest of the teams in the league with their form for their 5 previous games (only games played in the Premier League) in the \"Top Teams Tab\".\n\nI've been learning a lot about code quality and whatnot so I wanted to share how I implemented some of my learnings.\n\nFlowchart has been updated:  \n\n\n[Flowchart](https://preview.redd.it/j35bye26wuta1.png?width=1796&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=340250f3d2f43200e57ca248cb1935d91a200aba)\n\nThanks \ud83e\udee1", "author_fullname": "t2_bix7v2w5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Premier League Project Infrastructure Update", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 110, "top_awarded_type": null, "hide_score": false, "media_metadata": {"j35bye26wuta1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 85, "x": 108, "u": "https://preview.redd.it/j35bye26wuta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=691d81b1d99bccdabc41dab92775c53968142e69"}, {"y": 171, "x": 216, "u": "https://preview.redd.it/j35bye26wuta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b899741e6ca5b8546d32ca0d11c4187deb1b6d1c"}, {"y": 253, "x": 320, "u": "https://preview.redd.it/j35bye26wuta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=40c8e3d5f84f987f65399e0d39341b343418eb00"}, {"y": 506, "x": 640, "u": "https://preview.redd.it/j35bye26wuta1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b0d4f5541797f574c5ba298341fb5779a6261c8"}, {"y": 760, "x": 960, "u": "https://preview.redd.it/j35bye26wuta1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=193b6ac8b2279f9dd78eb44b379b53a9456b4881"}, {"y": 855, "x": 1080, "u": "https://preview.redd.it/j35bye26wuta1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9099ab1a74b6e285acfce07e937880be3f086f64"}], "s": {"y": 1422, "x": 1796, "u": "https://preview.redd.it/j35bye26wuta1.png?width=1796&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=340250f3d2f43200e57ca248cb1935d91a200aba"}, "id": "j35bye26wuta1"}}, "name": "t3_12lyhbw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Personal Project Showcase", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/mPkJXkG_MsFx917j3RCKy1vvH253lbEjuLnd6TBcCZs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1681481275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For anyone that has any interest, I&amp;#39;ve updated the backend of my Premier League Visualization (Football Data Pipeline) project with the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Implemented code formatting with &lt;a href=\"https://github.com/psf/black\"&gt;Black&lt;/a&gt; and linting with &lt;a href=\"https://github.com/pylint-dev/pylint\"&gt;Pylint&lt;/a&gt; in my CI pipeline.\n\n&lt;ul&gt;\n&lt;li&gt;Here is my updated GitHub Actions Workflow file: &lt;a href=\"https://github.com/digitalghost-dev/premier-league/blob/main/.github/workflows/ci.yml\"&gt;ci.yml&lt;/a&gt; &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Split up the data endpoints into their own Docker images to achieve more of a &amp;quot;micro-services&amp;quot; architecture. Previously, I had one Docker image for all endpoints and made troubleshooting a bit tougher.\n\n&lt;ul&gt;\n&lt;li&gt;The files are under the &lt;code&gt;/data&lt;/code&gt; folder in my &lt;a href=\"https://github.com/digitalghost-dev/premier-league/tree/main/data\"&gt;repo&lt;/a&gt;.&lt;/li&gt;\n&lt;li&gt;I run the containers twice a day now. I&amp;#39;m thinking of upgrading my subscription to allow more calls for more frequent updates.&lt;/li&gt;\n&lt;li&gt;I also plan to bring in a &amp;quot;fixtures&amp;quot; tab to show game scores and history.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;I also updated the &lt;a href=\"https://premierleague.streamlit.app\"&gt;Streamlit dashboard&lt;/a&gt; to include the rest of the teams in the league with their form for their 5 previous games (only games played in the Premier League) in the &amp;quot;Top Teams Tab&amp;quot;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve been learning a lot about code quality and whatnot so I wanted to share how I implemented some of my learnings.&lt;/p&gt;\n\n&lt;p&gt;Flowchart has been updated:  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/j35bye26wuta1.png?width=1796&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=340250f3d2f43200e57ca248cb1935d91a200aba\"&gt;Flowchart&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks \ud83e\udee1&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/E5RkEeNtiOx_n3keeFD7y2mZVFNB5FygQ9Y6q2dIenY.jpg?auto=webp&amp;v=enabled&amp;s=2c587632c67a07b89e6842a9dd3f37419c65d61b", "width": 1280, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/E5RkEeNtiOx_n3keeFD7y2mZVFNB5FygQ9Y6q2dIenY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=002f43b3448db5e6361301bdbaead77d63601f6d", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/E5RkEeNtiOx_n3keeFD7y2mZVFNB5FygQ9Y6q2dIenY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=952476ece38f5de8797306207f7c39c3d462b7b1", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/E5RkEeNtiOx_n3keeFD7y2mZVFNB5FygQ9Y6q2dIenY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a6ac2f02e0e585c439bfbaf1c4c8accefe7254b", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/E5RkEeNtiOx_n3keeFD7y2mZVFNB5FygQ9Y6q2dIenY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98ce183d313105e69598ee6e8900a64614d8bd74", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/E5RkEeNtiOx_n3keeFD7y2mZVFNB5FygQ9Y6q2dIenY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c63feafe7fa664a917fda5dfa034816f2cbdea49", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/E5RkEeNtiOx_n3keeFD7y2mZVFNB5FygQ9Y6q2dIenY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f05ea347bf85e629969925af89607a39b309dab3", "width": 1080, "height": 540}], "variants": {}, "id": "3gZoHpwb4UeknyioIxffQdkrkZhaIE_OItkbDbkxS3E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4134b452-dc3b-11ec-a21a-0262096eec38", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ddbd37", "id": "12lyhbw", "is_robot_indexable": true, "report_reasons": null, "author": "digitalghost-dev", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12lyhbw/premier_league_project_infrastructure_update/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lyhbw/premier_league_project_infrastructure_update/", "subreddit_subscribers": 98990, "created_utc": 1681481275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI am new to AWS and I am reaching out to the community to explore our options for building data pipelines. \n\nWe need to export metrics from AWS Prometheus to S3 every 5 minutes and then use this data in Sagemaker to build some ML models. The pipelines should be declarative in the sense that we want to specify what metrics to query. Also there is the possibility that the bussines will want historical data from Prometheus. The data will be either accesed via Athena or we will send it to Redshift. We haven't decided yet. \n\nWhat would be the best services to use to achieve this? My approach would be to use AWS Airflow and just build custom data pipelines. Is there a better way?\n\nThanks!", "author_fullname": "t2_7sdm747h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS data pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lnebj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681455178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I am new to AWS and I am reaching out to the community to explore our options for building data pipelines. &lt;/p&gt;\n\n&lt;p&gt;We need to export metrics from AWS Prometheus to S3 every 5 minutes and then use this data in Sagemaker to build some ML models. The pipelines should be declarative in the sense that we want to specify what metrics to query. Also there is the possibility that the bussines will want historical data from Prometheus. The data will be either accesed via Athena or we will send it to Redshift. We haven&amp;#39;t decided yet. &lt;/p&gt;\n\n&lt;p&gt;What would be the best services to use to achieve this? My approach would be to use AWS Airflow and just build custom data pipelines. Is there a better way?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12lnebj", "is_robot_indexable": true, "report_reasons": null, "author": "aliuta", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12lnebj/aws_data_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lnebj/aws_data_pipelines/", "subreddit_subscribers": 98990, "created_utc": 1681455178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Just a bit of user research\n\n[View Poll](https://www.reddit.com/poll/12lwprj)", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "For those of you with Lakehouse Architectures, how do you handle duplicate records?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lwprj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681477676.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just a bit of user research&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/12lwprj\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12lwprj", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 16, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1681736876996, "options": [{"text": "Architecture never allows for duplicate records", "id": "22556891"}, {"text": "Upserts via table format like Iceberg, Delta, Hudi, etc", "id": "22556892"}, {"text": "Deduplicate at query time", "id": "22556893"}, {"text": "Other (elaborate in comments)", "id": "22556894"}, {"text": "No answer, just want to see results", "id": "22556895"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 366, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12lwprj/for_those_of_you_with_lakehouse_architectures_how/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/12lwprj/for_those_of_you_with_lakehouse_architectures_how/", "subreddit_subscribers": 98990, "created_utc": 1681477676.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, \n\nI was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.\n\nAs I've had no whatsoever experience with BigQuery and didn't really know where or how to start, I asked you guys around here some questions:\n\n* [What should I know before using BigQuery, having traditional MySQL knowledge?](https://www.reddit.com/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/)\n* [How should I understand query limitations in BigQuery?](https://www.reddit.com/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/)\n* [Using redis to batch data from various sources and then bulk insert into BigQuery, is this common?](https://www.reddit.com/r/dataengineering/comments/12dh2ht/using_redis_to_batch_data_from_various_sources/)\n\nWith the help and lead from you folks, I think I finally managed to get the bigger picture. At the same time, I've been writing things down which will hopefully help other software developers or traditional database users get into BigQuery, without going through several courses and documentation to get this \"Ahaaa, that's what it is used for, how it works, and how we should use it\" moment.\n\nI've written such down [as a guide here (preview)](https://hashnode.com/preview/643975a89ce48a000fb1867b), and would love to have some feedback (if possible), just to make sure that I'm not spreading misinformation.\n\nAnyway, thanks so much to this subreddit for all the help you have already given!", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Need feedback] I wrote a guide about the fundamentals of BigQuery for software developers &amp; traditional database users", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mccxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681504289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, &lt;/p&gt;\n\n&lt;p&gt;I was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.&lt;/p&gt;\n\n&lt;p&gt;As I&amp;#39;ve had no whatsoever experience with BigQuery and didn&amp;#39;t really know where or how to start, I asked you guys around here some questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/\"&gt;What should I know before using BigQuery, having traditional MySQL knowledge?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/\"&gt;How should I understand query limitations in BigQuery?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/12dh2ht/using_redis_to_batch_data_from_various_sources/\"&gt;Using redis to batch data from various sources and then bulk insert into BigQuery, is this common?&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With the help and lead from you folks, I think I finally managed to get the bigger picture. At the same time, I&amp;#39;ve been writing things down which will hopefully help other software developers or traditional database users get into BigQuery, without going through several courses and documentation to get this &amp;quot;Ahaaa, that&amp;#39;s what it is used for, how it works, and how we should use it&amp;quot; moment.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve written such down &lt;a href=\"https://hashnode.com/preview/643975a89ce48a000fb1867b\"&gt;as a guide here (preview)&lt;/a&gt;, and would love to have some feedback (if possible), just to make sure that I&amp;#39;m not spreading misinformation.&lt;/p&gt;\n\n&lt;p&gt;Anyway, thanks so much to this subreddit for all the help you have already given!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?auto=webp&amp;v=enabled&amp;s=62defc0be251764d5b096ae39b8f3a0b70085635", "width": 1549, "height": 840}, "resolutions": [{"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=125555f86fe45e004504f7773a2f63e82f1c6bd4", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1c8335002e4f1dd15e3155836126761a017f92c", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac474d0f76b6dc0b062dca3efe72a57762ee3751", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2386e612d99fc461faac9b23f8e31365cf50333", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f88e776b52d428bc55ecc1e19740a898b4d82fb8", "width": 960, "height": 520}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e695de79f1de20268e94f94b0f7a180c17e4d9b9", "width": 1080, "height": 585}], "variants": {}, "id": "Sv3ekwo8yyXSEkV18TtP4eDCKZ_ZqF5Ig7c267Z19ow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mccxv", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mccxv/need_feedback_i_wrote_a_guide_about_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mccxv/need_feedback_i_wrote_a_guide_about_the/", "subreddit_subscribers": 98990, "created_utc": 1681504289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "cc /u/cosmicBb0y", "author_fullname": "t2_3qlqubb2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ArjanCodes: How to Use Pandas With Pandera to Validate Your Data in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12malx1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-tU7fuUiq7w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to Use Pandas With Pandera to Validate Your Data in Python\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to Use Pandas With Pandera to Validate Your Data in Python", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-tU7fuUiq7w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to Use Pandas With Pandera to Validate Your Data in Python\"&gt;&lt;/iframe&gt;", "author_name": "ArjanCodes", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/-tU7fuUiq7w/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ArjanCodes"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-tU7fuUiq7w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to Use Pandas With Pandera to Validate Your Data in Python\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12malx1", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/pJryRzjFi8keLzZTP6aHwFCOIIEqur6MaBTgKpOZPO4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681501223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;cc &lt;a href=\"/u/cosmicBb0y\"&gt;/u/cosmicBb0y&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/-tU7fuUiq7w", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KQk5h-3dDIbI0zxJoeYHj7YOnAcc5X76Rjd-taBTIbQ.jpg?auto=webp&amp;v=enabled&amp;s=9a94916997565943bda38b1a61e337546537998d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/KQk5h-3dDIbI0zxJoeYHj7YOnAcc5X76Rjd-taBTIbQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6220ed74050f390859a25279a58f6417e4c1f93e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/KQk5h-3dDIbI0zxJoeYHj7YOnAcc5X76Rjd-taBTIbQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8e62a1f6e14b71c2f3bcfd371e8f5c35d7b5a33", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/KQk5h-3dDIbI0zxJoeYHj7YOnAcc5X76Rjd-taBTIbQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a44a3a4b66cf77773d1f41656b54d1b58099740", "width": 320, "height": 240}], "variants": {}, "id": "bEK7On-ViuRVvW7es_cui0WHxoZKbaxvhRL-8wwjTxc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12malx1", "is_robot_indexable": true, "report_reasons": null, "author": "EarthGoddessDude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12malx1/arjancodes_how_to_use_pandas_with_pandera_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/-tU7fuUiq7w", "subreddit_subscribers": 98990, "created_utc": 1681501223.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to Use Pandas With Pandera to Validate Your Data in Python", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-tU7fuUiq7w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to Use Pandas With Pandera to Validate Your Data in Python\"&gt;&lt;/iframe&gt;", "author_name": "ArjanCodes", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/-tU7fuUiq7w/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ArjanCodes"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How would you achieve this?   \n\n\nFor the brave, I will add some caveats, but I am still interested in your approach in the simple case described in the title.\n\nCaveats:\n\n\\- Postgres uses table partitioning (this means, that the WAL changes are associated with the partition tables and not the top-level table)\n\n\\- No dupes in Redshift. Redshift doesn't enforce primary key uniqueness, and so you may have duplicate entries for the same ID. That is undesirable.\n\n\\- Can it be done in AWS only?", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Move data from Postgres to Redshift with CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m2ukb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681489717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How would you achieve this?   &lt;/p&gt;\n\n&lt;p&gt;For the brave, I will add some caveats, but I am still interested in your approach in the simple case described in the title.&lt;/p&gt;\n\n&lt;p&gt;Caveats:&lt;/p&gt;\n\n&lt;p&gt;- Postgres uses table partitioning (this means, that the WAL changes are associated with the partition tables and not the top-level table)&lt;/p&gt;\n\n&lt;p&gt;- No dupes in Redshift. Redshift doesn&amp;#39;t enforce primary key uniqueness, and so you may have duplicate entries for the same ID. That is undesirable.&lt;/p&gt;\n\n&lt;p&gt;- Can it be done in AWS only?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12m2ukb", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12m2ukb/move_data_from_postgres_to_redshift_with_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12m2ukb/move_data_from_postgres_to_redshift_with_cdc/", "subreddit_subscribers": 98990, "created_utc": 1681489717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In software engineering we have sdet and QA''s to do testing so in same way do we have specific people to do testing.\nIn my squad we data engineer are it self doing testing for our projects and in other squad data analyst does that\nIs it same in your company also?", "author_fullname": "t2_7yh1jlaz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Who does testing in data teams", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lnev8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681455216.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In software engineering we have sdet and QA&amp;#39;&amp;#39;s to do testing so in same way do we have specific people to do testing.\nIn my squad we data engineer are it self doing testing for our projects and in other squad data analyst does that\nIs it same in your company also?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12lnev8", "is_robot_indexable": true, "report_reasons": null, "author": "Foot_Straight", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12lnev8/who_does_testing_in_data_teams/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lnev8/who_does_testing_in_data_teams/", "subreddit_subscribers": 98990, "created_utc": 1681455216.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7e04ujnq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One day we\u2019ll get the respect we deserve \ud83e\udd72", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_12meohj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "transparent", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GmsK2oEav88xxMD7QOlgMfPZV2WFRX1Bpqxo7d7bKYU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681508747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/pqafjpltoyta1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?auto=webp&amp;v=enabled&amp;s=11ef40cedf5bfac7083596aa648a9f410b82384c", "width": 1098, "height": 1226}, "resolutions": [{"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66ff07132e1bc0507ed0cd9c59a3ed0f161d6a15", "width": 108, "height": 120}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12448513d4d5ccd06f7d98fc8b445c195b90b606", "width": 216, "height": 241}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a74f3a1d8588ed2465d697fc918633baf921f5b2", "width": 320, "height": 357}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b41b4d9a92b0ae85633b9cfe33f9248e6720cdd", "width": 640, "height": 714}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=950423ddfb43696d3aee15c1b088be56fd774027", "width": 960, "height": 1071}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=381d5638798b83bbfc888e8bb60a898876b27f3e", "width": 1080, "height": 1205}], "variants": {}, "id": "jh4DkOdZ72CqqpKCLgwjLYq3-IWOCCRc5lIMe3RgrhI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer That Broke All ETLs", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12meohj", "is_robot_indexable": true, "report_reasons": null, "author": "ThatGrayZ", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12meohj/one_day_well_get_the_respect_we_deserve/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/pqafjpltoyta1.jpg", "subreddit_subscribers": 98990, "created_utc": 1681508747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nLooking for some suggestions. We get Kafka data landed directly in Snowflake. A table per event with two variant columns:\n\n- meta_data;\n- event_content.\n\nThe first contains a json payload with partition, offset, timestamp. The second contains the actual payload.\n\nThe first step is to flatten the data, which is performed as a view. We then have an incrementally loaded table of the flattened data, that uses a QUALIFY statement to remove duplicates, as Kafka will deliver at least once.\n\nWith the timestamp being contained in a variant column, how much overhead are we likely eating up each time we need to incrementally load our table? Has this been architected badly? It was done before my time. It might be fine, but it seems we might be chewing through a fair amount of compute as a result of this design.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advise on incremental process of Kafka data on Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m2azk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681488660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Looking for some suggestions. We get Kafka data landed directly in Snowflake. A table per event with two variant columns:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;meta_data;&lt;/li&gt;\n&lt;li&gt;event_content.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The first contains a json payload with partition, offset, timestamp. The second contains the actual payload.&lt;/p&gt;\n\n&lt;p&gt;The first step is to flatten the data, which is performed as a view. We then have an incrementally loaded table of the flattened data, that uses a QUALIFY statement to remove duplicates, as Kafka will deliver at least once.&lt;/p&gt;\n\n&lt;p&gt;With the timestamp being contained in a variant column, how much overhead are we likely eating up each time we need to incrementally load our table? Has this been architected badly? It was done before my time. It might be fine, but it seems we might be chewing through a fair amount of compute as a result of this design.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12m2azk", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12m2azk/advise_on_incremental_process_of_kafka_data_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12m2azk/advise_on_incremental_process_of_kafka_data_on/", "subreddit_subscribers": 98990, "created_utc": 1681488660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As stated in the title, there is the need to handle image data in a new project. There are 3 termo-cameras that acquire a frame 640x480 for the visual information and another layer 640x480 for the termo information. Each pixel of the visual information takes 3/4 bytes (I suppose, 3 for the RGB, 1 for the alpha) and the termo information takes 3 bytes (I suppose, as there is a sensitivity of 35mK and the possible range of acquisition is around 800 C\u00b0).\n\nFrom my calculation the worst case scenario of the size of each image, in MB,  is as follows:\n\nVisual: (640x480x4)/(1024x1024) = 1.17 MB\n\nTermo: (640x480x3)/(1024x1024) =  0.88 MB\n\nTotal = 2.05 MB\n\nThe worst-case sampling rate to date is 1Hz as the termo dynamics of the process is slow and the visual information is not exploited for the running algorithms. Taking into account this condition, and the will to save the raw image information, there would be a stream of data as follows:\n\n2.05 \\[MB/camera\\] x 3 \\[camera\\] = 6.15 MB/s\n\nThere is the need to think about an architecture to store these images, because we want to save interesting data. The process could last up to five days, which brings to:\n\n6.15 \\[MB/s\\] x 3600 \\[s/hour\\] x 24 \\[hour/day\\] x 5 \\[day\\] =  2595 \\[GB/process\\] = 2.53 \\[TB/process\\]\n\nWhat would be the best architecture to handle this stream of data and the storage need? In the future we might also access this data, but I think for post-processing analysis, e.g. the access would be sporadic just to perform some data analysis by data scientists or review the process data. Also at the moment, there will be also a edge computing, dedicated to process the acquired frames on-line in order to analysise the process.\n\nThis is my first project with image data, feel free to tell me anything, I'm exploring.", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Review needed: first project with data images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lmjex", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681453004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As stated in the title, there is the need to handle image data in a new project. There are 3 termo-cameras that acquire a frame 640x480 for the visual information and another layer 640x480 for the termo information. Each pixel of the visual information takes 3/4 bytes (I suppose, 3 for the RGB, 1 for the alpha) and the termo information takes 3 bytes (I suppose, as there is a sensitivity of 35mK and the possible range of acquisition is around 800 C\u00b0).&lt;/p&gt;\n\n&lt;p&gt;From my calculation the worst case scenario of the size of each image, in MB,  is as follows:&lt;/p&gt;\n\n&lt;p&gt;Visual: (640x480x4)/(1024x1024) = 1.17 MB&lt;/p&gt;\n\n&lt;p&gt;Termo: (640x480x3)/(1024x1024) =  0.88 MB&lt;/p&gt;\n\n&lt;p&gt;Total = 2.05 MB&lt;/p&gt;\n\n&lt;p&gt;The worst-case sampling rate to date is 1Hz as the termo dynamics of the process is slow and the visual information is not exploited for the running algorithms. Taking into account this condition, and the will to save the raw image information, there would be a stream of data as follows:&lt;/p&gt;\n\n&lt;p&gt;2.05 [MB/camera] x 3 [camera] = 6.15 MB/s&lt;/p&gt;\n\n&lt;p&gt;There is the need to think about an architecture to store these images, because we want to save interesting data. The process could last up to five days, which brings to:&lt;/p&gt;\n\n&lt;p&gt;6.15 [MB/s] x 3600 [s/hour] x 24 [hour/day] x 5 [day] =  2595 [GB/process] = 2.53 [TB/process]&lt;/p&gt;\n\n&lt;p&gt;What would be the best architecture to handle this stream of data and the storage need? In the future we might also access this data, but I think for post-processing analysis, e.g. the access would be sporadic just to perform some data analysis by data scientists or review the process data. Also at the moment, there will be also a edge computing, dedicated to process the acquired frames on-line in order to analysise the process.&lt;/p&gt;\n\n&lt;p&gt;This is my first project with image data, feel free to tell me anything, I&amp;#39;m exploring.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12lmjex", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12lmjex/review_needed_first_project_with_data_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lmjex/review_needed_first_project_with_data_images/", "subreddit_subscribers": 98990, "created_utc": 1681453004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Im currently a uni student and realised that DE was not an entry level role out of college, and instead something you transition into. So I wanted to know if the DE knowledge I've accumulated would make me competitive in getting into a DA role by any chance? By knowledge I mean like data warehousing, ETL, data modelling and some experience with using ETL tools?", "author_fullname": "t2_76fvluuq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would DE skills be somewhat useful in a DA role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lkpdd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681448534.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im currently a uni student and realised that DE was not an entry level role out of college, and instead something you transition into. So I wanted to know if the DE knowledge I&amp;#39;ve accumulated would make me competitive in getting into a DA role by any chance? By knowledge I mean like data warehousing, ETL, data modelling and some experience with using ETL tools?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12lkpdd", "is_robot_indexable": true, "report_reasons": null, "author": "notGaruda1", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12lkpdd/would_de_skills_be_somewhat_useful_in_a_da_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lkpdd/would_de_skills_be_somewhat_useful_in_a_da_role/", "subreddit_subscribers": 98990, "created_utc": 1681448534.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How to use Chat GPT or suggest any other online free tool for query tuning.\n\nI have data analysts writing a bunch of queries that uses same set tables of tables over and over again in left joins and CTEs with full table scans \ud83d\ude35\u200d\ud83d\udcab\ud83e\udd74", "author_fullname": "t2_f2obp4el", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "T-Sql Query Tuning", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12l8ryw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681424647.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How to use Chat GPT or suggest any other online free tool for query tuning.&lt;/p&gt;\n\n&lt;p&gt;I have data analysts writing a bunch of queries that uses same set tables of tables over and over again in left joins and CTEs with full table scans \ud83d\ude35\u200d\ud83d\udcab\ud83e\udd74&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12l8ryw", "is_robot_indexable": true, "report_reasons": null, "author": "InterestingDot8089", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12l8ryw/tsql_query_tuning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12l8ryw/tsql_query_tuning/", "subreddit_subscribers": 98990, "created_utc": 1681424647.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the folks who are self-hosting Dremio, what are your preferred method of hosting in AWS?\n\nI have deployed via the AWS Market option for the demo but long term will probably want some other option for deployment other than the AWS Market Cloudformation.\n\n&amp;#x200B;\n\nAre folks deploying Dremio via EC2 instances (in which we would define the infra with Terraform)?  Anyone running Dremio on Kubernetes?", "author_fullname": "t2_fwy0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dremio - Preferred Deployment Method for Self-Hosting in AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12mcuus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681505274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the folks who are self-hosting Dremio, what are your preferred method of hosting in AWS?&lt;/p&gt;\n\n&lt;p&gt;I have deployed via the AWS Market option for the demo but long term will probably want some other option for deployment other than the AWS Market Cloudformation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Are folks deploying Dremio via EC2 instances (in which we would define the infra with Terraform)?  Anyone running Dremio on Kubernetes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mcuus", "is_robot_indexable": true, "report_reasons": null, "author": "exemaitch", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mcuus/dremio_preferred_deployment_method_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mcuus/dremio_preferred_deployment_method_for/", "subreddit_subscribers": 98990, "created_utc": 1681505274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We are currently looking for an alternative to our aging hadoop cluster (very tiny, \\~6 instances)\n\nDo you think Databend.rs would be an alternative? Is is production ready? Or is it too early?", "author_fullname": "t2_5o9ebpsl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Databend production ready?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m0mup", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681485422.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We are currently looking for an alternative to our aging hadoop cluster (very tiny, ~6 instances)&lt;/p&gt;\n\n&lt;p&gt;Do you think Databend.rs would be an alternative? Is is production ready? Or is it too early?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12m0mup", "is_robot_indexable": true, "report_reasons": null, "author": "mosquitsch", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12m0mup/is_databend_production_ready/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12m0mup/is_databend_production_ready/", "subreddit_subscribers": 98990, "created_utc": 1681485422.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey guys - I got a question for you experts out there. I was trying to build a thesis on Modern Data Stack and I wanted to understand why does Snowflake-like products get big (in the data warehousing space) when Google, Amazon, Microsoft all have similar offerings available and all of them have one less challenge to face i.e. the distribution.\n\nAlso, now that Snowflake, Bigquery et al have made their own niche for so many years, why do new companies like Firebolt (an Israeli unicorn, started in 2019) come up every now and then and get so much funding?\n\nCan anyone please explain. TIA", "author_fullname": "t2_a4d2j3fh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Traditional Data tools vs new start-ups that are getting funded", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lvapt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681474642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys - I got a question for you experts out there. I was trying to build a thesis on Modern Data Stack and I wanted to understand why does Snowflake-like products get big (in the data warehousing space) when Google, Amazon, Microsoft all have similar offerings available and all of them have one less challenge to face i.e. the distribution.&lt;/p&gt;\n\n&lt;p&gt;Also, now that Snowflake, Bigquery et al have made their own niche for so many years, why do new companies like Firebolt (an Israeli unicorn, started in 2019) come up every now and then and get so much funding?&lt;/p&gt;\n\n&lt;p&gt;Can anyone please explain. TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12lvapt", "is_robot_indexable": true, "report_reasons": null, "author": "Living-Nobody-2727", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12lvapt/traditional_data_tools_vs_new_startups_that_are/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lvapt/traditional_data_tools_vs_new_startups_that_are/", "subreddit_subscribers": 98990, "created_utc": 1681474642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am at the early stage in my career, currently looking for a data engineer role, and I am not sure what I should call my first job in my resume.\n\nIn this role I stayed for a year out of college, I primarily (maybe 70% of the time) wrote lots of queries to deliver raw excel files (sometimes very minor visualizations - but never did \u2018analysis\u2019) to all kind of different teams. Our team name was DevOps but I think it was close to what DBA team looks like. I was tasked with auditing/db health monitoring/backup or batch maintenance/etc, those are what I can think of now. (30% of the time)\n\nMy role was called as something like data specialist, but I don\u2019t think that\u2019s a common name. Should I list myself as DBA? What would you say my role was?", "author_fullname": "t2_5pafg8ca6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What would you call my first job?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lmmjx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681453225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am at the early stage in my career, currently looking for a data engineer role, and I am not sure what I should call my first job in my resume.&lt;/p&gt;\n\n&lt;p&gt;In this role I stayed for a year out of college, I primarily (maybe 70% of the time) wrote lots of queries to deliver raw excel files (sometimes very minor visualizations - but never did \u2018analysis\u2019) to all kind of different teams. Our team name was DevOps but I think it was close to what DBA team looks like. I was tasked with auditing/db health monitoring/backup or batch maintenance/etc, those are what I can think of now. (30% of the time)&lt;/p&gt;\n\n&lt;p&gt;My role was called as something like data specialist, but I don\u2019t think that\u2019s a common name. Should I list myself as DBA? What would you say my role was?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12lmmjx", "is_robot_indexable": true, "report_reasons": null, "author": "TaxGreat2308", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12lmmjx/what_would_you_call_my_first_job/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lmmjx/what_would_you_call_my_first_job/", "subreddit_subscribers": 98990, "created_utc": 1681453225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm deciding on the best tool to grab Jira data. I was originally going to use the Jira api and clean up the data myself but I read that airbyte Jira connector is very useful. \n\nUnfortunately I have been unable to connect it, I keep getting authentication error. I saw some comments on other forums that mentioned the connector only works for Jira cloud not self hosted. Is this true? \n\nIf so is it better to make a custom connector in airbyte or just go with my original plan of grabbing the data via Jira api?", "author_fullname": "t2_cyr5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is airbyte Jira connector only for SaaS version of jira?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ld8fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681432553.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m deciding on the best tool to grab Jira data. I was originally going to use the Jira api and clean up the data myself but I read that airbyte Jira connector is very useful. &lt;/p&gt;\n\n&lt;p&gt;Unfortunately I have been unable to connect it, I keep getting authentication error. I saw some comments on other forums that mentioned the connector only works for Jira cloud not self hosted. Is this true? &lt;/p&gt;\n\n&lt;p&gt;If so is it better to make a custom connector in airbyte or just go with my original plan of grabbing the data via Jira api?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ld8fw", "is_robot_indexable": true, "report_reasons": null, "author": "bigYman", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ld8fw/is_airbyte_jira_connector_only_for_saas_version/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ld8fw/is_airbyte_jira_connector_only_for_saas_version/", "subreddit_subscribers": 98990, "created_utc": 1681432553.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "ETL can be one of the most expensive costs of data engineering for data warehousing.  Today, Databricks announced they were able to perform the typical ETL of an EDW, with all the transformations and rules, at breakneck speeds, and cheap cost.  Would love your thoughts on this, and can you try it out for yourselves and let us know what you think!  \n\n&amp;#x200B;\n\n[https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html](https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html)\n\n&amp;#x200B;\n\nDirect link to Repo to Repro: [https://github.com/shannon-barrow/databricks-tpc-di](https://github.com/shannon-barrow/databricks-tpc-di)", "author_fullname": "t2_8ke8s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ETL 1 Billion rows for less than $1 with Delta Lives Tables on Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lx0mt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681478295.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ETL can be one of the most expensive costs of data engineering for data warehousing.  Today, Databricks announced they were able to perform the typical ETL of an EDW, with all the transformations and rules, at breakneck speeds, and cheap cost.  Would love your thoughts on this, and can you try it out for yourselves and let us know what you think!  &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html\"&gt;https://www.databricks.com/blog/2023/04/14/how-we-performed-etl-one-billion-records-under-1-delta-live-tables.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Direct link to Repo to Repro: &lt;a href=\"https://github.com/shannon-barrow/databricks-tpc-di\"&gt;https://github.com/shannon-barrow/databricks-tpc-di&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?auto=webp&amp;v=enabled&amp;s=15e7319434e1e103352a37e7fabfbd9456a168ef", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1176850e76031e71bb122f9c353101bd7abe6bf", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=429d70d1e08de4ce9c49426ac4caa101f4c3e264", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=29cde5f1616959571c9b58b8c1c1900201c77f7e", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b58b543aa8701ba0a87a3198960697d53ff22c", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd2d8ab37cf854034f841dea22a655dc91a5f3b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/oXmGBjnOGbMLNj6ZyveOgQGOrgxxEmgC0EwkH-pffPo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47ceb6115a4ccc0e21696967727505ec48f78f37", "width": 1080, "height": 567}], "variants": {}, "id": "RDPFo3n-9ZSpTUT0k9sCNnHc7tSD0wBu2TyDFfITIDs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12lx0mt", "is_robot_indexable": true, "report_reasons": null, "author": "letmebefrankwithyou", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12lx0mt/etl_1_billion_rows_for_less_than_1_with_delta/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lx0mt/etl_1_billion_rows_for_less_than_1_with_delta/", "subreddit_subscribers": 98990, "created_utc": 1681478295.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello:  \nMy employer has hired a data engineer/architect. We went over his grand design to replace the firms litany of legacy applications that handle everything from real-time transaction data to automated report-making, to email notifications, to FTP, to orchestration. Overall probably 10-14 different applications. Our architect's design is as follows:  \n\n\n* Azure Data Factory to handle literally everything.\n* Logic Apps to handle email notifications, since that's the one thing ADF can't do.\n* A SQL database. \n* Power BI Paginated for reporting.\n\nThat's it. That's all the tools we shall ever require. I think this stems from a phobia of coding, some of his behavior and opinions corroborate with this.   \n\n\nI'm writing to ask you all: is Azure Data Factory really that good? Is it typical \"best practice\" \"industry standard\" to not involve any amount of code? What's your thoughts on low-code? Personally, I think there's some glaring issues with the architecture, but I want to see if I'm missing something not-obvious before opening my mouth.\n\nThanks!", "author_fullname": "t2_lwmkqytr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineer is terrified of programming", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lobgb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "author_cakeday": true, "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681457482.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello:&lt;br/&gt;\nMy employer has hired a data engineer/architect. We went over his grand design to replace the firms litany of legacy applications that handle everything from real-time transaction data to automated report-making, to email notifications, to FTP, to orchestration. Overall probably 10-14 different applications. Our architect&amp;#39;s design is as follows:  &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Azure Data Factory to handle literally everything.&lt;/li&gt;\n&lt;li&gt;Logic Apps to handle email notifications, since that&amp;#39;s the one thing ADF can&amp;#39;t do.&lt;/li&gt;\n&lt;li&gt;A SQL database. &lt;/li&gt;\n&lt;li&gt;Power BI Paginated for reporting.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That&amp;#39;s it. That&amp;#39;s all the tools we shall ever require. I think this stems from a phobia of coding, some of his behavior and opinions corroborate with this.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m writing to ask you all: is Azure Data Factory really that good? Is it typical &amp;quot;best practice&amp;quot; &amp;quot;industry standard&amp;quot; to not involve any amount of code? What&amp;#39;s your thoughts on low-code? Personally, I think there&amp;#39;s some glaring issues with the architecture, but I want to see if I&amp;#39;m missing something not-obvious before opening my mouth.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12lobgb", "is_robot_indexable": true, "report_reasons": null, "author": "c0ntrap0sitive", "discussion_type": null, "num_comments": 37, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12lobgb/data_engineer_is_terrified_of_programming/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12lobgb/data_engineer_is_terrified_of_programming/", "subreddit_subscribers": 98990, "created_utc": 1681457482.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm currently in the US Army and plan on becoming a data engineer after I get out. My plan is to take the Data Science: Analytics course on Codecademy, then start applying for part time jobs (I'm assuming I'd still be in the army).\n\n After the codecademy course, I plan on taking the new Google Data Analytics course to strengthen my data science skills. \n\nI'm assuming I'll get my foot in the door as an entry level data analyst/data scientist and then work my way up to data engineer.\n\nI'm also going to be working on the data anlytics/data engineering cloud certs from the big three (AWS, Azure, and GCP)\n\nWhat do ya'll think of my plan?", "author_fullname": "t2_7s8rukjx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My plan to become a data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ldrgk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.43, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681433634.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently in the US Army and plan on becoming a data engineer after I get out. My plan is to take the Data Science: Analytics course on Codecademy, then start applying for part time jobs (I&amp;#39;m assuming I&amp;#39;d still be in the army).&lt;/p&gt;\n\n&lt;p&gt;After the codecademy course, I plan on taking the new Google Data Analytics course to strengthen my data science skills. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m assuming I&amp;#39;ll get my foot in the door as an entry level data analyst/data scientist and then work my way up to data engineer.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also going to be working on the data anlytics/data engineering cloud certs from the big three (AWS, Azure, and GCP)&lt;/p&gt;\n\n&lt;p&gt;What do ya&amp;#39;ll think of my plan?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ldrgk", "is_robot_indexable": true, "report_reasons": null, "author": "african_kid_1", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ldrgk/my_plan_to_become_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ldrgk/my_plan_to_become_a_data_engineer/", "subreddit_subscribers": 98990, "created_utc": 1681433634.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}