{"kind": "Listing", "data": {"after": "t3_12lfh0n", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Like many others I got laid off in December. Been struggling finding work. Interviews have slowed much since q1 and starting to get worried. Anyone have any luck finding a job? Any tips?", "author_fullname": "t2_8o0eldke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Anyone else struggling to find work?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kmpif", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 126, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 126, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681387415.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like many others I got laid off in December. Been struggling finding work. Interviews have slowed much since q1 and starting to get worried. Anyone have any luck finding a job? Any tips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12kmpif", "is_robot_indexable": true, "report_reasons": null, "author": "djaycat", "discussion_type": null, "num_comments": 66, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12kmpif/anyone_else_struggling_to_find_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12kmpif/anyone_else_struggling_to_find_work/", "subreddit_subscribers": 872694, "created_utc": 1681387415.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm curious to hear your experiences on how the hiring bar has changed, especially for those that have been in the field 5+ years.\n\nFrom a purely anecdotal perspective, it feels that the hiring bar for data scientists has gone up and is all over the place. I might be wrong, but it felt that 5 years ago if you knew A/B testing + jupyter notebooks it was good enough.\n\nDisclaimer: there's no one definitive definition of what a data scientist is, so each company/field will have different criteria. Obviously you can't thoroughly test for everything, sometimes it's just a \"Can you tell me what X is and how it's used?\".\n\nThese are the interview elements where I feel the bar has gone up:\n\n* More leetcode medium, DS/ML coding level is moving closer to SWE. Definitely ran into some leetcode hards. \n* MLE leaning roles will ask for knowledge on how to productize DS projects. Knowledge of AWS and containers is sort of expected.\n* DS roles expect at least some idea of how neural networks work, sometimes how transformers work.\n\nWhat has not changed and will probably remain the same:\n\n* SQL still very relevant, not much change in difficulty.\n* Classification metrics.\n* Ability to translate business problems into DS action items.\n* Hypothesis testing.", "author_fullname": "t2_4oceb7wh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reflecting on the Changing Hiring Bar", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12l8o0k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 104, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 104, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681424518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious to hear your experiences on how the hiring bar has changed, especially for those that have been in the field 5+ years.&lt;/p&gt;\n\n&lt;p&gt;From a purely anecdotal perspective, it feels that the hiring bar for data scientists has gone up and is all over the place. I might be wrong, but it felt that 5 years ago if you knew A/B testing + jupyter notebooks it was good enough.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: there&amp;#39;s no one definitive definition of what a data scientist is, so each company/field will have different criteria. Obviously you can&amp;#39;t thoroughly test for everything, sometimes it&amp;#39;s just a &amp;quot;Can you tell me what X is and how it&amp;#39;s used?&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;These are the interview elements where I feel the bar has gone up:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;More leetcode medium, DS/ML coding level is moving closer to SWE. Definitely ran into some leetcode hards. &lt;/li&gt;\n&lt;li&gt;MLE leaning roles will ask for knowledge on how to productize DS projects. Knowledge of AWS and containers is sort of expected.&lt;/li&gt;\n&lt;li&gt;DS roles expect at least some idea of how neural networks work, sometimes how transformers work.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What has not changed and will probably remain the same:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;SQL still very relevant, not much change in difficulty.&lt;/li&gt;\n&lt;li&gt;Classification metrics.&lt;/li&gt;\n&lt;li&gt;Ability to translate business problems into DS action items.&lt;/li&gt;\n&lt;li&gt;Hypothesis testing.&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12l8o0k", "is_robot_indexable": true, "report_reasons": null, "author": "HummusEconomics", "discussion_type": null, "num_comments": 30, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12l8o0k/reflecting_on_the_changing_hiring_bar/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12l8o0k/reflecting_on_the_changing_hiring_bar/", "subreddit_subscribers": 872694, "created_utc": 1681424518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have \\~6k positive samples of a fraud class; and \\~110k unlabeled  samples of mostly negative classes. Although I don't have labels for these 110k samples, I assume that the majority belongs to the negative class.  However, in my assumption, I know that there are some positive samples  in this unlabeled data set. \n\nWhat do you think it would be the best approach to detect these fraud samples in the unlabeled data set?    \n1- I was thinking in a binary classification approach after removing samples that have the highest chance of being outlier/anomaly on the unlabeled data set;  \n2- Maybe go for an anomaly detection model only or one-class classification\n\nThanks in advance!", "author_fullname": "t2_9o8ch0c3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help on this fraud detection problem", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lahml", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681427122.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have ~6k positive samples of a fraud class; and ~110k unlabeled  samples of mostly negative classes. Although I don&amp;#39;t have labels for these 110k samples, I assume that the majority belongs to the negative class.  However, in my assumption, I know that there are some positive samples  in this unlabeled data set. &lt;/p&gt;\n\n&lt;p&gt;What do you think it would be the best approach to detect these fraud samples in the unlabeled data set?&lt;br/&gt;\n1- I was thinking in a binary classification approach after removing samples that have the highest chance of being outlier/anomaly on the unlabeled data set;&lt;br/&gt;\n2- Maybe go for an anomaly detection model only or one-class classification&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lahml", "is_robot_indexable": true, "report_reasons": null, "author": "le_bebop", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lahml/help_on_this_fraud_detection_problem/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lahml/help_on_this_fraud_detection_problem/", "subreddit_subscribers": 872694, "created_utc": 1681427122.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I graduated May 2021 and started working as a Data Scientist at General Motors. I was given a project but there was no project manager, customer, deadline or anything. I held myself accountable for 5 months setting my own deadlines etc until I realized that this just wasn't a real project and the company was terribly mismanaged. I took initiative to find other projects and each project I got just kept getting canceled or there wasn't any data. \n\nThen I switched to a Solutions Data Scientist role July 2022 at a bigger tech company last year. I spent the first 3 months in training (completely irrelevant to Data Science) and then had absolutely no work. I networked like crazy and got a project on another team last fall which was somewhat exciting but this year there has been absolutely nothing. The other teams seem to have work but they won't let me on to even shadow because they want to prioritize utilization for their employees. My team has no data science work or data to play with. The Solutions Data Scientist role is weird because it is client-facing so if the clients don't want to engage, there is no work. Another challenge is that even when the client engage, our priority is selling the company's AI products so there is minimal actual data science involved.\n\nI'm kind of concerned that I'm going to end up with years of Data Science experience with nothing to show for it. What should I do? If I applied for Data Science at another company now, it would be hard given the recessionary environment and the fact that I've probably gotten pretty technically soft now.\n\nIs this the reality with Data Science roles? Is it like a fake job? I'm wondering if I should move to a different role, like Product Management, where I might be guaranteed a steady flow of work to do.\n\nI'm also kind of concerned about Chat GPT4. My company sells products in the NLP space and with Chat GPT4, I feel like our products will be pretty obsolete soon.", "author_fullname": "t2_8cg2z0mf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Haven't had any real work for the last 2 years at 2 different companies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12li3r3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 22, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 22, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681442744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I graduated May 2021 and started working as a Data Scientist at General Motors. I was given a project but there was no project manager, customer, deadline or anything. I held myself accountable for 5 months setting my own deadlines etc until I realized that this just wasn&amp;#39;t a real project and the company was terribly mismanaged. I took initiative to find other projects and each project I got just kept getting canceled or there wasn&amp;#39;t any data. &lt;/p&gt;\n\n&lt;p&gt;Then I switched to a Solutions Data Scientist role July 2022 at a bigger tech company last year. I spent the first 3 months in training (completely irrelevant to Data Science) and then had absolutely no work. I networked like crazy and got a project on another team last fall which was somewhat exciting but this year there has been absolutely nothing. The other teams seem to have work but they won&amp;#39;t let me on to even shadow because they want to prioritize utilization for their employees. My team has no data science work or data to play with. The Solutions Data Scientist role is weird because it is client-facing so if the clients don&amp;#39;t want to engage, there is no work. Another challenge is that even when the client engage, our priority is selling the company&amp;#39;s AI products so there is minimal actual data science involved.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m kind of concerned that I&amp;#39;m going to end up with years of Data Science experience with nothing to show for it. What should I do? If I applied for Data Science at another company now, it would be hard given the recessionary environment and the fact that I&amp;#39;ve probably gotten pretty technically soft now.&lt;/p&gt;\n\n&lt;p&gt;Is this the reality with Data Science roles? Is it like a fake job? I&amp;#39;m wondering if I should move to a different role, like Product Management, where I might be guaranteed a steady flow of work to do.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also kind of concerned about Chat GPT4. My company sells products in the NLP space and with Chat GPT4, I feel like our products will be pretty obsolete soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12li3r3", "is_robot_indexable": true, "report_reasons": null, "author": "Legitimate_Ebb3623", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12li3r3/havent_had_any_real_work_for_the_last_2_years_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12li3r3/havent_had_any_real_work_for_the_last_2_years_at/", "subreddit_subscribers": 872694, "created_utc": 1681442744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "TLDR: I projected some performance improvement from my model but when put into production it was a complete shitshow. And now im scared I will either get piped or fired.", "author_fullname": "t2_7uv7n9v6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I worked on a DS project for the last 4 months in my new role ( been here since 7ish months approx). Model was used to basically identify a targeted group of customers to send out mktg message to. But when we put this targeting into place we are not reaching to even 1% of the expected customers.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kq8wu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681394481.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: I projected some performance improvement from my model but when put into production it was a complete shitshow. And now im scared I will either get piped or fired.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12kq8wu", "is_robot_indexable": true, "report_reasons": null, "author": "Financial_Ad7856", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12kq8wu/i_worked_on_a_ds_project_for_the_last_4_months_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12kq8wu/i_worked_on_a_ds_project_for_the_last_4_months_in/", "subreddit_subscribers": 872694, "created_utc": 1681394481.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am interviewing for a job next week. It\u2019s a really unique roll that requires dealing with data but also requires a law degree. I\u2019m a lawyer and I happen to be in a masters program for data science so I\u2019m oddly perfectly qualified for this job. There\u2019s some other requirements that I\u2019m even more uniquely qualified for but its too much detail for a throw away account. \n\nI frankly don\u2019t know how anyone could have my unique combination of requirements to meet this role. The concern I have is that I\u2019m interviewing with folks that are all lawyers. How do I both impress them with my data science knowledge but not overwhelm them with terminology they aren\u2019t familiar with. I know what the goals of the data analysis is for the role, have some thoughts on data they could gather that they probably aren\u2019t (I have found some reports they\u2019ve released in the past), and have some ideas on how machine learning could be used to meet the end goal. I\u2019m just concerned about translating that for non-data science people, especially the machine learning piece. \n\nI\u2019d really just love any tips for interviewing for a data heavy role with non-data folks. How do you leave an impression without them thinking you are just making up things? \ud83d\ude02", "author_fullname": "t2_mrw1n7xg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview tips for the job I am so oddly qualified for it\u2019s scaring", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kp6d2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681392414.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am interviewing for a job next week. It\u2019s a really unique roll that requires dealing with data but also requires a law degree. I\u2019m a lawyer and I happen to be in a masters program for data science so I\u2019m oddly perfectly qualified for this job. There\u2019s some other requirements that I\u2019m even more uniquely qualified for but its too much detail for a throw away account. &lt;/p&gt;\n\n&lt;p&gt;I frankly don\u2019t know how anyone could have my unique combination of requirements to meet this role. The concern I have is that I\u2019m interviewing with folks that are all lawyers. How do I both impress them with my data science knowledge but not overwhelm them with terminology they aren\u2019t familiar with. I know what the goals of the data analysis is for the role, have some thoughts on data they could gather that they probably aren\u2019t (I have found some reports they\u2019ve released in the past), and have some ideas on how machine learning could be used to meet the end goal. I\u2019m just concerned about translating that for non-data science people, especially the machine learning piece. &lt;/p&gt;\n\n&lt;p&gt;I\u2019d really just love any tips for interviewing for a data heavy role with non-data folks. How do you leave an impression without them thinking you are just making up things? \ud83d\ude02&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12kp6d2", "is_robot_indexable": true, "report_reasons": null, "author": "Comfortable-Lie-2326", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12kp6d2/interview_tips_for_the_job_i_am_so_oddly/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12kp6d2/interview_tips_for_the_job_i_am_so_oddly/", "subreddit_subscribers": 872694, "created_utc": 1681392414.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello folks!\n\nI am attempting to perform DBSCAN on a dataset with approximately 2.5 million rows and 23 columns. After reading many places online, I understand that memory allocation is a problem for performing DBSCAN on such a huge dataset. Does anyone know how to do it, and in addition to it, can DBSCAN be used with parallel processing?\n\nThanks!", "author_fullname": "t2_3wr0pzmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch processing for DBSCAN", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12logrh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681457850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks!&lt;/p&gt;\n\n&lt;p&gt;I am attempting to perform DBSCAN on a dataset with approximately 2.5 million rows and 23 columns. After reading many places online, I understand that memory allocation is a problem for performing DBSCAN on such a huge dataset. Does anyone know how to do it, and in addition to it, can DBSCAN be used with parallel processing?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12logrh", "is_robot_indexable": true, "report_reasons": null, "author": "sARUcasm", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12logrh/batch_processing_for_dbscan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12logrh/batch_processing_for_dbscan/", "subreddit_subscribers": 872694, "created_utc": 1681457850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In February I started a project in my university about air quality. My team downloaded the data from the Spanish Government web and we started cleaning data from then on.\n\nAlmost two months have passed and we haven't finished the data cleaning process. I have to mention that we had filtered the data through many python scripts (w/ pandas) and we have cleaned a lot. I consider that in one week we'll have finished.\n\nThe point here is that everyone else was already analyzing data like a month ago, but I feel that they do not have so much data like us. Moreover, their data comes from some suspicious websites.\n\nMy question is, is it ok? Or should I have reduced the quantity of data? \n\nPD: sorry if my English is wrong, I'm still learning!", "author_fullname": "t2_ulhyxkkk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm in my first project and the data cleaning process is taking sooo long, it is ok?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lnxaw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681456497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In February I started a project in my university about air quality. My team downloaded the data from the Spanish Government web and we started cleaning data from then on.&lt;/p&gt;\n\n&lt;p&gt;Almost two months have passed and we haven&amp;#39;t finished the data cleaning process. I have to mention that we had filtered the data through many python scripts (w/ pandas) and we have cleaned a lot. I consider that in one week we&amp;#39;ll have finished.&lt;/p&gt;\n\n&lt;p&gt;The point here is that everyone else was already analyzing data like a month ago, but I feel that they do not have so much data like us. Moreover, their data comes from some suspicious websites.&lt;/p&gt;\n\n&lt;p&gt;My question is, is it ok? Or should I have reduced the quantity of data? &lt;/p&gt;\n\n&lt;p&gt;PD: sorry if my English is wrong, I&amp;#39;m still learning!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lnxaw", "is_robot_indexable": true, "report_reasons": null, "author": "LaiqianDS", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lnxaw/im_in_my_first_project_and_the_data_cleaning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lnxaw/im_in_my_first_project_and_the_data_cleaning/", "subreddit_subscribers": 872694, "created_utc": 1681456497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have been playing around with Causal Forests through the econML package but causal inference in general is quite new to me.\n\nI've read some interesting literature about how these types of random forest models can be thought of as an adaptive nearest neighbor approach which \"learns\" which features are most important in determining neighborhoods, rather than just using a standard distance calc across all features.\n\nThere are lots of tools around determining which features are important (SHAP values, feature importances, etc.), but I was wondering if there was a way to use these models to determine the most similar data points based on this adaptive neighborhood idea (i.e. something like how many leaves they are in together, or % of times they are on the same side of a split, etc.).\n\nI know I can compare on the actual output, but I was also thinking there would be cases where different subgroups might have similar outcomes spuriously despite having different features (i.e. w.r.t. housing price regression: small shore houses having similar price ranges to large suburban homes, despite being very different). \n\nSo I was thinking there must be a better way to do this. Thanks!", "author_fullname": "t2_8nyuj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any way to \"recover\" nearest neighbors from a Random Forest/Causal Forest model?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12l65yl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681421748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been playing around with Causal Forests through the econML package but causal inference in general is quite new to me.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read some interesting literature about how these types of random forest models can be thought of as an adaptive nearest neighbor approach which &amp;quot;learns&amp;quot; which features are most important in determining neighborhoods, rather than just using a standard distance calc across all features.&lt;/p&gt;\n\n&lt;p&gt;There are lots of tools around determining which features are important (SHAP values, feature importances, etc.), but I was wondering if there was a way to use these models to determine the most similar data points based on this adaptive neighborhood idea (i.e. something like how many leaves they are in together, or % of times they are on the same side of a split, etc.).&lt;/p&gt;\n\n&lt;p&gt;I know I can compare on the actual output, but I was also thinking there would be cases where different subgroups might have similar outcomes spuriously despite having different features (i.e. w.r.t. housing price regression: small shore houses having similar price ranges to large suburban homes, despite being very different). &lt;/p&gt;\n\n&lt;p&gt;So I was thinking there must be a better way to do this. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12l65yl", "is_robot_indexable": true, "report_reasons": null, "author": "metsfan1025", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12l65yl/any_way_to_recover_nearest_neighbors_from_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12l65yl/any_way_to_recover_nearest_neighbors_from_a/", "subreddit_subscribers": 872694, "created_utc": 1681421748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Lowering the price will likely increase sales but could decrease margin. Vice versa for raising the price. Changing prices then measuring the impact seems inefficient especially since I can only change prices once per month and each product/location could react differently.", "author_fullname": "t2_kcl3tfwe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good algorithms/techniques for pricing and revenue optimization strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kqu24", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681395599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lowering the price will likely increase sales but could decrease margin. Vice versa for raising the price. Changing prices then measuring the impact seems inefficient especially since I can only change prices once per month and each product/location could react differently.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12kqu24", "is_robot_indexable": true, "report_reasons": null, "author": "NewEcho2940", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12kqu24/good_algorithmstechniques_for_pricing_and_revenue/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12kqu24/good_algorithmstechniques_for_pricing_and_revenue/", "subreddit_subscribers": 872694, "created_utc": 1681395599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_qqy6or6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the advantage of using Machine learning in Azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12luj28", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681472838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12luj28", "is_robot_indexable": true, "report_reasons": null, "author": "star-lord-98", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12luj28/what_is_the_advantage_of_using_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12luj28/what_is_the_advantage_of_using_machine_learning/", "subreddit_subscribers": 872694, "created_utc": 1681472838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I need help with this, I don\u2019t understand what\u2019s going on in the image. I know the l1 norm gives the diamond shape and the l2 norm gives the circle and lasso regression uses the l1 to help remove unimportant features but I don\u2019t understand the image", "author_fullname": "t2_9ple7b7g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Not my homework mod", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "name": "t3_12ltfud", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/OwQO4mGc4JQON2Zs8DWipFthKmq18aiHU8Ypfyza_As.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681470080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need help with this, I don\u2019t understand what\u2019s going on in the image. I know the l1 norm gives the diamond shape and the l2 norm gives the circle and lasso regression uses the l1 to help remove unimportant features but I don\u2019t understand the image&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/n6keucauhvta1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/n6keucauhvta1.jpg?auto=webp&amp;v=enabled&amp;s=765aeaf72f8afe81f63de2068ae0663051b04bb4", "width": 1159, "height": 1915}, "resolutions": [{"url": "https://preview.redd.it/n6keucauhvta1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d48cc44fc670f5c84d95b945d7a421b1aebc2818", "width": 108, "height": 178}, {"url": "https://preview.redd.it/n6keucauhvta1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=559ef7e5cff62028be18c689a481bb726a815d84", "width": 216, "height": 356}, {"url": "https://preview.redd.it/n6keucauhvta1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3784847e95fe34da4f87e2cb1cbbe74ebbb3b85b", "width": 320, "height": 528}, {"url": "https://preview.redd.it/n6keucauhvta1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4698f7e2e9e7da8c03d3cb54c6ea00471eba7de1", "width": 640, "height": 1057}, {"url": "https://preview.redd.it/n6keucauhvta1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=465e90427a466e6e841040e88927467964a11331", "width": 960, "height": 1586}, {"url": "https://preview.redd.it/n6keucauhvta1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=353da7901f9006f7e9b5a4d0e38474be35f49c85", "width": 1080, "height": 1784}], "variants": {}, "id": "adKAhRcszAwWI6nXPo_lUTGlKsRwVoytUfcRDb1rcZU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12ltfud", "is_robot_indexable": true, "report_reasons": null, "author": "Longjumping_Ad_7053", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12ltfud/not_my_homework_mod/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/n6keucauhvta1.jpg", "subreddit_subscribers": 872694, "created_utc": 1681470080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_jlunblur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Neuropizza: Models for spike train classification and machine learning parameter identifiability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12lqcjr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gmCBxvCmf3E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Neuropizza: Models for spike train classification and machine learning parameter identifiability\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Neuropizza: Models for spike train classification and machine learning parameter identifiability", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gmCBxvCmf3E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Neuropizza: Models for spike train classification and machine learning parameter identifiability\"&gt;&lt;/iframe&gt;", "author_name": "Alessandro Crimi", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/gmCBxvCmf3E/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@alecrimi"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gmCBxvCmf3E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Neuropizza: Models for spike train classification and machine learning parameter identifiability\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12lqcjr", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8lNZGkMOprrdB1ghwXatBtA54J4xbLDYB9CzK2c3Czw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681462593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=gmCBxvCmf3E", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L-fyEboM97IOvjzJEFcLUBwj7NiSfQjKykCz78nHuQA.jpg?auto=webp&amp;v=enabled&amp;s=1cf2643d9230ada28a2f582a146b3e5908603f83", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/L-fyEboM97IOvjzJEFcLUBwj7NiSfQjKykCz78nHuQA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d62acf4c880618e6540135b3f3cd3a00fbe2045c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/L-fyEboM97IOvjzJEFcLUBwj7NiSfQjKykCz78nHuQA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8feb3843340e170c5bd3fd3553efb7c0186021b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/L-fyEboM97IOvjzJEFcLUBwj7NiSfQjKykCz78nHuQA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6633094d776054bca9a80602c94d88296fa2224", "width": 320, "height": 240}], "variants": {}, "id": "ISUixBol5tj9cIpOo7ZBV3bQ-qx8V_DTqB4IYfkumJA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lqcjr", "is_robot_indexable": true, "report_reasons": null, "author": "rottoneuro", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lqcjr/neuropizza_models_for_spike_train_classification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=gmCBxvCmf3E", "subreddit_subscribers": 872694, "created_utc": 1681462593.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Neuropizza: Models for spike train classification and machine learning parameter identifiability", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gmCBxvCmf3E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Neuropizza: Models for spike train classification and machine learning parameter identifiability\"&gt;&lt;/iframe&gt;", "author_name": "Alessandro Crimi", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/gmCBxvCmf3E/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@alecrimi"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I saw a plot today and for some reason, after over a decade in the profession, thought that the standard axes might not be the norm. I was brought up with the standard X-Y axes, but might not be the case in other countries where left to right is not the norm.\n\nSo for people writing in non-latin scripts, Arabic, Hebrew, Standard Chinese, etc, do you draw your plots the same way?\n\nDo you plot time series plots with time going from left to right?", "author_fullname": "t2_v1t0s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non left-to-right writers: how do you plot time-series?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12loszb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681458693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw a plot today and for some reason, after over a decade in the profession, thought that the standard axes might not be the norm. I was brought up with the standard X-Y axes, but might not be the case in other countries where left to right is not the norm.&lt;/p&gt;\n\n&lt;p&gt;So for people writing in non-latin scripts, Arabic, Hebrew, Standard Chinese, etc, do you draw your plots the same way?&lt;/p&gt;\n\n&lt;p&gt;Do you plot time series plots with time going from left to right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12loszb", "is_robot_indexable": true, "report_reasons": null, "author": "philwinder", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12loszb/non_lefttoright_writers_how_do_you_plot_timeseries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12loszb/non_lefttoright_writers_how_do_you_plot_timeseries/", "subreddit_subscribers": 872694, "created_utc": 1681458693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "CPA that works at a old culture company that not as tech savvy. I\u2019m trying to get more skills that will help me for my next job.\n\nI think we have some pretty rich data and I really want to learn and use python. Is it worth it for me to learn plotty/dash to create dashboards for comparisons?\n\nI thought about some use cases where it could add value.\n\n1. Balance Sheet account analysis: Where we look at balance sheet accounts and compare them across periods. (I could see having a stacked bar graph showing values of certain line items like Accounts receivable and using the visualization to quickly find which accounts are outliers)\n\n2. OPEX Analysis (similar to above for quick identification of movers) \n\n3. Impairment Analysis(we have a large portfolio of assets so maybe it possible to map out their profitability and using figure out which assets could qualify for an asset writedown)\n\nI\u2019m sure there more, but i\u2019m just curious if it becomes a valuable set of skills.", "author_fullname": "t2_4m8xm989", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accountant here, should I learn Plotty and Dash?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lm76g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681452163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;CPA that works at a old culture company that not as tech savvy. I\u2019m trying to get more skills that will help me for my next job.&lt;/p&gt;\n\n&lt;p&gt;I think we have some pretty rich data and I really want to learn and use python. Is it worth it for me to learn plotty/dash to create dashboards for comparisons?&lt;/p&gt;\n\n&lt;p&gt;I thought about some use cases where it could add value.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Balance Sheet account analysis: Where we look at balance sheet accounts and compare them across periods. (I could see having a stacked bar graph showing values of certain line items like Accounts receivable and using the visualization to quickly find which accounts are outliers)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;OPEX Analysis (similar to above for quick identification of movers) &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Impairment Analysis(we have a large portfolio of assets so maybe it possible to map out their profitability and using figure out which assets could qualify for an asset writedown)&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I\u2019m sure there more, but i\u2019m just curious if it becomes a valuable set of skills.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lm76g", "is_robot_indexable": true, "report_reasons": null, "author": "hsidbjakoxj", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lm76g/accountant_here_should_i_learn_plotty_and_dash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lm76g/accountant_here_should_i_learn_plotty_and_dash/", "subreddit_subscribers": 872694, "created_utc": 1681452163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey, I'm having trouble understanding how to incremental load from multiple source tables that join on each other to a target table. I was wondering if anyone has had to do something similar and has any ideas they could share. I'm using databricks as a platform if that matters.\n\nExample:\n\nLarge Table: basicStats with columns (nameID,name,fatherID)\n\nLarge Table: moreStats with columns (nameID,height)\n\nLooking to implement incremental loading on a new table:\n\nselect sha1(basicStats.nameID) nameHashKey,\n\n[basicStats.name](https://basicstats.name/) name,\n\nbasicStatsFather[.name](https://father.name/) as fatherName,\n\nmoreStats.height as height\n\nfrom basicStats join moreStats\n\non [basicStats.nameID = moreStats.name](https://table1.name%3Dtable2.name/)ID\n\njoin basicStats as basicStatsFather\n\non basicStats.nameID = basicStatsFather.fatherID", "author_fullname": "t2_6pedjzz0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental loading from multiple tables into a target table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lkw6s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681448977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I&amp;#39;m having trouble understanding how to incremental load from multiple source tables that join on each other to a target table. I was wondering if anyone has had to do something similar and has any ideas they could share. I&amp;#39;m using databricks as a platform if that matters.&lt;/p&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;p&gt;Large Table: basicStats with columns (nameID,name,fatherID)&lt;/p&gt;\n\n&lt;p&gt;Large Table: moreStats with columns (nameID,height)&lt;/p&gt;\n\n&lt;p&gt;Looking to implement incremental loading on a new table:&lt;/p&gt;\n\n&lt;p&gt;select sha1(basicStats.nameID) nameHashKey,&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://basicstats.name/\"&gt;basicStats.name&lt;/a&gt; name,&lt;/p&gt;\n\n&lt;p&gt;basicStatsFather&lt;a href=\"https://father.name/\"&gt;.name&lt;/a&gt; as fatherName,&lt;/p&gt;\n\n&lt;p&gt;moreStats.height as height&lt;/p&gt;\n\n&lt;p&gt;from basicStats join moreStats&lt;/p&gt;\n\n&lt;p&gt;on &lt;a href=\"https://table1.name%3Dtable2.name/\"&gt;basicStats.nameID = moreStats.name&lt;/a&gt;ID&lt;/p&gt;\n\n&lt;p&gt;join basicStats as basicStatsFather&lt;/p&gt;\n\n&lt;p&gt;on basicStats.nameID = basicStatsFather.fatherID&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lkw6s", "is_robot_indexable": true, "report_reasons": null, "author": "Simp4ABGs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lkw6s/incremental_loading_from_multiple_tables_into_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lkw6s/incremental_loading_from_multiple_tables_into_a/", "subreddit_subscribers": 872694, "created_utc": 1681448977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been working as radiological imaging technologist for past two years without any increments and it doesn't add any new experience at all. Now the company planning to laid me off soon and to regret they informed that they won't provide any experience certificates too.\n\nNow I'm intend to start from the scratch again, I'm highly interested in data science, I did few internships in data science. But it is really hard to land into the job. I constantly applying for job but didn't even got single interview.\n\nI did few own projects but I have no idea how to build a portfolio.\n\nWhat should I do?", "author_fullname": "t2_tesccx8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is it possible for a medical Imaging Tech to be a data scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lkaej", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681447583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as radiological imaging technologist for past two years without any increments and it doesn&amp;#39;t add any new experience at all. Now the company planning to laid me off soon and to regret they informed that they won&amp;#39;t provide any experience certificates too.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m intend to start from the scratch again, I&amp;#39;m highly interested in data science, I did few internships in data science. But it is really hard to land into the job. I constantly applying for job but didn&amp;#39;t even got single interview.&lt;/p&gt;\n\n&lt;p&gt;I did few own projects but I have no idea how to build a portfolio.&lt;/p&gt;\n\n&lt;p&gt;What should I do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lkaej", "is_robot_indexable": true, "report_reasons": null, "author": "Dilly_03", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lkaej/is_it_possible_for_a_medical_imaging_tech_to_be_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lkaej/is_it_possible_for_a_medical_imaging_tech_to_be_a/", "subreddit_subscribers": 872694, "created_utc": 1681447583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hey all,\n\nI need to predict when a machine will hit a threshold for wear amount (The machine will be replaced once the threshold is met), where the current wear of the machine is measured about once a month. One of the biggest causes of wear is when the machine is in use, which happens a couple times a month. There are also other factors which affect this machine's wear rate, including temperature, ect.\n\nBy looking at the scatter graph of wear amount against time, it looks to be mostly linear, although the rate is different depending on which machine I am looking at (because of the previously mentioned wear rate factors), the rate for one machine also periodically changes based on other factors not mentioned.\n\nI was going to go down the RUL approach for this problem with Survival Analysis, however before I do this, I was wondering if anyone had any advice or a better approach to use (neural network or some other form of regression). Since the current wear amount is not measured very frequently, how should the input data for the model be structured to account for these gaps of data?\n\nThanks for the help.", "author_fullname": "t2_cvsyi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which Predictive Maintenance method to use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12led2o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681434862.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I need to predict when a machine will hit a threshold for wear amount (The machine will be replaced once the threshold is met), where the current wear of the machine is measured about once a month. One of the biggest causes of wear is when the machine is in use, which happens a couple times a month. There are also other factors which affect this machine&amp;#39;s wear rate, including temperature, ect.&lt;/p&gt;\n\n&lt;p&gt;By looking at the scatter graph of wear amount against time, it looks to be mostly linear, although the rate is different depending on which machine I am looking at (because of the previously mentioned wear rate factors), the rate for one machine also periodically changes based on other factors not mentioned.&lt;/p&gt;\n\n&lt;p&gt;I was going to go down the RUL approach for this problem with Survival Analysis, however before I do this, I was wondering if anyone had any advice or a better approach to use (neural network or some other form of regression). Since the current wear amount is not measured very frequently, how should the input data for the model be structured to account for these gaps of data?&lt;/p&gt;\n\n&lt;p&gt;Thanks for the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12led2o", "is_robot_indexable": true, "report_reasons": null, "author": "Shuhandler", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12led2o/which_predictive_maintenance_method_to_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12led2o/which_predictive_maintenance_method_to_use/", "subreddit_subscribers": 872694, "created_utc": 1681434862.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I wonder on which texts should TfidfVectorizer be fitted when using TF-IDF cosine for text similarity. Should TfidfVectorizer be fitted on the texts that are analyzed for text similarity, or some other texts (if so, which one)?\n\n---\n\nI follow [ogrisel](https://stackoverflow.com/users/163740/ogrisel)'s [code](https://stackoverflow.com/a/12128777/395857) to compute text similarity via TF-IDF cosine, which fits the `TfidfVectorizer` on the texts that are analyzed for text similarity (`fetch_20newsgroups()` in that example):\n\n\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.datasets import fetch_20newsgroups\n    twenty = fetch_20newsgroups()\n    tfidf = TfidfVectorizer().fit_transform(twenty.data)\n    from sklearn.metrics.pairwise import linear_kernel\n    cosine_similarities = linear_kernel(tfidf[0], tfidf[1]).flatten()\n    print(cosine_similarities) # print TF-IDF cosine similarity between text 1 and 2.", "author_fullname": "t2_kprlc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On which texts should TfidfVectorizer be fitted when using TF-IDF cosine for text similarity?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lktao", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681448785.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wonder on which texts should TfidfVectorizer be fitted when using TF-IDF cosine for text similarity. Should TfidfVectorizer be fitted on the texts that are analyzed for text similarity, or some other texts (if so, which one)?&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;I follow &lt;a href=\"https://stackoverflow.com/users/163740/ogrisel\"&gt;ogrisel&lt;/a&gt;&amp;#39;s &lt;a href=\"https://stackoverflow.com/a/12128777/395857\"&gt;code&lt;/a&gt; to compute text similarity via TF-IDF cosine, which fits the &lt;code&gt;TfidfVectorizer&lt;/code&gt; on the texts that are analyzed for text similarity (&lt;code&gt;fetch_20newsgroups()&lt;/code&gt; in that example):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.datasets import fetch_20newsgroups\ntwenty = fetch_20newsgroups()\ntfidf = TfidfVectorizer().fit_transform(twenty.data)\nfrom sklearn.metrics.pairwise import linear_kernel\ncosine_similarities = linear_kernel(tfidf[0], tfidf[1]).flatten()\nprint(cosine_similarities) # print TF-IDF cosine similarity between text 1 and 2.\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?auto=webp&amp;v=enabled&amp;s=19b4a59f036ea2f314ff2033c11e54cdc240f8d8", "width": 316, "height": 316}, "resolutions": [{"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d93e783257998ff2ed865c359d9a00312a5412d7", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/QgPvRTknlY3rMNDqH1k4I37XGiq9tZF_FsygC_Xht4o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90216f7dc897a869ee852791bafa1e00667cdf07", "width": 216, "height": 216}], "variants": {}, "id": "nfayPavSUB5ngYv6-19UHNBThsXfcLIDQl4HkEe3Cv0"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lktao", "is_robot_indexable": true, "report_reasons": null, "author": "Franck_Dernoncourt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lktao/on_which_texts_should_tfidfvectorizer_be_fitted/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lktao/on_which_texts_should_tfidfvectorizer_be_fitted/", "subreddit_subscribers": 872694, "created_utc": 1681448785.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Anyone else applying for jobs right now? I\u2019ve noticed that salaries look overall lower than the last time I applied (2021), there are less postings overall, and most are requiring in office days\u2026\n\nIm also getting less callbacks vs 2021 when I had random recruiters reach out to me! When I do get a callback I\u2018m been getting lowball offers that are less than what I\u2019m making now...\n\nOnly a year or two ago, I feel like there was enough demand where you could ask for a crazy high salary and companies would agree.\u2026\n\nim lucky to be employed but I\u2019m stuck in a very stressful job with long hours.  Wish I had planned a bit better\u2026wonder if the market is going to pick up again or markets officially over saturated\u2026to apply or wait it out?", "author_fullname": "t2_bdnm56dh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Job market seems to be getting less favorable by the day\u2026 do we still have negotiating power?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lfmd8", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681437564.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone else applying for jobs right now? I\u2019ve noticed that salaries look overall lower than the last time I applied (2021), there are less postings overall, and most are requiring in office days\u2026&lt;/p&gt;\n\n&lt;p&gt;Im also getting less callbacks vs 2021 when I had random recruiters reach out to me! When I do get a callback I\u2018m been getting lowball offers that are less than what I\u2019m making now...&lt;/p&gt;\n\n&lt;p&gt;Only a year or two ago, I feel like there was enough demand where you could ask for a crazy high salary and companies would agree.\u2026&lt;/p&gt;\n\n&lt;p&gt;im lucky to be employed but I\u2019m stuck in a very stressful job with long hours.  Wish I had planned a bit better\u2026wonder if the market is going to pick up again or markets officially over saturated\u2026to apply or wait it out?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lfmd8", "is_robot_indexable": true, "report_reasons": null, "author": "Cultural-Gear-1323", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lfmd8/job_market_seems_to_be_getting_less_favorable_by/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lfmd8/job_market_seems_to_be_getting_less_favorable_by/", "subreddit_subscribers": 872694, "created_utc": 1681437564.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I really want to figure out how to do this. I can totally write the code but I just need help starting out and getting a plan together. Any ideas? I'd like to consider things such as the historical drop off rate of other top NBA players and Lebron's season averages.\n\nEdit: Downvotes are leading me to believe this is a stupid question. Although that may be the case all I'm trying to do is find a jumping off point so I can google/research to figure out the rest.", "author_fullname": "t2_3uk2pfvx6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I want to build a project to determine the age that Lebron would score less than 10 points. Just looking for a rough guideline. I want to write it all myself obviously.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12kyc5o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681415802.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681410243.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really want to figure out how to do this. I can totally write the code but I just need help starting out and getting a plan together. Any ideas? I&amp;#39;d like to consider things such as the historical drop off rate of other top NBA players and Lebron&amp;#39;s season averages.&lt;/p&gt;\n\n&lt;p&gt;Edit: Downvotes are leading me to believe this is a stupid question. Although that may be the case all I&amp;#39;m trying to do is find a jumping off point so I can google/research to figure out the rest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12kyc5o", "is_robot_indexable": true, "report_reasons": null, "author": "GlenSheen", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12kyc5o/i_want_to_build_a_project_to_determine_the_age/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12kyc5o/i_want_to_build_a_project_to_determine_the_age/", "subreddit_subscribers": 872694, "created_utc": 1681410243.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Apologies for cross-posting with /r/Python, but this is particularly relevant to many data science practitioners.\n\nWe're excited to announce the release of a new data visualization toolkit: Highcharts for Python!\n\nIt's a collection of Python libraries designed to provide turn-key comprehensive support for Highcharts data visualizations in Python, including native integrations with Pandas, Jupyter, PySpark, GeoPandas, ESRI, and more. If you're unfamiliar with Highcharts, it is one of the leading Javascript data visualization solutions, used to easily create highly interactive, beautifully styled data visualizations with over 150 different chart types. Using the Highcharts for Python toolkit, you can easily integrate this rich set of visualization capabilities into your Notebooks and analytical apps.\n\nThe toolkit includes full support for the Highcharts suite of data visualization libraries, including Highcharts Core, Highcharts Stock, Highcharts Maps, and Highcharts Gantt. Full disclosure, the libraries are free to try, but like all of the Highcharts visualization products, using them commercially requires a license with extensive (human) support.\n\nYou can find the Github repos for the full toolkit at [https://github.com/highcharts-for-python](https://github.com/highcharts-for-python), more details about what the toolkit can do at [https://www.highcharts.com/blog/integrations/python/](https://www.highcharts.com/blog/integrations/python/), and extensive documentation at [https://highchartspython.com/get-help](https://highchartspython.com/get-help).\n\nGiven that this is v.1.0 of our Python toolkit, it's obviously early days, so we'd love your thoughts and perspectives on the library!", "author_fullname": "t2_slz15ug2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Highcharts for Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lo5c9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681471868.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681457064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies for cross-posting with &lt;a href=\"/r/Python\"&gt;/r/Python&lt;/a&gt;, but this is particularly relevant to many data science practitioners.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re excited to announce the release of a new data visualization toolkit: Highcharts for Python!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a collection of Python libraries designed to provide turn-key comprehensive support for Highcharts data visualizations in Python, including native integrations with Pandas, Jupyter, PySpark, GeoPandas, ESRI, and more. If you&amp;#39;re unfamiliar with Highcharts, it is one of the leading Javascript data visualization solutions, used to easily create highly interactive, beautifully styled data visualizations with over 150 different chart types. Using the Highcharts for Python toolkit, you can easily integrate this rich set of visualization capabilities into your Notebooks and analytical apps.&lt;/p&gt;\n\n&lt;p&gt;The toolkit includes full support for the Highcharts suite of data visualization libraries, including Highcharts Core, Highcharts Stock, Highcharts Maps, and Highcharts Gantt. Full disclosure, the libraries are free to try, but like all of the Highcharts visualization products, using them commercially requires a license with extensive (human) support.&lt;/p&gt;\n\n&lt;p&gt;You can find the Github repos for the full toolkit at &lt;a href=\"https://github.com/highcharts-for-python\"&gt;https://github.com/highcharts-for-python&lt;/a&gt;, more details about what the toolkit can do at &lt;a href=\"https://www.highcharts.com/blog/integrations/python/\"&gt;https://www.highcharts.com/blog/integrations/python/&lt;/a&gt;, and extensive documentation at &lt;a href=\"https://highchartspython.com/get-help\"&gt;https://highchartspython.com/get-help&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Given that this is v.1.0 of our Python toolkit, it&amp;#39;s obviously early days, so we&amp;#39;d love your thoughts and perspectives on the library!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_3LlpUUp6HRq5Z8CqXOcf3_dCusLNtl5ctelt09hDJk.jpg?auto=webp&amp;v=enabled&amp;s=ca049026258518f8138fd7240933069a6a3be7fd", "width": 280, "height": 280}, "resolutions": [{"url": "https://external-preview.redd.it/_3LlpUUp6HRq5Z8CqXOcf3_dCusLNtl5ctelt09hDJk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68a16a84c27767e5de0f2d9819019a88d82bdead", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/_3LlpUUp6HRq5Z8CqXOcf3_dCusLNtl5ctelt09hDJk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e77aeb240f13fef2e0772cc3baf5ca57bdbddf50", "width": 216, "height": 216}], "variants": {}, "id": "TR-WAEGmKFLYlEzFxgs36Pc3Ai1bg-nSDR-ZU89qSIs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lo5c9", "is_robot_indexable": true, "report_reasons": null, "author": "highcharts", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lo5c9/highcharts_for_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lo5c9/highcharts_for_python/", "subreddit_subscribers": 872694, "created_utc": 1681457064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_pses1cx1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Create Custom Vision Applications with No Code/Data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12kop98", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7SMySnRRTew?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"This is the Remyx!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "This is the Remyx!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7SMySnRRTew?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"This is the Remyx!\"&gt;&lt;/iframe&gt;", "author_name": "Smells Like ML", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7SMySnRRTew/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@smellslikeml"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7SMySnRRTew?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"This is the Remyx!\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12kop98", "height": 200}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/Y921ytB_CBY6x5dgdVRCINpUolMwggpEFvA5BgtLzF8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681391457.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=7SMySnRRTew", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KQmBzm2bojsN_YZ93vIwEvRD1_BXy_WX24-eW-hW_Io.jpg?auto=webp&amp;v=enabled&amp;s=7033f42e0cc95164865faef3cf7a38cf07d5b200", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/KQmBzm2bojsN_YZ93vIwEvRD1_BXy_WX24-eW-hW_Io.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=679bdbdb49df61f212d12447691161a97d926f8f", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/KQmBzm2bojsN_YZ93vIwEvRD1_BXy_WX24-eW-hW_Io.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9b6b0e9a10ae522f6efe79883034220799ab617", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/KQmBzm2bojsN_YZ93vIwEvRD1_BXy_WX24-eW-hW_Io.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94b832106a26a52282ccbb65c9dd414023c22da4", "width": 320, "height": 240}], "variants": {}, "id": "zV1S95EK_4BSAZsbgDTKRZgh18Hm3L8zbrOwkM81qIg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12kop98", "is_robot_indexable": true, "report_reasons": null, "author": "remyxai", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12kop98/create_custom_vision_applications_with_no_codedata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=7SMySnRRTew", "subreddit_subscribers": 872694, "created_utc": 1681391457.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "This is the Remyx!", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7SMySnRRTew?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"This is the Remyx!\"&gt;&lt;/iframe&gt;", "author_name": "Smells Like ML", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/7SMySnRRTew/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@smellslikeml"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nI'm working in a data set and I'm trying to use groupby() and sum() and column that I'm using has a data type of int32 that is a new column created from a datetime with the function .dt.month\n\nclean\\_all\\_month\\_df\\['MonthNum'\\] = clean\\_all\\_month\\_df\\['Order Date'\\].dt.month\n\n&amp;#x200B;\n\nhttps://preview.redd.it/eggr7gkp1qta1.png?width=848&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2763a0dcf5985482c5897ce5f1ca0ed9e4e62100\n\nwhen trying to do the groupby() I receive an error regarding the data type.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/vl7t1l4x1qta1.png?width=1290&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4b00ce5bb3762a181aace082c0bac1eb3ec3f33a\n\nTypeError: datetime64 type does not support sum operations\n\nAny hint?", "author_fullname": "t2_dkpbwjdv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hints on Data Types", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 68, "top_awarded_type": null, "hide_score": false, "media_metadata": {"eggr7gkp1qta1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/eggr7gkp1qta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a6abe0491ed304fd7d8ee4ba90b3ab3ab7fe6906"}, {"y": 105, "x": 216, "u": "https://preview.redd.it/eggr7gkp1qta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c128caf4d86a18123721c38c773bb3b5f0404b0b"}, {"y": 156, "x": 320, "u": "https://preview.redd.it/eggr7gkp1qta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=581a4e326776ee9d4a0215ebed79ed7e60a4b7a7"}, {"y": 313, "x": 640, "u": "https://preview.redd.it/eggr7gkp1qta1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6edd63ce8e025195fa20d1eea2015b6e54f0aa8e"}], "s": {"y": 416, "x": 848, "u": "https://preview.redd.it/eggr7gkp1qta1.png?width=848&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2763a0dcf5985482c5897ce5f1ca0ed9e4e62100"}, "id": "eggr7gkp1qta1"}, "vl7t1l4x1qta1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 21, "x": 108, "u": "https://preview.redd.it/vl7t1l4x1qta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f58a8b1ff87bbd3b2911491bf767508adb5de4af"}, {"y": 43, "x": 216, "u": "https://preview.redd.it/vl7t1l4x1qta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6dcb4020148a440fd965d0a44141a554afa9f1ae"}, {"y": 64, "x": 320, "u": "https://preview.redd.it/vl7t1l4x1qta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a174d47382d09ccf34d688d39676bc3f9ad1d4b1"}, {"y": 128, "x": 640, "u": "https://preview.redd.it/vl7t1l4x1qta1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=854e3768cf62d020c612c8f2e608bbc71ecbe8e5"}, {"y": 192, "x": 960, "u": "https://preview.redd.it/vl7t1l4x1qta1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a81abfc0f371c56cdd4e4b26b16c8f14a4d7be55"}, {"y": 216, "x": 1080, "u": "https://preview.redd.it/vl7t1l4x1qta1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=87c8080b48a3fd41fc547f6e7ddd831815778528"}], "s": {"y": 258, "x": 1290, "u": "https://preview.redd.it/vl7t1l4x1qta1.png?width=1290&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4b00ce5bb3762a181aace082c0bac1eb3ec3f33a"}, "id": "vl7t1l4x1qta1"}}, "name": "t3_12l6mfo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/BdxiJ7Gwn6MkNG-bWE-1cNuUkHzor1y3zTIuxGZXQxA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681422245.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working in a data set and I&amp;#39;m trying to use groupby() and sum() and column that I&amp;#39;m using has a data type of int32 that is a new column created from a datetime with the function .dt.month&lt;/p&gt;\n\n&lt;p&gt;clean_all_month_df[&amp;#39;MonthNum&amp;#39;] = clean_all_month_df[&amp;#39;Order Date&amp;#39;].dt.month&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/eggr7gkp1qta1.png?width=848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2763a0dcf5985482c5897ce5f1ca0ed9e4e62100\"&gt;https://preview.redd.it/eggr7gkp1qta1.png?width=848&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2763a0dcf5985482c5897ce5f1ca0ed9e4e62100&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;when trying to do the groupby() I receive an error regarding the data type.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/vl7t1l4x1qta1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=4b00ce5bb3762a181aace082c0bac1eb3ec3f33a\"&gt;https://preview.redd.it/vl7t1l4x1qta1.png?width=1290&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=4b00ce5bb3762a181aace082c0bac1eb3ec3f33a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TypeError: datetime64 type does not support sum operations&lt;/p&gt;\n\n&lt;p&gt;Any hint?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12l6mfo", "is_robot_indexable": true, "report_reasons": null, "author": "nzenzo_209", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12l6mfo/hints_on_data_types/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12l6mfo/hints_on_data_types/", "subreddit_subscribers": 872694, "created_utc": 1681422245.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I work at big bank. I get to work with lot of collections data. Think people who have defaulted on a loan. Can you guys give me ideas on how can I use predictive modeling on such data?\n\nEg - How to predict who is going to default with high accuracy? What models can be suitable for such classification?\n\nA book or resource with such ideas will also be very helpful!\nThank you!", "author_fullname": "t2_a0h39pj0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Modeling hard?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lfh0n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.27, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681437240.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at big bank. I get to work with lot of collections data. Think people who have defaulted on a loan. Can you guys give me ideas on how can I use predictive modeling on such data?&lt;/p&gt;\n\n&lt;p&gt;Eg - How to predict who is going to default with high accuracy? What models can be suitable for such classification?&lt;/p&gt;\n\n&lt;p&gt;A book or resource with such ideas will also be very helpful!\nThank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lfh0n", "is_robot_indexable": true, "report_reasons": null, "author": "wealthyinvestor999", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lfh0n/is_modeling_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lfh0n/is_modeling_hard/", "subreddit_subscribers": 872694, "created_utc": 1681437240.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}