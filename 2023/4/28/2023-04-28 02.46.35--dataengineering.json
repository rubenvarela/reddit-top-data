{"kind": "Listing", "data": {"after": "t3_130link", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My personal favorite... A man's health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.", "author_fullname": "t2_3xnau4cx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your favorite data quality horror story?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130rfc2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 141, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 141, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682609164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My personal favorite... A man&amp;#39;s health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130rfc2", "is_robot_indexable": true, "report_reasons": null, "author": "superconductiveKyle", "discussion_type": null, "num_comments": 42, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "subreddit_subscribers": 102932, "created_utc": 1682609164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm in a company that uses pip.\nSince my senior colleague went away 1.5 years ago, my manager instead of hiring another person, he made me became an one man band by doing data engineer / analyst / PMO / SA with tight deadlines.\n\nWhen there's something more complex that he doesn't know, he doesn't want to use it or to make the client pay to use the right tools.\nHe insist that these things can be done in one day.\n\nDon't know what to say to him.", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to vent - manager that doesn't like complex things", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130he0d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682592163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in a company that uses pip.\nSince my senior colleague went away 1.5 years ago, my manager instead of hiring another person, he made me became an one man band by doing data engineer / analyst / PMO / SA with tight deadlines.&lt;/p&gt;\n\n&lt;p&gt;When there&amp;#39;s something more complex that he doesn&amp;#39;t know, he doesn&amp;#39;t want to use it or to make the client pay to use the right tools.\nHe insist that these things can be done in one day.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t know what to say to him.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130he0d", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130he0d/need_to_vent_manager_that_doesnt_like_complex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130he0d/need_to_vent_manager_that_doesnt_like_complex/", "subreddit_subscribers": 102932, "created_utc": 1682592163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is transitioning into medallion architecture making it the first learning for alot of us. \n\nFolks who have grinded their ass off with this architecture, how has your experience been?", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How has your experience working with the Databrick medallion architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130lbzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682600232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is transitioning into medallion architecture making it the first learning for alot of us. &lt;/p&gt;\n\n&lt;p&gt;Folks who have grinded their ass off with this architecture, how has your experience been?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130lbzy", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130lbzy/how_has_your_experience_working_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130lbzy/how_has_your_experience_working_with_the/", "subreddit_subscribers": 102932, "created_utc": 1682600232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i started recently as a DE and wondered if the speed our ELT-Pipeline is slow?\n\nIts a pipeline from an IBM DB2 table to SQL Server table (once full, not per delta), which is used as a data warehouse. We pull the data via a SQL Command, put the data into a temporal table, then add 2 Columns (HashKey and LoadDate) and then insert it into a persisted table. The whole process took us 6 hours!\n\nRow count : 27 Million\n\nNumber of Columns of the table: 95\n\nCompression type: Column Store (just from temporal table to persisted table)\n\nData space: 2584 MB\n\nIndex space: 1561 MB\n\nETL Tool: Microsoft Integration Services\n\nTime from Source to temporal table: 6 hours\n\nTime from temporal table to persisted table: 39 min\n\nFull Process from Source to temporal table (SSIS data flow within a loop, 10.000 rows per batch):\n\n1. truncate temporal table\n2. connect to source via sql command source query (SELECT 95 attributes FROM source)\n3. do some TRIM operations and type conversions\n4. add a hash key column and loaddate column\n5. Insert the data into the temporal table\n\n&amp;#x200B;\n\nHow much time should this take and what could we do to make the process faster?", "author_fullname": "t2_jo31wrc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the Speed of our ELT Pipeline too slow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130ff1p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682597170.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682588080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i started recently as a DE and wondered if the speed our ELT-Pipeline is slow?&lt;/p&gt;\n\n&lt;p&gt;Its a pipeline from an IBM DB2 table to SQL Server table (once full, not per delta), which is used as a data warehouse. We pull the data via a SQL Command, put the data into a temporal table, then add 2 Columns (HashKey and LoadDate) and then insert it into a persisted table. The whole process took us 6 hours!&lt;/p&gt;\n\n&lt;p&gt;Row count : 27 Million&lt;/p&gt;\n\n&lt;p&gt;Number of Columns of the table: 95&lt;/p&gt;\n\n&lt;p&gt;Compression type: Column Store (just from temporal table to persisted table)&lt;/p&gt;\n\n&lt;p&gt;Data space: 2584 MB&lt;/p&gt;\n\n&lt;p&gt;Index space: 1561 MB&lt;/p&gt;\n\n&lt;p&gt;ETL Tool: Microsoft Integration Services&lt;/p&gt;\n\n&lt;p&gt;Time from Source to temporal table: 6 hours&lt;/p&gt;\n\n&lt;p&gt;Time from temporal table to persisted table: 39 min&lt;/p&gt;\n\n&lt;p&gt;Full Process from Source to temporal table (SSIS data flow within a loop, 10.000 rows per batch):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;truncate temporal table&lt;/li&gt;\n&lt;li&gt;connect to source via sql command source query (SELECT 95 attributes FROM source)&lt;/li&gt;\n&lt;li&gt;do some TRIM operations and type conversions&lt;/li&gt;\n&lt;li&gt;add a hash key column and loaddate column&lt;/li&gt;\n&lt;li&gt;Insert the data into the temporal table&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How much time should this take and what could we do to make the process faster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130ff1p", "is_robot_indexable": true, "report_reasons": null, "author": "Elegant_Good6212", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130ff1p/is_the_speed_of_our_elt_pipeline_too_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130ff1p/is_the_speed_of_our_elt_pipeline_too_slow/", "subreddit_subscribers": 102932, "created_utc": 1682588080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work in data professionally with a team who only knows SQL whereas I know less SQL than they do and a lot more Python. We're using Spark for data transformations and are currently trying to upsert to Delta Lake files from existing parquet EDIT using notebooks in Synapse. \n\nAs my team only knows SQL (as does the lead), the Lead thinks it'd be best if we only use SQL to minimise the amount of time required for everybody to get up and running.\n\nI'm currently reading in the parquet file using the syntax:\n\n`CREATE TABLE IF NOT EXISTS SourceTable USING Parquet LOCATION &lt;location&gt;`\n\nand add in a column\n\n`ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))`\n\nI now want to populate this column with values with `UPDATE SourceTable SET NewColumn = 'test'` just to check it works and I'm getting the error:\n\n`Error: UPDATE destination only supports Delta sources.`\n\nI get this and it's music to my ears because data manipulation in PySpark is infinitely easier for me (this is going to be dynamic so ease of string interpolation and variable injection is what I want).\n\nThis leads me to have a few questions:\n\n* Is it really impossible to `UPDATE` a table object created from a Parquet file using Spark SQL?\n\n* Under the hood, I get that in PySpark the parquet file is converted into a DataFrame which is an object in memory. As you can't this using Spark SQL, does this mean the parquet file doesn't get converted into memory and you are, in fact, hitting the file directly?\n\n* I'm trying to build a strong case where we use PySpark 90%+ of the time for these kinds of transformations and, of course, being unable to view changes in memory is a pretty big shitter.  Are there any other massive limitations of Spark SQL I should know about?\n\nThank you!\n\nEDIT\n\nMore detail.\n\nWe currently use the Data Lakehouse paradigm in Azure Synapse. We take parquet files (bronze) and then essentially upsert them into Delta files (silver) based off a key. This key is a hash of a column/set of columns.\n\nThis was previously achieved using Azure Data Flows although we're trying to move away from using these because they're a massive pain in the tits to maintain and shift into notebooks/code. The Data flow has the following pattern for the row hash:\n\n* Reads in the parquet file\n\n* Creates a new column with an expression which is essentially hashing a column/set of columns\n\n* Does an upsert against the delta file, checking if the row hash exists as the condition.\n\nThe issue:\n\n* In note books using Spark SQL, I can't recreate this pattern because when I add a new column, I can't populate it with values as I'm, effectively, trying to \"edit\" the parquet file.\n\n* This is confusing for me because one of the questions I have at the top is \"Why can I do this in a data frame, but not in a table created using Spark SQL syntax? Is it tables created from Parquets using Spark SQL aren't the same as dataframes?\".\n\n* I can do this all in Pyspark, but ideally, I'd like to use Spark SQL because my team can't do it in Python.\n\nEDIT 2:\n\nSample code comparison:\n\n    from pyspark.sql.functions import sha2\n    \n    source_df = spark.read.load(\"path/to/parquet/file.parquet\", format=\"parquet\")\n    source_df = source_df.withColumn(\"NewColumn\", sha2(source_df.ColumnToHash, 256))\n\nI can then do the upsert to the Delta file.\n\nMy Spark SQL code:\n\n    CREATE TABLE IF NOT EXISTS SourceTable USING Parquet\n    LOCATION \"path/to/parquet/file.parquet\"\n\n    ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))\n\nBut then I can't do anything to `NewColumn`.  Worth adding a minor correction - it's not a row hash, I'm trying to hash a specific column/set of columns.", "author_fullname": "t2_bqhp2bh8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Limitations of Spark SQL vs. PySpark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130nxgp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682611347.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682605793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work in data professionally with a team who only knows SQL whereas I know less SQL than they do and a lot more Python. We&amp;#39;re using Spark for data transformations and are currently trying to upsert to Delta Lake files from existing parquet EDIT using notebooks in Synapse. &lt;/p&gt;\n\n&lt;p&gt;As my team only knows SQL (as does the lead), the Lead thinks it&amp;#39;d be best if we only use SQL to minimise the amount of time required for everybody to get up and running.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently reading in the parquet file using the syntax:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;CREATE TABLE IF NOT EXISTS SourceTable USING Parquet LOCATION &amp;lt;location&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;and add in a column&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I now want to populate this column with values with &lt;code&gt;UPDATE SourceTable SET NewColumn = &amp;#39;test&amp;#39;&lt;/code&gt; just to check it works and I&amp;#39;m getting the error:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Error: UPDATE destination only supports Delta sources.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I get this and it&amp;#39;s music to my ears because data manipulation in PySpark is infinitely easier for me (this is going to be dynamic so ease of string interpolation and variable injection is what I want).&lt;/p&gt;\n\n&lt;p&gt;This leads me to have a few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Is it really impossible to &lt;code&gt;UPDATE&lt;/code&gt; a table object created from a Parquet file using Spark SQL?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Under the hood, I get that in PySpark the parquet file is converted into a DataFrame which is an object in memory. As you can&amp;#39;t this using Spark SQL, does this mean the parquet file doesn&amp;#39;t get converted into memory and you are, in fact, hitting the file directly?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I&amp;#39;m trying to build a strong case where we use PySpark 90%+ of the time for these kinds of transformations and, of course, being unable to view changes in memory is a pretty big shitter.  Are there any other massive limitations of Spark SQL I should know about?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;EDIT&lt;/p&gt;\n\n&lt;p&gt;More detail.&lt;/p&gt;\n\n&lt;p&gt;We currently use the Data Lakehouse paradigm in Azure Synapse. We take parquet files (bronze) and then essentially upsert them into Delta files (silver) based off a key. This key is a hash of a column/set of columns.&lt;/p&gt;\n\n&lt;p&gt;This was previously achieved using Azure Data Flows although we&amp;#39;re trying to move away from using these because they&amp;#39;re a massive pain in the tits to maintain and shift into notebooks/code. The Data flow has the following pattern for the row hash:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Reads in the parquet file&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Creates a new column with an expression which is essentially hashing a column/set of columns&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Does an upsert against the delta file, checking if the row hash exists as the condition.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The issue:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;In note books using Spark SQL, I can&amp;#39;t recreate this pattern because when I add a new column, I can&amp;#39;t populate it with values as I&amp;#39;m, effectively, trying to &amp;quot;edit&amp;quot; the parquet file.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This is confusing for me because one of the questions I have at the top is &amp;quot;Why can I do this in a data frame, but not in a table created using Spark SQL syntax? Is it tables created from Parquets using Spark SQL aren&amp;#39;t the same as dataframes?&amp;quot;.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I can do this all in Pyspark, but ideally, I&amp;#39;d like to use Spark SQL because my team can&amp;#39;t do it in Python.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;EDIT 2:&lt;/p&gt;\n\n&lt;p&gt;Sample code comparison:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from pyspark.sql.functions import sha2\n\nsource_df = spark.read.load(&amp;quot;path/to/parquet/file.parquet&amp;quot;, format=&amp;quot;parquet&amp;quot;)\nsource_df = source_df.withColumn(&amp;quot;NewColumn&amp;quot;, sha2(source_df.ColumnToHash, 256))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I can then do the upsert to the Delta file.&lt;/p&gt;\n\n&lt;p&gt;My Spark SQL code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE IF NOT EXISTS SourceTable USING Parquet\nLOCATION &amp;quot;path/to/parquet/file.parquet&amp;quot;\n\nALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But then I can&amp;#39;t do anything to &lt;code&gt;NewColumn&lt;/code&gt;.  Worth adding a minor correction - it&amp;#39;s not a row hash, I&amp;#39;m trying to hash a specific column/set of columns.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130nxgp", "is_robot_indexable": true, "report_reasons": null, "author": "standard_throw", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130nxgp/limitations_of_spark_sql_vs_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130nxgp/limitations_of_spark_sql_vs_pyspark/", "subreddit_subscribers": 102932, "created_utc": 1682605793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, sorry if this is a stupid question. But I'd love to know why companies would pay for both DBT cloud and Fivetran now that Fivetran has built in DBT core with scheduling?\n\nIt seems like there's an ever growing number of data tools, each with their own expensive price tag. So I am trying to understand which tools are necessary and which aren't. Thanks!", "author_fullname": "t2_mytvjynu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why pay for DBT cloud when Fivetran has built in DBT Core?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1318f7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682632179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, sorry if this is a stupid question. But I&amp;#39;d love to know why companies would pay for both DBT cloud and Fivetran now that Fivetran has built in DBT core with scheduling?&lt;/p&gt;\n\n&lt;p&gt;It seems like there&amp;#39;s an ever growing number of data tools, each with their own expensive price tag. So I am trying to understand which tools are necessary and which aren&amp;#39;t. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1318f7s", "is_robot_indexable": true, "report_reasons": null, "author": "a-layerup", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1318f7s/why_pay_for_dbt_cloud_when_fivetran_has_built_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1318f7s/why_pay_for_dbt_cloud_when_fivetran_has_built_in/", "subreddit_subscribers": 102932, "created_utc": 1682632179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company has a well-structured data set which is about 10TB in size. Each file in this dataset represents a \"parameter\" which has two columns (x and y).\n\nManagement has requested me to research about making a data visualization software that can be accessed through the intranet by multiple departments within the company. \n\nSkillwise, I would rate myself about 6/10 in JS and Python. I do most of my work in C/C++. I am given about 3 months to do an initial draft of the software (the time frame was given based on my request as I have to do a lot of learning).\n\nI did some research and came up with Django for backend and React for front end with Chart.js for handling the bulk of data visualization. MySQL may be used for data storage. The VIZ is mostly simple line charts (max 50 parameters, each with 2000 data points), scatter plots and bar plots. Bonus points if user can interact in many ways with the charts.\n\nAm I on the right track? Can anyone give me some suggestions regarding the tech stack I am planning to use and suggest additional components for say, optimizing data retrieval?", "author_fullname": "t2_2teg11zt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestion regarding making a data visualization tool", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1307v04", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682564248.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company has a well-structured data set which is about 10TB in size. Each file in this dataset represents a &amp;quot;parameter&amp;quot; which has two columns (x and y).&lt;/p&gt;\n\n&lt;p&gt;Management has requested me to research about making a data visualization software that can be accessed through the intranet by multiple departments within the company. &lt;/p&gt;\n\n&lt;p&gt;Skillwise, I would rate myself about 6/10 in JS and Python. I do most of my work in C/C++. I am given about 3 months to do an initial draft of the software (the time frame was given based on my request as I have to do a lot of learning).&lt;/p&gt;\n\n&lt;p&gt;I did some research and came up with Django for backend and React for front end with Chart.js for handling the bulk of data visualization. MySQL may be used for data storage. The VIZ is mostly simple line charts (max 50 parameters, each with 2000 data points), scatter plots and bar plots. Bonus points if user can interact in many ways with the charts.&lt;/p&gt;\n\n&lt;p&gt;Am I on the right track? Can anyone give me some suggestions regarding the tech stack I am planning to use and suggest additional components for say, optimizing data retrieval?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1307v04", "is_robot_indexable": true, "report_reasons": null, "author": "ciado63", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1307v04/need_suggestion_regarding_making_a_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1307v04/need_suggestion_regarding_making_a_data/", "subreddit_subscribers": 102932, "created_utc": 1682564248.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know a lot of you will say you can learn for free, but my employer will pay for it, and I\u2019d like to take advantage of that money.", "author_fullname": "t2_ulshlx1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any paid bootcamp or certification worth taking for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131bffg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682639069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a lot of you will say you can learn for free, but my employer will pay for it, and I\u2019d like to take advantage of that money.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "131bffg", "is_robot_indexable": true, "report_reasons": null, "author": "__academic__", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131bffg/is_there_any_paid_bootcamp_or_certification_worth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131bffg/is_there_any_paid_bootcamp_or_certification_worth/", "subreddit_subscribers": 102932, "created_utc": 1682639069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. \n\nOne of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.\n\nIn practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.\n\nWith that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?", "author_fullname": "t2_14aeem", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reporting for application using a CQRS event store", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1313uw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. &lt;/p&gt;\n\n&lt;p&gt;One of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.&lt;/p&gt;\n\n&lt;p&gt;In practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.&lt;/p&gt;\n\n&lt;p&gt;With that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313uw5", "is_robot_indexable": true, "report_reasons": null, "author": "raginjason", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "subreddit_subscribers": 102932, "created_utc": 1682624515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically detecting breaking changes in SQL queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1310k7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682618763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tobikodata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1310k7b", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1310k7b/automatically_detecting_breaking_changes_in_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "subreddit_subscribers": 102932, "created_utc": 1682618763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I've heard bad things about Informatica, but I didn't even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?", "author_fullname": "t2_4rifsjav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IBM DB2 Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130tg7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682611175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I&amp;#39;ve heard bad things about Informatica, but I didn&amp;#39;t even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130tg7s", "is_robot_indexable": true, "report_reasons": null, "author": "Techthrowaway2222888", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "subreddit_subscribers": 102932, "created_utc": 1682611175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. \n\nDoes anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. \n\nAddl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.", "author_fullname": "t2_36yd2hgi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your company handle replication and CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130ls2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682601191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. &lt;/p&gt;\n\n&lt;p&gt;Addl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130ls2m", "is_robot_indexable": true, "report_reasons": null, "author": "PROTECTyaNECK44", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "subreddit_subscribers": 102932, "created_utc": 1682601191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wish I could find some references or use-cases of architectures about data engineering for manufactoring IoT, these are my goals:\n\n1. Some sensors are 1Hz (LF), others run at 20Hz (HF)\n2. Some sensors need some ingestion pipelines (thinking about map/reduce/filter/apply functions on the data stream) or processing pipelines\n3. How to handle the stream of data/mini-batch/batches of data with queues or topics and similar tools\n4. How to handle different industrial protocols like MQTT, MODBUS, OPCUA etc.\n5. What  should be done edge/on-premise vs on the cloud\n\nI read about Apache Kafka, ActiveMQ, RabbitMQ, then about Apache Spark for the processing but I don't want to dig into those frameworks without knowing what are the right tools since I don't have that much time to dedicate to my education, sadly.\n\nAlso I wish I could understand the best way to think about where, in the pipelines, the smart features should be provided, e.g. close to the machine assets or close to the module that sends the data to the datalake, if I should think about hot processing (processing stream of data) vs cooler processing (processing mini-batches of data). Do I need to handle messages and stream in a pub/sub architecture or do I need to handle dataframes of data? Do I need read or write performances? (I read about avro and parquet recently). I'm a bit lost where to start.", "author_fullname": "t2_vlk568vv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best tools about data engineering architecture for IoT (manufactoring): stream processing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130bncu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682575469.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wish I could find some references or use-cases of architectures about data engineering for manufactoring IoT, these are my goals:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Some sensors are 1Hz (LF), others run at 20Hz (HF)&lt;/li&gt;\n&lt;li&gt;Some sensors need some ingestion pipelines (thinking about map/reduce/filter/apply functions on the data stream) or processing pipelines&lt;/li&gt;\n&lt;li&gt;How to handle the stream of data/mini-batch/batches of data with queues or topics and similar tools&lt;/li&gt;\n&lt;li&gt;How to handle different industrial protocols like MQTT, MODBUS, OPCUA etc.&lt;/li&gt;\n&lt;li&gt;What  should be done edge/on-premise vs on the cloud&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I read about Apache Kafka, ActiveMQ, RabbitMQ, then about Apache Spark for the processing but I don&amp;#39;t want to dig into those frameworks without knowing what are the right tools since I don&amp;#39;t have that much time to dedicate to my education, sadly.&lt;/p&gt;\n\n&lt;p&gt;Also I wish I could understand the best way to think about where, in the pipelines, the smart features should be provided, e.g. close to the machine assets or close to the module that sends the data to the datalake, if I should think about hot processing (processing stream of data) vs cooler processing (processing mini-batches of data). Do I need to handle messages and stream in a pub/sub architecture or do I need to handle dataframes of data? Do I need read or write performances? (I read about avro and parquet recently). I&amp;#39;m a bit lost where to start.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130bncu", "is_robot_indexable": true, "report_reasons": null, "author": "Plenty-Button8465", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130bncu/best_tools_about_data_engineering_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130bncu/best_tools_about_data_engineering_architecture/", "subreddit_subscribers": 102932, "created_utc": 1682575469.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently encountered Step Functions and can\u2019t believe how easy it is to use, want to understand how easy it is to use compared to something like Airflow or MWAA in my case.\n\nI\u2019ve not heard of a situation when someone had used StepFunctions for Complex ETL, any thoughts ?", "author_fullname": "t2_1nawf7b9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow vs Step functions ??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_131dxyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682645783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently encountered Step Functions and can\u2019t believe how easy it is to use, want to understand how easy it is to use compared to something like Airflow or MWAA in my case.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve not heard of a situation when someone had used StepFunctions for Complex ETL, any thoughts ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131dxyl", "is_robot_indexable": true, "report_reasons": null, "author": "Exciting-Garlic8360", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131dxyl/airflow_vs_step_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131dxyl/airflow_vs_step_functions/", "subreddit_subscribers": 102932, "created_utc": 1682645783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you guys store your audit logs and orchestration metadata in snowflake environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1313y1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313y1r", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "subreddit_subscribers": 102932, "created_utc": 1682624668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to get the group\u2019s experiences on what they\u2019ve done and recommend.\n\n My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.\n\nWe\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.\n\nWe\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. \n\nWhat\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.\n\n[View Poll](https://www.reddit.com/poll/1312r30)", "author_fullname": "t2_6nrc61lj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DW Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312r30", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to get the group\u2019s experiences on what they\u2019ve done and recommend.&lt;/p&gt;\n\n&lt;p&gt;My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1312r30\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1312r30", "is_robot_indexable": true, "report_reasons": null, "author": "paulypavilion", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1682881813052, "options": [{"text": "Star schema", "id": "22770462"}, {"text": "Snowflake Schema", "id": "22770463"}, {"text": "Data Lake only", "id": "22770464"}, {"text": "Data lake +views", "id": "22770465"}, {"text": "Other", "id": "22770466"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 78, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312r30/dw_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1312r30/dw_architecture/", "subreddit_subscribers": 102932, "created_utc": 1682622613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was given a test task that definitely wants to test my skills in data processing optimization. I have some `.csv` data and need to run several transformations and count statistics on it. \n\nIf I just do stuff with plain pandas on `.csv` files it will be ineffective. Any good tips? I have few ideas like converting `.csv` to parquet or `avro`. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   \n\n\nThe data itself is small and in `.csv` but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.", "author_fullname": "t2_lyf92k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on processing 10M+ records locally with python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312gxj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was given a test task that definitely wants to test my skills in data processing optimization. I have some &lt;code&gt;.csv&lt;/code&gt; data and need to run several transformations and count statistics on it. &lt;/p&gt;\n\n&lt;p&gt;If I just do stuff with plain pandas on &lt;code&gt;.csv&lt;/code&gt; files it will be ineffective. Any good tips? I have few ideas like converting &lt;code&gt;.csv&lt;/code&gt; to parquet or &lt;code&gt;avro&lt;/code&gt;. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   &lt;/p&gt;\n\n&lt;p&gt;The data itself is small and in &lt;code&gt;.csv&lt;/code&gt; but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1312gxj", "is_robot_indexable": true, "report_reasons": null, "author": "chelicerae-aureus", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "subreddit_subscribers": 102932, "created_utc": 1682622124.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I'm a little stuck on how exactly to do this. \n\nFor example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?\n\nHopefully this isn't too many questions all at once. I guess I'm just overwhelmed with the sheer number of options.", "author_fullname": "t2_thw4nqfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you organize your extract/load pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130qgti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682608241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I&amp;#39;m a little stuck on how exactly to do this. &lt;/p&gt;\n\n&lt;p&gt;For example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?&lt;/p&gt;\n\n&lt;p&gt;Hopefully this isn&amp;#39;t too many questions all at once. I guess I&amp;#39;m just overwhelmed with the sheer number of options.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130qgti", "is_robot_indexable": true, "report_reasons": null, "author": "BoofThatShit720", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "subreddit_subscribers": 102932, "created_utc": 1682608241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My company in the process of migrating to an AWS service that dumps JSON log data to S3 buckets.\n\nI need to get the data in these logs, transform and store on an on premise database to use in analytical processes.  At some point in the future this will be output to a cloud database.\n\nThe steer from our Technical Architects is that if we want access to these logs we need to do it via AWS Athena as they will not allow direct access to the S3 buckets that store the log data.\n\nWe are allowed to define the Athena tables ourselves so we could pull all the log contents.\n\nIn my reasearch it seems like AWS is primarily a UI for querying data in S3, similar to Hue, and not really designed to serve data to downstream processes.  It looks like it can be done, either by retrieving the query results that are stored in S3 or by setting up an API using other AWS services.\n\nI already have developed a small PySpark pipeline to process the JSON data and would prefer to access the data direct in the S3 buckets.\n\nTLDR: i need a pipepline to transform JSON data stored in S3 and stored in an on premise database.  What would be the best approach?  Is AWS Athena a good fit in this pipeline?", "author_fullname": "t2_h5e9i", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automating retrieval of query results from AWS Athean", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130cuof", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682579401.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My company in the process of migrating to an AWS service that dumps JSON log data to S3 buckets.&lt;/p&gt;\n\n&lt;p&gt;I need to get the data in these logs, transform and store on an on premise database to use in analytical processes.  At some point in the future this will be output to a cloud database.&lt;/p&gt;\n\n&lt;p&gt;The steer from our Technical Architects is that if we want access to these logs we need to do it via AWS Athena as they will not allow direct access to the S3 buckets that store the log data.&lt;/p&gt;\n\n&lt;p&gt;We are allowed to define the Athena tables ourselves so we could pull all the log contents.&lt;/p&gt;\n\n&lt;p&gt;In my reasearch it seems like AWS is primarily a UI for querying data in S3, similar to Hue, and not really designed to serve data to downstream processes.  It looks like it can be done, either by retrieving the query results that are stored in S3 or by setting up an API using other AWS services.&lt;/p&gt;\n\n&lt;p&gt;I already have developed a small PySpark pipeline to process the JSON data and would prefer to access the data direct in the S3 buckets.&lt;/p&gt;\n\n&lt;p&gt;TLDR: i need a pipepline to transform JSON data stored in S3 and stored in an on premise database.  What would be the best approach?  Is AWS Athena a good fit in this pipeline?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130cuof", "is_robot_indexable": true, "report_reasons": null, "author": "the_glover", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130cuof/automating_retrieval_of_query_results_from_aws/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130cuof/automating_retrieval_of_query_results_from_aws/", "subreddit_subscribers": 102932, "created_utc": 1682579401.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm sure I'm not the only one in this situation, what is the best approach when working for a company who insists on using software that is extremely niche to pull data from? In that companies such as Fivetran or Zapier don't have connectors for them. Luckily these software packages have API's, however, you have to apply for their developer program to gain access to them. How do you deal with this in a company setting?\n\nWhen applying for these developer programs it seems they are wanting you to build apps for their marketplace, however, I would only be using these internally. I'm asking as I don't want to apply for the developer program and then 6-12 months down the line I get a call asking where my end product is.", "author_fullname": "t2_60zzpr2k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pulling Data from Software without connectors on popular cloud services (Fivetran, Zapier etc.)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_131d5h2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "19bba012-ac9d-11eb-b77b-0eec37c01719", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682643625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure I&amp;#39;m not the only one in this situation, what is the best approach when working for a company who insists on using software that is extremely niche to pull data from? In that companies such as Fivetran or Zapier don&amp;#39;t have connectors for them. Luckily these software packages have API&amp;#39;s, however, you have to apply for their developer program to gain access to them. How do you deal with this in a company setting?&lt;/p&gt;\n\n&lt;p&gt;When applying for these developer programs it seems they are wanting you to build apps for their marketplace, however, I would only be using these internally. I&amp;#39;m asking as I don&amp;#39;t want to apply for the developer program and then 6-12 months down the line I get a call asking where my end product is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Analyst", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "131d5h2", "is_robot_indexable": true, "report_reasons": null, "author": "Jehhred", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/131d5h2/pulling_data_from_software_without_connectors_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131d5h2/pulling_data_from_software_without_connectors_on/", "subreddit_subscribers": 102932, "created_utc": 1682643625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For most (non-data) projects some level of testing happens but it's still in its infancy for our data engineering side of things. \n\nOur \"architecture\" is as follows: Bunch of microservices where Clojure streams into an object store. Pandas + SQLalchemy (legacy)  places a schema on everything (batch). Finally Polars + raw SQL gets it ready for our downstream tasks.\n\n&amp;#x200B;\n\n1. We validate a bit manually by skimming what comes in every so often (in terms of raw data) but this isn't tractable.\n2. When going from raw to schema and then to clean we *can* do assertions to check our data quality. For example, if object A that we're measuring had 250 events of type B after processing it should be the same. We kind of do this ad-hoc with group by's and joins but it would be cool if this is in some test suite somewhere. \n3. We have logs / notifications to tell us if a pipeline run failed. What we did not forsee is to add notifications if the pipeline came up empty handed (because upstream, out of organisation sources are failing).\n\nI think we want to monitor each source individually but also ensure that they're all \"complete\" in the sense that I'd like it to be visible if they're discrepancies in the events in source 1 vs source 2 vs source 3.\n\nAt the end of the day this is software and I see a bunch of pre-conditions, post-conditions and assertions I want to test. I've been resisting the temptation to reinvent a shitty version of the wheel by handrolling unit tests in Python that run raw-SQL / object store querys to validate that the data is parsed correctly. So far I've looked at great expectations and it seems to be close to what we need.", "author_fullname": "t2_8rjci796o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle tests, assertions and data quality management in your data pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131ar6j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682637385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For most (non-data) projects some level of testing happens but it&amp;#39;s still in its infancy for our data engineering side of things. &lt;/p&gt;\n\n&lt;p&gt;Our &amp;quot;architecture&amp;quot; is as follows: Bunch of microservices where Clojure streams into an object store. Pandas + SQLalchemy (legacy)  places a schema on everything (batch). Finally Polars + raw SQL gets it ready for our downstream tasks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We validate a bit manually by skimming what comes in every so often (in terms of raw data) but this isn&amp;#39;t tractable.&lt;/li&gt;\n&lt;li&gt;When going from raw to schema and then to clean we &lt;em&gt;can&lt;/em&gt; do assertions to check our data quality. For example, if object A that we&amp;#39;re measuring had 250 events of type B after processing it should be the same. We kind of do this ad-hoc with group by&amp;#39;s and joins but it would be cool if this is in some test suite somewhere. &lt;/li&gt;\n&lt;li&gt;We have logs / notifications to tell us if a pipeline run failed. What we did not forsee is to add notifications if the pipeline came up empty handed (because upstream, out of organisation sources are failing).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I think we want to monitor each source individually but also ensure that they&amp;#39;re all &amp;quot;complete&amp;quot; in the sense that I&amp;#39;d like it to be visible if they&amp;#39;re discrepancies in the events in source 1 vs source 2 vs source 3.&lt;/p&gt;\n\n&lt;p&gt;At the end of the day this is software and I see a bunch of pre-conditions, post-conditions and assertions I want to test. I&amp;#39;ve been resisting the temptation to reinvent a shitty version of the wheel by handrolling unit tests in Python that run raw-SQL / object store querys to validate that the data is parsed correctly. So far I&amp;#39;ve looked at great expectations and it seems to be close to what we need.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131ar6j", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-One8023", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131ar6j/how_do_you_handle_tests_assertions_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131ar6j/how_do_you_handle_tests_assertions_and_data/", "subreddit_subscribers": 102932, "created_utc": 1682637385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My past company uses Informatica for data catalog, but I\u2019m curious what popular tools are out there? How should I help my company to decide which data catalog to choose?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you choose which data catalog tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1319zta", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682635615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My past company uses Informatica for data catalog, but I\u2019m curious what popular tools are out there? How should I help my company to decide which data catalog to choose?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1319zta", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1319zta/how_do_you_choose_which_data_catalog_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1319zta/how_do_you_choose_which_data_catalog_tool/", "subreddit_subscribers": 102932, "created_utc": 1682635615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Im a super junior data engineer and my boss wanted this specificly:\n\nOur pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.  \nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.\n\nOur dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.\n\nBoss wants the pipeline to succeed, even though the dataflow should fail.\n\nCan anyone share some insight into which would be the best way to handle this?", "author_fullname": "t2_xcbzsw9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best/easiest way to handle failed dataflows in a ADF pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130yccc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682616467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a super junior data engineer and my boss wanted this specificly:&lt;/p&gt;\n\n&lt;p&gt;Our pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.&lt;br/&gt;\nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.&lt;/p&gt;\n\n&lt;p&gt;Our dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.&lt;/p&gt;\n\n&lt;p&gt;Boss wants the pipeline to succeed, even though the dataflow should fail.&lt;/p&gt;\n\n&lt;p&gt;Can anyone share some insight into which would be the best way to handle this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130yccc", "is_robot_indexable": true, "report_reasons": null, "author": "useyourname89", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "subreddit_subscribers": 102932, "created_utc": 1682616467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "i'm looking for advice to make my development more seamless.\n\nmy idea is, have my IDE tied to wsl (not via ssh -- vm option is out because of vpn), create a project folder, write whatever python scripts and other stuff into there...\n\nthen have it run in a docker container.\n\n&amp;#x200B;\n\nso, the project folder would include a dockerfile, which i would have to build to run the scripts.\n\nAre you guys updating dockerfiles, or are you just creating images out of your containers?\n\nAlso, i noticed it's not really possible to use pipenv when sshing into the container for an interpreter.  So, i'm just looking for detailed examples of how you all do it.", "author_fullname": "t2_5qteskd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what's your dev workflow like", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130tmqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682611503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m looking for advice to make my development more seamless.&lt;/p&gt;\n\n&lt;p&gt;my idea is, have my IDE tied to wsl (not via ssh -- vm option is out because of vpn), create a project folder, write whatever python scripts and other stuff into there...&lt;/p&gt;\n\n&lt;p&gt;then have it run in a docker container.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;so, the project folder would include a dockerfile, which i would have to build to run the scripts.&lt;/p&gt;\n\n&lt;p&gt;Are you guys updating dockerfiles, or are you just creating images out of your containers?&lt;/p&gt;\n\n&lt;p&gt;Also, i noticed it&amp;#39;s not really possible to use pipenv when sshing into the container for an interpreter.  So, i&amp;#39;m just looking for detailed examples of how you all do it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130tmqy", "is_robot_indexable": true, "report_reasons": null, "author": "iseestupid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130tmqy/whats_your_dev_workflow_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130tmqy/whats_your_dev_workflow_like/", "subreddit_subscribers": 102932, "created_utc": 1682611503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In general which cloud vendor has best data engineering services?\n\nA little bit context about where this question is coming from. So I used MWAA and it has a lot of stability issues. So just wanted to know how is the product experience with different cloud vendors when it comes to data engineering ones", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud vendor with best data engineering/data science tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130link", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682608946.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682600625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In general which cloud vendor has best data engineering services?&lt;/p&gt;\n\n&lt;p&gt;A little bit context about where this question is coming from. So I used MWAA and it has a lot of stability issues. So just wanted to know how is the product experience with different cloud vendors when it comes to data engineering ones&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130link", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130link/cloud_vendor_with_best_data_engineeringdata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130link/cloud_vendor_with_best_data_engineeringdata/", "subreddit_subscribers": 102932, "created_utc": 1682600625.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}