{"kind": "Listing", "data": {"after": "t3_131h7dn", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Have you ever interviewed candidates that acted so poorly on an interpersonal level that you had to cut the interview short? If so, how have you handled the situation?", "author_fullname": "t2_l79yml7q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interviews with dick candidates", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1311zdz", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 181, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 181, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682621271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Have you ever interviewed candidates that acted so poorly on an interpersonal level that you had to cut the interview short? If so, how have you handled the situation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1311zdz", "is_robot_indexable": true, "report_reasons": null, "author": "Academic_Baker_9233", "discussion_type": null, "num_comments": 84, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1311zdz/interviews_with_dick_candidates/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1311zdz/interviews_with_dick_candidates/", "subreddit_subscribers": 883552, "created_utc": 1682621271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_5y5y24cy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hey, what is wrong with me? I sent many applications, and lately, I am getting a bunch of no'es, even for entry positions. Not even getting a chance for an interview. Is there something wrong with it? I have a hided bunch of identifying information.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_131bl0x", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "ups": 52, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 52, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7erjq-uYKkqltMVO1L6X9ToKrDx1A2NcZhbfWtkXAhk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682639467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/oy5cmuehliwa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/oy5cmuehliwa1.png?auto=webp&amp;v=enabled&amp;s=1f0c766919f648071f84049a1685047fda08e4c6", "width": 1123, "height": 1580}, "resolutions": [{"url": "https://preview.redd.it/oy5cmuehliwa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=126a57cd1474e5f8e9d558db4fccbcd91deb2b51", "width": 108, "height": 151}, {"url": "https://preview.redd.it/oy5cmuehliwa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=272a284d77279b98b94fce73fba45d9e74b5ecb1", "width": 216, "height": 303}, {"url": "https://preview.redd.it/oy5cmuehliwa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85aa7b1e51b88f691a3b882b79ca8e0ce67f7d96", "width": 320, "height": 450}, {"url": "https://preview.redd.it/oy5cmuehliwa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ae8f4dfb0f2ed2199f5f4fd404a8542aa07837f", "width": 640, "height": 900}, {"url": "https://preview.redd.it/oy5cmuehliwa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b5ba1c94bfc127d29dcdbdfd59fe7b0121f6319", "width": 960, "height": 1350}, {"url": "https://preview.redd.it/oy5cmuehliwa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2fe5cc8a2d38d016d908c556340dbad4ccca39a5", "width": 1080, "height": 1519}], "variants": {}, "id": "PeZkRMDbCHTwwaxzG38aaIFR8EmSGC_VMtgHqntSUsQ"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "131bl0x", "is_robot_indexable": true, "report_reasons": null, "author": "Parazic", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131bl0x/hey_what_is_wrong_with_me_i_sent_many/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/oy5cmuehliwa1.png", "subreddit_subscribers": 883552, "created_utc": 1682639467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "When I was in high school, my algebra 2 schools were weak because I slacked off in Algebra 1 and the highest math course I took was a combo course of algebra 1 and 2 my senior year.\n\nEDIT: I'm actually doing my Masters in Data Science and highest math I took was Calculus 1 in college. I'm just curious if any other Redditors struggled with math.", "author_fullname": "t2_2avd4wfl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Were any of you DS bad at math at some point?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1315m8l", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 46, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 46, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682629181.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682627420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I was in high school, my algebra 2 schools were weak because I slacked off in Algebra 1 and the highest math course I took was a combo course of algebra 1 and 2 my senior year.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I&amp;#39;m actually doing my Masters in Data Science and highest math I took was Calculus 1 in college. I&amp;#39;m just curious if any other Redditors struggled with math.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1315m8l", "is_robot_indexable": true, "report_reasons": null, "author": "Javilism", "discussion_type": null, "num_comments": 58, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1315m8l/were_any_of_you_ds_bad_at_math_at_some_point/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1315m8l/were_any_of_you_ds_bad_at_math_at_some_point/", "subreddit_subscribers": 883552, "created_utc": 1682627420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've seen in the programming humor sub that experienced SWEs have recruiters constantly messaging them for roles. Does this happen with data science too and if so at what level?", "author_fullname": "t2_mkdw0oz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "At what experience level do recruiters reach out", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130z4ni", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682617128.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen in the programming humor sub that experienced SWEs have recruiters constantly messaging them for roles. Does this happen with data science too and if so at what level?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "130z4ni", "is_robot_indexable": true, "report_reasons": null, "author": "send_math_equations", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/130z4ni/at_what_experience_level_do_recruiters_reach_out/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/130z4ni/at_what_experience_level_do_recruiters_reach_out/", "subreddit_subscribers": 883552, "created_utc": 1682617128.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I volunteered at a middle school career fair to discuss my job as a data scientist and created this neat poster. FYI it was way way overkill and most kids didn't read the through much of it. Next year I'll try to reduce the text and incorporate more games / activities. \n\nhttps://preview.redd.it/pnze9c42tgwa1.png?width=2304&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f1faed07e646caa16a56eb5b00839198f51816f8", "author_fullname": "t2_6hvdm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Poster I created for a middle school career fair", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "media_metadata": {"pnze9c42tgwa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 162, "x": 108, "u": "https://preview.redd.it/pnze9c42tgwa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56436100f67b467d23e5afe872863ffa3c814042"}, {"y": 324, "x": 216, "u": "https://preview.redd.it/pnze9c42tgwa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f173bebed98ea86ea0bc01f38e442dd9fecdf3e8"}, {"y": 480, "x": 320, "u": "https://preview.redd.it/pnze9c42tgwa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95cf79ac4dc76f107800b500e11f338fbf9d8668"}, {"y": 960, "x": 640, "u": "https://preview.redd.it/pnze9c42tgwa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=777d23f978ad3f359d14863acb78cdf5cc009096"}, {"y": 1440, "x": 960, "u": "https://preview.redd.it/pnze9c42tgwa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=866b37a9b98d5611962794b4a020fdf5cb1f7a71"}, {"y": 1620, "x": 1080, "u": "https://preview.redd.it/pnze9c42tgwa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7d434d9b97b58217093cb60043ef88207fe4796"}], "s": {"y": 3456, "x": 2304, "u": "https://preview.redd.it/pnze9c42tgwa1.png?width=2304&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f1faed07e646caa16a56eb5b00839198f51816f8"}, "id": "pnze9c42tgwa1"}}, "name": "t3_13105ot", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": true, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/XfVPKB5bCClu8xJGpsDhdorivewk7LwcbpFQckKDodU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682618054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I volunteered at a middle school career fair to discuss my job as a data scientist and created this neat poster. FYI it was way way overkill and most kids didn&amp;#39;t read the through much of it. Next year I&amp;#39;ll try to reduce the text and incorporate more games / activities. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/pnze9c42tgwa1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=f1faed07e646caa16a56eb5b00839198f51816f8\"&gt;https://preview.redd.it/pnze9c42tgwa1.png?width=2304&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=f1faed07e646caa16a56eb5b00839198f51816f8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13105ot", "is_robot_indexable": true, "report_reasons": null, "author": "hipstahs", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13105ot/poster_i_created_for_a_middle_school_career_fair/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13105ot/poster_i_created_for_a_middle_school_career_fair/", "subreddit_subscribers": 883552, "created_utc": 1682618054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "TLDR: Know R + Tidyverse, want to get better at Python + numpy/pandas (or arrow is that better?) + PyData Stack. How?  \n\n\nI'm mainly an R + SQL coder. It gets me through a lot of things... definitely better for visualizations and tidyverse allows for a lot of quick/dirty things to be done adhoc. I have used Python in the past but it always felt cludgy and I never really forced myself to get to where I have near effortless mastery of it.   \n\n\nI'm increasingly, doing XAI and causal inference, and will be productionizing some models in the near future. R works but the packages available seem slow (evtree+GRF vs GOSDT+EconML+CausalML). As such I'm looking towards Python. Also, everyone except for my manager mainly uses Python now. They all know R so it's not a problem but I'm also trying to NOT be the odd man out. \n\nAny tips/tricks for getting good at Python for DS tasks? Should I cram leetcode/hackerrank questions? Maybe push myself to do a few public facing projects on github?", "author_fullname": "t2_9zr0in21s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to get better at Python and the PyData stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131gnf1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682653506.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: Know R + Tidyverse, want to get better at Python + numpy/pandas (or arrow is that better?) + PyData Stack. How?  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m mainly an R + SQL coder. It gets me through a lot of things... definitely better for visualizations and tidyverse allows for a lot of quick/dirty things to be done adhoc. I have used Python in the past but it always felt cludgy and I never really forced myself to get to where I have near effortless mastery of it.   &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m increasingly, doing XAI and causal inference, and will be productionizing some models in the near future. R works but the packages available seem slow (evtree+GRF vs GOSDT+EconML+CausalML). As such I&amp;#39;m looking towards Python. Also, everyone except for my manager mainly uses Python now. They all know R so it&amp;#39;s not a problem but I&amp;#39;m also trying to NOT be the odd man out. &lt;/p&gt;\n\n&lt;p&gt;Any tips/tricks for getting good at Python for DS tasks? Should I cram leetcode/hackerrank questions? Maybe push myself to do a few public facing projects on github?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131gnf1", "is_robot_indexable": true, "report_reasons": null, "author": "ramblinginternetgeek", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131gnf1/looking_to_get_better_at_python_and_the_pydata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131gnf1/looking_to_get_better_at_python_and_the_pydata/", "subreddit_subscribers": 883552, "created_utc": 1682653506.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "What are your experiences using some tool or system to track issues and tasks related to data projects. I\u2019m coming across small projects (maybe 1-3 weeks) but there\u2019s lots of stakeholders who provide data inputs, or want particular outputs, or other teams need to do subtasks and such. So I\u2019m looking for something to just have a common url that people can view and add info to. \n\nI\u2019ve used jira in the past but it gets expensive with lots of users, especially infrequent users who might only want to see a single project in a year. \n\nI\u2019ve used GitLab and GitHub issues pretty effectively but non-devs seem to have a tough time accessing and working on items. \n\nMy org uses ServiceNow but it seems more like just a ticketing system than a collaboration tool. And the UI is really unpleasant looking and difficult to navigate. \n\nAny particular tools that are lightweight and easy for diverse stakeholder groups to connect to and use without a lot of cost and training.", "author_fullname": "t2_6r40s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Good issue tracker for data projects?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131dbqo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682644082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are your experiences using some tool or system to track issues and tasks related to data projects. I\u2019m coming across small projects (maybe 1-3 weeks) but there\u2019s lots of stakeholders who provide data inputs, or want particular outputs, or other teams need to do subtasks and such. So I\u2019m looking for something to just have a common url that people can view and add info to. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve used jira in the past but it gets expensive with lots of users, especially infrequent users who might only want to see a single project in a year. &lt;/p&gt;\n\n&lt;p&gt;I\u2019ve used GitLab and GitHub issues pretty effectively but non-devs seem to have a tough time accessing and working on items. &lt;/p&gt;\n\n&lt;p&gt;My org uses ServiceNow but it seems more like just a ticketing system than a collaboration tool. And the UI is really unpleasant looking and difficult to navigate. &lt;/p&gt;\n\n&lt;p&gt;Any particular tools that are lightweight and easy for diverse stakeholder groups to connect to and use without a lot of cost and training.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131dbqo", "is_robot_indexable": true, "report_reasons": null, "author": "prepend", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131dbqo/good_issue_tracker_for_data_projects/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131dbqo/good_issue_tracker_for_data_projects/", "subreddit_subscribers": 883552, "created_utc": 1682644082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_3xnau4cx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Blog] What is data profiling?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 95, "top_awarded_type": null, "hide_score": false, "name": "t3_130rrma", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.72, "author_flair_background_color": null, "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/lbuo8g_XYpLxzss2nTCWeq5d1o0s5Ur7okJtR1JvwL8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682609497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "greatexpectations.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://greatexpectations.io/blog/what-is-data-profiling", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/QzCFFHyO3P4mKv-lAT0OPjmAHjo-syNkFBo5XINOzzA.jpg?auto=webp&amp;v=enabled&amp;s=478ff82196dc2d5c08e0e2b1292645b89aa7e765", "width": 1283, "height": 879}, "resolutions": [{"url": "https://external-preview.redd.it/QzCFFHyO3P4mKv-lAT0OPjmAHjo-syNkFBo5XINOzzA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1e4b121a74009aa153184bee58fcd9fbf6d25cbf", "width": 108, "height": 73}, {"url": "https://external-preview.redd.it/QzCFFHyO3P4mKv-lAT0OPjmAHjo-syNkFBo5XINOzzA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=79f149d5ddcc1f6498057a915dc6deeb39ac764c", "width": 216, "height": 147}, {"url": "https://external-preview.redd.it/QzCFFHyO3P4mKv-lAT0OPjmAHjo-syNkFBo5XINOzzA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66f512efe91aa6b70088207aa205b65097d3192c", "width": 320, "height": 219}, {"url": "https://external-preview.redd.it/QzCFFHyO3P4mKv-lAT0OPjmAHjo-syNkFBo5XINOzzA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ebae9673c60a857a8dcbf352abab4ed3ca9bf1e8", "width": 640, "height": 438}, {"url": "https://external-preview.redd.it/QzCFFHyO3P4mKv-lAT0OPjmAHjo-syNkFBo5XINOzzA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1c1f8d1b0550c08c806ac8768c72c036bf75d4fa", "width": 960, "height": 657}, {"url": "https://external-preview.redd.it/QzCFFHyO3P4mKv-lAT0OPjmAHjo-syNkFBo5XINOzzA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b1ab29bc6c9fbe4cb45716bee7b7763563e1dcc1", "width": 1080, "height": 739}], "variants": {}, "id": "zFFdiW1ZD1OMCqzYT7L-pdQo1l5aeUIM-LWZp_-9Vzw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "130rrma", "is_robot_indexable": true, "report_reasons": null, "author": "superconductiveKyle", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/130rrma/blog_what_is_data_profiling/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://greatexpectations.io/blog/what-is-data-profiling", "subreddit_subscribers": 883552, "created_utc": 1682609497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on processing 10M+ records locally with python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312hry", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_lyf92k", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was given a test task that definitely wants to test my skills in data processing optimization. I have some `.csv` data and need to run several transformations and count statistics on it. \n\nIf I just do stuff with plain pandas on `.csv` files it will be ineffective. Any good tips? I have few ideas like converting `.csv` to parquet or `avro`. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   \n\n\nThe data itself is small and in `.csv` but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.", "author_fullname": "t2_lyf92k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on processing 10M+ records locally with python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312gxj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was given a test task that definitely wants to test my skills in data processing optimization. I have some &lt;code&gt;.csv&lt;/code&gt; data and need to run several transformations and count statistics on it. &lt;/p&gt;\n\n&lt;p&gt;If I just do stuff with plain pandas on &lt;code&gt;.csv&lt;/code&gt; files it will be ineffective. Any good tips? I have few ideas like converting &lt;code&gt;.csv&lt;/code&gt; to parquet or &lt;code&gt;avro&lt;/code&gt;. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   &lt;/p&gt;\n\n&lt;p&gt;The data itself is small and in &lt;code&gt;.csv&lt;/code&gt; but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1312gxj", "is_robot_indexable": true, "report_reasons": null, "author": "chelicerae-aureus", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "subreddit_subscribers": 102989, "created_utc": 1682622124.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1682622163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1312hry", "is_robot_indexable": true, "report_reasons": null, "author": "chelicerae-aureus", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_1312gxj", "author_flair_text_color": null, "permalink": "/r/datascience/comments/1312hry/tips_on_processing_10m_records_locally_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "subreddit_subscribers": 883552, "created_utc": 1682622163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I applied to NEU MSDS program sometime in the first week of April. I know there are people still waiting from dec to hear back. Have decisions in the previous years been given in mid-late may as well? (Asking if I should keep hope)", "author_fullname": "t2_pwd61hqc", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NEU MSDS Decision delay", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131fxjh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682651360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I applied to NEU MSDS program sometime in the first week of April. I know there are people still waiting from dec to hear back. Have decisions in the previous years been given in mid-late may as well? (Asking if I should keep hope)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131fxjh", "is_robot_indexable": true, "report_reasons": null, "author": "Sherlock-1899", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131fxjh/neu_msds_decision_delay/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131fxjh/neu_msds_decision_delay/", "subreddit_subscribers": 883552, "created_utc": 1682651360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm currently in the last semester of my undergraduate program, and I've been learning about Data Science and Machine Learning a lot these days. I've completed a couple of courses from Coursera (Supervised and Unsupervised Learning) and Udemy (DS and ML Bootcamp- Jose P.). I also got the opportunity to do a remote DS Internship.  \nMy problem at hand is how should I put those skills to use since I don't have a job related to the field yet. I want to post everything I do related to DS and ML on my GitHub. Where should I start? with respect to building a portfolio that attracts hirers.", "author_fullname": "t2_3ihaa088", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DS and ML Portfolio.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131c302", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682640743.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently in the last semester of my undergraduate program, and I&amp;#39;ve been learning about Data Science and Machine Learning a lot these days. I&amp;#39;ve completed a couple of courses from Coursera (Supervised and Unsupervised Learning) and Udemy (DS and ML Bootcamp- Jose P.). I also got the opportunity to do a remote DS Internship.&lt;br/&gt;\nMy problem at hand is how should I put those skills to use since I don&amp;#39;t have a job related to the field yet. I want to post everything I do related to DS and ML on my GitHub. Where should I start? with respect to building a portfolio that attracts hirers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131c302", "is_robot_indexable": true, "report_reasons": null, "author": "Shwifty_MO", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131c302/ds_and_ml_portfolio/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131c302/ds_and_ml_portfolio/", "subreddit_subscribers": 883552, "created_utc": 1682640743.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have weekly sales data for a restaurant, with 113 observations, and every year in January, sales drop significantly and then recover after two or four weeks. I applied a SARIMA model with auto-arima in R, but when I evaluate my model, I realize that the residuals are not constant and not normally distributed. Box-Cox Transformation was already used, lamba was 1.99 but it did not improve the model.\n\nUpon reviewing a time series text, I learned that I need to create a dummy variable. I looked at the observations where the pattern breaks in January and classified them as \"1\", with the others as zero. I estimated my model again and it resulted in a SARIMA with rgex, but when I reviewed the residuals, I still have extremely high residuals in those periods. Residuals used to be -10000, now they are mostly -5000, so they were reduced. Do you have any suggestions? What types of interventions exist? I placed \"1\" for the first few weeks of January, then \"0\" when I see that sales have reached their normal level.\n\nTo test for stationarity, I used the Augmented Dickey-Fuller test and found that the data was not stationary. Therefore, I differenced the time series and checked for stationarity again using the ADF test.\n\nNext, I decomposed the time series into its seasonal, trend, and residual components and plotted the results. I then fit a SARIMA model using auto.arima and checked its diagnostic tests for accuracy. Additionally, I  performed a Box-Ljung test to check for autocorrelation in the residuals and a Jarque-Bera test for normality. Box-Ljung was fine, but Jarque-Bera was not.\n\nI then added a dummy variable for intervention analysis to account for outliers or extreme values in the data. I fit a second SARIMA model using the dummy variable and checked its diagnostic tests for accuracy. However, yI found that the variance was still high, and upon further investigation, I found that the highest residuals corresponded to the same period that I alreadt included in the dummy variable for intervention. What else could I do? any suggestion? thanks!", "author_fullname": "t2_440m0av6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "how to apply intervention analysis to my time series?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1316e2u", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682628678.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have weekly sales data for a restaurant, with 113 observations, and every year in January, sales drop significantly and then recover after two or four weeks. I applied a SARIMA model with auto-arima in R, but when I evaluate my model, I realize that the residuals are not constant and not normally distributed. Box-Cox Transformation was already used, lamba was 1.99 but it did not improve the model.&lt;/p&gt;\n\n&lt;p&gt;Upon reviewing a time series text, I learned that I need to create a dummy variable. I looked at the observations where the pattern breaks in January and classified them as &amp;quot;1&amp;quot;, with the others as zero. I estimated my model again and it resulted in a SARIMA with rgex, but when I reviewed the residuals, I still have extremely high residuals in those periods. Residuals used to be -10000, now they are mostly -5000, so they were reduced. Do you have any suggestions? What types of interventions exist? I placed &amp;quot;1&amp;quot; for the first few weeks of January, then &amp;quot;0&amp;quot; when I see that sales have reached their normal level.&lt;/p&gt;\n\n&lt;p&gt;To test for stationarity, I used the Augmented Dickey-Fuller test and found that the data was not stationary. Therefore, I differenced the time series and checked for stationarity again using the ADF test.&lt;/p&gt;\n\n&lt;p&gt;Next, I decomposed the time series into its seasonal, trend, and residual components and plotted the results. I then fit a SARIMA model using auto.arima and checked its diagnostic tests for accuracy. Additionally, I  performed a Box-Ljung test to check for autocorrelation in the residuals and a Jarque-Bera test for normality. Box-Ljung was fine, but Jarque-Bera was not.&lt;/p&gt;\n\n&lt;p&gt;I then added a dummy variable for intervention analysis to account for outliers or extreme values in the data. I fit a second SARIMA model using the dummy variable and checked its diagnostic tests for accuracy. However, yI found that the variance was still high, and upon further investigation, I found that the highest residuals corresponded to the same period that I alreadt included in the dummy variable for intervention. What else could I do? any suggestion? thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1316e2u", "is_robot_indexable": true, "report_reasons": null, "author": "sircapital97", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1316e2u/how_to_apply_intervention_analysis_to_my_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1316e2u/how_to_apply_intervention_analysis_to_my_time/", "subreddit_subscribers": 883552, "created_utc": 1682628678.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "  Hi there\n\nI am not used to working with machine learning models, and are currently sitting with an issue i hope you can help me with.\n\nI am sitting with a multi classifciation problem, where i try to predict whether the bitcoin will either \u2019increase\u2019, \u2019decrease\u2019 or \u2019neutral\u2019 (neutral is a change in price within -0.5 % to 0.5%). The data I use are based on titles and subtitles from news articles from various media sources. This is definitely a hard machine learning task and i am not expecting a high accuracy.\n\nI\u2019m doing data preprocessing where I doing a number of NLP-tasks such as removing stopwords, stemming etc. I am creating a countvectorizer with n-grams which results in a sparse matrix, that i train the model on. I end up with 1.095 instances. I am training the models Random-forest, KNN, Logistic regression and MLP. I am using gridsearchCV on all the different models.\n\nMost of the models i train are overfitted on the training-data and are getting a very bad result on our test data.\n\nI am trying to prevent the models from overfitting, but no matter which hyperparameters i use, it seems to be overfitting. \n\nCan i lower the accuracy on the model when predicting on training data? Or can i simply conclude, that the overfitting is caused by the data having low prediction power towards the target variable? Or would this rather be shown through low results on both the training and the test data?", "author_fullname": "t2_b5jaztvr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Huge overfit on prediction model - due to data with low predictive power or can this be fixed?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1315nh6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682627472.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there&lt;/p&gt;\n\n&lt;p&gt;I am not used to working with machine learning models, and are currently sitting with an issue i hope you can help me with.&lt;/p&gt;\n\n&lt;p&gt;I am sitting with a multi classifciation problem, where i try to predict whether the bitcoin will either \u2019increase\u2019, \u2019decrease\u2019 or \u2019neutral\u2019 (neutral is a change in price within -0.5 % to 0.5%). The data I use are based on titles and subtitles from news articles from various media sources. This is definitely a hard machine learning task and i am not expecting a high accuracy.&lt;/p&gt;\n\n&lt;p&gt;I\u2019m doing data preprocessing where I doing a number of NLP-tasks such as removing stopwords, stemming etc. I am creating a countvectorizer with n-grams which results in a sparse matrix, that i train the model on. I end up with 1.095 instances. I am training the models Random-forest, KNN, Logistic regression and MLP. I am using gridsearchCV on all the different models.&lt;/p&gt;\n\n&lt;p&gt;Most of the models i train are overfitted on the training-data and are getting a very bad result on our test data.&lt;/p&gt;\n\n&lt;p&gt;I am trying to prevent the models from overfitting, but no matter which hyperparameters i use, it seems to be overfitting. &lt;/p&gt;\n\n&lt;p&gt;Can i lower the accuracy on the model when predicting on training data? Or can i simply conclude, that the overfitting is caused by the data having low prediction power towards the target variable? Or would this rather be shown through low results on both the training and the test data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "1315nh6", "is_robot_indexable": true, "report_reasons": null, "author": "JPReuther", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/1315nh6/huge_overfit_on_prediction_model_due_to_data_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/1315nh6/huge_overfit_on_prediction_model_due_to_data_with/", "subreddit_subscribers": 883552, "created_utc": 1682627472.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm currently working on a project that involves analyzing a dataset with lots of different variables, and I'm hoping to find a software that can help me identify correlations between them. The data looks akin to movie rating/ movie stats database where I want to figure out what movie would a person like depending on previous ratings. I would also like it to be something I can use as API from programming language that is more universal (unlike R for example) so I can build upon it more easily.\n\nThanks for help!", "author_fullname": "t2_lyhdhzbm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for a software that can automatically find correlations between different types of data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13134ku", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682623273.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently working on a project that involves analyzing a dataset with lots of different variables, and I&amp;#39;m hoping to find a software that can help me identify correlations between them. The data looks akin to movie rating/ movie stats database where I want to figure out what movie would a person like depending on previous ratings. I would also like it to be something I can use as API from programming language that is more universal (unlike R for example) so I can build upon it more easily.&lt;/p&gt;\n\n&lt;p&gt;Thanks for help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "13134ku", "is_robot_indexable": true, "report_reasons": null, "author": "TopPaleontologist185", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/13134ku/looking_for_a_software_that_can_automatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/13134ku/looking_for_a_software_that_can_automatically/", "subreddit_subscribers": 883552, "created_utc": 1682623273.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a masters in electrical engineering with a focus on ML. I did work with ML work during my masters program for two years. I am currently in consulting and i don't like it, as i couldn't be further away from data analysis and machine learning. So i want to leave this job.\n\nI recently got a job offer with some data analysis, but the main part will be programming embedded software in c. The overall offer is an improvement from my current job (and importantly, it would involve software development, which is quite lacking in my current job).\n\nThe problem i have is that i am really lacking good programming experience, as i didn't do much coding professionally (i did mostly stick to python during my university time and did some python programming at the beginning of my consulting career).\n\nI am really not sure if i should accept this offer or if i should continue looking for alternatives.", "author_fullname": "t2_syipk1yv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Want to do DS, but got a job offer as embedded developer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131288d", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682621708.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a masters in electrical engineering with a focus on ML. I did work with ML work during my masters program for two years. I am currently in consulting and i don&amp;#39;t like it, as i couldn&amp;#39;t be further away from data analysis and machine learning. So i want to leave this job.&lt;/p&gt;\n\n&lt;p&gt;I recently got a job offer with some data analysis, but the main part will be programming embedded software in c. The overall offer is an improvement from my current job (and importantly, it would involve software development, which is quite lacking in my current job).&lt;/p&gt;\n\n&lt;p&gt;The problem i have is that i am really lacking good programming experience, as i didn&amp;#39;t do much coding professionally (i did mostly stick to python during my university time and did some python programming at the beginning of my consulting career).&lt;/p&gt;\n\n&lt;p&gt;I am really not sure if i should accept this offer or if i should continue looking for alternatives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131288d", "is_robot_indexable": true, "report_reasons": null, "author": "GroschenG", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131288d/want_to_do_ds_but_got_a_job_offer_as_embedded/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131288d/want_to_do_ds_but_got_a_job_offer_as_embedded/", "subreddit_subscribers": 883552, "created_utc": 1682621708.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " I want to use GridsearchCV for hyperparametertuning, but in my code  there are several parts that I want to try different parameters for. I  first want to use SMOTE for oversampling, and there the parameters would  be the k-nearest neighbours (either 3, 5 or 7), and sampling\\_strategy  which will be either 0.1, 0.15, or 0.2. Then when I use LDA (latent  Dirichlet allocation) I want to set the topic count to either 5, 10 or  15. And then I want to use a multinomial na\u00efve bayes classifier, and  either do the classification with only the descriptions as predictor,  the descriptions and the publisher, or descriptions, publisher,  pagecount, and title.  \n\nCan I do all of this is the same GridSearchCV algorithm? Cause so far I have only found examples where they use one classifier and then use GridsearchCV to try different parameters for that single model.", "author_fullname": "t2_2uua33bh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using GridsearchCV for several parts of code?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131279p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682621659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to use GridsearchCV for hyperparametertuning, but in my code  there are several parts that I want to try different parameters for. I  first want to use SMOTE for oversampling, and there the parameters would  be the k-nearest neighbours (either 3, 5 or 7), and sampling_strategy  which will be either 0.1, 0.15, or 0.2. Then when I use LDA (latent  Dirichlet allocation) I want to set the topic count to either 5, 10 or  15. And then I want to use a multinomial na\u00efve bayes classifier, and  either do the classification with only the descriptions as predictor,  the descriptions and the publisher, or descriptions, publisher,  pagecount, and title.  &lt;/p&gt;\n\n&lt;p&gt;Can I do all of this is the same GridSearchCV algorithm? Cause so far I have only found examples where they use one classifier and then use GridsearchCV to try different parameters for that single model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131279p", "is_robot_indexable": true, "report_reasons": null, "author": "Romcom1398", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131279p/using_gridsearchcv_for_several_parts_of_code/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131279p/using_gridsearchcv_for_several_parts_of_code/", "subreddit_subscribers": 883552, "created_utc": 1682621659.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi, I've been a data engineer for about 2 years now and am currently getting a masters in data analytics.\n\nI use snowflake sql, python, and aws in my current role.\n\nIn my coursework for my class we go over network science (doesn't even seem related to the field except in niche roles it seems?), ML, data visualization, cloud, sas, and spark to name a few tools.\n\nA few questions- is there any difference between data analyst/ data scientist/ ML engineer / MLops roles? Or are they pretty much synonymous and more company dependent? I'm more interested in getting into data prep, model training and model deployment as opposed to visualization. Is there a specific role I should be targeting that would fit my interests? From some research it seems like I should be targeting data science roles? Not sure if It would be too much of a jump in terms of lack of experience to jump into the role as my professional experience with modeling is only part time teaching kids ML and an internship where I did some computer vision stuff.\n\nWhat's the day to day look like in the role? Is most of your work spend on the data prep/ model training/ model deployment?\n\nWhere can I go to learn about industry standards about model creation and deployment? If there's one thing I've learned from undergrad, school did not prepare me well for that and I don't expect grad school to be different.", "author_fullname": "t2_7170e", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What steps should I take to transition my data engineering career to a data analyst/ML engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130u52o", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682613264.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682612454.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;ve been a data engineer for about 2 years now and am currently getting a masters in data analytics.&lt;/p&gt;\n\n&lt;p&gt;I use snowflake sql, python, and aws in my current role.&lt;/p&gt;\n\n&lt;p&gt;In my coursework for my class we go over network science (doesn&amp;#39;t even seem related to the field except in niche roles it seems?), ML, data visualization, cloud, sas, and spark to name a few tools.&lt;/p&gt;\n\n&lt;p&gt;A few questions- is there any difference between data analyst/ data scientist/ ML engineer / MLops roles? Or are they pretty much synonymous and more company dependent? I&amp;#39;m more interested in getting into data prep, model training and model deployment as opposed to visualization. Is there a specific role I should be targeting that would fit my interests? From some research it seems like I should be targeting data science roles? Not sure if It would be too much of a jump in terms of lack of experience to jump into the role as my professional experience with modeling is only part time teaching kids ML and an internship where I did some computer vision stuff.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the day to day look like in the role? Is most of your work spend on the data prep/ model training/ model deployment?&lt;/p&gt;\n\n&lt;p&gt;Where can I go to learn about industry standards about model creation and deployment? If there&amp;#39;s one thing I&amp;#39;ve learned from undergrad, school did not prepare me well for that and I don&amp;#39;t expect grad school to be different.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "130u52o", "is_robot_indexable": true, "report_reasons": null, "author": "wcb98", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/130u52o/what_steps_should_i_take_to_transition_my_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/130u52o/what_steps_should_i_take_to_transition_my_data/", "subreddit_subscribers": 883552, "created_utc": 1682612454.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am going into my third year of a maths degree and wanted some advice on which modules would be most useful for a data science career. If you believe they are all essential, then which ones are the hardest to self-study?\n\nI can **choose 2 from the following:**\n\n&amp;#x200B;\n\n* **Frequentist and Bayesian Inference**\n   * Builds on previous studies of classical (frequentist) inference and introduces  Bayesian inference\n   * Topics such as sufficiency, estimating equations, likelihood ratio tests and best-unbiased estimators are explored in detail.\n   * Will also use statistical packages\n   * I imagine this would be useful as a product analyst running experiments\n\n&amp;#x200B;\n\n* **Optimisation**\n   * Centred around classical optimization problems such as linear programming and nonlinear regression problems arising in a myriad of areas including operations research, computational data science, and financial mathematics, among many others\n   * Could be useful for understanding machine learning algorithms - things like gradient descent.\n   * Known for being an easier module\n\n&amp;#x200B;\n\n* **Time Series Analysis**\n   * concepts of stationary and non-stationary time series;\n   * philosophy of model building in the context of time series analysis;\n   * simple time series models and their properties;\n   * the model identification process;\n   * estimation of parameters;\n   * assessing the goodness of fit;\n   * methods for forecasting;\n   * use of a statistical package\n\n&amp;#x200B;\n\n* **Stochastic Models**\n   * probably the least practical module, but I did enjoy studying Markov chains...\n   * Builds on previous studies of discrete-time Markov chains.\n   * homogeneous Poisson processes and their elementary properties;\n   * birth-and-death processes - forward and backward equations, extinction probability;\n   * epidemic processes - chain-binomial models, parameter estimation, deterministic and stochastic general epidemic, threshold behaviour, carrier-borne epidemics;\n   * queueing processes - equilibrium behaviour of single server queues;\n   * queues with priorities;\n   * component reliability and replacement schemes;\n   * system reliability;\n   * Stochastic differential equations and Ito's lemma.", "author_fullname": "t2_i4soxrm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Which modules would you recommend for an aspiring data scientist?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130q332", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682615599.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682607874.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am going into my third year of a maths degree and wanted some advice on which modules would be most useful for a data science career. If you believe they are all essential, then which ones are the hardest to self-study?&lt;/p&gt;\n\n&lt;p&gt;I can &lt;strong&gt;choose 2 from the following:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Frequentist and Bayesian Inference&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Builds on previous studies of classical (frequentist) inference and introduces  Bayesian inference&lt;/li&gt;\n&lt;li&gt;Topics such as sufficiency, estimating equations, likelihood ratio tests and best-unbiased estimators are explored in detail.&lt;/li&gt;\n&lt;li&gt;Will also use statistical packages&lt;/li&gt;\n&lt;li&gt;I imagine this would be useful as a product analyst running experiments&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Optimisation&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Centred around classical optimization problems such as linear programming and nonlinear regression problems arising in a myriad of areas including operations research, computational data science, and financial mathematics, among many others&lt;/li&gt;\n&lt;li&gt;Could be useful for understanding machine learning algorithms - things like gradient descent.&lt;/li&gt;\n&lt;li&gt;Known for being an easier module&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Time Series Analysis&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;concepts of stationary and non-stationary time series;&lt;/li&gt;\n&lt;li&gt;philosophy of model building in the context of time series analysis;&lt;/li&gt;\n&lt;li&gt;simple time series models and their properties;&lt;/li&gt;\n&lt;li&gt;the model identification process;&lt;/li&gt;\n&lt;li&gt;estimation of parameters;&lt;/li&gt;\n&lt;li&gt;assessing the goodness of fit;&lt;/li&gt;\n&lt;li&gt;methods for forecasting;&lt;/li&gt;\n&lt;li&gt;use of a statistical package&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Stochastic Models&lt;/strong&gt;\n\n&lt;ul&gt;\n&lt;li&gt;probably the least practical module, but I did enjoy studying Markov chains...&lt;/li&gt;\n&lt;li&gt;Builds on previous studies of discrete-time Markov chains.&lt;/li&gt;\n&lt;li&gt;homogeneous Poisson processes and their elementary properties;&lt;/li&gt;\n&lt;li&gt;birth-and-death processes - forward and backward equations, extinction probability;&lt;/li&gt;\n&lt;li&gt;epidemic processes - chain-binomial models, parameter estimation, deterministic and stochastic general epidemic, threshold behaviour, carrier-borne epidemics;&lt;/li&gt;\n&lt;li&gt;queueing processes - equilibrium behaviour of single server queues;&lt;/li&gt;\n&lt;li&gt;queues with priorities;&lt;/li&gt;\n&lt;li&gt;component reliability and replacement schemes;&lt;/li&gt;\n&lt;li&gt;system reliability;&lt;/li&gt;\n&lt;li&gt;Stochastic differential equations and Ito&amp;#39;s lemma.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "130q332", "is_robot_indexable": true, "report_reasons": null, "author": "poorname", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/130q332/which_modules_would_you_recommend_for_an_aspiring/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/130q332/which_modules_would_you_recommend_for_an_aspiring/", "subreddit_subscribers": 883552, "created_utc": 1682607874.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_7qfjqq47", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I have data for altitude for a single location, but with time it's fluctuating a lot (data_max-data_min = 77 ideally, it should be 0). Is there any way to confidently find the value of altitude?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130jqkt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682596718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "130jqkt", "is_robot_indexable": true, "report_reasons": null, "author": "Anu_Rag9704", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/130jqkt/i_have_data_for_altitude_for_a_single_location/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/130jqkt/i_have_data_for_altitude_for_a_single_location/", "subreddit_subscribers": 883552, "created_utc": 1682596718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I need to predict end of quarter customer retention by the end of each week. Ideally I would train separate models on weekly historic data but that would be too much to handle so I'm thinking of training on end of month data and feed the weekly data to the model for predictions. That way I'll have 3 models instead of ~13 models/quarter. \n\nBut the training data has same customer population multiple times. For example two rows of the data (only predictors) would be like: \n\n| ID  | Age | Total Spending  | Month Seq |\n|-----|-----|-----------------|-----------|\n| 007 | 45  | 150             | 1         |   \n| 007 | 45  | 220             | 2         |   \n\nMy understanding is that panel data breaks the assumption that each row is an independent record but how bad would it be if it's broken?", "author_fullname": "t2_9axqyq8u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How bad a idea is it to use panel data for traditional predictive model training?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130j9h4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682595620.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to predict end of quarter customer retention by the end of each week. Ideally I would train separate models on weekly historic data but that would be too much to handle so I&amp;#39;m thinking of training on end of month data and feed the weekly data to the model for predictions. That way I&amp;#39;ll have 3 models instead of ~13 models/quarter. &lt;/p&gt;\n\n&lt;p&gt;But the training data has same customer population multiple times. For example two rows of the data (only predictors) would be like: &lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;ID&lt;/th&gt;\n&lt;th&gt;Age&lt;/th&gt;\n&lt;th&gt;Total Spending&lt;/th&gt;\n&lt;th&gt;Month Seq&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;007&lt;/td&gt;\n&lt;td&gt;45&lt;/td&gt;\n&lt;td&gt;150&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;007&lt;/td&gt;\n&lt;td&gt;45&lt;/td&gt;\n&lt;td&gt;220&lt;/td&gt;\n&lt;td&gt;2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;My understanding is that panel data breaks the assumption that each row is an independent record but how bad would it be if it&amp;#39;s broken?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "130j9h4", "is_robot_indexable": true, "report_reasons": null, "author": "Difficult-Big-3890", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/130j9h4/how_bad_a_idea_is_it_to_use_panel_data_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/130j9h4/how_bad_a_idea_is_it_to_use_panel_data_for/", "subreddit_subscribers": 883552, "created_utc": 1682595620.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " \n\nHello. One of our colleagues in Ubuntu put together this blog about Kubeflow Pipelines. Why reading it?\n\n* Learn more about Kubeflow Pipelines\n* Use cases for Kubeflow Pipelines\n* Learn how to build Kubeflow Pipelines\n\nRead now: https://medium.com/ubuntu-ai/how-to-build-and-share-components-for-kubeflow-pipelines-86f2c8f40de5", "author_fullname": "t2_3z4miuvs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Kubeflow Pipelines", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_131n8x9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682675990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. One of our colleagues in Ubuntu put together this blog about Kubeflow Pipelines. Why reading it?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Learn more about Kubeflow Pipelines&lt;/li&gt;\n&lt;li&gt;Use cases for Kubeflow Pipelines&lt;/li&gt;\n&lt;li&gt;Learn how to build Kubeflow Pipelines&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Read now: &lt;a href=\"https://medium.com/ubuntu-ai/how-to-build-and-share-components-for-kubeflow-pipelines-86f2c8f40de5\"&gt;https://medium.com/ubuntu-ai/how-to-build-and-share-components-for-kubeflow-pipelines-86f2c8f40de5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/iyJUytMmyIxQSfR1yV1gRIYHVfKJi4G4Qo3Ww6vpr0g.jpg?auto=webp&amp;v=enabled&amp;s=b8b09233d939f013786f17ca0d7e833bfbdb413a", "width": 954, "height": 663}, "resolutions": [{"url": "https://external-preview.redd.it/iyJUytMmyIxQSfR1yV1gRIYHVfKJi4G4Qo3Ww6vpr0g.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=edb3927aa8a47fc64c2d5e22a71668a554013c7e", "width": 108, "height": 75}, {"url": "https://external-preview.redd.it/iyJUytMmyIxQSfR1yV1gRIYHVfKJi4G4Qo3Ww6vpr0g.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b582fd40ecbf794d21e29b762c9a978c392e5bf", "width": 216, "height": 150}, {"url": "https://external-preview.redd.it/iyJUytMmyIxQSfR1yV1gRIYHVfKJi4G4Qo3Ww6vpr0g.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66bd4d2363a65312f19f9e2cadbc46e4fefcdd1b", "width": 320, "height": 222}, {"url": "https://external-preview.redd.it/iyJUytMmyIxQSfR1yV1gRIYHVfKJi4G4Qo3Ww6vpr0g.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ff1ee941308edd95b4c670e3ac552ee4739c29a2", "width": 640, "height": 444}], "variants": {}, "id": "755gnZZmByOM3cPaYjn_pcB5n0sXtWHGGu-XWiCN0Eo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131n8x9", "is_robot_indexable": true, "report_reasons": null, "author": "andreea-mun", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131n8x9/kubeflow_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131n8x9/kubeflow_pipelines/", "subreddit_subscribers": 883552, "created_utc": 1682675990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "We just published our new Data Science project, divided into 3 Python notebooks (for easier reading).\n\n&amp;#x200B;\n\nTechnologies used:\n\n* PostgreSQL\n* AWS Sagemaker\n* Catboost\n* AutoGluon\n\n&amp;#x200B;\n\nStart from the first notebook, link to others are into the notebooks:\n\n[https://github.com/MandM-DataScience/house-price-prediction/blob/master/notebooks/house\\_price\\_prediction.ipynb](https://github.com/MandM-DataScience/house-price-prediction/blob/master/notebooks/house_price_prediction.ipynb)\n\n&amp;#x200B;\n\nEvery and each comment / feedback is greatly appreciated!\n\n&amp;#x200B;\n\nThank you!\n\nM&amp;M\n\nhttps://preview.redd.it/bkvxwqmxklwa1.png?width=612&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a3d1a9873ce9be5ee0f669e69c72dfe93bc10e5", "author_fullname": "t2_81hr0pq4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Real Estate house price prediction - How to predict house prices with Machine Learning?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": true, "media_metadata": {"bkvxwqmxklwa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 115, "x": 108, "u": "https://preview.redd.it/bkvxwqmxklwa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47da63bf5248df761ecda9f237ffaf18f2acf389"}, {"y": 231, "x": 216, "u": "https://preview.redd.it/bkvxwqmxklwa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1d225f3aeff3c9d7bf136698e2da31ba068106a3"}, {"y": 343, "x": 320, "u": "https://preview.redd.it/bkvxwqmxklwa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a24b08ea3c0cfa31c5153406bdcccb14557abf3"}], "s": {"y": 657, "x": 612, "u": "https://preview.redd.it/bkvxwqmxklwa1.png?width=612&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a3d1a9873ce9be5ee0f669e69c72dfe93bc10e5"}, "id": "bkvxwqmxklwa1"}}, "name": "t3_131n4yi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/v0JpTri2sPLq21EfkTnUGH8tR2Inso8ofyPqsqiuC5U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682675631.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We just published our new Data Science project, divided into 3 Python notebooks (for easier reading).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Technologies used:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;PostgreSQL&lt;/li&gt;\n&lt;li&gt;AWS Sagemaker&lt;/li&gt;\n&lt;li&gt;Catboost&lt;/li&gt;\n&lt;li&gt;AutoGluon&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Start from the first notebook, link to others are into the notebooks:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/MandM-DataScience/house-price-prediction/blob/master/notebooks/house_price_prediction.ipynb\"&gt;https://github.com/MandM-DataScience/house-price-prediction/blob/master/notebooks/house_price_prediction.ipynb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Every and each comment / feedback is greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;M&amp;amp;M&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bkvxwqmxklwa1.png?width=612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a3d1a9873ce9be5ee0f669e69c72dfe93bc10e5\"&gt;https://preview.redd.it/bkvxwqmxklwa1.png?width=612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a3d1a9873ce9be5ee0f669e69c72dfe93bc10e5&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131n4yi", "is_robot_indexable": true, "report_reasons": null, "author": "MandM-DataScience", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131n4yi/real_estate_house_price_prediction_how_to_predict/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131n4yi/real_estate_house_price_prediction_how_to_predict/", "subreddit_subscribers": 883552, "created_utc": 1682675631.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all,\n\nI am working in consultancy inside one of the big4 companies. I am still junior but quite lucky to be considered one of the most technical employee of the data scientists therefore I am now starting to have more and more technical projects which are often DS related. \n\nMost of the times I am working as the only DS of the team, so I can structure my work how I want and everyone has been happy with the results so far. \n\nBut it happens quite frequently that I am given super unclear requirements (or no requirements at all) for the analysis that I should provide or the way the code should be implemented. Because of that I often have to change things in the code which I did not anticipate, just because someone wants a special graph or something like that, and I feel that this is really not efficient because if I knew it at first, I would have chosen another structure for the code.\n\nIt's hard for me to keep structured in that kind of setting, I need to have a clear objective to be able to \"decompose\" the task in subtasks and tackle it correctly. \n\nHow do you cope with that ? Interested to see what everyone thinks about that topic. Is it a problem coming from me ?", "author_fullname": "t2_mi18lm4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to deal with unclear requirements ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131ky3p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682667780.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am working in consultancy inside one of the big4 companies. I am still junior but quite lucky to be considered one of the most technical employee of the data scientists therefore I am now starting to have more and more technical projects which are often DS related. &lt;/p&gt;\n\n&lt;p&gt;Most of the times I am working as the only DS of the team, so I can structure my work how I want and everyone has been happy with the results so far. &lt;/p&gt;\n\n&lt;p&gt;But it happens quite frequently that I am given super unclear requirements (or no requirements at all) for the analysis that I should provide or the way the code should be implemented. Because of that I often have to change things in the code which I did not anticipate, just because someone wants a special graph or something like that, and I feel that this is really not efficient because if I knew it at first, I would have chosen another structure for the code.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s hard for me to keep structured in that kind of setting, I need to have a clear objective to be able to &amp;quot;decompose&amp;quot; the task in subtasks and tackle it correctly. &lt;/p&gt;\n\n&lt;p&gt;How do you cope with that ? Interested to see what everyone thinks about that topic. Is it a problem coming from me ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131ky3p", "is_robot_indexable": true, "report_reasons": null, "author": "GuinsooIsOverrated", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131ky3p/how_to_deal_with_unclear_requirements/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131ky3p/how_to_deal_with_unclear_requirements/", "subreddit_subscribers": 883552, "created_utc": 1682667780.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I used to be a mechanical engineer and worked as an applications engineer, serving as the technical expert for the sales team. I got to speak with customers every day, solve problems, and enjoy relatively low stress since there wasn't a quote because I was not an outside sales person.\n\nAre there similar roles in DS/DE, and if so what are the titles I could search to find them?", "author_fullname": "t2_j3ecksk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pre Sales Technical Support Roles?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131huok", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682657237.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used to be a mechanical engineer and worked as an applications engineer, serving as the technical expert for the sales team. I got to speak with customers every day, solve problems, and enjoy relatively low stress since there wasn&amp;#39;t a quote because I was not an outside sales person.&lt;/p&gt;\n\n&lt;p&gt;Are there similar roles in DS/DE, and if so what are the titles I could search to find them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "131huok", "is_robot_indexable": true, "report_reasons": null, "author": "SellGameRent", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131huok/pre_sales_technical_support_roles/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/131huok/pre_sales_technical_support_roles/", "subreddit_subscribers": 883552, "created_utc": 1682657237.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_azskf4p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hidet: A Deep Learning Compiler for Efficient Model Serving", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_131h7dn", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/fAm7P-UlVf_qaQcB_jDTo1q6r3pSQWpQu8f3sKVBVIE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682655182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "pytorch.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://pytorch.org/blog/introducing-hidet/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?auto=webp&amp;v=enabled&amp;s=2e9c9da7a7990b70b830924b475efc9eca5512c5", "width": 2500, "height": 2500}, "resolutions": [{"url": "https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=774518c93b542d9d3fc262559bde3c68148d2337", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e7473f2ce34370311755c5b6ccaa6a735448984", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=64f3a434b60840e394100bcfdb44e0295b7a77e7", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eeb080bfe5ec842b00fdf5f4bd059944e140c7e9", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=99f38a48a5e67cbb850407729193536d53fa058d", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/AOO-W-0wH9-gVmVZfqTZzYzrYLawr2I0pntQzUTRsek.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d21e837a6bbb9e9ebf4af658356b7dcd9b657c4d", "width": 1080, "height": 1080}], "variants": {}, "id": "Mg0HAz_PmYkYWkaGTVyhCEzp15PpLW_TWWqmn9VNVFs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "131h7dn", "is_robot_indexable": true, "report_reasons": null, "author": "ashvar", "discussion_type": null, "num_comments": 0, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/131h7dn/hidet_a_deep_learning_compiler_for_efficient/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://pytorch.org/blog/introducing-hidet/", "subreddit_subscribers": 883552, "created_utc": 1682655182.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}