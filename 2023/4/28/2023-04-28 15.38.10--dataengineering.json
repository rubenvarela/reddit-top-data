{"kind": "Listing", "data": {"after": "t3_130yccc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My personal favorite... A man's health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.", "author_fullname": "t2_3xnau4cx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your favorite data quality horror story?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130rfc2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 210, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 210, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682609164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My personal favorite... A man&amp;#39;s health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130rfc2", "is_robot_indexable": true, "report_reasons": null, "author": "superconductiveKyle", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "subreddit_subscribers": 103018, "created_utc": 1682609164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dzfsy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT core v1.5 released", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_131okrw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bLH7aUGThmTtYxWuUgXrXurKo3TMCVwULoPk57XDgVs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682680127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/dbt-labs/dbt-core/releases/tag/v1.5.0", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?auto=webp&amp;v=enabled&amp;s=51c20d5fe12d6c5c2e06e42b504b68de4a2b8175", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e8d1cf3943cc053c060e9c1d46c48cb0b24f8d4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b3a3aa073ca5c7f038cc01f5d085736e9b2aca5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b9720e0cd577e8d05e12bedaf0fa8ee68d109e6", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f623ac4d0b73d217141fc13af2b1a2751e69bce8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8dbad338b85e1daa4a3927ab85f7f1939c5fa839", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97fa8149e133339c0f836077ef3ac8665e5dad6c", "width": 1080, "height": 540}], "variants": {}, "id": "tQphSKEWxPuFtXmFU_-tHL65XBnxhvlVI4_nmg5KSew"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "131okrw", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_Again", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131okrw/dbt_core_v15_released/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/dbt-labs/dbt-core/releases/tag/v1.5.0", "subreddit_subscribers": 103018, "created_utc": 1682680127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, sorry if this is a stupid question. But I'd love to know why companies would pay for both DBT cloud and Fivetran now that Fivetran has built in DBT core with scheduling?\n\nIt seems like there's an ever growing number of data tools, each with their own expensive price tag. So I am trying to understand which tools are necessary and which aren't. Thanks!", "author_fullname": "t2_mytvjynu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why pay for DBT cloud when Fivetran has built in DBT Core?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1318f7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682632179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, sorry if this is a stupid question. But I&amp;#39;d love to know why companies would pay for both DBT cloud and Fivetran now that Fivetran has built in DBT core with scheduling?&lt;/p&gt;\n\n&lt;p&gt;It seems like there&amp;#39;s an ever growing number of data tools, each with their own expensive price tag. So I am trying to understand which tools are necessary and which aren&amp;#39;t. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1318f7s", "is_robot_indexable": true, "report_reasons": null, "author": "a-layerup", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1318f7s/why_pay_for_dbt_cloud_when_fivetran_has_built_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1318f7s/why_pay_for_dbt_cloud_when_fivetran_has_built_in/", "subreddit_subscribers": 103018, "created_utc": 1682632179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am a cloud/data architect with 7+ years of experience working at Microsoft. I love helping, guiding, and mentoring students and working professionals on technical or personal topics.\n\nDuring my career in cloud and data engineering, I've received invaluable support from fellow professionals, and I'm eager to pay it forward. That's why I'm offering free mentorship to anyone who may benefit from it in these fields.\n\nOur weekly 20-minute calls can cover a range of topics, such as\n\n1. Transitioning into cloud and data engineering,\n2. Strategies for career growth,\n3. Tackling technical challenges, and\n4. Receiving feedback on your work.\n\nAs your mentor, I'll draw on my experience to provide guidance and support tailored to your needs. I'm passionate about helping others succeed in these fields, and I believe that mentorship can make a significant difference.\n\nIf you're interested in learning more or scheduling a call, please feel free to direct message me. I'm excited to hear from you and help you achieve your goals in cloud and data engineering!", "author_fullname": "t2_rrbmofj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Free Mentorship in Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_131sj88", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682690275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am a cloud/data architect with 7+ years of experience working at Microsoft. I love helping, guiding, and mentoring students and working professionals on technical or personal topics.&lt;/p&gt;\n\n&lt;p&gt;During my career in cloud and data engineering, I&amp;#39;ve received invaluable support from fellow professionals, and I&amp;#39;m eager to pay it forward. That&amp;#39;s why I&amp;#39;m offering free mentorship to anyone who may benefit from it in these fields.&lt;/p&gt;\n\n&lt;p&gt;Our weekly 20-minute calls can cover a range of topics, such as&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Transitioning into cloud and data engineering,&lt;/li&gt;\n&lt;li&gt;Strategies for career growth,&lt;/li&gt;\n&lt;li&gt;Tackling technical challenges, and&lt;/li&gt;\n&lt;li&gt;Receiving feedback on your work.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;As your mentor, I&amp;#39;ll draw on my experience to provide guidance and support tailored to your needs. I&amp;#39;m passionate about helping others succeed in these fields, and I believe that mentorship can make a significant difference.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in learning more or scheduling a call, please feel free to direct message me. I&amp;#39;m excited to hear from you and help you achieve your goals in cloud and data engineering!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "131sj88", "is_robot_indexable": true, "report_reasons": null, "author": "Anishekkamal", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131sj88/free_mentorship_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131sj88/free_mentorship_in_data_engineering/", "subreddit_subscribers": 103018, "created_utc": 1682690275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I didn't think I would get the position. All I have is a technical degree in data analysis, a couple of years of experience in the same area, and some certs. Extremely happy.\n\nWondering if I should get a degree, or if I can continue to grow with just experience and certifications.", "author_fullname": "t2_8kf3kc84t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No bachelor degree, got offered a data engineer position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131fdyu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682649789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I didn&amp;#39;t think I would get the position. All I have is a technical degree in data analysis, a couple of years of experience in the same area, and some certs. Extremely happy.&lt;/p&gt;\n\n&lt;p&gt;Wondering if I should get a degree, or if I can continue to grow with just experience and certifications.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "131fdyu", "is_robot_indexable": true, "report_reasons": null, "author": "Pinkypie_15", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131fdyu/no_bachelor_degree_got_offered_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131fdyu/no_bachelor_degree_got_offered_a_data_engineer/", "subreddit_subscribers": 103018, "created_utc": 1682649789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know a lot of you will say you can learn for free, but my employer will pay for it, and I\u2019d like to take advantage of that money.", "author_fullname": "t2_ulshlx1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any paid bootcamp or certification worth taking for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131bffg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682639069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a lot of you will say you can learn for free, but my employer will pay for it, and I\u2019d like to take advantage of that money.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "131bffg", "is_robot_indexable": true, "report_reasons": null, "author": "__academic__", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131bffg/is_there_any_paid_bootcamp_or_certification_worth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131bffg/is_there_any_paid_bootcamp_or_certification_worth/", "subreddit_subscribers": 103018, "created_utc": 1682639069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm pretty comfortable with Terraform, but not to the same extent as an engineer who works with it every day.  I've worked at bigger companies with massive Terraform code bases that always fascinated me about how they were built over time.\n\n\n\n\n\nHow would building data infrastructure work if you were one of the first few data engineers at a startup?  Would there be pressure to deliver results immediately and would you just quickly manually provision resources on the cloud so you can get things up and running, or do everything in Terraform first?  Would your priorities be just setting up all the data infrastructure first long before you start building data pipelines and reporting?", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does data infrastructure/infrastructure as code at a startup evolve over time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131oyl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682681472.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682681269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m pretty comfortable with Terraform, but not to the same extent as an engineer who works with it every day.  I&amp;#39;ve worked at bigger companies with massive Terraform code bases that always fascinated me about how they were built over time.&lt;/p&gt;\n\n&lt;p&gt;How would building data infrastructure work if you were one of the first few data engineers at a startup?  Would there be pressure to deliver results immediately and would you just quickly manually provision resources on the cloud so you can get things up and running, or do everything in Terraform first?  Would your priorities be just setting up all the data infrastructure first long before you start building data pipelines and reporting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131oyl3", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131oyl3/how_does_data_infrastructureinfrastructure_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131oyl3/how_does_data_infrastructureinfrastructure_as/", "subreddit_subscribers": 103018, "created_utc": 1682681269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As part of a hubby project I am working on, I need to be able to take SQL strings supplied by users, and then run it.\n\nBut before running it, i will like to do some manipulations to the SQL. For example, I may want to have a copy of the SQL that does a COUNT query version of the original query...I might want to insert LIMIT clauses if not provided, or I might want to manipulate the columns specified, rename or even insert columns.\n\nNow the question is: I am not sure how to go able this and what sort of tools to even look for. I think in essence I want to be able to manipulate the SQL not as a string but as a data structure I can manipulate\n\nUsing regex and string manipulation does not sound like the right tool for the job. What will you say is the right way to go about this?", "author_fullname": "t2_x8zrrj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manipulate SQL string programmatically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131j6ee", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682661573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As part of a hubby project I am working on, I need to be able to take SQL strings supplied by users, and then run it.&lt;/p&gt;\n\n&lt;p&gt;But before running it, i will like to do some manipulations to the SQL. For example, I may want to have a copy of the SQL that does a COUNT query version of the original query...I might want to insert LIMIT clauses if not provided, or I might want to manipulate the columns specified, rename or even insert columns.&lt;/p&gt;\n\n&lt;p&gt;Now the question is: I am not sure how to go able this and what sort of tools to even look for. I think in essence I want to be able to manipulate the SQL not as a string but as a data structure I can manipulate&lt;/p&gt;\n\n&lt;p&gt;Using regex and string manipulation does not sound like the right tool for the job. What will you say is the right way to go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131j6ee", "is_robot_indexable": true, "report_reasons": null, "author": "io_geekabyte", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131j6ee/how_to_manipulate_sql_string_programmatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131j6ee/how_to_manipulate_sql_string_programmatically/", "subreddit_subscribers": 103018, "created_utc": 1682661573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently encountered Step Functions and can\u2019t believe how easy it is to use, want to understand how easy it is to use compared to something like Airflow or MWAA in my case.\n\nI\u2019ve not heard of a situation when someone had used StepFunctions for Complex ETL, any thoughts ?", "author_fullname": "t2_1nawf7b9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow vs Step functions ??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131dxyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682645783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently encountered Step Functions and can\u2019t believe how easy it is to use, want to understand how easy it is to use compared to something like Airflow or MWAA in my case.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve not heard of a situation when someone had used StepFunctions for Complex ETL, any thoughts ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131dxyl", "is_robot_indexable": true, "report_reasons": null, "author": "Exciting-Garlic8360", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131dxyl/airflow_vs_step_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131dxyl/airflow_vs_step_functions/", "subreddit_subscribers": 103018, "created_utc": 1682645783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Note: i have posted in Germany subredit, but i want to take you opinion as data engineers if by any chance one of you already working in German. \n\n\nI have recently been offered a position as a data engineer in Berlin, and I am considering accepting it. However, I am not sure whether the salary that I have been offered is sufficient and whether it's worth it to move to Germany.\n\nAbout my current situation: I [27, m, single] work in a multinational company, with a great salary for a third world country but of course much less than what i have been offered. \n\nThe company has offered me a salary of 75k\u20ac per year, and they will also provide me with a Blue Card, which is a special type of work and residence permit for highly skilled non-EU citizens. With the Blue Card, I will have the right to work and live in Germany, as well as move freely within the EU.\n\nI have several questions about this job offer, and I would appreciate any advice or insights that you can provide:\n\n* Is the salary that I have been offered sufficient for a data engineering position in Berlin? What is the average salary for data engineers in Berlin?\n\n* What are the career opportunities for data engineers in Germany? Will I have the opportunity to grow my career and develop my skills further?\n\n* What is the cost of living like in Berlin? Will my salary be enough to cover my living expenses?\n\n* What is the work culture like in Berlin? Will I be able to adjust to the new work environment and lifestyle?\n\n* Are there any other factors that I should consider before making a decision?\n\nAny advice or insights that you can provide would be greatly appreciated. Thank you in advance for your help!", "author_fullname": "t2_a7urc8tl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Considering an offer as data engineer in Berlin", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131mjyj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682673731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Note: i have posted in Germany subredit, but i want to take you opinion as data engineers if by any chance one of you already working in German. &lt;/p&gt;\n\n&lt;p&gt;I have recently been offered a position as a data engineer in Berlin, and I am considering accepting it. However, I am not sure whether the salary that I have been offered is sufficient and whether it&amp;#39;s worth it to move to Germany.&lt;/p&gt;\n\n&lt;p&gt;About my current situation: I [27, m, single] work in a multinational company, with a great salary for a third world country but of course much less than what i have been offered. &lt;/p&gt;\n\n&lt;p&gt;The company has offered me a salary of 75k\u20ac per year, and they will also provide me with a Blue Card, which is a special type of work and residence permit for highly skilled non-EU citizens. With the Blue Card, I will have the right to work and live in Germany, as well as move freely within the EU.&lt;/p&gt;\n\n&lt;p&gt;I have several questions about this job offer, and I would appreciate any advice or insights that you can provide:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Is the salary that I have been offered sufficient for a data engineering position in Berlin? What is the average salary for data engineers in Berlin?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What are the career opportunities for data engineers in Germany? Will I have the opportunity to grow my career and develop my skills further?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What is the cost of living like in Berlin? Will my salary be enough to cover my living expenses?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What is the work culture like in Berlin? Will I be able to adjust to the new work environment and lifestyle?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are there any other factors that I should consider before making a decision?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice or insights that you can provide would be greatly appreciated. Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "131mjyj", "is_robot_indexable": true, "report_reasons": null, "author": "Taylankab", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131mjyj/considering_an_offer_as_data_engineer_in_berlin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131mjyj/considering_an_offer_as_data_engineer_in_berlin/", "subreddit_subscribers": 103018, "created_utc": 1682673731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For most (non-data) projects some level of testing happens but it's still in its infancy for our data engineering side of things. \n\nOur \"architecture\" is as follows: Bunch of microservices where Clojure streams into an object store. Pandas + SQLalchemy (legacy)  places a schema on everything (batch). Finally Polars + raw SQL gets it ready for our downstream tasks.\n\n&amp;#x200B;\n\n1. We validate a bit manually by skimming what comes in every so often (in terms of raw data) but this isn't tractable.\n2. When going from raw to schema and then to clean we *can* do assertions to check our data quality. For example, if object A that we're measuring had 250 events of type B after processing it should be the same. We kind of do this ad-hoc with group by's and joins but it would be cool if this is in some test suite somewhere. \n3. We have logs / notifications to tell us if a pipeline run failed. What we did not forsee is to add notifications if the pipeline came up empty handed (because upstream, out of organisation sources are failing).\n\nI think we want to monitor each source individually but also ensure that they're all \"complete\" in the sense that I'd like it to be visible if they're discrepancies in the events in source 1 vs source 2 vs source 3.\n\nAt the end of the day this is software and I see a bunch of pre-conditions, post-conditions and assertions I want to test. I've been resisting the temptation to reinvent a shitty version of the wheel by handrolling unit tests in Python that run raw-SQL / object store querys to validate that the data is parsed correctly. So far I've looked at great expectations and it seems to be close to what we need.", "author_fullname": "t2_8rjci796o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle tests, assertions and data quality management in your data pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131ar6j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682637385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For most (non-data) projects some level of testing happens but it&amp;#39;s still in its infancy for our data engineering side of things. &lt;/p&gt;\n\n&lt;p&gt;Our &amp;quot;architecture&amp;quot; is as follows: Bunch of microservices where Clojure streams into an object store. Pandas + SQLalchemy (legacy)  places a schema on everything (batch). Finally Polars + raw SQL gets it ready for our downstream tasks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We validate a bit manually by skimming what comes in every so often (in terms of raw data) but this isn&amp;#39;t tractable.&lt;/li&gt;\n&lt;li&gt;When going from raw to schema and then to clean we &lt;em&gt;can&lt;/em&gt; do assertions to check our data quality. For example, if object A that we&amp;#39;re measuring had 250 events of type B after processing it should be the same. We kind of do this ad-hoc with group by&amp;#39;s and joins but it would be cool if this is in some test suite somewhere. &lt;/li&gt;\n&lt;li&gt;We have logs / notifications to tell us if a pipeline run failed. What we did not forsee is to add notifications if the pipeline came up empty handed (because upstream, out of organisation sources are failing).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I think we want to monitor each source individually but also ensure that they&amp;#39;re all &amp;quot;complete&amp;quot; in the sense that I&amp;#39;d like it to be visible if they&amp;#39;re discrepancies in the events in source 1 vs source 2 vs source 3.&lt;/p&gt;\n\n&lt;p&gt;At the end of the day this is software and I see a bunch of pre-conditions, post-conditions and assertions I want to test. I&amp;#39;ve been resisting the temptation to reinvent a shitty version of the wheel by handrolling unit tests in Python that run raw-SQL / object store querys to validate that the data is parsed correctly. So far I&amp;#39;ve looked at great expectations and it seems to be close to what we need.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131ar6j", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-One8023", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131ar6j/how_do_you_handle_tests_assertions_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131ar6j/how_do_you_handle_tests_assertions_and_data/", "subreddit_subscribers": 103018, "created_utc": 1682637385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you extract data from primary source database in \"lakehouse\" setup? You dump it into CSV into the raw/bronze layer and then process it further?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Load primary source DB to WH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131qhuq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682685392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you extract data from primary source database in &amp;quot;lakehouse&amp;quot; setup? You dump it into CSV into the raw/bronze layer and then process it further?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131qhuq", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131qhuq/load_primary_source_db_to_wh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131qhuq/load_primary_source_db_to_wh/", "subreddit_subscribers": 103018, "created_utc": 1682685392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. \n\nOne of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.\n\nIn practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.\n\nWith that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?", "author_fullname": "t2_14aeem", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reporting for application using a CQRS event store", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1313uw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. &lt;/p&gt;\n\n&lt;p&gt;One of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.&lt;/p&gt;\n\n&lt;p&gt;In practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.&lt;/p&gt;\n\n&lt;p&gt;With that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313uw5", "is_robot_indexable": true, "report_reasons": null, "author": "raginjason", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "subreddit_subscribers": 103018, "created_utc": 1682624515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to get the group\u2019s experiences on what they\u2019ve done and recommend.\n\n My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.\n\nWe\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.\n\nWe\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. \n\nWhat\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.\n\n[View Poll](https://www.reddit.com/poll/1312r30)", "author_fullname": "t2_6nrc61lj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DW Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312r30", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682653672.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to get the group\u2019s experiences on what they\u2019ve done and recommend.&lt;/p&gt;\n\n&lt;p&gt;My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1312r30\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1312r30", "is_robot_indexable": true, "report_reasons": null, "author": "paulypavilion", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1682881813052, "options": [{"text": "Star schema", "id": "22770462"}, {"text": "Snowflake Schema", "id": "22770463"}, {"text": "Data Lake only", "id": "22770464"}, {"text": "Data lake +views", "id": "22770465"}, {"text": "Other", "id": "22770466"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 133, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312r30/dw_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1312r30/dw_architecture/", "subreddit_subscribers": 103018, "created_utc": 1682622613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically detecting breaking changes in SQL queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1310k7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682618763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tobikodata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1310k7b", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1310k7b/automatically_detecting_breaking_changes_in_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "subreddit_subscribers": 103018, "created_utc": 1682618763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I've heard bad things about Informatica, but I didn't even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?", "author_fullname": "t2_4rifsjav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IBM DB2 Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130tg7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682611175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I&amp;#39;ve heard bad things about Informatica, but I didn&amp;#39;t even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130tg7s", "is_robot_indexable": true, "report_reasons": null, "author": "Techthrowaway2222888", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "subreddit_subscribers": 103018, "created_utc": 1682611175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On my project would like to apply some data profiling/data quality technics. I would like to identify columns on my data frame that have mixed type of values, like date and strings\u2013 PipeRider would work here? And what I can use to declare a desired mask for specific type of values and transform it if is not what I expect? I do not ingest it from a API, so no pydantic here, i guess. What would fit better \u2013dbt, gx or pandera?\n\n&amp;#x200B;\n\nextra. Theres an article, github repo where I can found some good example how to apply to real world projects?", "author_fullname": "t2_za6q9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What library would work better for my project for Data Quality?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131gjsb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682653200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On my project would like to apply some data profiling/data quality technics. I would like to identify columns on my data frame that have mixed type of values, like date and strings\u2013 PipeRider would work here? And what I can use to declare a desired mask for specific type of values and transform it if is not what I expect? I do not ingest it from a API, so no pydantic here, i guess. What would fit better \u2013dbt, gx or pandera?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;extra. Theres an article, github repo where I can found some good example how to apply to real world projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131gjsb", "is_robot_indexable": true, "report_reasons": null, "author": "imloualvaro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131gjsb/what_library_would_work_better_for_my_project_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131gjsb/what_library_would_work_better_for_my_project_for/", "subreddit_subscribers": 103018, "created_utc": 1682653200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My past company uses Informatica for data catalog, but I\u2019m curious what popular tools are out there? How should I help my company to decide which data catalog to choose?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you choose which data catalog tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1319zta", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682635615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My past company uses Informatica for data catalog, but I\u2019m curious what popular tools are out there? How should I help my company to decide which data catalog to choose?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1319zta", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1319zta/how_do_you_choose_which_data_catalog_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1319zta/how_do_you_choose_which_data_catalog_tool/", "subreddit_subscribers": 103018, "created_utc": 1682635615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you guys store your audit logs and orchestration metadata in snowflake environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1313y1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313y1r", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "subreddit_subscribers": 103018, "created_utc": 1682624668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was given a test task that definitely wants to test my skills in data processing optimization. I have some `.csv` data and need to run several transformations and count statistics on it. \n\nIf I just do stuff with plain pandas on `.csv` files it will be ineffective. Any good tips? I have few ideas like converting `.csv` to parquet or `avro`. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   \n\n\nThe data itself is small and in `.csv` but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.", "author_fullname": "t2_lyf92k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on processing 10M+ records locally with python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312gxj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was given a test task that definitely wants to test my skills in data processing optimization. I have some &lt;code&gt;.csv&lt;/code&gt; data and need to run several transformations and count statistics on it. &lt;/p&gt;\n\n&lt;p&gt;If I just do stuff with plain pandas on &lt;code&gt;.csv&lt;/code&gt; files it will be ineffective. Any good tips? I have few ideas like converting &lt;code&gt;.csv&lt;/code&gt; to parquet or &lt;code&gt;avro&lt;/code&gt;. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   &lt;/p&gt;\n\n&lt;p&gt;The data itself is small and in &lt;code&gt;.csv&lt;/code&gt; but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1312gxj", "is_robot_indexable": true, "report_reasons": null, "author": "chelicerae-aureus", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "subreddit_subscribers": 103018, "created_utc": 1682622124.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I'm a little stuck on how exactly to do this. \n\nFor example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?\n\nHopefully this isn't too many questions all at once. I guess I'm just overwhelmed with the sheer number of options.", "author_fullname": "t2_thw4nqfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you organize your extract/load pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130qgti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682608241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I&amp;#39;m a little stuck on how exactly to do this. &lt;/p&gt;\n\n&lt;p&gt;For example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?&lt;/p&gt;\n\n&lt;p&gt;Hopefully this isn&amp;#39;t too many questions all at once. I guess I&amp;#39;m just overwhelmed with the sheer number of options.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130qgti", "is_robot_indexable": true, "report_reasons": null, "author": "BoofThatShit720", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "subreddit_subscribers": 103018, "created_utc": 1682608241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Every tool does in memory processing in one way or the other, i don't think any tool can function without computing stuffs in it's memory. \n\nLet's take Hadoop, Hadoop does in memory processing too then why is spark given so much of an attention due to its in memory processing? \n\nCan you give me a example how in memory processing in spark is different from tools", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Every tool does in memory processing, then why so much attention to Spak's in memory processing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131qmto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682685731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every tool does in memory processing in one way or the other, i don&amp;#39;t think any tool can function without computing stuffs in it&amp;#39;s memory. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s take Hadoop, Hadoop does in memory processing too then why is spark given so much of an attention due to its in memory processing? &lt;/p&gt;\n\n&lt;p&gt;Can you give me a example how in memory processing in spark is different from tools&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131qmto", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131qmto/every_tool_does_in_memory_processing_then_why_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131qmto/every_tool_does_in_memory_processing_then_why_so/", "subreddit_subscribers": 103018, "created_utc": 1682685731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9hqzxpiv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building an End-to-End ETL pipeline on Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_131pswt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YOirNd7oBdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Building an End-to-End ETL pipeline on Databricks\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"oembed": {"provider_url": "https://www.youtube.com/", "title": "Building an End-to-End ETL pipeline on Databricks", "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YOirNd7oBdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Building an End-to-End ETL pipeline on Databricks\"&gt;&lt;/iframe&gt;", "thumbnail_width": 480, "height": 200, "width": 356, "version": "1.0", "author_name": "BuildwithJay", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/YOirNd7oBdA/hqdefault.jpg", "type": "video", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@jay_reddy"}, "type": "youtube.com"}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YOirNd7oBdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Building an End-to-End ETL pipeline on Databricks\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/131pswt", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/hKEMgKLAi6pl4YuPSFlcgF8-Pq8k8zhWm7GLOj3QYo4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682683609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/YOirNd7oBdA", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/230vMI8n8Gz7kSolmEU5a6iIKFC6JzJUEzrgcLNqMSg.jpg?auto=webp&amp;v=enabled&amp;s=270855078f98585e2d0d4447f377f58fce6b64b4", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/230vMI8n8Gz7kSolmEU5a6iIKFC6JzJUEzrgcLNqMSg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=213ef0690c7d5b6774c21a5fb4124da7b8be2cda", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/230vMI8n8Gz7kSolmEU5a6iIKFC6JzJUEzrgcLNqMSg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1f777d2a323e01dd27ff3abaaab49de66cbcd8d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/230vMI8n8Gz7kSolmEU5a6iIKFC6JzJUEzrgcLNqMSg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5b1edb1946793b1815faed65197638e0a28dc42", "width": 320, "height": 240}], "variants": {}, "id": "FHOh8EODqi4Qrl3POiR47dAID77q8bJoFtYk5fwI1kk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "131pswt", "is_robot_indexable": true, "report_reasons": null, "author": "jay_reddy9", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131pswt/building_an_endtoend_etl_pipeline_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/YOirNd7oBdA", "subreddit_subscribers": 103018, "created_utc": 1682683609.0, "num_crossposts": 0, "media": {"oembed": {"provider_url": "https://www.youtube.com/", "title": "Building an End-to-End ETL pipeline on Databricks", "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YOirNd7oBdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Building an End-to-End ETL pipeline on Databricks\"&gt;&lt;/iframe&gt;", "thumbnail_width": 480, "height": 200, "width": 356, "version": "1.0", "author_name": "BuildwithJay", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/YOirNd7oBdA/hqdefault.jpg", "type": "video", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@jay_reddy"}, "type": "youtube.com"}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What is Z-Order?\n\nZ-Order is a method of sorting data to cluster data based on multiple fields equally. So instead of sorting by X and then Y, you sort by X and Y equally. This can be great when data is often searched based on X and Y.\n\nHow does this work?\n\nEssentially imagine all the data is sorted into four quadrants, something like this array:\n\n[\n  [], [],\n  [], []\n]\n\nWe sort the data as following:\n\n- If X is 1-50 and Y is 1-50 it will be sorted into the upper left quadrant (first sub-array)\n\n- If X is 51-100 and Y is 1-50 it will be sorted into the upper right quadrant (second sub-array)\n\n- If X is 1-50 and Y is 51-100 it will be sorted into the lower left quadrant (third sub-array)\n\n- If X is 51-100 and Y is 51-100 it will be sorted into the upper left quadrant (fourth sub-array)\n\nBy clustering the data in this manner if I search for data where X is 32 and Y is 58, I can search only the relevant subdivision of clustered data eliminating the other 3 subdivisions from my search saving me lots of time.", "author_fullname": "t2_lnwagoki", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is Z-Order?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_1310uyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/B9VzKMkx7d-pVMPXFPE2ZJwSzXAU_ty9fK_moYsRXSA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682619292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What is Z-Order?&lt;/p&gt;\n\n&lt;p&gt;Z-Order is a method of sorting data to cluster data based on multiple fields equally. So instead of sorting by X and then Y, you sort by X and Y equally. This can be great when data is often searched based on X and Y.&lt;/p&gt;\n\n&lt;p&gt;How does this work?&lt;/p&gt;\n\n&lt;p&gt;Essentially imagine all the data is sorted into four quadrants, something like this array:&lt;/p&gt;\n\n&lt;p&gt;[\n  [], [],\n  [], []\n]&lt;/p&gt;\n\n&lt;p&gt;We sort the data as following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;If X is 1-50 and Y is 1-50 it will be sorted into the upper left quadrant (first sub-array)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If X is 51-100 and Y is 1-50 it will be sorted into the upper right quadrant (second sub-array)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If X is 1-50 and Y is 51-100 it will be sorted into the lower left quadrant (third sub-array)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If X is 51-100 and Y is 51-100 it will be sorted into the upper left quadrant (fourth sub-array)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;By clustering the data in this manner if I search for data where X is 32 and Y is 58, I can search only the relevant subdivision of clustered data eliminating the other 3 subdivisions from my search saving me lots of time.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/r73mubi1fiwa1.jpg", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?auto=webp&amp;v=enabled&amp;s=36c4f2a4f1f8367d856ebfde2aa45c24c184a672", "width": 1280, "height": 720}, "resolutions": [{"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4022b6cca5be0aa17d58f0a99bf48b4e3c1f999a", "width": 108, "height": 60}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0b4e3b42cccb2bb995150c3b1fe6a787733ac6f0", "width": 216, "height": 121}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49b1e0781008c474b79ec58c7ac8baa0bfa3ac6f", "width": 320, "height": 180}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af092c6d3abbae936f02f4b9a9da99b607f0bb37", "width": 640, "height": 360}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cce0f5fa538fbc81b2f05ac83ab5d44008d1fcff", "width": 960, "height": 540}, {"url": "https://preview.redd.it/r73mubi1fiwa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0e705fabdded159b201ad80f7b2b504109eb0755", "width": 1080, "height": 607}], "variants": {}, "id": "uoNoQIEnawZLpQEzA5CKW_ItMzpDn8Ksz7iYEFE0VbI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1310uyl", "is_robot_indexable": true, "report_reasons": null, "author": "AMDataLake", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1310uyl/what_is_zorder/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/r73mubi1fiwa1.jpg", "subreddit_subscribers": 103018, "created_utc": 1682619292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Im a super junior data engineer and my boss wanted this specificly:\n\nOur pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.  \nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.\n\nOur dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.\n\nBoss wants the pipeline to succeed, even though the dataflow should fail.\n\nCan anyone share some insight into which would be the best way to handle this?", "author_fullname": "t2_xcbzsw9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best/easiest way to handle failed dataflows in a ADF pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130yccc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682616467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a super junior data engineer and my boss wanted this specificly:&lt;/p&gt;\n\n&lt;p&gt;Our pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.&lt;br/&gt;\nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.&lt;/p&gt;\n\n&lt;p&gt;Our dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.&lt;/p&gt;\n\n&lt;p&gt;Boss wants the pipeline to succeed, even though the dataflow should fail.&lt;/p&gt;\n\n&lt;p&gt;Can anyone share some insight into which would be the best way to handle this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130yccc", "is_robot_indexable": true, "report_reasons": null, "author": "useyourname89", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "subreddit_subscribers": 103018, "created_utc": 1682616467.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}