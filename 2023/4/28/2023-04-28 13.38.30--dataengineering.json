{"kind": "Listing", "data": {"after": "t3_130yccc", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My personal favorite... A man's health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.", "author_fullname": "t2_3xnau4cx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your favorite data quality horror story?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130rfc2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.99, "author_flair_background_color": null, "subreddit_type": "public", "ups": 203, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 203, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682609164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My personal favorite... A man&amp;#39;s health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130rfc2", "is_robot_indexable": true, "report_reasons": null, "author": "superconductiveKyle", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "subreddit_subscribers": 102999, "created_utc": 1682609164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, sorry if this is a stupid question. But I'd love to know why companies would pay for both DBT cloud and Fivetran now that Fivetran has built in DBT core with scheduling?\n\nIt seems like there's an ever growing number of data tools, each with their own expensive price tag. So I am trying to understand which tools are necessary and which aren't. Thanks!", "author_fullname": "t2_mytvjynu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why pay for DBT cloud when Fivetran has built in DBT Core?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1318f7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682632179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, sorry if this is a stupid question. But I&amp;#39;d love to know why companies would pay for both DBT cloud and Fivetran now that Fivetran has built in DBT core with scheduling?&lt;/p&gt;\n\n&lt;p&gt;It seems like there&amp;#39;s an ever growing number of data tools, each with their own expensive price tag. So I am trying to understand which tools are necessary and which aren&amp;#39;t. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1318f7s", "is_robot_indexable": true, "report_reasons": null, "author": "a-layerup", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1318f7s/why_pay_for_dbt_cloud_when_fivetran_has_built_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1318f7s/why_pay_for_dbt_cloud_when_fivetran_has_built_in/", "subreddit_subscribers": 102999, "created_utc": 1682632179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know a lot of you will say you can learn for free, but my employer will pay for it, and I\u2019d like to take advantage of that money.", "author_fullname": "t2_ulshlx1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any paid bootcamp or certification worth taking for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131bffg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682639069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a lot of you will say you can learn for free, but my employer will pay for it, and I\u2019d like to take advantage of that money.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "131bffg", "is_robot_indexable": true, "report_reasons": null, "author": "__academic__", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131bffg/is_there_any_paid_bootcamp_or_certification_worth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131bffg/is_there_any_paid_bootcamp_or_certification_worth/", "subreddit_subscribers": 102999, "created_utc": 1682639069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work in data professionally with a team who only knows SQL whereas I know less SQL than they do and a lot more Python. We're using Spark for data transformations and are currently trying to upsert to Delta Lake files from existing parquet EDIT using notebooks in Synapse. \n\nAs my team only knows SQL (as does the lead), the Lead thinks it'd be best if we only use SQL to minimise the amount of time required for everybody to get up and running.\n\nI'm currently reading in the parquet file using the syntax:\n\n`CREATE TABLE IF NOT EXISTS SourceTable USING Parquet LOCATION &lt;location&gt;`\n\nand add in a column\n\n`ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))`\n\nI now want to populate this column with values with `UPDATE SourceTable SET NewColumn = 'test'` just to check it works and I'm getting the error:\n\n`Error: UPDATE destination only supports Delta sources.`\n\nI get this and it's music to my ears because data manipulation in PySpark is infinitely easier for me (this is going to be dynamic so ease of string interpolation and variable injection is what I want).\n\nThis leads me to have a few questions:\n\n* Is it really impossible to `UPDATE` a table object created from a Parquet file using Spark SQL?\n\n* Under the hood, I get that in PySpark the parquet file is converted into a DataFrame which is an object in memory. As you can't this using Spark SQL, does this mean the parquet file doesn't get converted into memory and you are, in fact, hitting the file directly?\n\n* I'm trying to build a strong case where we use PySpark 90%+ of the time for these kinds of transformations and, of course, being unable to view changes in memory is a pretty big shitter.  Are there any other massive limitations of Spark SQL I should know about?\n\nThank you!\n\nEDIT\n\nMore detail.\n\nWe currently use the Data Lakehouse paradigm in Azure Synapse. We take parquet files (bronze) and then essentially upsert them into Delta files (silver) based off a key. This key is a hash of a column/set of columns.\n\nThis was previously achieved using Azure Data Flows although we're trying to move away from using these because they're a massive pain in the tits to maintain and shift into notebooks/code. The Data flow has the following pattern for the row hash:\n\n* Reads in the parquet file\n\n* Creates a new column with an expression which is essentially hashing a column/set of columns\n\n* Does an upsert against the delta file, checking if the row hash exists as the condition.\n\nThe issue:\n\n* In note books using Spark SQL, I can't recreate this pattern because when I add a new column, I can't populate it with values as I'm, effectively, trying to \"edit\" the parquet file.\n\n* This is confusing for me because one of the questions I have at the top is \"Why can I do this in a data frame, but not in a table created using Spark SQL syntax? Is it tables created from Parquets using Spark SQL aren't the same as dataframes?\".\n\n* I can do this all in Pyspark, but ideally, I'd like to use Spark SQL because my team can't do it in Python.\n\nEDIT 2:\n\nSample code comparison:\n\n    from pyspark.sql.functions import sha2\n    \n    source_df = spark.read.load(\"path/to/parquet/file.parquet\", format=\"parquet\")\n    source_df = source_df.withColumn(\"NewColumn\", sha2(source_df.ColumnToHash, 256))\n\nI can then do the upsert to the Delta file.\n\nMy Spark SQL code:\n\n    CREATE TABLE IF NOT EXISTS SourceTable USING Parquet\n    LOCATION \"path/to/parquet/file.parquet\"\n\n    ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))\n\nBut then I can't do anything to `NewColumn`.  Worth adding a minor correction - it's not a row hash, I'm trying to hash a specific column/set of columns.", "author_fullname": "t2_bqhp2bh8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Limitations of Spark SQL vs. PySpark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130nxgp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682611347.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682605793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work in data professionally with a team who only knows SQL whereas I know less SQL than they do and a lot more Python. We&amp;#39;re using Spark for data transformations and are currently trying to upsert to Delta Lake files from existing parquet EDIT using notebooks in Synapse. &lt;/p&gt;\n\n&lt;p&gt;As my team only knows SQL (as does the lead), the Lead thinks it&amp;#39;d be best if we only use SQL to minimise the amount of time required for everybody to get up and running.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently reading in the parquet file using the syntax:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;CREATE TABLE IF NOT EXISTS SourceTable USING Parquet LOCATION &amp;lt;location&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;and add in a column&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I now want to populate this column with values with &lt;code&gt;UPDATE SourceTable SET NewColumn = &amp;#39;test&amp;#39;&lt;/code&gt; just to check it works and I&amp;#39;m getting the error:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Error: UPDATE destination only supports Delta sources.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I get this and it&amp;#39;s music to my ears because data manipulation in PySpark is infinitely easier for me (this is going to be dynamic so ease of string interpolation and variable injection is what I want).&lt;/p&gt;\n\n&lt;p&gt;This leads me to have a few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Is it really impossible to &lt;code&gt;UPDATE&lt;/code&gt; a table object created from a Parquet file using Spark SQL?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Under the hood, I get that in PySpark the parquet file is converted into a DataFrame which is an object in memory. As you can&amp;#39;t this using Spark SQL, does this mean the parquet file doesn&amp;#39;t get converted into memory and you are, in fact, hitting the file directly?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I&amp;#39;m trying to build a strong case where we use PySpark 90%+ of the time for these kinds of transformations and, of course, being unable to view changes in memory is a pretty big shitter.  Are there any other massive limitations of Spark SQL I should know about?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;EDIT&lt;/p&gt;\n\n&lt;p&gt;More detail.&lt;/p&gt;\n\n&lt;p&gt;We currently use the Data Lakehouse paradigm in Azure Synapse. We take parquet files (bronze) and then essentially upsert them into Delta files (silver) based off a key. This key is a hash of a column/set of columns.&lt;/p&gt;\n\n&lt;p&gt;This was previously achieved using Azure Data Flows although we&amp;#39;re trying to move away from using these because they&amp;#39;re a massive pain in the tits to maintain and shift into notebooks/code. The Data flow has the following pattern for the row hash:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Reads in the parquet file&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Creates a new column with an expression which is essentially hashing a column/set of columns&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Does an upsert against the delta file, checking if the row hash exists as the condition.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The issue:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;In note books using Spark SQL, I can&amp;#39;t recreate this pattern because when I add a new column, I can&amp;#39;t populate it with values as I&amp;#39;m, effectively, trying to &amp;quot;edit&amp;quot; the parquet file.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This is confusing for me because one of the questions I have at the top is &amp;quot;Why can I do this in a data frame, but not in a table created using Spark SQL syntax? Is it tables created from Parquets using Spark SQL aren&amp;#39;t the same as dataframes?&amp;quot;.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I can do this all in Pyspark, but ideally, I&amp;#39;d like to use Spark SQL because my team can&amp;#39;t do it in Python.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;EDIT 2:&lt;/p&gt;\n\n&lt;p&gt;Sample code comparison:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from pyspark.sql.functions import sha2\n\nsource_df = spark.read.load(&amp;quot;path/to/parquet/file.parquet&amp;quot;, format=&amp;quot;parquet&amp;quot;)\nsource_df = source_df.withColumn(&amp;quot;NewColumn&amp;quot;, sha2(source_df.ColumnToHash, 256))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I can then do the upsert to the Delta file.&lt;/p&gt;\n\n&lt;p&gt;My Spark SQL code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE IF NOT EXISTS SourceTable USING Parquet\nLOCATION &amp;quot;path/to/parquet/file.parquet&amp;quot;\n\nALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But then I can&amp;#39;t do anything to &lt;code&gt;NewColumn&lt;/code&gt;.  Worth adding a minor correction - it&amp;#39;s not a row hash, I&amp;#39;m trying to hash a specific column/set of columns.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130nxgp", "is_robot_indexable": true, "report_reasons": null, "author": "standard_throw", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130nxgp/limitations_of_spark_sql_vs_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130nxgp/limitations_of_spark_sql_vs_pyspark/", "subreddit_subscribers": 102999, "created_utc": 1682605793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I didn't think I would get the position. All I have is a technical degree in data analysis, a couple of years of experience in the same area, and some certs. Extremely happy.\n\nWondering if I should get a degree, or if I can continue to grow with just experience and certifications.", "author_fullname": "t2_8kf3kc84t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No bachelor degree, got offered a data engineer position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131fdyu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682649789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I didn&amp;#39;t think I would get the position. All I have is a technical degree in data analysis, a couple of years of experience in the same area, and some certs. Extremely happy.&lt;/p&gt;\n\n&lt;p&gt;Wondering if I should get a degree, or if I can continue to grow with just experience and certifications.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "131fdyu", "is_robot_indexable": true, "report_reasons": null, "author": "Pinkypie_15", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131fdyu/no_bachelor_degree_got_offered_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131fdyu/no_bachelor_degree_got_offered_a_data_engineer/", "subreddit_subscribers": 102999, "created_utc": 1682649789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently encountered Step Functions and can\u2019t believe how easy it is to use, want to understand how easy it is to use compared to something like Airflow or MWAA in my case.\n\nI\u2019ve not heard of a situation when someone had used StepFunctions for Complex ETL, any thoughts ?", "author_fullname": "t2_1nawf7b9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow vs Step functions ??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131dxyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682645783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently encountered Step Functions and can\u2019t believe how easy it is to use, want to understand how easy it is to use compared to something like Airflow or MWAA in my case.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve not heard of a situation when someone had used StepFunctions for Complex ETL, any thoughts ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131dxyl", "is_robot_indexable": true, "report_reasons": null, "author": "Exciting-Garlic8360", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131dxyl/airflow_vs_step_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131dxyl/airflow_vs_step_functions/", "subreddit_subscribers": 102999, "created_utc": 1682645783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_dzfsy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DBT core v1.5 released", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_131okrw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/bLH7aUGThmTtYxWuUgXrXurKo3TMCVwULoPk57XDgVs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682680127.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "github.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://github.com/dbt-labs/dbt-core/releases/tag/v1.5.0", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?auto=webp&amp;v=enabled&amp;s=51c20d5fe12d6c5c2e06e42b504b68de4a2b8175", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e8d1cf3943cc053c060e9c1d46c48cb0b24f8d4", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b3a3aa073ca5c7f038cc01f5d085736e9b2aca5", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b9720e0cd577e8d05e12bedaf0fa8ee68d109e6", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f623ac4d0b73d217141fc13af2b1a2751e69bce8", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8dbad338b85e1daa4a3927ab85f7f1939c5fa839", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/q1RvEoBCWeIxOkaMd8WeLDrxDJNsPnIU2ggVRw2QGTk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=97fa8149e133339c0f836077ef3ac8665e5dad6c", "width": 1080, "height": 540}], "variants": {}, "id": "tQphSKEWxPuFtXmFU_-tHL65XBnxhvlVI4_nmg5KSew"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "131okrw", "is_robot_indexable": true, "report_reasons": null, "author": "Mr_Again", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131okrw/dbt_core_v15_released/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://github.com/dbt-labs/dbt-core/releases/tag/v1.5.0", "subreddit_subscribers": 102999, "created_utc": 1682680127.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Note: i have posted in Germany subredit, but i want to take you opinion as data engineers if by any chance one of you already working in German. \n\n\nI have recently been offered a position as a data engineer in Berlin, and I am considering accepting it. However, I am not sure whether the salary that I have been offered is sufficient and whether it's worth it to move to Germany.\n\nAbout my current situation: I [27, m, single] work in a multinational company, with a great salary for a third world country but of course much less than what i have been offered. \n\nThe company has offered me a salary of 75k\u20ac per year, and they will also provide me with a Blue Card, which is a special type of work and residence permit for highly skilled non-EU citizens. With the Blue Card, I will have the right to work and live in Germany, as well as move freely within the EU.\n\nI have several questions about this job offer, and I would appreciate any advice or insights that you can provide:\n\n* Is the salary that I have been offered sufficient for a data engineering position in Berlin? What is the average salary for data engineers in Berlin?\n\n* What are the career opportunities for data engineers in Germany? Will I have the opportunity to grow my career and develop my skills further?\n\n* What is the cost of living like in Berlin? Will my salary be enough to cover my living expenses?\n\n* What is the work culture like in Berlin? Will I be able to adjust to the new work environment and lifestyle?\n\n* Are there any other factors that I should consider before making a decision?\n\nAny advice or insights that you can provide would be greatly appreciated. Thank you in advance for your help!", "author_fullname": "t2_a7urc8tl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Considering an offer as data engineer in Berlin", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131mjyj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682673731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Note: i have posted in Germany subredit, but i want to take you opinion as data engineers if by any chance one of you already working in German. &lt;/p&gt;\n\n&lt;p&gt;I have recently been offered a position as a data engineer in Berlin, and I am considering accepting it. However, I am not sure whether the salary that I have been offered is sufficient and whether it&amp;#39;s worth it to move to Germany.&lt;/p&gt;\n\n&lt;p&gt;About my current situation: I [27, m, single] work in a multinational company, with a great salary for a third world country but of course much less than what i have been offered. &lt;/p&gt;\n\n&lt;p&gt;The company has offered me a salary of 75k\u20ac per year, and they will also provide me with a Blue Card, which is a special type of work and residence permit for highly skilled non-EU citizens. With the Blue Card, I will have the right to work and live in Germany, as well as move freely within the EU.&lt;/p&gt;\n\n&lt;p&gt;I have several questions about this job offer, and I would appreciate any advice or insights that you can provide:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Is the salary that I have been offered sufficient for a data engineering position in Berlin? What is the average salary for data engineers in Berlin?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What are the career opportunities for data engineers in Germany? Will I have the opportunity to grow my career and develop my skills further?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What is the cost of living like in Berlin? Will my salary be enough to cover my living expenses?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What is the work culture like in Berlin? Will I be able to adjust to the new work environment and lifestyle?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are there any other factors that I should consider before making a decision?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any advice or insights that you can provide would be greatly appreciated. Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "131mjyj", "is_robot_indexable": true, "report_reasons": null, "author": "Taylankab", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131mjyj/considering_an_offer_as_data_engineer_in_berlin/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131mjyj/considering_an_offer_as_data_engineer_in_berlin/", "subreddit_subscribers": 102999, "created_utc": 1682673731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As part of a hubby project I am working on, I need to be able to take SQL strings supplied by users, and then run it.\n\nBut before running it, i will like to do some manipulations to the SQL. For example, I may want to have a copy of the SQL that does a COUNT query version of the original query...I might want to insert LIMIT clauses if not provided, or I might want to manipulate the columns specified, rename or even insert columns.\n\nNow the question is: I am not sure how to go able this and what sort of tools to even look for. I think in essence I want to be able to manipulate the SQL not as a string but as a data structure I can manipulate\n\nUsing regex and string manipulation does not sound like the right tool for the job. What will you say is the right way to go about this?", "author_fullname": "t2_x8zrrj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manipulate SQL string programmatically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131j6ee", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682661573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As part of a hubby project I am working on, I need to be able to take SQL strings supplied by users, and then run it.&lt;/p&gt;\n\n&lt;p&gt;But before running it, i will like to do some manipulations to the SQL. For example, I may want to have a copy of the SQL that does a COUNT query version of the original query...I might want to insert LIMIT clauses if not provided, or I might want to manipulate the columns specified, rename or even insert columns.&lt;/p&gt;\n\n&lt;p&gt;Now the question is: I am not sure how to go able this and what sort of tools to even look for. I think in essence I want to be able to manipulate the SQL not as a string but as a data structure I can manipulate&lt;/p&gt;\n\n&lt;p&gt;Using regex and string manipulation does not sound like the right tool for the job. What will you say is the right way to go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131j6ee", "is_robot_indexable": true, "report_reasons": null, "author": "io_geekabyte", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131j6ee/how_to_manipulate_sql_string_programmatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131j6ee/how_to_manipulate_sql_string_programmatically/", "subreddit_subscribers": 102999, "created_utc": 1682661573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. \n\nDoes anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. \n\nAddl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.", "author_fullname": "t2_36yd2hgi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your company handle replication and CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130ls2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682601191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. &lt;/p&gt;\n\n&lt;p&gt;Addl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130ls2m", "is_robot_indexable": true, "report_reasons": null, "author": "PROTECTyaNECK44", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "subreddit_subscribers": 102999, "created_utc": 1682601191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For most (non-data) projects some level of testing happens but it's still in its infancy for our data engineering side of things. \n\nOur \"architecture\" is as follows: Bunch of microservices where Clojure streams into an object store. Pandas + SQLalchemy (legacy)  places a schema on everything (batch). Finally Polars + raw SQL gets it ready for our downstream tasks.\n\n&amp;#x200B;\n\n1. We validate a bit manually by skimming what comes in every so often (in terms of raw data) but this isn't tractable.\n2. When going from raw to schema and then to clean we *can* do assertions to check our data quality. For example, if object A that we're measuring had 250 events of type B after processing it should be the same. We kind of do this ad-hoc with group by's and joins but it would be cool if this is in some test suite somewhere. \n3. We have logs / notifications to tell us if a pipeline run failed. What we did not forsee is to add notifications if the pipeline came up empty handed (because upstream, out of organisation sources are failing).\n\nI think we want to monitor each source individually but also ensure that they're all \"complete\" in the sense that I'd like it to be visible if they're discrepancies in the events in source 1 vs source 2 vs source 3.\n\nAt the end of the day this is software and I see a bunch of pre-conditions, post-conditions and assertions I want to test. I've been resisting the temptation to reinvent a shitty version of the wheel by handrolling unit tests in Python that run raw-SQL / object store querys to validate that the data is parsed correctly. So far I've looked at great expectations and it seems to be close to what we need.", "author_fullname": "t2_8rjci796o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle tests, assertions and data quality management in your data pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131ar6j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682637385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For most (non-data) projects some level of testing happens but it&amp;#39;s still in its infancy for our data engineering side of things. &lt;/p&gt;\n\n&lt;p&gt;Our &amp;quot;architecture&amp;quot; is as follows: Bunch of microservices where Clojure streams into an object store. Pandas + SQLalchemy (legacy)  places a schema on everything (batch). Finally Polars + raw SQL gets it ready for our downstream tasks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We validate a bit manually by skimming what comes in every so often (in terms of raw data) but this isn&amp;#39;t tractable.&lt;/li&gt;\n&lt;li&gt;When going from raw to schema and then to clean we &lt;em&gt;can&lt;/em&gt; do assertions to check our data quality. For example, if object A that we&amp;#39;re measuring had 250 events of type B after processing it should be the same. We kind of do this ad-hoc with group by&amp;#39;s and joins but it would be cool if this is in some test suite somewhere. &lt;/li&gt;\n&lt;li&gt;We have logs / notifications to tell us if a pipeline run failed. What we did not forsee is to add notifications if the pipeline came up empty handed (because upstream, out of organisation sources are failing).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I think we want to monitor each source individually but also ensure that they&amp;#39;re all &amp;quot;complete&amp;quot; in the sense that I&amp;#39;d like it to be visible if they&amp;#39;re discrepancies in the events in source 1 vs source 2 vs source 3.&lt;/p&gt;\n\n&lt;p&gt;At the end of the day this is software and I see a bunch of pre-conditions, post-conditions and assertions I want to test. I&amp;#39;ve been resisting the temptation to reinvent a shitty version of the wheel by handrolling unit tests in Python that run raw-SQL / object store querys to validate that the data is parsed correctly. So far I&amp;#39;ve looked at great expectations and it seems to be close to what we need.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131ar6j", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-One8023", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131ar6j/how_do_you_handle_tests_assertions_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131ar6j/how_do_you_handle_tests_assertions_and_data/", "subreddit_subscribers": 102999, "created_utc": 1682637385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. \n\nOne of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.\n\nIn practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.\n\nWith that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?", "author_fullname": "t2_14aeem", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reporting for application using a CQRS event store", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1313uw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. &lt;/p&gt;\n\n&lt;p&gt;One of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.&lt;/p&gt;\n\n&lt;p&gt;In practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.&lt;/p&gt;\n\n&lt;p&gt;With that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313uw5", "is_robot_indexable": true, "report_reasons": null, "author": "raginjason", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "subreddit_subscribers": 102999, "created_utc": 1682624515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically detecting breaking changes in SQL queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1310k7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682618763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tobikodata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1310k7b", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1310k7b/automatically_detecting_breaking_changes_in_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "subreddit_subscribers": 102999, "created_utc": 1682618763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I've heard bad things about Informatica, but I didn't even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?", "author_fullname": "t2_4rifsjav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IBM DB2 Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130tg7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682611175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I&amp;#39;ve heard bad things about Informatica, but I didn&amp;#39;t even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130tg7s", "is_robot_indexable": true, "report_reasons": null, "author": "Techthrowaway2222888", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "subreddit_subscribers": 102999, "created_utc": 1682611175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm pretty comfortable with Terraform, but not to the same extent as an engineer who works with it every day.  I've worked at bigger companies with massive Terraform code bases that always fascinated me about how they were built over time.\n\n\n\n\n\nHow would building data infrastructure work if you were one of the first few data engineers at a startup?  Would there be pressure to deliver results immediately and would you just quickly manually provision resources on the cloud so you can get things up and running, or do everything in Terraform first?  Would your priorities be just setting up all the data infrastructure first long before you start building data pipelines and reporting?", "author_fullname": "t2_auf0obxj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does data infrastructure/infrastructure as code at a startup evolve over time?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131oyl3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682681472.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682681269.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m pretty comfortable with Terraform, but not to the same extent as an engineer who works with it every day.  I&amp;#39;ve worked at bigger companies with massive Terraform code bases that always fascinated me about how they were built over time.&lt;/p&gt;\n\n&lt;p&gt;How would building data infrastructure work if you were one of the first few data engineers at a startup?  Would there be pressure to deliver results immediately and would you just quickly manually provision resources on the cloud so you can get things up and running, or do everything in Terraform first?  Would your priorities be just setting up all the data infrastructure first long before you start building data pipelines and reporting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131oyl3", "is_robot_indexable": true, "report_reasons": null, "author": "level_126_programmer", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131oyl3/how_does_data_infrastructureinfrastructure_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131oyl3/how_does_data_infrastructureinfrastructure_as/", "subreddit_subscribers": 102999, "created_utc": 1682681269.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On my project would like to apply some data profiling/data quality technics. I would like to identify columns on my data frame that have mixed type of values, like date and strings\u2013 PipeRider would work here? And what I can use to declare a desired mask for specific type of values and transform it if is not what I expect? I do not ingest it from a API, so no pydantic here, i guess. What would fit better \u2013dbt, gx or pandera?\n\n&amp;#x200B;\n\nextra. Theres an article, github repo where I can found some good example how to apply to real world projects?", "author_fullname": "t2_za6q9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What library would work better for my project for Data Quality?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131gjsb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682653200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On my project would like to apply some data profiling/data quality technics. I would like to identify columns on my data frame that have mixed type of values, like date and strings\u2013 PipeRider would work here? And what I can use to declare a desired mask for specific type of values and transform it if is not what I expect? I do not ingest it from a API, so no pydantic here, i guess. What would fit better \u2013dbt, gx or pandera?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;extra. Theres an article, github repo where I can found some good example how to apply to real world projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131gjsb", "is_robot_indexable": true, "report_reasons": null, "author": "imloualvaro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131gjsb/what_library_would_work_better_for_my_project_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131gjsb/what_library_would_work_better_for_my_project_for/", "subreddit_subscribers": 102999, "created_utc": 1682653200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My past company uses Informatica for data catalog, but I\u2019m curious what popular tools are out there? How should I help my company to decide which data catalog to choose?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you choose which data catalog tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1319zta", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682635615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My past company uses Informatica for data catalog, but I\u2019m curious what popular tools are out there? How should I help my company to decide which data catalog to choose?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1319zta", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1319zta/how_do_you_choose_which_data_catalog_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1319zta/how_do_you_choose_which_data_catalog_tool/", "subreddit_subscribers": 102999, "created_utc": 1682635615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you guys store your audit logs and orchestration metadata in snowflake environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1313y1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313y1r", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "subreddit_subscribers": 102999, "created_utc": 1682624668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to get the group\u2019s experiences on what they\u2019ve done and recommend.\n\n My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.\n\nWe\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.\n\nWe\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. \n\nWhat\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.\n\n[View Poll](https://www.reddit.com/poll/1312r30)", "author_fullname": "t2_6nrc61lj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DW Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312r30", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682653672.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to get the group\u2019s experiences on what they\u2019ve done and recommend.&lt;/p&gt;\n\n&lt;p&gt;My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1312r30\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1312r30", "is_robot_indexable": true, "report_reasons": null, "author": "paulypavilion", "discussion_type": null, "num_comments": 21, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1682881813052, "options": [{"text": "Star schema", "id": "22770462"}, {"text": "Snowflake Schema", "id": "22770463"}, {"text": "Data Lake only", "id": "22770464"}, {"text": "Data lake +views", "id": "22770465"}, {"text": "Other", "id": "22770466"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 127, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312r30/dw_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1312r30/dw_architecture/", "subreddit_subscribers": 102999, "created_utc": 1682622613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was given a test task that definitely wants to test my skills in data processing optimization. I have some `.csv` data and need to run several transformations and count statistics on it. \n\nIf I just do stuff with plain pandas on `.csv` files it will be ineffective. Any good tips? I have few ideas like converting `.csv` to parquet or `avro`. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   \n\n\nThe data itself is small and in `.csv` but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.", "author_fullname": "t2_lyf92k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on processing 10M+ records locally with python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312gxj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was given a test task that definitely wants to test my skills in data processing optimization. I have some &lt;code&gt;.csv&lt;/code&gt; data and need to run several transformations and count statistics on it. &lt;/p&gt;\n\n&lt;p&gt;If I just do stuff with plain pandas on &lt;code&gt;.csv&lt;/code&gt; files it will be ineffective. Any good tips? I have few ideas like converting &lt;code&gt;.csv&lt;/code&gt; to parquet or &lt;code&gt;avro&lt;/code&gt;. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   &lt;/p&gt;\n\n&lt;p&gt;The data itself is small and in &lt;code&gt;.csv&lt;/code&gt; but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1312gxj", "is_robot_indexable": true, "report_reasons": null, "author": "chelicerae-aureus", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "subreddit_subscribers": 102999, "created_utc": 1682622124.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I'm a little stuck on how exactly to do this. \n\nFor example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?\n\nHopefully this isn't too many questions all at once. I guess I'm just overwhelmed with the sheer number of options.", "author_fullname": "t2_thw4nqfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you organize your extract/load pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130qgti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682608241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I&amp;#39;m a little stuck on how exactly to do this. &lt;/p&gt;\n\n&lt;p&gt;For example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?&lt;/p&gt;\n\n&lt;p&gt;Hopefully this isn&amp;#39;t too many questions all at once. I guess I&amp;#39;m just overwhelmed with the sheer number of options.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130qgti", "is_robot_indexable": true, "report_reasons": null, "author": "BoofThatShit720", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "subreddit_subscribers": 102999, "created_utc": 1682608241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Every tool does in memory processing in one way or the other, i don't think any tool can function without computing stuffs in it's memory. \n\nLet's take Hadoop, Hadoop does in memory processing too then why is spark given so much of an attention due to its in memory processing? \n\nCan you give me a example how in memory processing in spark is different from tools", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Every tool does in memory processing, then why so much attention to Spak's in memory processing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_131qmto", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682685731.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every tool does in memory processing in one way or the other, i don&amp;#39;t think any tool can function without computing stuffs in it&amp;#39;s memory. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s take Hadoop, Hadoop does in memory processing too then why is spark given so much of an attention due to its in memory processing? &lt;/p&gt;\n\n&lt;p&gt;Can you give me a example how in memory processing in spark is different from tools&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131qmto", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131qmto/every_tool_does_in_memory_processing_then_why_so/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131qmto/every_tool_does_in_memory_processing_then_why_so/", "subreddit_subscribers": 102999, "created_utc": 1682685731.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How do you extract data from primary source database in \"lakehouse\" setup? You dump it into CSV into the raw/bronze layer and then process it further?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Load primary source DB to WH", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_131qhuq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682685392.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How do you extract data from primary source database in &amp;quot;lakehouse&amp;quot; setup? You dump it into CSV into the raw/bronze layer and then process it further?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131qhuq", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131qhuq/load_primary_source_db_to_wh/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131qhuq/load_primary_source_db_to_wh/", "subreddit_subscribers": 102999, "created_utc": 1682685392.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_9hqzxpiv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Building an End-to-End ETL pipeline on Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": true, "name": "t3_131pswt", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YOirNd7oBdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Building an End-to-End ETL pipeline on Databricks\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Building an End-to-End ETL pipeline on Databricks", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YOirNd7oBdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Building an End-to-End ETL pipeline on Databricks\"&gt;&lt;/iframe&gt;", "author_name": "BuildwithJay", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/YOirNd7oBdA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@jay_reddy"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YOirNd7oBdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Building an End-to-End ETL pipeline on Databricks\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/131pswt", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/hKEMgKLAi6pl4YuPSFlcgF8-Pq8k8zhWm7GLOj3QYo4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682683609.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/YOirNd7oBdA", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/230vMI8n8Gz7kSolmEU5a6iIKFC6JzJUEzrgcLNqMSg.jpg?auto=webp&amp;v=enabled&amp;s=270855078f98585e2d0d4447f377f58fce6b64b4", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/230vMI8n8Gz7kSolmEU5a6iIKFC6JzJUEzrgcLNqMSg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=213ef0690c7d5b6774c21a5fb4124da7b8be2cda", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/230vMI8n8Gz7kSolmEU5a6iIKFC6JzJUEzrgcLNqMSg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1f777d2a323e01dd27ff3abaaab49de66cbcd8d", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/230vMI8n8Gz7kSolmEU5a6iIKFC6JzJUEzrgcLNqMSg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a5b1edb1946793b1815faed65197638e0a28dc42", "width": 320, "height": 240}], "variants": {}, "id": "FHOh8EODqi4Qrl3POiR47dAID77q8bJoFtYk5fwI1kk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "131pswt", "is_robot_indexable": true, "report_reasons": null, "author": "jay_reddy9", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131pswt/building_an_endtoend_etl_pipeline_on_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/YOirNd7oBdA", "subreddit_subscribers": 102999, "created_utc": 1682683609.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Building an End-to-End ETL pipeline on Databricks", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/YOirNd7oBdA?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Building an End-to-End ETL pipeline on Databricks\"&gt;&lt;/iframe&gt;", "author_name": "BuildwithJay", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/YOirNd7oBdA/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@jay_reddy"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Im a super junior data engineer and my boss wanted this specificly:\n\nOur pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.  \nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.\n\nOur dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.\n\nBoss wants the pipeline to succeed, even though the dataflow should fail.\n\nCan anyone share some insight into which would be the best way to handle this?", "author_fullname": "t2_xcbzsw9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best/easiest way to handle failed dataflows in a ADF pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130yccc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682616467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a super junior data engineer and my boss wanted this specificly:&lt;/p&gt;\n\n&lt;p&gt;Our pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.&lt;br/&gt;\nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.&lt;/p&gt;\n\n&lt;p&gt;Our dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.&lt;/p&gt;\n\n&lt;p&gt;Boss wants the pipeline to succeed, even though the dataflow should fail.&lt;/p&gt;\n\n&lt;p&gt;Can anyone share some insight into which would be the best way to handle this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130yccc", "is_robot_indexable": true, "report_reasons": null, "author": "useyourname89", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "subreddit_subscribers": 102999, "created_utc": 1682616467.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}