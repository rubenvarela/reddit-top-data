{"kind": "Listing", "data": {"after": "t3_131ew3n", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_1nj2uu8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The community group Forest of Illusion, dedicated to preserving Nintendo's history, has announced the end of their work", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 96, "top_awarded_type": null, "hide_score": false, "name": "t3_131omwm", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 503, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 503, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/1xLkdU9LjrOdJ0JVcivUCmg3gFoiQYIkLR4ZnJQp0Z4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682680301.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "archive.org", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://archive.org/details/readme_20230428", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/YiQRc1Kh_LhZKnW0P8o13a-1VrZudI26GKsmOhr6Glw.jpg?auto=webp&amp;v=enabled&amp;s=6a11bfe99205212e71e4602d351ddac52a48141f", "width": 160, "height": 110}, "resolutions": [{"url": "https://external-preview.redd.it/YiQRc1Kh_LhZKnW0P8o13a-1VrZudI26GKsmOhr6Glw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b80e296b21f03abd06e1d1dd0f93880a18650230", "width": 108, "height": 74}], "variants": {}, "id": "te8dS_8tDwV7a4I5wDwIv-RCBsf1rpL-BYZPIq0-t7k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131omwm", "is_robot_indexable": true, "report_reasons": null, "author": "bmacabeus", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131omwm/the_community_group_forest_of_illusion_dedicated/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://archive.org/details/readme_20230428", "subreddit_subscribers": 680129, "created_utc": 1682680301.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Has anyone saved this master piece?", "author_fullname": "t2_51p8x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Farscape 4K Remaster AI was recently uploaded to youtube. DMCA shut it down.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131f4mp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 189, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 189, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682649054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone saved this master piece?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "20c598d2-b3f5-11ea-8f7a-0e7cd54fad9b", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#dadada", "id": "131f4mp", "is_robot_indexable": true, "report_reasons": null, "author": "fmjk45a", "discussion_type": null, "num_comments": 133, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131f4mp/farscape_4k_remaster_ai_was_recently_uploaded_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131f4mp/farscape_4k_remaster_ai_was_recently_uploaded_to/", "subreddit_subscribers": 680129, "created_utc": 1682649054.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_qwl19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Always 1 quarter away", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 75, "top_awarded_type": null, "hide_score": false, "name": "t3_1325e06", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.94, "author_flair_background_color": null, "ups": 112, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 112, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/WPAdM3H_QFlb6OybLJBRd9brhtqaORU6YJL5t3R56Hk.jpg", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682705666.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/iq3d1ag62owa1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?auto=webp&amp;v=enabled&amp;s=e41a777c91e4a811c9730b18c16d42e67237845f", "width": 679, "height": 367}, "resolutions": [{"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca9c461fb7b7b326eb3f74c73d7183c013a238e2", "width": 108, "height": 58}, {"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=42faf4d41eb14cc851d1ccda0214a6f8f484f53e", "width": 216, "height": 116}, {"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e6792352e7612eb9762debfb8578a2d893cf195", "width": 320, "height": 172}, {"url": "https://preview.redd.it/iq3d1ag62owa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b86691a85e11a11f703e1209fd25d75bccd0d849", "width": 640, "height": 345}], "variants": {}, "id": "LA3HM47MTukpVPigpb3QnEmUY5k8DuEtAayBhsEcsHc"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "18TB", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1325e06", "is_robot_indexable": true, "report_reasons": null, "author": "ZGorlock", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/1325e06/always_1_quarter_away/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/iq3d1ag62owa1.jpg", "subreddit_subscribers": 680129, "created_utc": 1682705666.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Since the first release (in December 2021), SCrawler has been expanding and improving. I have implemented many of the user requests. I want to say thank you to everyone who uses my program, who likes it and who finds it useful. I really appreciate your kind words when you DM me. It makes my day :-)\n\nThe new release contains new sites: **YouTube, YouTube music, Mastodon, Pinterest** and **ThisVid**. It also contains many improvements and bug fixes.\n\n[https://github.com/AAndyProgram/SCrawler](https://github.com/AAndyProgram/SCrawler)\n\nThe program is completely free. I hope you will like it ;-)", "author_fullname": "t2_bfhlgnl4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SCrawler. Reddit, Twitter, Instagram, YouTube and any other sites downloader. New grand update.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131m8h3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "265b199a-b98c-11e2-8300-12313b0b21ae", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "cloud", "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682672597.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since the first release (in December 2021), SCrawler has been expanding and improving. I have implemented many of the user requests. I want to say thank you to everyone who uses my program, who likes it and who finds it useful. I really appreciate your kind words when you DM me. It makes my day :-)&lt;/p&gt;\n\n&lt;p&gt;The new release contains new sites: &lt;strong&gt;YouTube, YouTube music, Mastodon, Pinterest&lt;/strong&gt; and &lt;strong&gt;ThisVid&lt;/strong&gt;. It also contains many improvements and bug fixes.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/AAndyProgram/SCrawler\"&gt;https://github.com/AAndyProgram/SCrawler&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The program is completely free. I hope you will like it ;-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?auto=webp&amp;v=enabled&amp;s=7762203f5e277671722427ba8b8059db62a80ca6", "width": 1283, "height": 640}, "resolutions": [{"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfd65c9e04800e8b1b9def6d61ba13455cf7836a", "width": 108, "height": 53}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aecc109537c4353bb2aa96edfcd99aabea23614b", "width": 216, "height": 107}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2de3fc7a72ad3e6c9e70b9aa1ad88d5fdd02a213", "width": 320, "height": 159}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92ebb7d1e8d6b78d61545c05ce2cd18cca004443", "width": 640, "height": 319}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55c3b20e58f54a3a864249853303c1c5f62bd4e5", "width": 960, "height": 478}, {"url": "https://external-preview.redd.it/94HJHI8TvGTgmz8wWQbg_a-Nai34jS3HNnEMp9D0z74.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4da6f9f905c18158e2ccbf373eda80877ad17760", "width": 1080, "height": 538}], "variants": {}, "id": "_pHtIQKoFHtVPlwEO3-8R5Db1nV-A5okGhAwJKRuSSk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "32TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131m8h3", "is_robot_indexable": true, "report_reasons": null, "author": "AndyGay06", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/131m8h3/scrawler_reddit_twitter_instagram_youtube_and_any/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131m8h3/scrawler_reddit_twitter_instagram_youtube_and_any/", "subreddit_subscribers": 680129, "created_utc": 1682672597.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'll be going with a 2TB 980 Pro otherwise", "author_fullname": "t2_9bci9al5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any 4TB M.2 SSD's that use TLC NAND or are they all QLC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131daak", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682643967.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll be going with a 2TB 980 Pro otherwise&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131daak", "is_robot_indexable": true, "report_reasons": null, "author": "CompleteMoron_203", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131daak/are_there_any_4tb_m2_ssds_that_use_tlc_nand_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131daak/are_there_any_4tb_m2_ssds_that_use_tlc_nand_or/", "subreddit_subscribers": 680129, "created_utc": 1682643967.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "&amp;#x200B;\n\nhttps://preview.redd.it/jp9ci1ommowa1.jpg?width=628&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7fc95f04bba701916da3faedeb9a1b9316883b78", "author_fullname": "t2_76pgn19", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My hoarder subtype is 'Purist'.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": 111, "top_awarded_type": null, "hide_score": false, "media_metadata": {"jp9ci1ommowa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 85, "x": 108, "u": "https://preview.redd.it/jp9ci1ommowa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36eb008968880f44e9801b269b404aa453079bcf"}, {"y": 171, "x": 216, "u": "https://preview.redd.it/jp9ci1ommowa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05f431d4287239dcff8eb9c01c4786fa199e063b"}, {"y": 254, "x": 320, "u": "https://preview.redd.it/jp9ci1ommowa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1b5964b1b74d880809f4522562ead4630e7fd31d"}], "s": {"y": 500, "x": 628, "u": "https://preview.redd.it/jp9ci1ommowa1.jpg?width=628&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7fc95f04bba701916da3faedeb9a1b9316883b78"}, "id": "jp9ci1ommowa1"}}, "name": "t3_1328ca3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jJFC6ARZ8jfdkXST19GFI3q1rO4zpIg35EC6A8JW4TY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682712518.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/jp9ci1ommowa1.jpg?width=628&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7fc95f04bba701916da3faedeb9a1b9316883b78\"&gt;https://preview.redd.it/jp9ci1ommowa1.jpg?width=628&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=7fc95f04bba701916da3faedeb9a1b9316883b78&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1328ca3", "is_robot_indexable": true, "report_reasons": null, "author": "AshleyUncia", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1328ca3/my_hoarder_subtype_is_purist/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1328ca3/my_hoarder_subtype_is_purist/", "subreddit_subscribers": 680129, "created_utc": 1682712518.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My family has been using Marco Polo to share videos. I'd like to download the videos from the last year and copy them on to thumb drives I can give to family members. \n\nIt would be nice if there is a \"portable apps\" kind of program like google photos that would let me tag videos with people and location, etc, that would make searching through the videos easier.", "author_fullname": "t2_8looig91", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Photos like software to manage videos on a USB thumb drive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131asur", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682637499.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My family has been using Marco Polo to share videos. I&amp;#39;d like to download the videos from the last year and copy them on to thumb drives I can give to family members. &lt;/p&gt;\n\n&lt;p&gt;It would be nice if there is a &amp;quot;portable apps&amp;quot; kind of program like google photos that would let me tag videos with people and location, etc, that would make searching through the videos easier.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131asur", "is_robot_indexable": true, "report_reasons": null, "author": "RedbloodJarvey", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131asur/google_photos_like_software_to_manage_videos_on_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131asur/google_photos_like_software_to_manage_videos_on_a/", "subreddit_subscribers": 680129, "created_utc": 1682637499.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all. Figured this would be the place to ask for this. See if anyone else has a clue what this program was.\n\nSo, there was this contact sheet/video thumbnail program I was using for a bit, then I reinstalled my OS, and was going about installing my usual programs, and realized I wanted to add that one again, but had no idea what it was called. So here's it's description:\n\nI believe it might've been a Windows program, and I used it through wine on Linux. It had a GUI, and it allowed drag and drop with video files, plus batch loaded files. On-top of that, the second you added files into it, it would begin the process of creating the contact sheets (typically creates them within the same folder). The location of the files you uploaded is on the left, with the settings and progress on the right.\n\nBest part is, the output (contact sheet afterwards) didn't have a little watermark or any such logo or reference to the program, unlike something like Video Thumbnail Maker.\n\nThis is a shot in the dark, but I've tried searching for this for a few days now, trying everything, and no single program I've come across is it. Trying my hand at all the experts in this sub to see if anyone came across this or knows of it.\n\nThank you!", "author_fullname": "t2_yirnb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Trying to remember what THIS software is called, and what it's name is", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131ijzu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682659490.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. Figured this would be the place to ask for this. See if anyone else has a clue what this program was.&lt;/p&gt;\n\n&lt;p&gt;So, there was this contact sheet/video thumbnail program I was using for a bit, then I reinstalled my OS, and was going about installing my usual programs, and realized I wanted to add that one again, but had no idea what it was called. So here&amp;#39;s it&amp;#39;s description:&lt;/p&gt;\n\n&lt;p&gt;I believe it might&amp;#39;ve been a Windows program, and I used it through wine on Linux. It had a GUI, and it allowed drag and drop with video files, plus batch loaded files. On-top of that, the second you added files into it, it would begin the process of creating the contact sheets (typically creates them within the same folder). The location of the files you uploaded is on the left, with the settings and progress on the right.&lt;/p&gt;\n\n&lt;p&gt;Best part is, the output (contact sheet afterwards) didn&amp;#39;t have a little watermark or any such logo or reference to the program, unlike something like Video Thumbnail Maker.&lt;/p&gt;\n\n&lt;p&gt;This is a shot in the dark, but I&amp;#39;ve tried searching for this for a few days now, trying everything, and no single program I&amp;#39;ve come across is it. Trying my hand at all the experts in this sub to see if anyone came across this or knows of it.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "VHS", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131ijzu", "is_robot_indexable": true, "report_reasons": null, "author": "prodigalkal7", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/131ijzu/trying_to_remember_what_this_software_is_called/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131ijzu/trying_to_remember_what_this_software_is_called/", "subreddit_subscribers": 680129, "created_utc": 1682659490.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Facebook displays a page's videos on a single screen that keeps adding thumbnail results as you scroll down. Because this page has so many videos, I haven't been able to reach the end of the video list (read: load thumbnails of all the page's videos) without something breaking.\n\nI tried using Youtube-DL and yt-dlp. They can download individual videos from Facebook, but they can't download all videos from a particular Facebook page.\n\nI also tried using JDownloader 2. LinkCrawler only found a handful of videos.\n\nSuggestions?", "author_fullname": "t2_ix7smrj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need to archive all of a Facebook page's public videos... but the page has 9,000+ videos and I can't even load all their thumbnails to grab their URLs. Suggestions?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131i2wj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682657978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Facebook displays a page&amp;#39;s videos on a single screen that keeps adding thumbnail results as you scroll down. Because this page has so many videos, I haven&amp;#39;t been able to reach the end of the video list (read: load thumbnails of all the page&amp;#39;s videos) without something breaking.&lt;/p&gt;\n\n&lt;p&gt;I tried using Youtube-DL and yt-dlp. They can download individual videos from Facebook, but they can&amp;#39;t download all videos from a particular Facebook page.&lt;/p&gt;\n\n&lt;p&gt;I also tried using JDownloader 2. LinkCrawler only found a handful of videos.&lt;/p&gt;\n\n&lt;p&gt;Suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131i2wj", "is_robot_indexable": true, "report_reasons": null, "author": "SociopathicProTips", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131i2wj/i_need_to_archive_all_of_a_facebook_pages_public/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131i2wj/i_need_to_archive_all_of_a_facebook_pages_public/", "subreddit_subscribers": 680129, "created_utc": 1682657978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Early today \"The Roddenberry Archive\" released a bunch of explorable 3D models of the bridges from different iterations of Star Trek. I've only ever taken 3D models from video games, but these models are imbedded into the site, so I'm not even sure where I'd start. [https://roddenberry.x.io/2344-uss-enterprise-ncc-1701-c/](https://roddenberry.x.io/2344-uss-enterprise-ncc-1701-c/)\n\nYou're able to move around these environments, so they're not just panoramic images, it's just a bit fiddly to get the controls to come up.", "author_fullname": "t2_rax11", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any idea how to RIP the 3D models of the Star Trek bridges from this site?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131at1n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.63, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682637514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Early today &amp;quot;The Roddenberry Archive&amp;quot; released a bunch of explorable 3D models of the bridges from different iterations of Star Trek. I&amp;#39;ve only ever taken 3D models from video games, but these models are imbedded into the site, so I&amp;#39;m not even sure where I&amp;#39;d start. &lt;a href=\"https://roddenberry.x.io/2344-uss-enterprise-ncc-1701-c/\"&gt;https://roddenberry.x.io/2344-uss-enterprise-ncc-1701-c/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re able to move around these environments, so they&amp;#39;re not just panoramic images, it&amp;#39;s just a bit fiddly to get the controls to come up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lNVAondnD_4iGCsxpS_E4ayX5-fWriXVesTKnefxddA.jpg?auto=webp&amp;v=enabled&amp;s=496bc1673d5d4f599eeff3b494407bd810957592", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/lNVAondnD_4iGCsxpS_E4ayX5-fWriXVesTKnefxddA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6d9911065e88a1326beaa3e8851999933d1d4626", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/lNVAondnD_4iGCsxpS_E4ayX5-fWriXVesTKnefxddA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe94783866fed13af2f13aa59c9bf539741bde2b", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/lNVAondnD_4iGCsxpS_E4ayX5-fWriXVesTKnefxddA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68b37b42b2622e37a0925e32882bb4ab3c75902b", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/lNVAondnD_4iGCsxpS_E4ayX5-fWriXVesTKnefxddA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1bda51ade9ef7c9e8f3bba5f1bbcef48d61b2885", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/lNVAondnD_4iGCsxpS_E4ayX5-fWriXVesTKnefxddA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=905a9a139e8f51fc09633a7c0eec265818285b78", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/lNVAondnD_4iGCsxpS_E4ayX5-fWriXVesTKnefxddA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5358b683a5abaeeda37141ecbce8807b2a710333", "width": 1080, "height": 567}], "variants": {}, "id": "o68SdpvNOZ2SnNbQNs_v8GjPmMpUcMmVzv0KSaJDiUM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131at1n", "is_robot_indexable": true, "report_reasons": null, "author": "Sparksighs", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131at1n/any_idea_how_to_rip_the_3d_models_of_the_star/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131at1n/any_idea_how_to_rip_the_3d_models_of_the_star/", "subreddit_subscribers": 680129, "created_utc": 1682637514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Thanks to the help of a fellow anonymous Redditor I've released a new version of [RedditScrape](https://github.com/NSFWUTILS/RedditScrape). This new version now uses the push shift API to gather gigantic levels of data for you to download. This means we no longer need to provide any form of reddit credentials. \n\nWhile the previous version was hard capped at 1,000 posts using the Reddit API, this new version has no limits at all, other than what resources and disk space you have. \n\nFor example, if you're brave enough to try and scrape something like *gonewild*, you'll find it takes DAYS just to get all of the data back from push shift. The JSON text alone is over 9 gigs (3.3 million posts) and climbing. \n\nRunning this is now a two step process, but results in a substantially larger set of media from your favorite subs. \n\nInstructions can be found [here](https://github.com/NSFWUTILS/RedditScrape#setup-the-script). I hope I've fixed a few of the problems that people had with the first iteration along the way.", "author_fullname": "t2_9ptn7jbjf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Improved version of RedditScrape for backing up your favorite subreddits (Now with Push Shift)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1328pmj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682713371.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thanks to the help of a fellow anonymous Redditor I&amp;#39;ve released a new version of &lt;a href=\"https://github.com/NSFWUTILS/RedditScrape\"&gt;RedditScrape&lt;/a&gt;. This new version now uses the push shift API to gather gigantic levels of data for you to download. This means we no longer need to provide any form of reddit credentials. &lt;/p&gt;\n\n&lt;p&gt;While the previous version was hard capped at 1,000 posts using the Reddit API, this new version has no limits at all, other than what resources and disk space you have. &lt;/p&gt;\n\n&lt;p&gt;For example, if you&amp;#39;re brave enough to try and scrape something like &lt;em&gt;gonewild&lt;/em&gt;, you&amp;#39;ll find it takes DAYS just to get all of the data back from push shift. The JSON text alone is over 9 gigs (3.3 million posts) and climbing. &lt;/p&gt;\n\n&lt;p&gt;Running this is now a two step process, but results in a substantially larger set of media from your favorite subs. &lt;/p&gt;\n\n&lt;p&gt;Instructions can be found &lt;a href=\"https://github.com/NSFWUTILS/RedditScrape#setup-the-script\"&gt;here&lt;/a&gt;. I hope I&amp;#39;ve fixed a few of the problems that people had with the first iteration along the way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?auto=webp&amp;v=enabled&amp;s=a9d046c146e14504b168c70d1517d2f199b346da", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f2782224869661f9692c787f848cf59658a1a623", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d713d0d33677e933d08cf907a437af1418c21fb6", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8821ffcd392ee268954af03e37077434fa4fba81", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ec6c7fa7fd452582dde828a06b7fd94cc151ce36", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8e168f78bf0b53ce0898d0b6dece21febaf9db97", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/V4TgjTn8NB7Ah_sSeZFBcGtw4ofeunxRMCJD00AsA_Y.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8fb80ce0b9ae3a830f64719e823dd2600cfda42", "width": 1080, "height": 540}], "variants": {}, "id": "voDkpEv7h5mZTSSyu9yag2yl_npE2zRAsJkQ6NkYOz4"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1328pmj", "is_robot_indexable": true, "report_reasons": null, "author": "nsfwutils", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1328pmj/improved_version_of_redditscrape_for_backing_up/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1328pmj/improved_version_of_redditscrape_for_backing_up/", "subreddit_subscribers": 680129, "created_utc": 1682713371.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm on Windows 11... Is it possible to generate a single hash for an entire folder of data rather than for each individual file within the folder? There could be hundreds of thousands of files in a folder of a few hundred gigs for example. Ideally I'd like something easy to use with a GUI if possible, that I could run on source files and copied files after doing a backup to an external drive. Thanks.", "author_fullname": "t2_hylomcx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Verifying Hashes/Checksums for Entire Folders on Windows 11", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1325uru", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682706717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m on Windows 11... Is it possible to generate a single hash for an entire folder of data rather than for each individual file within the folder? There could be hundreds of thousands of files in a folder of a few hundred gigs for example. Ideally I&amp;#39;d like something easy to use with a GUI if possible, that I could run on source files and copied files after doing a backup to an external drive. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1325uru", "is_robot_indexable": true, "report_reasons": null, "author": "Celcius_87", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1325uru/verifying_hasheschecksums_for_entire_folders_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1325uru/verifying_hasheschecksums_for_entire_folders_on/", "subreddit_subscribers": 680129, "created_utc": 1682706717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hello, I am very new to this so I'm sorry if this is a dumb question. But I have to format my computer and I have around 750GB of anime that I need to move over to another drive or online. Is there a way for me to calculate how long it would take to transfer it all? If it will take more than a week I will just go scorched earth and not move anything over before formatting.  \n\n\nHDD:  Hitachi HUA722010CLA330 1TB, Internal Hard Drive  \n\n\nAll help is greatly appreciated.", "author_fullname": "t2_7aygqar2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hard Drive Switching", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131f5jl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.56, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682649118.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am very new to this so I&amp;#39;m sorry if this is a dumb question. But I have to format my computer and I have around 750GB of anime that I need to move over to another drive or online. Is there a way for me to calculate how long it would take to transfer it all? If it will take more than a week I will just go scorched earth and not move anything over before formatting.  &lt;/p&gt;\n\n&lt;p&gt;HDD:  Hitachi HUA722010CLA330 1TB, Internal Hard Drive  &lt;/p&gt;\n\n&lt;p&gt;All help is greatly appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131f5jl", "is_robot_indexable": true, "report_reasons": null, "author": "_TheOneTrueBean_", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131f5jl/hard_drive_switching/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131f5jl/hard_drive_switching/", "subreddit_subscribers": 680129, "created_utc": 1682649118.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been looking for some drives to act as the backup copies for my WD Red Pro's which i got for a similar price a while back.. but i've really not paid attention to things..\n\nFrom what i've read the Exos are better than the Iron Wolf pro... but either way they both come with 5 years warranty..\n\ni picked up 2 x 16TB exos for \u00a3450.. so we're at \u00a314 per TB..\n\nThese drives are going to be sat in a microserver and powered up once a week to have thr weeks worth of \"acquisitions\" written to them.. total power on time will be about 7 hours a week.\n\nHave a gotten at least an acceptable deal at this price for these drives?", "author_fullname": "t2_4cfky", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Seagate Exos 16tb - \u00a3225.. was this a good deal?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131920j", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682633525.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been looking for some drives to act as the backup copies for my WD Red Pro&amp;#39;s which i got for a similar price a while back.. but i&amp;#39;ve really not paid attention to things..&lt;/p&gt;\n\n&lt;p&gt;From what i&amp;#39;ve read the Exos are better than the Iron Wolf pro... but either way they both come with 5 years warranty..&lt;/p&gt;\n\n&lt;p&gt;i picked up 2 x 16TB exos for \u00a3450.. so we&amp;#39;re at \u00a314 per TB..&lt;/p&gt;\n\n&lt;p&gt;These drives are going to be sat in a microserver and powered up once a week to have thr weeks worth of &amp;quot;acquisitions&amp;quot; written to them.. total power on time will be about 7 hours a week.&lt;/p&gt;\n\n&lt;p&gt;Have a gotten at least an acceptable deal at this price for these drives?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "42 TB (Usable)", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131920j", "is_robot_indexable": true, "report_reasons": null, "author": "d4nm3d", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/131920j/seagate_exos_16tb_225_was_this_a_good_deal/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131920j/seagate_exos_16tb_225_was_this_a_good_deal/", "subreddit_subscribers": 680129, "created_utc": 1682633525.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Currently I use syncthing to two-way sync a folder on my macbook with a folder on my PC. I then have google drive for desktop sync the same folder on my PC to my google drive so that I can access the files if i'm away from both computers. This seems to work OK for now but I have problems if the PC is off (google drive is not updated). Plus there seems to be a delay in google drive syncing too.\n\nIs there a better solution/software I can use?", "author_fullname": "t2_6zxmlbrb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best way to way to sync computers with cloud access.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1328yop", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682713948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently I use syncthing to two-way sync a folder on my macbook with a folder on my PC. I then have google drive for desktop sync the same folder on my PC to my google drive so that I can access the files if i&amp;#39;m away from both computers. This seems to work OK for now but I have problems if the PC is off (google drive is not updated). Plus there seems to be a delay in google drive syncing too.&lt;/p&gt;\n\n&lt;p&gt;Is there a better solution/software I can use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1328yop", "is_robot_indexable": true, "report_reasons": null, "author": "Kommanchi", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1328yop/best_way_to_way_to_sync_computers_with_cloud/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1328yop/best_way_to_way_to_sync_computers_with_cloud/", "subreddit_subscribers": 680129, "created_utc": 1682713948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm wanting to download and view 360 videos from YouTube with HD quality but am having trouble finding a downloader OR a video player. I've used \"Video Keeper\" to download the videos from YouTube and Windows media player's 360 function. I'm not sure if it's Video Keeper or windows media player, but something's leaving a swirly at the apex of the video. \n\nI've looked online for suggestions but most posts are 5+ years old and I feel like are a bit outdated. \n\nIf you guys have any suggestions or maybe tips I can do to get better video quality, I would love to try them out!! Thanks a bunch!!", "author_fullname": "t2_4qcjjq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Suggestions both free and paid to download AND view 360 videos from YTube.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1326vuw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682708996.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wanting to download and view 360 videos from YouTube with HD quality but am having trouble finding a downloader OR a video player. I&amp;#39;ve used &amp;quot;Video Keeper&amp;quot; to download the videos from YouTube and Windows media player&amp;#39;s 360 function. I&amp;#39;m not sure if it&amp;#39;s Video Keeper or windows media player, but something&amp;#39;s leaving a swirly at the apex of the video. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked online for suggestions but most posts are 5+ years old and I feel like are a bit outdated. &lt;/p&gt;\n\n&lt;p&gt;If you guys have any suggestions or maybe tips I can do to get better video quality, I would love to try them out!! Thanks a bunch!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1326vuw", "is_robot_indexable": true, "report_reasons": null, "author": "American-Omar", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1326vuw/suggestions_both_free_and_paid_to_download_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1326vuw/suggestions_both_free_and_paid_to_download_and/", "subreddit_subscribers": 680129, "created_utc": 1682708996.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have a lot of offline storage. I need to send out files a lot but I only need to send out new or changed files since the last time I sent them out. \n\nI use Powershell to generate SHA256 checksums of the offline media and the full file path and file name in a CSV file.  I first use \"Highlight Duplicates\" and \"Remove Duplicates\" function of Excel in the checksums of the local folder to dedupe it. I then compare the offline media with the Local folder's hash and determine which files are not among the files I sent out using the \"Highlight Duplicate\" function. I then paste the paths into FreeFileSync's exclusion list and copy everything else. \n\nDoes this method copy all the new files or am I missing some files? I did some small-scale tests and it seems to have worked. \n\nThanks.", "author_fullname": "t2_lunshccg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using checksums and complete file paths and names in a CSV file to keeping track of offline files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1325i95", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682705937.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a lot of offline storage. I need to send out files a lot but I only need to send out new or changed files since the last time I sent them out. &lt;/p&gt;\n\n&lt;p&gt;I use Powershell to generate SHA256 checksums of the offline media and the full file path and file name in a CSV file.  I first use &amp;quot;Highlight Duplicates&amp;quot; and &amp;quot;Remove Duplicates&amp;quot; function of Excel in the checksums of the local folder to dedupe it. I then compare the offline media with the Local folder&amp;#39;s hash and determine which files are not among the files I sent out using the &amp;quot;Highlight Duplicate&amp;quot; function. I then paste the paths into FreeFileSync&amp;#39;s exclusion list and copy everything else. &lt;/p&gt;\n\n&lt;p&gt;Does this method copy all the new files or am I missing some files? I did some small-scale tests and it seems to have worked. &lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1325i95", "is_robot_indexable": true, "report_reasons": null, "author": "Mewto17", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1325i95/using_checksums_and_complete_file_paths_and_names/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1325i95/using_checksums_and_complete_file_paths_and_names/", "subreddit_subscribers": 680129, "created_utc": 1682705937.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "New to external data storage solutions, and I was wondering, if I host a website on the NAS can I have all data remotely accessible through said website? This is not sensitive data so security is not a big issue, the only requirement is that it is only accessible by certain users.", "author_fullname": "t2_5i0xv5j9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "NAS remote access", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1325b64", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682705479.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;New to external data storage solutions, and I was wondering, if I host a website on the NAS can I have all data remotely accessible through said website? This is not sensitive data so security is not a big issue, the only requirement is that it is only accessible by certain users.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1325b64", "is_robot_indexable": true, "report_reasons": null, "author": "Latea987", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1325b64/nas_remote_access/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1325b64/nas_remote_access/", "subreddit_subscribers": 680129, "created_utc": 1682705479.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using ZFS in a RAIDZ pool configuration which I think is discouraged now in favor of mirroring for both my server and backup server.  I'm questioning the whole point of RAID in a home environment right now.   If I have backups, do I really need to waste  50% of my disk space via mirroring?    If a drive goes down, restore from backup.   If the backup goes down, backup your stuff again after repairing the backup server. \n\nMost of my storage is due to media server files.  I was thinking of reconfiguring storage for JBOD &amp; mergerfs.  And maybe doing the same with the backup server but adding redundancy via Snapraid with a parity disk on the backup server.\n\nAnything extremely important like financial records are also out on the cloud.   Is RAID redundancy really that important in the home?", "author_fullname": "t2_11l32p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RAID - home servers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_13229fy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682701396.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using ZFS in a RAIDZ pool configuration which I think is discouraged now in favor of mirroring for both my server and backup server.  I&amp;#39;m questioning the whole point of RAID in a home environment right now.   If I have backups, do I really need to waste  50% of my disk space via mirroring?    If a drive goes down, restore from backup.   If the backup goes down, backup your stuff again after repairing the backup server. &lt;/p&gt;\n\n&lt;p&gt;Most of my storage is due to media server files.  I was thinking of reconfiguring storage for JBOD &amp;amp; mergerfs.  And maybe doing the same with the backup server but adding redundancy via Snapraid with a parity disk on the backup server.&lt;/p&gt;\n\n&lt;p&gt;Anything extremely important like financial records are also out on the cloud.   Is RAID redundancy really that important in the home?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "13229fy", "is_robot_indexable": true, "report_reasons": null, "author": "mlcarson", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/13229fy/raid_home_servers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/13229fy/raid_home_servers/", "subreddit_subscribers": 680129, "created_utc": 1682701396.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi guys just a simple question I recently installed stable bits drive pool and drive scanner. And it works wonderfully, however I have a shit 3tb drive that ive been using knowing it might fail imminently and this is just for a server with plex in it so worst case scenario I just have to re download everything which is not much either. But the scanner has seen that the drive is on its last legs so it has initiated a file evacuation and now I cant download anything. I know its doing its job but is there any way to bypass this. I know the drive is dying but the media in there is not even important so can I pause this until I get a good drive? My other drive is WD RED. I dont see any settings and can rebalance the pool manualy. Appreciate any help!", "author_fullname": "t2_3nvcxuhv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "File evacuation", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1321z4t", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682701125.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys just a simple question I recently installed stable bits drive pool and drive scanner. And it works wonderfully, however I have a shit 3tb drive that ive been using knowing it might fail imminently and this is just for a server with plex in it so worst case scenario I just have to re download everything which is not much either. But the scanner has seen that the drive is on its last legs so it has initiated a file evacuation and now I cant download anything. I know its doing its job but is there any way to bypass this. I know the drive is dying but the media in there is not even important so can I pause this until I get a good drive? My other drive is WD RED. I dont see any settings and can rebalance the pool manualy. Appreciate any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "1321z4t", "is_robot_indexable": true, "report_reasons": null, "author": "nicolas19961805", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/1321z4t/file_evacuation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/1321z4t/file_evacuation/", "subreddit_subscribers": 680129, "created_utc": 1682701125.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm trying to speed up a 10TB migration off iSCSI as the array throughput is less than ideal \n\nThe file sync is very slow to build the index of the volume which is the main hurdle of starting the migration \n\nBuild index - takes days and disk time away from the running app\n\nCheck diffs - again takes forever \n\nCopy - taking forever (150KB/sec)\n\nWith that, exploring an idea to see if an app exists that can watch a folder and replicate the files being read/written to another share?", "author_fullname": "t2_12koak", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there an app that can replicate files on read/write?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131z02f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "eac073cc-b98a-11e2-84c9-12313d1841d1", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "vhs", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682698047.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to speed up a 10TB migration off iSCSI as the array throughput is less than ideal &lt;/p&gt;\n\n&lt;p&gt;The file sync is very slow to build the index of the volume which is the main hurdle of starting the migration &lt;/p&gt;\n\n&lt;p&gt;Build index - takes days and disk time away from the running app&lt;/p&gt;\n\n&lt;p&gt;Check diffs - again takes forever &lt;/p&gt;\n\n&lt;p&gt;Copy - taking forever (150KB/sec)&lt;/p&gt;\n\n&lt;p&gt;With that, exploring an idea to see if an app exists that can watch a folder and replicate the files being read/written to another share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "40TB + 14TB Storj ", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131z02f", "is_robot_indexable": true, "report_reasons": null, "author": "techtornado", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/131z02f/is_there_an_app_that_can_replicate_files_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131z02f/is_there_an_app_that_can_replicate_files_on/", "subreddit_subscribers": 680129, "created_utc": 1682698047.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i have a book collection, created as a resource to be shared with pretty much anyone, that's currently uploaded to mega, sorted into various folders by genre, topic, etc. this works fine, but whenever a book fits multiple categories, the only way i've found to have it under multiple folders is to duplicate it into both folders, which does takes up extra storage space. this is quickly becoming a problem as the collection grows; i know i could just make a second account when the one i'm using runs out of space, but i'm afraid it would be confusing for your average person accessing the collection.\n\nmy question is, does anyone know of any sites that allow sorting files into multiple categories/tags, don't require hosting a server (as it's not really possible for me at the moment), and are generally user-friendly? sorry if this has been answered before here, search only shows posts that are related, but not quite what i'm looking for. thanks in advance.", "author_fullname": "t2_a6isq25uy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "sorting book collection in cloud service in an accessible way", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131vd81", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682693725.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i have a book collection, created as a resource to be shared with pretty much anyone, that&amp;#39;s currently uploaded to mega, sorted into various folders by genre, topic, etc. this works fine, but whenever a book fits multiple categories, the only way i&amp;#39;ve found to have it under multiple folders is to duplicate it into both folders, which does takes up extra storage space. this is quickly becoming a problem as the collection grows; i know i could just make a second account when the one i&amp;#39;m using runs out of space, but i&amp;#39;m afraid it would be confusing for your average person accessing the collection.&lt;/p&gt;\n\n&lt;p&gt;my question is, does anyone know of any sites that allow sorting files into multiple categories/tags, don&amp;#39;t require hosting a server (as it&amp;#39;s not really possible for me at the moment), and are generally user-friendly? sorry if this has been answered before here, search only shows posts that are related, but not quite what i&amp;#39;m looking for. thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131vd81", "is_robot_indexable": true, "report_reasons": null, "author": "passifloreae", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131vd81/sorting_book_collection_in_cloud_service_in_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131vd81/sorting_book_collection_in_cloud_service_in_an/", "subreddit_subscribers": 680129, "created_utc": 1682693725.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi all, not sure if I'm overthinking this but I've found very few technical details about how Google Drive is actually working with regards to offline files and misc behaviour.\n\nHere's my use case:\n\n\\- Unify all my files and folder into one \"location\"\n\n\\- Sync access across devices, with offline availability for certain folders.\n\n\\- Point certain programs on Windows to absolute file paths for G \"virtual\" Drive e.g music library for Ableton Live.\n\n\\- Using Obsidian Sync (a syncing/backup function integrated into the Obsidian text editor for file syncing across devices - I don't want to use Syncthing or P2P for misc reasons)\n\nPreface: I am keeping actual program files local on system (for obvious reasons related to the way programs get installed across many file directories and cache paths etc)\n\nI currently use an array of different software tools e.g. video editors, music production software and so it's critical that many of my files are offline first to avoid issues with \"streaming\" into realtime applications.\n\n**Concerns with Google Drive file \"streaming\":**\n\nThe \"obvious\" solution seems to be to make some folders available \"offline\"\n\nHowever as I understand it, when you make available \"offline\", these files are not actually stored in your virtual \"G Drive\", but rather point to an obfuscated caching folder inside Windows. This doesn't \"seem\" to cause issues when just pointing to a correct path inside the virtual \"G Drive\" (assuming Google drive client is running) but am concerned I may run into issues with certain programs in the future due to the trickery that's going on? I really don't want to run into various unexplained issues?\n\nI'm also using Obsidian Sync. This just syncs a local folder (\"Vault\") into Obsidian servers.\n\nFrom their web page:\n\n\"Some cloud storage services, such as OneDrive, allow you to only download files when you use them and later remove them locally to free up space. Since the files are no longer available locally, Obsidian Sync believes they've been deleted and removes them from your remote vault. To use Obsidian Sync together with Files On-Demand and similar features, make sure to configure the service to always keep the files on the device.\"\n\nAgain here, I can make my Obsidian Vault available \"offline\" inside my virtual G \"Drive\" but worried that if something \"goes wrong\" with offline access, I won't actually be syncing my files properly to the Obsidian servers.\n\n**Proposal: Is this \"good enough\"?**\n\n\\-On primary system, mirror instead of stream all my files to a unique drive so everything is \"local first\". I'm guessing caching is still used, but only for uploading to Google drive?\n\nThis would obviously mean I have to store everything in my Google Drive locally, but it seems like a small price to pay.\n\n\\- Backup this drive locally periodically, in the event of corruption, ransom etc.\n\n\\- Use File stream with \"offline access\" for other systems where necessary (ie local storage limitations) and hope for the best?\n\n&amp;#x200B;\n\nSorry it's a all a bit vague but I'm trying to streamline as much as possible.", "author_fullname": "t2_4gxdcxd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Google Drive for Desktop confusion(cache, offline files) - How foolproof is it.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131rb3r", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682691545.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682687418.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, not sure if I&amp;#39;m overthinking this but I&amp;#39;ve found very few technical details about how Google Drive is actually working with regards to offline files and misc behaviour.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s my use case:&lt;/p&gt;\n\n&lt;p&gt;- Unify all my files and folder into one &amp;quot;location&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;- Sync access across devices, with offline availability for certain folders.&lt;/p&gt;\n\n&lt;p&gt;- Point certain programs on Windows to absolute file paths for G &amp;quot;virtual&amp;quot; Drive e.g music library for Ableton Live.&lt;/p&gt;\n\n&lt;p&gt;- Using Obsidian Sync (a syncing/backup function integrated into the Obsidian text editor for file syncing across devices - I don&amp;#39;t want to use Syncthing or P2P for misc reasons)&lt;/p&gt;\n\n&lt;p&gt;Preface: I am keeping actual program files local on system (for obvious reasons related to the way programs get installed across many file directories and cache paths etc)&lt;/p&gt;\n\n&lt;p&gt;I currently use an array of different software tools e.g. video editors, music production software and so it&amp;#39;s critical that many of my files are offline first to avoid issues with &amp;quot;streaming&amp;quot; into realtime applications.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Concerns with Google Drive file &amp;quot;streaming&amp;quot;:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The &amp;quot;obvious&amp;quot; solution seems to be to make some folders available &amp;quot;offline&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;However as I understand it, when you make available &amp;quot;offline&amp;quot;, these files are not actually stored in your virtual &amp;quot;G Drive&amp;quot;, but rather point to an obfuscated caching folder inside Windows. This doesn&amp;#39;t &amp;quot;seem&amp;quot; to cause issues when just pointing to a correct path inside the virtual &amp;quot;G Drive&amp;quot; (assuming Google drive client is running) but am concerned I may run into issues with certain programs in the future due to the trickery that&amp;#39;s going on? I really don&amp;#39;t want to run into various unexplained issues?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also using Obsidian Sync. This just syncs a local folder (&amp;quot;Vault&amp;quot;) into Obsidian servers.&lt;/p&gt;\n\n&lt;p&gt;From their web page:&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Some cloud storage services, such as OneDrive, allow you to only download files when you use them and later remove them locally to free up space. Since the files are no longer available locally, Obsidian Sync believes they&amp;#39;ve been deleted and removes them from your remote vault. To use Obsidian Sync together with Files On-Demand and similar features, make sure to configure the service to always keep the files on the device.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Again here, I can make my Obsidian Vault available &amp;quot;offline&amp;quot; inside my virtual G &amp;quot;Drive&amp;quot; but worried that if something &amp;quot;goes wrong&amp;quot; with offline access, I won&amp;#39;t actually be syncing my files properly to the Obsidian servers.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Proposal: Is this &amp;quot;good enough&amp;quot;?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;-On primary system, mirror instead of stream all my files to a unique drive so everything is &amp;quot;local first&amp;quot;. I&amp;#39;m guessing caching is still used, but only for uploading to Google drive?&lt;/p&gt;\n\n&lt;p&gt;This would obviously mean I have to store everything in my Google Drive locally, but it seems like a small price to pay.&lt;/p&gt;\n\n&lt;p&gt;- Backup this drive locally periodically, in the event of corruption, ransom etc.&lt;/p&gt;\n\n&lt;p&gt;- Use File stream with &amp;quot;offline access&amp;quot; for other systems where necessary (ie local storage limitations) and hope for the best?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Sorry it&amp;#39;s a all a bit vague but I&amp;#39;m trying to streamline as much as possible.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131rb3r", "is_robot_indexable": true, "report_reasons": null, "author": "AstroflashReddit", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131rb3r/google_drive_for_desktop_confusioncache_offline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131rb3r/google_drive_for_desktop_confusioncache_offline/", "subreddit_subscribers": 680129, "created_utc": 1682687418.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "So HDDs are starting to go the way of the dodo, less and less devices come with them and experience tells me thats how it starts before a technology becomes obsolete, just like CDs.\n\nBut now I'm left wondering, if you Google around you'll find thousands of articles telling you SSDs in cold storage do not last, but to me that has been proven somewhat false, I've had a few SSDs unpowered for around a year without issues.\n\nSSDs have been around for more than 10 years now so there should be some actual experimental data around yet I cannot find any actual long term studies or people relating their experience, hence this post, what has been your experience? How long your data lasted? Under what conditions?", "author_fullname": "t2_lcjhu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SSDs vs Time, what's your experience?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131irwi", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682660197.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So HDDs are starting to go the way of the dodo, less and less devices come with them and experience tells me thats how it starts before a technology becomes obsolete, just like CDs.&lt;/p&gt;\n\n&lt;p&gt;But now I&amp;#39;m left wondering, if you Google around you&amp;#39;ll find thousands of articles telling you SSDs in cold storage do not last, but to me that has been proven somewhat false, I&amp;#39;ve had a few SSDs unpowered for around a year without issues.&lt;/p&gt;\n\n&lt;p&gt;SSDs have been around for more than 10 years now so there should be some actual experimental data around yet I cannot find any actual long term studies or people relating their experience, hence this post, what has been your experience? How long your data lasted? Under what conditions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131irwi", "is_robot_indexable": true, "report_reasons": null, "author": "otoko_no_hito", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131irwi/ssds_vs_time_whats_your_experience/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131irwi/ssds_vs_time_whats_your_experience/", "subreddit_subscribers": 680129, "created_utc": 1682660197.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've been using this PCIe firewire  (IEEE 1394 ) card to archive home video for 3 years now. Used it all  last week and now suddenly windows has stopped detecting my camera.  It'll detect the card in device manager no problem but not the camera.  These are all the thing I've tried with no success.\n\nMy main pc hasn't been updated since March 30th so I don't suspect a windows update issue\n\nTried 2 different cards from different manufacturers\n\nLegacy Drivers\n\n2 different cameras\n\n2 different software's (Premiere Pro &amp; WinDV)\n\n2 different firewire cables\n\n2 different pc's (one intel, one Ryzen) (Both running windows 10)\n\n&amp;#x200B;\n\n Any advice or help would be very appreciated as I feel I've tried everything.", "author_fullname": "t2_12fujk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Firewire Device stopped working suddenly (Windows 10)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131ew3n", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682648397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been using this PCIe firewire  (IEEE 1394 ) card to archive home video for 3 years now. Used it all  last week and now suddenly windows has stopped detecting my camera.  It&amp;#39;ll detect the card in device manager no problem but not the camera.  These are all the thing I&amp;#39;ve tried with no success.&lt;/p&gt;\n\n&lt;p&gt;My main pc hasn&amp;#39;t been updated since March 30th so I don&amp;#39;t suspect a windows update issue&lt;/p&gt;\n\n&lt;p&gt;Tried 2 different cards from different manufacturers&lt;/p&gt;\n\n&lt;p&gt;Legacy Drivers&lt;/p&gt;\n\n&lt;p&gt;2 different cameras&lt;/p&gt;\n\n&lt;p&gt;2 different software&amp;#39;s (Premiere Pro &amp;amp; WinDV)&lt;/p&gt;\n\n&lt;p&gt;2 different firewire cables&lt;/p&gt;\n\n&lt;p&gt;2 different pc&amp;#39;s (one intel, one Ryzen) (Both running windows 10)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any advice or help would be very appreciated as I feel I&amp;#39;ve tried everything.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "131ew3n", "is_robot_indexable": true, "report_reasons": null, "author": "hindersin1080", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/131ew3n/firewire_device_stopped_working_suddenly_windows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/131ew3n/firewire_device_stopped_working_suddenly_windows/", "subreddit_subscribers": 680129, "created_utc": 1682648397.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}