{"kind": "Listing", "data": {"after": "t3_130h06j", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My personal favorite... A man's health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.", "author_fullname": "t2_3xnau4cx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's your favorite data quality horror story?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130rfc2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 179, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 179, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682609164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My personal favorite... A man&amp;#39;s health insurance bill went up astronomically after moving from the EU to the USA because his height was listed at 1.8ft instead of meters. Needless to say, the insurance company decided someone shaped like a 180lb pancake is a high-risk individual to insure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130rfc2", "is_robot_indexable": true, "report_reasons": null, "author": "superconductiveKyle", "discussion_type": null, "num_comments": 59, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130rfc2/whats_your_favorite_data_quality_horror_story/", "subreddit_subscribers": 102975, "created_utc": 1682609164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm in a company that uses pip.\nSince my senior colleague went away 1.5 years ago, my manager instead of hiring another person, he made me became an one man band by doing data engineer / analyst / PMO / SA with tight deadlines.\n\nWhen there's something more complex that he doesn't know, he doesn't want to use it or to make the client pay to use the right tools.\nHe insist that these things can be done in one day.\n\nDon't know what to say to him.", "author_fullname": "t2_fsoatxql", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need to vent - manager that doesn't like complex things", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130he0d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682592163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m in a company that uses pip.\nSince my senior colleague went away 1.5 years ago, my manager instead of hiring another person, he made me became an one man band by doing data engineer / analyst / PMO / SA with tight deadlines.&lt;/p&gt;\n\n&lt;p&gt;When there&amp;#39;s something more complex that he doesn&amp;#39;t know, he doesn&amp;#39;t want to use it or to make the client pay to use the right tools.\nHe insist that these things can be done in one day.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t know what to say to him.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130he0d", "is_robot_indexable": true, "report_reasons": null, "author": "CauliflowerJolly4599", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130he0d/need_to_vent_manager_that_doesnt_like_complex/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130he0d/need_to_vent_manager_that_doesnt_like_complex/", "subreddit_subscribers": 102975, "created_utc": 1682592163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, sorry if this is a stupid question. But I'd love to know why companies would pay for both DBT cloud and Fivetran now that Fivetran has built in DBT core with scheduling?\n\nIt seems like there's an ever growing number of data tools, each with their own expensive price tag. So I am trying to understand which tools are necessary and which aren't. Thanks!", "author_fullname": "t2_mytvjynu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why pay for DBT cloud when Fivetran has built in DBT Core?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1318f7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.82, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682632179.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, sorry if this is a stupid question. But I&amp;#39;d love to know why companies would pay for both DBT cloud and Fivetran now that Fivetran has built in DBT core with scheduling?&lt;/p&gt;\n\n&lt;p&gt;It seems like there&amp;#39;s an ever growing number of data tools, each with their own expensive price tag. So I am trying to understand which tools are necessary and which aren&amp;#39;t. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1318f7s", "is_robot_indexable": true, "report_reasons": null, "author": "a-layerup", "discussion_type": null, "num_comments": 33, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1318f7s/why_pay_for_dbt_cloud_when_fivetran_has_built_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1318f7s/why_pay_for_dbt_cloud_when_fivetran_has_built_in/", "subreddit_subscribers": 102975, "created_utc": 1682632179.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My org is transitioning into medallion architecture making it the first learning for alot of us. \n\nFolks who have grinded their ass off with this architecture, how has your experience been?", "author_fullname": "t2_aqp7hdzb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How has your experience working with the Databrick medallion architecture?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130lbzy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682600232.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My org is transitioning into medallion architecture making it the first learning for alot of us. &lt;/p&gt;\n\n&lt;p&gt;Folks who have grinded their ass off with this architecture, how has your experience been?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130lbzy", "is_robot_indexable": true, "report_reasons": null, "author": "johnyjohnyespappa", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130lbzy/how_has_your_experience_working_with_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130lbzy/how_has_your_experience_working_with_the/", "subreddit_subscribers": 102975, "created_utc": 1682600232.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, i started recently as a DE and wondered if the speed our ELT-Pipeline is slow?\n\nIts a pipeline from an IBM DB2 table to SQL Server table (once full, not per delta), which is used as a data warehouse. We pull the data via a SQL Command, put the data into a separate table (stage/landing zone), then add 2 Columns (HashKey and LoadDate) and then insert it into a persisted table. The whole process took us 6 hours!\n\nRow count : 27 Million\n\nNumber of Columns of the table: 95\n\nCompression type: Column Store (just on persisted table)\n\nData space: 2584 MB\n\nIndex space: 1561 MB\n\nETL Tool: Microsoft Integration Services\n\nTime from Source to temporal table: 6 hours\n\nTime from stage table to persisted table: 39 min\n\nFull Process from Source to temporal table (SSIS data flow within a loop, 10.000 rows per batch):\n\n1. truncate temporal table\n2. connect to source via sql command source query (SELECT 95 attributes FROM source)\n3. do some TRIM operations and type conversions\n4. add a hash key column and loaddate column\n5. Insert the data into the temporal table\n\n&amp;#x200B;\n\nHow much time should this take and what could we do to make the process faster?", "author_fullname": "t2_jo31wrc3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is the Speed of our ELT Pipeline too slow?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130ff1p", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682666596.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682588080.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, i started recently as a DE and wondered if the speed our ELT-Pipeline is slow?&lt;/p&gt;\n\n&lt;p&gt;Its a pipeline from an IBM DB2 table to SQL Server table (once full, not per delta), which is used as a data warehouse. We pull the data via a SQL Command, put the data into a separate table (stage/landing zone), then add 2 Columns (HashKey and LoadDate) and then insert it into a persisted table. The whole process took us 6 hours!&lt;/p&gt;\n\n&lt;p&gt;Row count : 27 Million&lt;/p&gt;\n\n&lt;p&gt;Number of Columns of the table: 95&lt;/p&gt;\n\n&lt;p&gt;Compression type: Column Store (just on persisted table)&lt;/p&gt;\n\n&lt;p&gt;Data space: 2584 MB&lt;/p&gt;\n\n&lt;p&gt;Index space: 1561 MB&lt;/p&gt;\n\n&lt;p&gt;ETL Tool: Microsoft Integration Services&lt;/p&gt;\n\n&lt;p&gt;Time from Source to temporal table: 6 hours&lt;/p&gt;\n\n&lt;p&gt;Time from stage table to persisted table: 39 min&lt;/p&gt;\n\n&lt;p&gt;Full Process from Source to temporal table (SSIS data flow within a loop, 10.000 rows per batch):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;truncate temporal table&lt;/li&gt;\n&lt;li&gt;connect to source via sql command source query (SELECT 95 attributes FROM source)&lt;/li&gt;\n&lt;li&gt;do some TRIM operations and type conversions&lt;/li&gt;\n&lt;li&gt;add a hash key column and loaddate column&lt;/li&gt;\n&lt;li&gt;Insert the data into the temporal table&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;How much time should this take and what could we do to make the process faster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130ff1p", "is_robot_indexable": true, "report_reasons": null, "author": "Elegant_Good6212", "discussion_type": null, "num_comments": 28, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130ff1p/is_the_speed_of_our_elt_pipeline_too_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130ff1p/is_the_speed_of_our_elt_pipeline_too_slow/", "subreddit_subscribers": 102975, "created_utc": 1682588080.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work in data professionally with a team who only knows SQL whereas I know less SQL than they do and a lot more Python. We're using Spark for data transformations and are currently trying to upsert to Delta Lake files from existing parquet EDIT using notebooks in Synapse. \n\nAs my team only knows SQL (as does the lead), the Lead thinks it'd be best if we only use SQL to minimise the amount of time required for everybody to get up and running.\n\nI'm currently reading in the parquet file using the syntax:\n\n`CREATE TABLE IF NOT EXISTS SourceTable USING Parquet LOCATION &lt;location&gt;`\n\nand add in a column\n\n`ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))`\n\nI now want to populate this column with values with `UPDATE SourceTable SET NewColumn = 'test'` just to check it works and I'm getting the error:\n\n`Error: UPDATE destination only supports Delta sources.`\n\nI get this and it's music to my ears because data manipulation in PySpark is infinitely easier for me (this is going to be dynamic so ease of string interpolation and variable injection is what I want).\n\nThis leads me to have a few questions:\n\n* Is it really impossible to `UPDATE` a table object created from a Parquet file using Spark SQL?\n\n* Under the hood, I get that in PySpark the parquet file is converted into a DataFrame which is an object in memory. As you can't this using Spark SQL, does this mean the parquet file doesn't get converted into memory and you are, in fact, hitting the file directly?\n\n* I'm trying to build a strong case where we use PySpark 90%+ of the time for these kinds of transformations and, of course, being unable to view changes in memory is a pretty big shitter.  Are there any other massive limitations of Spark SQL I should know about?\n\nThank you!\n\nEDIT\n\nMore detail.\n\nWe currently use the Data Lakehouse paradigm in Azure Synapse. We take parquet files (bronze) and then essentially upsert them into Delta files (silver) based off a key. This key is a hash of a column/set of columns.\n\nThis was previously achieved using Azure Data Flows although we're trying to move away from using these because they're a massive pain in the tits to maintain and shift into notebooks/code. The Data flow has the following pattern for the row hash:\n\n* Reads in the parquet file\n\n* Creates a new column with an expression which is essentially hashing a column/set of columns\n\n* Does an upsert against the delta file, checking if the row hash exists as the condition.\n\nThe issue:\n\n* In note books using Spark SQL, I can't recreate this pattern because when I add a new column, I can't populate it with values as I'm, effectively, trying to \"edit\" the parquet file.\n\n* This is confusing for me because one of the questions I have at the top is \"Why can I do this in a data frame, but not in a table created using Spark SQL syntax? Is it tables created from Parquets using Spark SQL aren't the same as dataframes?\".\n\n* I can do this all in Pyspark, but ideally, I'd like to use Spark SQL because my team can't do it in Python.\n\nEDIT 2:\n\nSample code comparison:\n\n    from pyspark.sql.functions import sha2\n    \n    source_df = spark.read.load(\"path/to/parquet/file.parquet\", format=\"parquet\")\n    source_df = source_df.withColumn(\"NewColumn\", sha2(source_df.ColumnToHash, 256))\n\nI can then do the upsert to the Delta file.\n\nMy Spark SQL code:\n\n    CREATE TABLE IF NOT EXISTS SourceTable USING Parquet\n    LOCATION \"path/to/parquet/file.parquet\"\n\n    ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))\n\nBut then I can't do anything to `NewColumn`.  Worth adding a minor correction - it's not a row hash, I'm trying to hash a specific column/set of columns.", "author_fullname": "t2_bqhp2bh8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Limitations of Spark SQL vs. PySpark?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130nxgp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682611347.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682605793.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work in data professionally with a team who only knows SQL whereas I know less SQL than they do and a lot more Python. We&amp;#39;re using Spark for data transformations and are currently trying to upsert to Delta Lake files from existing parquet EDIT using notebooks in Synapse. &lt;/p&gt;\n\n&lt;p&gt;As my team only knows SQL (as does the lead), the Lead thinks it&amp;#39;d be best if we only use SQL to minimise the amount of time required for everybody to get up and running.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently reading in the parquet file using the syntax:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;CREATE TABLE IF NOT EXISTS SourceTable USING Parquet LOCATION &amp;lt;location&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;and add in a column&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I now want to populate this column with values with &lt;code&gt;UPDATE SourceTable SET NewColumn = &amp;#39;test&amp;#39;&lt;/code&gt; just to check it works and I&amp;#39;m getting the error:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Error: UPDATE destination only supports Delta sources.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I get this and it&amp;#39;s music to my ears because data manipulation in PySpark is infinitely easier for me (this is going to be dynamic so ease of string interpolation and variable injection is what I want).&lt;/p&gt;\n\n&lt;p&gt;This leads me to have a few questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Is it really impossible to &lt;code&gt;UPDATE&lt;/code&gt; a table object created from a Parquet file using Spark SQL?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Under the hood, I get that in PySpark the parquet file is converted into a DataFrame which is an object in memory. As you can&amp;#39;t this using Spark SQL, does this mean the parquet file doesn&amp;#39;t get converted into memory and you are, in fact, hitting the file directly?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I&amp;#39;m trying to build a strong case where we use PySpark 90%+ of the time for these kinds of transformations and, of course, being unable to view changes in memory is a pretty big shitter.  Are there any other massive limitations of Spark SQL I should know about?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;EDIT&lt;/p&gt;\n\n&lt;p&gt;More detail.&lt;/p&gt;\n\n&lt;p&gt;We currently use the Data Lakehouse paradigm in Azure Synapse. We take parquet files (bronze) and then essentially upsert them into Delta files (silver) based off a key. This key is a hash of a column/set of columns.&lt;/p&gt;\n\n&lt;p&gt;This was previously achieved using Azure Data Flows although we&amp;#39;re trying to move away from using these because they&amp;#39;re a massive pain in the tits to maintain and shift into notebooks/code. The Data flow has the following pattern for the row hash:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Reads in the parquet file&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Creates a new column with an expression which is essentially hashing a column/set of columns&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Does an upsert against the delta file, checking if the row hash exists as the condition.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The issue:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;In note books using Spark SQL, I can&amp;#39;t recreate this pattern because when I add a new column, I can&amp;#39;t populate it with values as I&amp;#39;m, effectively, trying to &amp;quot;edit&amp;quot; the parquet file.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This is confusing for me because one of the questions I have at the top is &amp;quot;Why can I do this in a data frame, but not in a table created using Spark SQL syntax? Is it tables created from Parquets using Spark SQL aren&amp;#39;t the same as dataframes?&amp;quot;.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I can do this all in Pyspark, but ideally, I&amp;#39;d like to use Spark SQL because my team can&amp;#39;t do it in Python.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;EDIT 2:&lt;/p&gt;\n\n&lt;p&gt;Sample code comparison:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;from pyspark.sql.functions import sha2\n\nsource_df = spark.read.load(&amp;quot;path/to/parquet/file.parquet&amp;quot;, format=&amp;quot;parquet&amp;quot;)\nsource_df = source_df.withColumn(&amp;quot;NewColumn&amp;quot;, sha2(source_df.ColumnToHash, 256))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I can then do the upsert to the Delta file.&lt;/p&gt;\n\n&lt;p&gt;My Spark SQL code:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CREATE TABLE IF NOT EXISTS SourceTable USING Parquet\nLOCATION &amp;quot;path/to/parquet/file.parquet&amp;quot;\n\nALTER TABLE SourceTable ADD COLUMNS (NewColumn VARCHAR(255))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;But then I can&amp;#39;t do anything to &lt;code&gt;NewColumn&lt;/code&gt;.  Worth adding a minor correction - it&amp;#39;s not a row hash, I&amp;#39;m trying to hash a specific column/set of columns.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130nxgp", "is_robot_indexable": true, "report_reasons": null, "author": "standard_throw", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130nxgp/limitations_of_spark_sql_vs_pyspark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130nxgp/limitations_of_spark_sql_vs_pyspark/", "subreddit_subscribers": 102975, "created_utc": 1682605793.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I know a lot of you will say you can learn for free, but my employer will pay for it, and I\u2019d like to take advantage of that money.", "author_fullname": "t2_ulshlx1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there any paid bootcamp or certification worth taking for DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131bffg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682639069.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know a lot of you will say you can learn for free, but my employer will pay for it, and I\u2019d like to take advantage of that money.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "131bffg", "is_robot_indexable": true, "report_reasons": null, "author": "__academic__", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131bffg/is_there_any_paid_bootcamp_or_certification_worth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131bffg/is_there_any_paid_bootcamp_or_certification_worth/", "subreddit_subscribers": 102975, "created_utc": 1682639069.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve recently encountered Step Functions and can\u2019t believe how easy it is to use, want to understand how easy it is to use compared to something like Airflow or MWAA in my case.\n\nI\u2019ve not heard of a situation when someone had used StepFunctions for Complex ETL, any thoughts ?", "author_fullname": "t2_1nawf7b9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Airflow vs Step functions ??", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131dxyl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682645783.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve recently encountered Step Functions and can\u2019t believe how easy it is to use, want to understand how easy it is to use compared to something like Airflow or MWAA in my case.&lt;/p&gt;\n\n&lt;p&gt;I\u2019ve not heard of a situation when someone had used StepFunctions for Complex ETL, any thoughts ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131dxyl", "is_robot_indexable": true, "report_reasons": null, "author": "Exciting-Garlic8360", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131dxyl/airflow_vs_step_functions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131dxyl/airflow_vs_step_functions/", "subreddit_subscribers": 102975, "created_utc": 1682645783.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As part of a hubby project I am working on, I need to be able to take SQL strings supplied by users, and then run it.\n\nBut before running it, i will like to do some manipulations to the SQL. For example, I may want to have a copy of the SQL that does a COUNT query version of the original query...I might want to insert LIMIT clauses if not provided, or I might want to manipulate the columns specified, rename or even insert columns.\n\nNow the question is: I am not sure how to go able this and what sort of tools to even look for. I think in essence I want to be able to manipulate the SQL not as a string but as a data structure I can manipulate\n\nUsing regex and string manipulation does not sound like the right tool for the job. What will you say is the right way to go about this?", "author_fullname": "t2_x8zrrj3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to manipulate SQL string programmatically?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131j6ee", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682661573.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As part of a hubby project I am working on, I need to be able to take SQL strings supplied by users, and then run it.&lt;/p&gt;\n\n&lt;p&gt;But before running it, i will like to do some manipulations to the SQL. For example, I may want to have a copy of the SQL that does a COUNT query version of the original query...I might want to insert LIMIT clauses if not provided, or I might want to manipulate the columns specified, rename or even insert columns.&lt;/p&gt;\n\n&lt;p&gt;Now the question is: I am not sure how to go able this and what sort of tools to even look for. I think in essence I want to be able to manipulate the SQL not as a string but as a data structure I can manipulate&lt;/p&gt;\n\n&lt;p&gt;Using regex and string manipulation does not sound like the right tool for the job. What will you say is the right way to go about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131j6ee", "is_robot_indexable": true, "report_reasons": null, "author": "io_geekabyte", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131j6ee/how_to_manipulate_sql_string_programmatically/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131j6ee/how_to_manipulate_sql_string_programmatically/", "subreddit_subscribers": 102975, "created_utc": 1682661573.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I didn't think I would get the position. All I have is a technical degree in data analysis, a couple of years of experience in the same area, and some certs. Extremely happy.\n\nWondering if I should get a degree, or if I can continue to grow with just experience and certifications.", "author_fullname": "t2_8kf3kc84t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "No bachelor degree, got offered a data engineer position", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131fdyu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682649789.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I didn&amp;#39;t think I would get the position. All I have is a technical degree in data analysis, a couple of years of experience in the same area, and some certs. Extremely happy.&lt;/p&gt;\n\n&lt;p&gt;Wondering if I should get a degree, or if I can continue to grow with just experience and certifications.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "131fdyu", "is_robot_indexable": true, "report_reasons": null, "author": "Pinkypie_15", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131fdyu/no_bachelor_degree_got_offered_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131fdyu/no_bachelor_degree_got_offered_a_data_engineer/", "subreddit_subscribers": 102975, "created_utc": 1682649789.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For most (non-data) projects some level of testing happens but it's still in its infancy for our data engineering side of things. \n\nOur \"architecture\" is as follows: Bunch of microservices where Clojure streams into an object store. Pandas + SQLalchemy (legacy)  places a schema on everything (batch). Finally Polars + raw SQL gets it ready for our downstream tasks.\n\n&amp;#x200B;\n\n1. We validate a bit manually by skimming what comes in every so often (in terms of raw data) but this isn't tractable.\n2. When going from raw to schema and then to clean we *can* do assertions to check our data quality. For example, if object A that we're measuring had 250 events of type B after processing it should be the same. We kind of do this ad-hoc with group by's and joins but it would be cool if this is in some test suite somewhere. \n3. We have logs / notifications to tell us if a pipeline run failed. What we did not forsee is to add notifications if the pipeline came up empty handed (because upstream, out of organisation sources are failing).\n\nI think we want to monitor each source individually but also ensure that they're all \"complete\" in the sense that I'd like it to be visible if they're discrepancies in the events in source 1 vs source 2 vs source 3.\n\nAt the end of the day this is software and I see a bunch of pre-conditions, post-conditions and assertions I want to test. I've been resisting the temptation to reinvent a shitty version of the wheel by handrolling unit tests in Python that run raw-SQL / object store querys to validate that the data is parsed correctly. So far I've looked at great expectations and it seems to be close to what we need.", "author_fullname": "t2_8rjci796o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you handle tests, assertions and data quality management in your data pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131ar6j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682637385.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For most (non-data) projects some level of testing happens but it&amp;#39;s still in its infancy for our data engineering side of things. &lt;/p&gt;\n\n&lt;p&gt;Our &amp;quot;architecture&amp;quot; is as follows: Bunch of microservices where Clojure streams into an object store. Pandas + SQLalchemy (legacy)  places a schema on everything (batch). Finally Polars + raw SQL gets it ready for our downstream tasks.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;We validate a bit manually by skimming what comes in every so often (in terms of raw data) but this isn&amp;#39;t tractable.&lt;/li&gt;\n&lt;li&gt;When going from raw to schema and then to clean we &lt;em&gt;can&lt;/em&gt; do assertions to check our data quality. For example, if object A that we&amp;#39;re measuring had 250 events of type B after processing it should be the same. We kind of do this ad-hoc with group by&amp;#39;s and joins but it would be cool if this is in some test suite somewhere. &lt;/li&gt;\n&lt;li&gt;We have logs / notifications to tell us if a pipeline run failed. What we did not forsee is to add notifications if the pipeline came up empty handed (because upstream, out of organisation sources are failing).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I think we want to monitor each source individually but also ensure that they&amp;#39;re all &amp;quot;complete&amp;quot; in the sense that I&amp;#39;d like it to be visible if they&amp;#39;re discrepancies in the events in source 1 vs source 2 vs source 3.&lt;/p&gt;\n\n&lt;p&gt;At the end of the day this is software and I see a bunch of pre-conditions, post-conditions and assertions I want to test. I&amp;#39;ve been resisting the temptation to reinvent a shitty version of the wheel by handrolling unit tests in Python that run raw-SQL / object store querys to validate that the data is parsed correctly. So far I&amp;#39;ve looked at great expectations and it seems to be close to what we need.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131ar6j", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-One8023", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131ar6j/how_do_you_handle_tests_assertions_and_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131ar6j/how_do_you_handle_tests_assertions_and_data/", "subreddit_subscribers": 102975, "created_utc": 1682637385.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My past company uses Informatica for data catalog, but I\u2019m curious what popular tools are out there? How should I help my company to decide which data catalog to choose?", "author_fullname": "t2_1xrjwd6k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you choose which data catalog tool?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1319zta", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682635615.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My past company uses Informatica for data catalog, but I\u2019m curious what popular tools are out there? How should I help my company to decide which data catalog to choose?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1319zta", "is_robot_indexable": true, "report_reasons": null, "author": "Fasthandman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1319zta/how_do_you_choose_which_data_catalog_tool/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1319zta/how_do_you_choose_which_data_catalog_tool/", "subreddit_subscribers": 102975, "created_utc": 1682635615.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. \n\nOne of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.\n\nIn practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.\n\nWith that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?", "author_fullname": "t2_14aeem", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Reporting for application using a CQRS event store", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1313uw5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624515.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019ve been a data engineer for a good amount of time, but have not had to deal with CQRS until now. I have been tasked with creating some kind of data pipeline and reporting datastore for one of our applications. This application has a CQRS event store that I am considering getting data from. The application team would write a projection that basically publishes all events to something (probably Kinesis, but doesn\u2019t really matter for this conversation), and I would pickup data from there and do whatever I need with it. &lt;/p&gt;\n\n&lt;p&gt;One of the things that seems awesome about an event store is that it would contain all events going back to the beginning of time. This simplifies reporting because we could incrementally pull/process whatever data we need at any point in time and not have to go with a \u201ccollect all the data, worry about use case later\u201d approach that datalakes push towards.&lt;/p&gt;\n\n&lt;p&gt;In practice, the application team has mentioned the idea of snapshotting the event store for performance purposes, which means that something would have to happen in order to retain history. They\u2019ve also mentioned the idea of having a second event store that would not be truncated, but that doesn\u2019t seem like a great idea to me. They also have issues where events sometimes get hung or stuck, so I question the resilience of this in general.&lt;/p&gt;\n\n&lt;p&gt;With that said, I\u2019m curious what other data teams are doing to get data out of CQRS event stores. Is a projection that basically blasts all the data out to some streaming technology a reasonable option, or would it make more sense to write a projection for say each dimension table/fact table/report? What has worked or not worked for you when it comes to reporting on data that exists in a CQRS event store?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313uw5", "is_robot_indexable": true, "report_reasons": null, "author": "raginjason", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313uw5/reporting_for_application_using_a_cqrs_event_store/", "subreddit_subscribers": 102975, "created_utc": 1682624515.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_56xhg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically detecting breaking changes in SQL queries", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1310k7b", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1682618763.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tobikodata.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "1310k7b", "is_robot_indexable": true, "report_reasons": null, "author": "captaintobs", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1310k7b/automatically_detecting_breaking_changes_in_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://tobikodata.com/automatically-detecting-breaking-changes-in-sql-queries.html", "subreddit_subscribers": 102975, "created_utc": 1682618763.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I've heard bad things about Informatica, but I didn't even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?", "author_fullname": "t2_4rifsjav", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "IBM DB2 Warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130tg7s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682611175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently interviewed with a company and their ETL/data warehouse stack is Informatica + IBM DB2.  I&amp;#39;ve heard bad things about Informatica, but I didn&amp;#39;t even know IBM had a data warehouse offering? Does anyone have any insights about it? How does it compare to other modern options like Snowflake or BQ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130tg7s", "is_robot_indexable": true, "report_reasons": null, "author": "Techthrowaway2222888", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130tg7s/ibm_db2_warehouse/", "subreddit_subscribers": 102975, "created_utc": 1682611175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. \n\nDoes anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. \n\nAddl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.", "author_fullname": "t2_36yd2hgi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How does your company handle replication and CDC?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130ls2m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682601191.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team has been struggling for months with finding a good replication and CDC solution. We are trying to replicate data from an app database and a few third party databases to BigQuery (or blob storage then eventually BQ). When I joined we had just signed a license with Fivetran which has worked seamlessly for the third party DBs but is extremely costly and prone to failure when using it for our application database. &lt;/p&gt;\n\n&lt;p&gt;Does anyone have a good solution for replication and CDC from Postgres to GCP? Our team is currently evaluating Confluent + Kafka. Ideally we want a solution that can handle a wide variety of types, handle schema drift, and won\u2019t break the bank. &lt;/p&gt;\n\n&lt;p&gt;Addl context:\nI am a senior data engineer at a ~300 person startup. DE is relatively new at my company having been around for a year. We use GCP, Postgres, Airflow, and are a heavy Python shop.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130ls2m", "is_robot_indexable": true, "report_reasons": null, "author": "PROTECTyaNECK44", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130ls2m/how_does_your_company_handle_replication_and_cdc/", "subreddit_subscribers": 102975, "created_utc": 1682601191.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "On my project would like to apply some data profiling/data quality technics. I would like to identify columns on my data frame that have mixed type of values, like date and strings\u2013 PipeRider would work here? And what I can use to declare a desired mask for specific type of values and transform it if is not what I expect? I do not ingest it from a API, so no pydantic here, i guess. What would fit better \u2013dbt, gx or pandera?\n\n&amp;#x200B;\n\nextra. Theres an article, github repo where I can found some good example how to apply to real world projects?", "author_fullname": "t2_za6q9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What library would work better for my project for Data Quality?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_131gjsb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682653200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;On my project would like to apply some data profiling/data quality technics. I would like to identify columns on my data frame that have mixed type of values, like date and strings\u2013 PipeRider would work here? And what I can use to declare a desired mask for specific type of values and transform it if is not what I expect? I do not ingest it from a API, so no pydantic here, i guess. What would fit better \u2013dbt, gx or pandera?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;extra. Theres an article, github repo where I can found some good example how to apply to real world projects?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "131gjsb", "is_robot_indexable": true, "report_reasons": null, "author": "imloualvaro", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/131gjsb/what_library_would_work_better_for_my_project_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/131gjsb/what_library_would_work_better_for_my_project_for/", "subreddit_subscribers": 102975, "created_utc": 1682653200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I don't think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you guys store your audit logs and orchestration metadata in snowflake environment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1313y1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682624668.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think snowflake is right place to put auditing information and should be in some OLTP database. Want to hear from you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1313y1r", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1313y1r/where_do_you_guys_store_your_audit_logs_and/", "subreddit_subscribers": 102975, "created_utc": 1682624668.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I want to get the group\u2019s experiences on what they\u2019ve done and recommend.\n\n My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.\n\nWe\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.\n\nWe\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. \n\nWhat\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.\n\n[View Poll](https://www.reddit.com/poll/1312r30)", "author_fullname": "t2_6nrc61lj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DW Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312r30", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682653672.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622613.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to get the group\u2019s experiences on what they\u2019ve done and recommend.&lt;/p&gt;\n\n&lt;p&gt;My company is undergoing the implementation of a data warehouse. We\u2019re probably middle of the road sized data sets but have decided on using snowflake as our storage. Almost all our transformations will be done in snowflake stored procs as well.&lt;/p&gt;\n\n&lt;p&gt;We\u2019ve brought in some consultants for evaluations and each have come back with different options from only using views out of a data lake to more standard approaches.&lt;/p&gt;\n\n&lt;p&gt;We\u2019re considering two of their recommendations, 1 is a kimball star schema and the other is Data vault 2.0.\nI\u2019m fairly familiar with the star schema but one of the consultants is labeling this as a legacy approach and data vault is more modern. I don\u2019t quite understand it because data vault isn\u2019t new. Also, the end state of a vault presents the data in a star schema. &lt;/p&gt;\n\n&lt;p&gt;What\u2019s models are you guys moving forward with? Asa side note, I\u2019d also like to hear some specifics , pros cons, you\u2019ve seen with data vault.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1312r30\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "1312r30", "is_robot_indexable": true, "report_reasons": null, "author": "paulypavilion", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1682881813052, "options": [{"text": "Star schema", "id": "22770462"}, {"text": "Snowflake Schema", "id": "22770463"}, {"text": "Data Lake only", "id": "22770464"}, {"text": "Data lake +views", "id": "22770465"}, {"text": "Other", "id": "22770466"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 111, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312r30/dw_architecture/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/1312r30/dw_architecture/", "subreddit_subscribers": 102975, "created_utc": 1682622613.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was given a test task that definitely wants to test my skills in data processing optimization. I have some `.csv` data and need to run several transformations and count statistics on it. \n\nIf I just do stuff with plain pandas on `.csv` files it will be ineffective. Any good tips? I have few ideas like converting `.csv` to parquet or `avro`. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   \n\n\nThe data itself is small and in `.csv` but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.", "author_fullname": "t2_lyf92k", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Tips on processing 10M+ records locally with python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1312gxj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682622124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was given a test task that definitely wants to test my skills in data processing optimization. I have some &lt;code&gt;.csv&lt;/code&gt; data and need to run several transformations and count statistics on it. &lt;/p&gt;\n\n&lt;p&gt;If I just do stuff with plain pandas on &lt;code&gt;.csv&lt;/code&gt; files it will be ineffective. Any good tips? I have few ideas like converting &lt;code&gt;.csv&lt;/code&gt; to parquet or &lt;code&gt;avro&lt;/code&gt;. I never practiced pySpark or Dask though, are they good tools for local machine (I know that Spark works on clusters)? Would appreciate any advice or personal experience.   &lt;/p&gt;\n\n&lt;p&gt;The data itself is small and in &lt;code&gt;.csv&lt;/code&gt; but the task specifically mentions to code with the thought that it could contain tens of millions records eventually.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "1312gxj", "is_robot_indexable": true, "report_reasons": null, "author": "chelicerae-aureus", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1312gxj/tips_on_processing_10m_records_locally_with_python/", "subreddit_subscribers": 102975, "created_utc": 1682622124.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I'm a little stuck on how exactly to do this. \n\nFor example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?\n\nHopefully this isn't too many questions all at once. I guess I'm just overwhelmed with the sheer number of options.", "author_fullname": "t2_thw4nqfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you organize your extract/load pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130qgti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682608241.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting to get more into the habit of breaking up larger DAGs/pipelines into smaller ones to accomplish more specific tasks which makes it easier to debug and troubleshoot. However, I&amp;#39;m a little stuck on how exactly to do this. &lt;/p&gt;\n\n&lt;p&gt;For example, is it a best practice to create a separate DAG for every data source? Every table? If I have a bunch of SharePoint lists scattered around on a bunch of different sites, should I set up one pipeline that loops through everything at once, or break it out by site, or some other way? Should the logic for which data to extract be embedded in the code itself, or a config file along with the code, or with the orchestrator itself completely separate from the code?&lt;/p&gt;\n\n&lt;p&gt;Hopefully this isn&amp;#39;t too many questions all at once. I guess I&amp;#39;m just overwhelmed with the sheer number of options.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130qgti", "is_robot_indexable": true, "report_reasons": null, "author": "BoofThatShit720", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130qgti/how_do_you_organize_your_extractload_pipelines/", "subreddit_subscribers": 102975, "created_utc": 1682608241.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Im a super junior data engineer and my boss wanted this specificly:\n\nOur pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.  \nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.\n\nOur dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.\n\nBoss wants the pipeline to succeed, even though the dataflow should fail.\n\nCan anyone share some insight into which would be the best way to handle this?", "author_fullname": "t2_xcbzsw9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the best/easiest way to handle failed dataflows in a ADF pipeline?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130yccc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682616467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im a super junior data engineer and my boss wanted this specificly:&lt;/p&gt;\n\n&lt;p&gt;Our pipeline currently fetches a JSON from an API. Normaly, its between 1-5 mb.&lt;br/&gt;\nHowever, sometimes, because of some error externally, we get an empty JSON file. This file is around 10 byte in size.&lt;/p&gt;\n\n&lt;p&gt;Our dataflow, which is part of the pipeline, cannot accept empty JSON files as input, it fails because of malformed SCHEMA.&lt;/p&gt;\n\n&lt;p&gt;Boss wants the pipeline to succeed, even though the dataflow should fail.&lt;/p&gt;\n\n&lt;p&gt;Can anyone share some insight into which would be the best way to handle this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130yccc", "is_robot_indexable": true, "report_reasons": null, "author": "useyourname89", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130yccc/what_is_the_besteasiest_way_to_handle_failed/", "subreddit_subscribers": 102975, "created_utc": 1682616467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "i'm looking for advice to make my development more seamless.\n\nmy idea is, have my IDE tied to wsl (not via ssh -- vm option is out because of vpn), create a project folder, write whatever python scripts and other stuff into there...\n\nthen have it run in a docker container.\n\n&amp;#x200B;\n\nso, the project folder would include a dockerfile, which i would have to build to run the scripts.\n\nAre you guys updating dockerfiles, or are you just creating images out of your containers?\n\nAlso, i noticed it's not really possible to use pipenv when sshing into the container for an interpreter.  So, i'm just looking for detailed examples of how you all do it.", "author_fullname": "t2_5qteskd9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "what's your dev workflow like", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130tmqy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682611503.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;m looking for advice to make my development more seamless.&lt;/p&gt;\n\n&lt;p&gt;my idea is, have my IDE tied to wsl (not via ssh -- vm option is out because of vpn), create a project folder, write whatever python scripts and other stuff into there...&lt;/p&gt;\n\n&lt;p&gt;then have it run in a docker container.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;so, the project folder would include a dockerfile, which i would have to build to run the scripts.&lt;/p&gt;\n\n&lt;p&gt;Are you guys updating dockerfiles, or are you just creating images out of your containers?&lt;/p&gt;\n\n&lt;p&gt;Also, i noticed it&amp;#39;s not really possible to use pipenv when sshing into the container for an interpreter.  So, i&amp;#39;m just looking for detailed examples of how you all do it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130tmqy", "is_robot_indexable": true, "report_reasons": null, "author": "iseestupid", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130tmqy/whats_your_dev_workflow_like/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130tmqy/whats_your_dev_workflow_like/", "subreddit_subscribers": 102975, "created_utc": 1682611503.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In general which cloud vendor has best data engineering services?\n\nA little bit context about where this question is coming from. So I used MWAA and it has a lot of stability issues. So just wanted to know how is the product experience with different cloud vendors when it comes to data engineering ones", "author_fullname": "t2_q27tep12", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cloud vendor with best data engineering/data science tools", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130link", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682608946.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682600625.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In general which cloud vendor has best data engineering services?&lt;/p&gt;\n\n&lt;p&gt;A little bit context about where this question is coming from. So I used MWAA and it has a lot of stability issues. So just wanted to know how is the product experience with different cloud vendors when it comes to data engineering ones&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "130link", "is_robot_indexable": true, "report_reasons": null, "author": "__albatross", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130link/cloud_vendor_with_best_data_engineeringdata/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130link/cloud_vendor_with_best_data_engineeringdata/", "subreddit_subscribers": 102975, "created_utc": 1682600625.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work for a startup and we need to deploy a presto cluster that can support at least 100QPS. The presto cluster will act as a query engine on top of Pinot. What are the things to consider for the presto cluster deployment on an EC2 instance. The reason for using presto is to have a complete MYSQL_ANSI compatibility which Pinot doesn\u2019t provide right now. Any help is appreciated.", "author_fullname": "t2_icq6ey6g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with deploying Presto in production", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_130h06j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682591465.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work for a startup and we need to deploy a presto cluster that can support at least 100QPS. The presto cluster will act as a query engine on top of Pinot. What are the things to consider for the presto cluster deployment on an EC2 instance. The reason for using presto is to have a complete MYSQL_ANSI compatibility which Pinot doesn\u2019t provide right now. Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "130h06j", "is_robot_indexable": true, "report_reasons": null, "author": "Direct-Wrongdoer-939", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/130h06j/need_help_with_deploying_presto_in_production/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/130h06j/need_help_with_deploying_presto_in_production/", "subreddit_subscribers": 102975, "created_utc": 1682591465.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}