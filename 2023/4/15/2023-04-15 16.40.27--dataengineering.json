{"kind": "Listing", "data": {"after": null, "dist": 21, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_16q5j0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Exporting to excel is always a people pleaser...", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12m8ml7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "ups": 614, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 614, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/vEaDT3jzNOo03b2pQKsXWm6Ub5wKYYAGzWEdyltISTY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681498344.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/6vtglxi2cwta1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/6vtglxi2cwta1.png?auto=webp&amp;v=enabled&amp;s=9cf19b47ab4a682995c2dc993bbab60faf5e6678", "width": 1182, "height": 1280}, "resolutions": [{"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=74e103d9a117ab2ea94bb361002ddf3cca4c34c9", "width": 108, "height": 116}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e62c682e16c902679a397e81e186cb076821ea6", "width": 216, "height": 233}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=61d45e345641564675e100b9818ec876e9286225", "width": 320, "height": 346}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e04a031c8f71e72bd1a6cb58439184f1045ed49", "width": 640, "height": 693}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8f1685f7d00c688a18cbc5efa2096749d6948205", "width": 960, "height": 1039}, {"url": "https://preview.redd.it/6vtglxi2cwta1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=32cd0519cfc66341a05a9ee9fb848816a3bbcb2c", "width": 1080, "height": 1169}], "variants": {}, "id": "4T_doMPlm1O7dNGlmOIpToj64res7j58NG4Xkccej7A"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12m8ml7", "is_robot_indexable": true, "report_reasons": null, "author": "audiologician", "discussion_type": null, "num_comments": 52, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12m8ml7/exporting_to_excel_is_always_a_people_pleaser/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/6vtglxi2cwta1.png", "subreddit_subscribers": 99214, "created_utc": 1681498344.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_7e04ujnq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "One day we\u2019ll get the respect we deserve \ud83e\udd72", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12meohj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": "transparent", "ups": 132, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "08789422-ac9d-11eb-aade-0e32c0bdd4fb", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 132, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/GmsK2oEav88xxMD7QOlgMfPZV2WFRX1Bpqxo7d7bKYU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681508747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/pqafjpltoyta1.jpg", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?auto=webp&amp;v=enabled&amp;s=11ef40cedf5bfac7083596aa648a9f410b82384c", "width": 1098, "height": 1226}, "resolutions": [{"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=66ff07132e1bc0507ed0cd9c59a3ed0f161d6a15", "width": 108, "height": 120}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=12448513d4d5ccd06f7d98fc8b445c195b90b606", "width": 216, "height": 241}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a74f3a1d8588ed2465d697fc918633baf921f5b2", "width": 320, "height": 357}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6b41b4d9a92b0ae85633b9cfe33f9248e6720cdd", "width": 640, "height": 714}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=950423ddfb43696d3aee15c1b088be56fd774027", "width": 960, "height": 1071}, {"url": "https://preview.redd.it/pqafjpltoyta1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=381d5638798b83bbfc888e8bb60a898876b27f3e", "width": 1080, "height": 1205}], "variants": {}, "id": "jh4DkOdZ72CqqpKCLgwjLYq3-IWOCCRc5lIMe3RgrhI"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Big Data Engineer That Broke All ETLs", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12meohj", "is_robot_indexable": true, "report_reasons": null, "author": "ThatGrayZ", "discussion_type": null, "num_comments": 35, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12meohj/one_day_well_get_the_respect_we_deserve/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/pqafjpltoyta1.jpg", "subreddit_subscribers": 99214, "created_utc": 1681508747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Test, fake, abstract, and synthetic data are becoming increasingly popular these days, especially for different types of testing. That's why we've put together some useful tips for generating test data using only SQL in our latest blog post - https://www.synthesized.io/post/test-data-generation-there-and-back\n\nWe also briefly mention [TDK](https://docs.synthesized.io/tdk/latest/?utm_source=reddit&amp;utm_medium=devrel&amp;utm_campaign=datagen) data tool by [Synthesized](https://www.synthesized.io/?utm_source=reddit&amp;utm_medium=devrel&amp;utm_campaign=datagen) at the very end of the post.\n\nFeel free to provide any critiques or feedback in the comments section!", "author_fullname": "t2_blc2ww3r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Test Data Generation - DIY", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12myeg9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681556059.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Test, fake, abstract, and synthetic data are becoming increasingly popular these days, especially for different types of testing. That&amp;#39;s why we&amp;#39;ve put together some useful tips for generating test data using only SQL in our latest blog post - &lt;a href=\"https://www.synthesized.io/post/test-data-generation-there-and-back\"&gt;https://www.synthesized.io/post/test-data-generation-there-and-back&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We also briefly mention &lt;a href=\"https://docs.synthesized.io/tdk/latest/?utm_source=reddit&amp;amp;utm_medium=devrel&amp;amp;utm_campaign=datagen\"&gt;TDK&lt;/a&gt; data tool by &lt;a href=\"https://www.synthesized.io/?utm_source=reddit&amp;amp;utm_medium=devrel&amp;amp;utm_campaign=datagen\"&gt;Synthesized&lt;/a&gt; at the very end of the post.&lt;/p&gt;\n\n&lt;p&gt;Feel free to provide any critiques or feedback in the comments section!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?auto=webp&amp;v=enabled&amp;s=5afa9f774adce8655a42a016ec74aa8cf57d3424", "width": 1000, "height": 562}, "resolutions": [{"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d4e2ee296cba0e3e8db5a5c46da148eb5fb012d", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=446b64730dc3df832b0def72b70726c4427b92e3", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df437f503e6c77d5aa91d56060b1be66b83b725f", "width": 320, "height": 179}, {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70f6e11efa878f0d2328754fdbabc760e529802f", "width": 640, "height": 359}, {"url": "https://external-preview.redd.it/UYFNvLXxcxDK03lAPU_u1cTfFbsDqLcXT-GF1kOGw5M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=83b5bd900c3b10c41fd95372afa30694b9af7c58", "width": 960, "height": 539}], "variants": {}, "id": "4XhbEotXIgY7_g3Io7-69b1-eAP8t6b-UxPWBNiJols"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12myeg9", "is_robot_indexable": true, "report_reasons": null, "author": "mgramin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12myeg9/test_data_generation_diy/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12myeg9/test_data_generation_diy/", "subreddit_subscribers": 99214, "created_utc": 1681556059.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI'm loooking for advice on switching to a more modern data architecture/stack.\nI'm a senior data analyst and currently building analytics capabilities for a bank \u00een the fraud domain. \nI want to challenge the current reference arhitecture from datalake to data lakehouse.\nWe have the datalake on-premise using IBM tools like Netezza and also a analytics platform build on open source where we do a lot of data science stuff (similar with Databricks)\nHow do I convinge my architects to build a data lakehouse on our analytics platform?\nI'm working on creating some slides about the data lakehouse benefits and share with them, what should I keep in mind when discussing this?", "author_fullname": "t2_6l3fp7bm", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Switching to data lakehouse?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mr7or", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681536224.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m loooking for advice on switching to a more modern data architecture/stack.\nI&amp;#39;m a senior data analyst and currently building analytics capabilities for a bank \u00een the fraud domain. \nI want to challenge the current reference arhitecture from datalake to data lakehouse.\nWe have the datalake on-premise using IBM tools like Netezza and also a analytics platform build on open source where we do a lot of data science stuff (similar with Databricks)\nHow do I convinge my architects to build a data lakehouse on our analytics platform?\nI&amp;#39;m working on creating some slides about the data lakehouse benefits and share with them, what should I keep in mind when discussing this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mr7or", "is_robot_indexable": true, "report_reasons": null, "author": "Wise_Solid1904", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mr7or/switching_to_data_lakehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mr7or/switching_to_data_lakehouse/", "subreddit_subscribers": 99214, "created_utc": 1681536224.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How would you achieve this?   \n\n\nFor the brave, I will add some caveats, but I am still interested in your approach in the simple case described in the title.\n\nCaveats:\n\n\\- Postgres uses table partitioning (this means, that the WAL changes are associated with the partition tables and not the top-level table)\n\n\\- No dupes in Redshift. Redshift doesn't enforce primary key uniqueness, and so you may have duplicate entries for the same ID. That is undesirable.\n\n\\- Can it be done in AWS only?", "author_fullname": "t2_3aird6b7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Move data from Postgres to Redshift with CDC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m2ukb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681489717.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How would you achieve this?   &lt;/p&gt;\n\n&lt;p&gt;For the brave, I will add some caveats, but I am still interested in your approach in the simple case described in the title.&lt;/p&gt;\n\n&lt;p&gt;Caveats:&lt;/p&gt;\n\n&lt;p&gt;- Postgres uses table partitioning (this means, that the WAL changes are associated with the partition tables and not the top-level table)&lt;/p&gt;\n\n&lt;p&gt;- No dupes in Redshift. Redshift doesn&amp;#39;t enforce primary key uniqueness, and so you may have duplicate entries for the same ID. That is undesirable.&lt;/p&gt;\n\n&lt;p&gt;- Can it be done in AWS only?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12m2ukb", "is_robot_indexable": true, "report_reasons": null, "author": "agsilvio", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12m2ukb/move_data_from_postgres_to_redshift_with_cdc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12m2ukb/move_data_from_postgres_to_redshift_with_cdc/", "subreddit_subscribers": 99214, "created_utc": 1681489717.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi team,\n\nWe are extracting Salesforce data using Fivetran to Snowflake and we have problems with orphans - rows hard deleted in the app are not deleted in our tables in snowflake and will always remain there...\n\nSo correct me if I am wrong but salesforce connectors cant spot HARD DELETED records.\n\nIn other words, if records are soft deleted in the Salesforce app , API calls will see them as rows with IS\\_DELETED = true flag. However if the row is hard deleted from the SF app rest api aka integration tools wont handle that and such 'orphans' will remain in our table using delta loads.\n\nWe end up with the situation where our 'analytical views' show 'orphans'- rows of data that no longer exist in the app.\n\n**How could we set a process to make sure our data in EDW = SF App as much as possible ;)?**\n\nWe use Fivetran as extraction tool and Snowflake as EDW.\n\nMy thought is to DROP / TRUNCACE SF tables in Snowflake and use rest api to call fivetran and force historical resync(**full reload**) once a week on saturday (off work not to impact business). This way we will have weekly process to make sure that our data is 'cleaned once every week'\n\n**Pros:**\n\nData cleansed once a week - acceptable by business\n\n**Cons:**\n\nif by any chance something will fail during 'historical resync' and our ladning table in Snowflake will remain empty, this might impact our 'analytical views' where they could show 'no data'\n\n&amp;#x200B;\n\nIs there any easier, better way, just to ensure our data will always be there ?\n\nI was thinking of RAW / STAGE table / schema...but I am getting loops in my thought process.\n\n&amp;#x200B;\n\nSomething like:\n\n1.Create RAW schema - landing point, for all Salesforce objects we extract from Fivetran. Still, DROP/TRUNCATE tables every saturday and perform full load to make sure we avoid orphans.\n\n2.Have STAGE schema. In order to MERGE data from RAW into STAGE (INSERT,UPDATE,DELETE)\n\n3.And I am getting lost here.....because if RAW will get truncated every saturday and will fail to extract data out of SF for god knows why reason....then merge (DELETE command will wipe entire STAGE table because RAW is empty).\n\n&amp;#x200B;\n\nGuys any ideas?", "author_fullname": "t2_do9wxbfu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Salesforce data extraction how to handle hard deletes in the app?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n0xnh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681562336.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681562124.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi team,&lt;/p&gt;\n\n&lt;p&gt;We are extracting Salesforce data using Fivetran to Snowflake and we have problems with orphans - rows hard deleted in the app are not deleted in our tables in snowflake and will always remain there...&lt;/p&gt;\n\n&lt;p&gt;So correct me if I am wrong but salesforce connectors cant spot HARD DELETED records.&lt;/p&gt;\n\n&lt;p&gt;In other words, if records are soft deleted in the Salesforce app , API calls will see them as rows with IS_DELETED = true flag. However if the row is hard deleted from the SF app rest api aka integration tools wont handle that and such &amp;#39;orphans&amp;#39; will remain in our table using delta loads.&lt;/p&gt;\n\n&lt;p&gt;We end up with the situation where our &amp;#39;analytical views&amp;#39; show &amp;#39;orphans&amp;#39;- rows of data that no longer exist in the app.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How could we set a process to make sure our data in EDW = SF App as much as possible ;)?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We use Fivetran as extraction tool and Snowflake as EDW.&lt;/p&gt;\n\n&lt;p&gt;My thought is to DROP / TRUNCACE SF tables in Snowflake and use rest api to call fivetran and force historical resync(&lt;strong&gt;full reload&lt;/strong&gt;) once a week on saturday (off work not to impact business). This way we will have weekly process to make sure that our data is &amp;#39;cleaned once every week&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Pros:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Data cleansed once a week - acceptable by business&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Cons:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;if by any chance something will fail during &amp;#39;historical resync&amp;#39; and our ladning table in Snowflake will remain empty, this might impact our &amp;#39;analytical views&amp;#39; where they could show &amp;#39;no data&amp;#39;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is there any easier, better way, just to ensure our data will always be there ?&lt;/p&gt;\n\n&lt;p&gt;I was thinking of RAW / STAGE table / schema...but I am getting loops in my thought process.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Something like:&lt;/p&gt;\n\n&lt;p&gt;1.Create RAW schema - landing point, for all Salesforce objects we extract from Fivetran. Still, DROP/TRUNCATE tables every saturday and perform full load to make sure we avoid orphans.&lt;/p&gt;\n\n&lt;p&gt;2.Have STAGE schema. In order to MERGE data from RAW into STAGE (INSERT,UPDATE,DELETE)&lt;/p&gt;\n\n&lt;p&gt;3.And I am getting lost here.....because if RAW will get truncated every saturday and will fail to extract data out of SF for god knows why reason....then merge (DELETE command will wipe entire STAGE table because RAW is empty).&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Guys any ideas?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12n0xnh", "is_robot_indexable": true, "report_reasons": null, "author": "87keicam", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n0xnh/salesforce_data_extraction_how_to_handle_hard/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12n0xnh/salesforce_data_extraction_how_to_handle_hard/", "subreddit_subscribers": 99214, "created_utc": 1681562124.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "cc /u/cosmicBb0y", "author_fullname": "t2_3qlqubb2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "ArjanCodes: How to Use Pandas With Pandera to Validate Your Data in Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12malx1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "ups": 5, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-tU7fuUiq7w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to Use Pandas With Pandera to Validate Your Data in Python\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to Use Pandas With Pandera to Validate Your Data in Python", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-tU7fuUiq7w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to Use Pandas With Pandera to Validate Your Data in Python\"&gt;&lt;/iframe&gt;", "author_name": "ArjanCodes", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/-tU7fuUiq7w/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ArjanCodes"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-tU7fuUiq7w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to Use Pandas With Pandera to Validate Your Data in Python\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12malx1", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/pJryRzjFi8keLzZTP6aHwFCOIIEqur6MaBTgKpOZPO4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681501223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;cc &lt;a href=\"/u/cosmicBb0y\"&gt;/u/cosmicBb0y&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/-tU7fuUiq7w", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/KQk5h-3dDIbI0zxJoeYHj7YOnAcc5X76Rjd-taBTIbQ.jpg?auto=webp&amp;v=enabled&amp;s=9a94916997565943bda38b1a61e337546537998d", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/KQk5h-3dDIbI0zxJoeYHj7YOnAcc5X76Rjd-taBTIbQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6220ed74050f390859a25279a58f6417e4c1f93e", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/KQk5h-3dDIbI0zxJoeYHj7YOnAcc5X76Rjd-taBTIbQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c8e62a1f6e14b71c2f3bcfd371e8f5c35d7b5a33", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/KQk5h-3dDIbI0zxJoeYHj7YOnAcc5X76Rjd-taBTIbQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8a44a3a4b66cf77773d1f41656b54d1b58099740", "width": 320, "height": 240}], "variants": {}, "id": "bEK7On-ViuRVvW7es_cui0WHxoZKbaxvhRL-8wwjTxc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12malx1", "is_robot_indexable": true, "report_reasons": null, "author": "EarthGoddessDude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12malx1/arjancodes_how_to_use_pandas_with_pandera_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/-tU7fuUiq7w", "subreddit_subscribers": 99214, "created_utc": 1681501223.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "How to Use Pandas With Pandera to Validate Your Data in Python", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/-tU7fuUiq7w?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"How to Use Pandas With Pandera to Validate Your Data in Python\"&gt;&lt;/iframe&gt;", "author_name": "ArjanCodes", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/-tU7fuUiq7w/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@ArjanCodes"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I see a Lead position with a range from 115k to 230K, \n\nHow many YoE does one need to max out that 230K, do DE really make these kind of money? Assuming it's 230K base.\n\nAlso anyone here working here for CVS, or went through their interview process, how hard is it to pass, get an offer and working there?\n\nThanks.", "author_fullname": "t2_1411ira9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CVS - Lead DE 115K to 230K?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mysus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681557074.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I see a Lead position with a range from 115k to 230K, &lt;/p&gt;\n\n&lt;p&gt;How many YoE does one need to max out that 230K, do DE really make these kind of money? Assuming it&amp;#39;s 230K base.&lt;/p&gt;\n\n&lt;p&gt;Also anyone here working here for CVS, or went through their interview process, how hard is it to pass, get an offer and working there?&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12mysus", "is_robot_indexable": true, "report_reasons": null, "author": "wisegeek57", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mysus/cvs_lead_de_115k_to_230k/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mysus/cvs_lead_de_115k_to_230k/", "subreddit_subscribers": 99214, "created_utc": 1681557074.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working for a company with a very unique use case in that my company receives data from clients on a weekly basis that is... generally the same population over and over but with slight variations in certain fields depending.  Mostly relates to people data, so changes in PII.  Our current tech stack and architecture is... bad.  It's a mostly MS Azure stack using SSIS between stages and WAY too much manual manipulation at each stage.  So much so that our \"bronze\" stage is an excel plug in that pulls from an ob prem ms sql server...and yes we have data sets that exceed excel data limits.\n\nI'm pushing to make major changes to the flow, but wanted to get a brain check before going to our CTO. I'm still limited to our Azure stack... maybe... hopefully the design can be tech agnostic so I can eventually push to replace parts...  \n\nMy biggest problem is that we use dynamics 365 for certain parts of business and right now its our \"gold\" stage.  We've bloated the he'll out of it.  We are moving to using Synapse for analytics but everything is still funneling through CRM first.  \n\nMy biggest recommendation is going to be to push through Synapse first and build a better \"silver\" stage there to manage transformations and enrichment of data that is automated to remove the manual efforts and only feed to the CRM what it needs to do its thing and remove the bloat.  Then use a microservices approach for periphery systems through apps and feed back most results to crm as needed and also back to a proper gold stage to use for analytics and BI reporting.\n\nClearly there is a lot of detail to fill in, but... am I off base here?  Are there other recommendations  i should consider?", "author_fullname": "t2_2v1p3nx2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CRM as source of truth?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n1a3j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681562861.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working for a company with a very unique use case in that my company receives data from clients on a weekly basis that is... generally the same population over and over but with slight variations in certain fields depending.  Mostly relates to people data, so changes in PII.  Our current tech stack and architecture is... bad.  It&amp;#39;s a mostly MS Azure stack using SSIS between stages and WAY too much manual manipulation at each stage.  So much so that our &amp;quot;bronze&amp;quot; stage is an excel plug in that pulls from an ob prem ms sql server...and yes we have data sets that exceed excel data limits.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m pushing to make major changes to the flow, but wanted to get a brain check before going to our CTO. I&amp;#39;m still limited to our Azure stack... maybe... hopefully the design can be tech agnostic so I can eventually push to replace parts...  &lt;/p&gt;\n\n&lt;p&gt;My biggest problem is that we use dynamics 365 for certain parts of business and right now its our &amp;quot;gold&amp;quot; stage.  We&amp;#39;ve bloated the he&amp;#39;ll out of it.  We are moving to using Synapse for analytics but everything is still funneling through CRM first.  &lt;/p&gt;\n\n&lt;p&gt;My biggest recommendation is going to be to push through Synapse first and build a better &amp;quot;silver&amp;quot; stage there to manage transformations and enrichment of data that is automated to remove the manual efforts and only feed to the CRM what it needs to do its thing and remove the bloat.  Then use a microservices approach for periphery systems through apps and feed back most results to crm as needed and also back to a proper gold stage to use for analytics and BI reporting.&lt;/p&gt;\n\n&lt;p&gt;Clearly there is a lot of detail to fill in, but... am I off base here?  Are there other recommendations  i should consider?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12n1a3j", "is_robot_indexable": true, "report_reasons": null, "author": "No-Current-7884", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n1a3j/crm_as_source_of_truth/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12n1a3j/crm_as_source_of_truth/", "subreddit_subscribers": 99214, "created_utc": 1681562861.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all! Hope you're all enjoying Saturday :)\n\nI recently accepted a junior DE position starting in 7 weeks from today. The job description was fairly general with the requirements including only python proficiency and PySpark being a desirable skill and familiarity with relational database structures.\n\nThe company had a couple of behavioural interviews and then a take home take on analysing a dataset which I did with Pandas and then prepared a report summarising my findings. The interview then was basically about my background and how I approached the data analysis task, but nothing about PySpark or databases.\n\nI am looking for some advice on how I can familiarise myself with PySpark enough to not make a fool out of myself on day 1 and any simple data pipeline tutorials which are recommended by the community.\n\nI came across the following 2 books which have decent reviews:\n\n* [Essential PySpark for Scalable Data Analytics](https://www.amazon.in/Essential-PySpark-Scalable-Data-Analytics-ebook/dp/B098C3PPSJ/ref=sr_1_1?crid=1YY53KSSBOYBF&amp;keywords=Essential+PySpark+for+Scalable+Data+Analytics&amp;qid=1681459147&amp;sprefix=essential+pyspark+for+scalable+data+analytics%2Caps%2C386&amp;sr=8-1#customerReviews)\n* [Data Algorithms with Spark](https://www.amazon.in/Data-Algorithms-Spark-Patterns-Grayscale/dp/9355420781/ref=cm_cr_arp_d_product_top?ie=UTF8)\n\nThank you very much in advance and any additional insight or tips would be greatly appreciated!\n\nPS: I am self-taught and over the past year have picked up Python and SQL and have worked with Pandas as well.", "author_fullname": "t2_ipo54", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help! I got a junior DE role!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mxv8x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681554623.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all! Hope you&amp;#39;re all enjoying Saturday :)&lt;/p&gt;\n\n&lt;p&gt;I recently accepted a junior DE position starting in 7 weeks from today. The job description was fairly general with the requirements including only python proficiency and PySpark being a desirable skill and familiarity with relational database structures.&lt;/p&gt;\n\n&lt;p&gt;The company had a couple of behavioural interviews and then a take home take on analysing a dataset which I did with Pandas and then prepared a report summarising my findings. The interview then was basically about my background and how I approached the data analysis task, but nothing about PySpark or databases.&lt;/p&gt;\n\n&lt;p&gt;I am looking for some advice on how I can familiarise myself with PySpark enough to not make a fool out of myself on day 1 and any simple data pipeline tutorials which are recommended by the community.&lt;/p&gt;\n\n&lt;p&gt;I came across the following 2 books which have decent reviews:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.in/Essential-PySpark-Scalable-Data-Analytics-ebook/dp/B098C3PPSJ/ref=sr_1_1?crid=1YY53KSSBOYBF&amp;amp;keywords=Essential+PySpark+for+Scalable+Data+Analytics&amp;amp;qid=1681459147&amp;amp;sprefix=essential+pyspark+for+scalable+data+analytics%2Caps%2C386&amp;amp;sr=8-1#customerReviews\"&gt;Essential PySpark for Scalable Data Analytics&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.amazon.in/Data-Algorithms-Spark-Patterns-Grayscale/dp/9355420781/ref=cm_cr_arp_d_product_top?ie=UTF8\"&gt;Data Algorithms with Spark&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thank you very much in advance and any additional insight or tips would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;PS: I am self-taught and over the past year have picked up Python and SQL and have worked with Pandas as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mxv8x", "is_robot_indexable": true, "report_reasons": null, "author": "darthvardhan95", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mxv8x/help_i_got_a_junior_de_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mxv8x/help_i_got_a_junior_de_role/", "subreddit_subscribers": 99214, "created_utc": 1681554623.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi there, \n\nI was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.\n\nAs I've had no whatsoever experience with BigQuery and didn't really know where or how to start, I asked you guys around here some questions:\n\n* [What should I know before using BigQuery, having traditional MySQL knowledge?](https://www.reddit.com/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/)\n* [How should I understand query limitations in BigQuery?](https://www.reddit.com/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/)\n* [Using redis to batch data from various sources and then bulk insert into BigQuery, is this common?](https://www.reddit.com/r/dataengineering/comments/12dh2ht/using_redis_to_batch_data_from_various_sources/)\n\nWith the help and lead from you folks, I think I finally managed to get the bigger picture. At the same time, I've been writing things down which will hopefully help other software developers or traditional database users get into BigQuery, without going through several courses and documentation to get this \"Ahaaa, that's what it is used for, how it works, and how we should use it\" moment.\n\nI've written such down [as a guide here (preview)](https://hashnode.com/preview/643975a89ce48a000fb1867b), and would love to have some feedback (if possible), just to make sure that I'm not spreading misinformation.\n\nAnyway, thanks so much to this subreddit for all the help you have already given!", "author_fullname": "t2_n726hnv4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[Need feedback] I wrote a guide about the fundamentals of BigQuery for software developers &amp; traditional database users", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mccxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681504289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, &lt;/p&gt;\n\n&lt;p&gt;I was tasked with implementing BigQuery to our web application and sending data to it for our analytics team.&lt;/p&gt;\n\n&lt;p&gt;As I&amp;#39;ve had no whatsoever experience with BigQuery and didn&amp;#39;t really know where or how to start, I asked you guys around here some questions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/12cbbw0/what_should_i_know_before_using_bigquery_having/\"&gt;What should I know before using BigQuery, having traditional MySQL knowledge?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/12cicp2/how_should_i_understand_query_limitations_in/\"&gt;How should I understand query limitations in BigQuery?&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.reddit.com/r/dataengineering/comments/12dh2ht/using_redis_to_batch_data_from_various_sources/\"&gt;Using redis to batch data from various sources and then bulk insert into BigQuery, is this common?&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With the help and lead from you folks, I think I finally managed to get the bigger picture. At the same time, I&amp;#39;ve been writing things down which will hopefully help other software developers or traditional database users get into BigQuery, without going through several courses and documentation to get this &amp;quot;Ahaaa, that&amp;#39;s what it is used for, how it works, and how we should use it&amp;quot; moment.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve written such down &lt;a href=\"https://hashnode.com/preview/643975a89ce48a000fb1867b\"&gt;as a guide here (preview)&lt;/a&gt;, and would love to have some feedback (if possible), just to make sure that I&amp;#39;m not spreading misinformation.&lt;/p&gt;\n\n&lt;p&gt;Anyway, thanks so much to this subreddit for all the help you have already given!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?auto=webp&amp;v=enabled&amp;s=62defc0be251764d5b096ae39b8f3a0b70085635", "width": 1549, "height": 840}, "resolutions": [{"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=125555f86fe45e004504f7773a2f63e82f1c6bd4", "width": 108, "height": 58}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1c8335002e4f1dd15e3155836126761a017f92c", "width": 216, "height": 117}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac474d0f76b6dc0b062dca3efe72a57762ee3751", "width": 320, "height": 173}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2386e612d99fc461faac9b23f8e31365cf50333", "width": 640, "height": 347}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f88e776b52d428bc55ecc1e19740a898b4d82fb8", "width": 960, "height": 520}, {"url": "https://external-preview.redd.it/8OvFWeDpodmiwEWwwL-nlJxeGVB5QZvLbIhWtfKapmo.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e695de79f1de20268e94f94b0f7a180c17e4d9b9", "width": 1080, "height": 585}], "variants": {}, "id": "Sv3ekwo8yyXSEkV18TtP4eDCKZ_ZqF5Ig7c267Z19ow"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mccxv", "is_robot_indexable": true, "report_reasons": null, "author": "Fit-Swordfish-5533", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mccxv/need_feedback_i_wrote_a_guide_about_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mccxv/need_feedback_i_wrote_a_guide_about_the/", "subreddit_subscribers": 99214, "created_utc": 1681504289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_17hfml", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Business Intelligence 101: Data within Multidimensional View - Part 2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 118, "top_awarded_type": null, "hide_score": false, "name": "t3_12mo5oe", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/B7NDWZLQaKnSVHRZ5rOhBEd2SmxpB816jc5X8EEw2O8.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681528836.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "datafriends.co", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.datafriends.co/categories/business-intelligence/business-intelligence-101-data-within-multidimensional-view-part-2/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?auto=webp&amp;v=enabled&amp;s=301554cca1dc6a23e5d53ec021f47f630abc40ed", "width": 1264, "height": 1073}, "resolutions": [{"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abe2774539d625801cfe30db6a52d76dd7cc49f5", "width": 108, "height": 91}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6fcdbe89d7c6e7e5e08cab15e9f97890f2162741", "width": 216, "height": 183}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3025c5ad8f96b061aa86f3385dc4e408a4cbaf0", "width": 320, "height": 271}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47c849a7937894bf66c9ea75fdff3ae0331bd332", "width": 640, "height": 543}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=484f2a81494fde0e4f96d9d9c04e3c210772780a", "width": 960, "height": 814}, {"url": "https://external-preview.redd.it/-vj2E3id8ebBR3Z1cfuqljJgFOQr6nGsefTY14fs-nw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=67eb40c789eec371fa96be888ddcf4bc101a7522", "width": 1080, "height": 916}], "variants": {}, "id": "9cmMtWi_38dCpM-4sDVMXNrBJgGYfdInfQ4omvFbmwc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12mo5oe", "is_robot_indexable": true, "report_reasons": null, "author": "harlkwin", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12mo5oe/business_intelligence_101_data_within/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.datafriends.co/categories/business-intelligence/business-intelligence-101-data-within-multidimensional-view-part-2/", "subreddit_subscribers": 99214, "created_utc": 1681528836.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need some guidance on who should be involved and what should be the responsibilities of this group? \n\nMembers:\nTechnical writer\nEngineer architect\nData engineer\nData Product mgr \n\nResponsibilities:\nSet guidelines for API standard format \nSet guidelines for standard field field format, definitions etc \nSign off on any APIs before they are release \n\nWhat else am I missing?", "author_fullname": "t2_6ighbrn4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need help with an API committee", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mgeab", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681512102.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need some guidance on who should be involved and what should be the responsibilities of this group? &lt;/p&gt;\n\n&lt;p&gt;Members:\nTechnical writer\nEngineer architect\nData engineer\nData Product mgr &lt;/p&gt;\n\n&lt;p&gt;Responsibilities:\nSet guidelines for API standard format \nSet guidelines for standard field field format, definitions etc \nSign off on any APIs before they are release &lt;/p&gt;\n\n&lt;p&gt;What else am I missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mgeab", "is_robot_indexable": true, "report_reasons": null, "author": "Particular-Essay-361", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mgeab/need_help_with_an_api_committee/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mgeab/need_help_with_an_api_committee/", "subreddit_subscribers": 99214, "created_utc": 1681512102.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nLooking for some suggestions. We get Kafka data landed directly in Snowflake. A table per event with two variant columns:\n\n- meta_data;\n- event_content.\n\nThe first contains a json payload with partition, offset, timestamp. The second contains the actual payload.\n\nThe first step is to flatten the data, which is performed as a view. We then have an incrementally loaded table of the flattened data, that uses a QUALIFY statement to remove duplicates, as Kafka will deliver at least once.\n\nWith the timestamp being contained in a variant column, how much overhead are we likely eating up each time we need to incrementally load our table? Has this been architected badly? It was done before my time. It might be fine, but it seems we might be chewing through a fair amount of compute as a result of this design.", "author_fullname": "t2_87zfi59a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advise on incremental process of Kafka data on Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m2azk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681488660.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Looking for some suggestions. We get Kafka data landed directly in Snowflake. A table per event with two variant columns:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;meta_data;&lt;/li&gt;\n&lt;li&gt;event_content.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The first contains a json payload with partition, offset, timestamp. The second contains the actual payload.&lt;/p&gt;\n\n&lt;p&gt;The first step is to flatten the data, which is performed as a view. We then have an incrementally loaded table of the flattened data, that uses a QUALIFY statement to remove duplicates, as Kafka will deliver at least once.&lt;/p&gt;\n\n&lt;p&gt;With the timestamp being contained in a variant column, how much overhead are we likely eating up each time we need to incrementally load our table? Has this been architected badly? It was done before my time. It might be fine, but it seems we might be chewing through a fair amount of compute as a result of this design.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12m2azk", "is_robot_indexable": true, "report_reasons": null, "author": "Cool_Telephone", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12m2azk/advise_on_incremental_process_of_kafka_data_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12m2azk/advise_on_incremental_process_of_kafka_data_on/", "subreddit_subscribers": 99214, "created_utc": 1681488660.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_5cgbpdbh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Diving into the Future: Serverless Data Warehouse Platform Architecture", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 59, "top_awarded_type": null, "hide_score": true, "name": "t3_12n6shn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Dpm5A_UJvVPcJoU5Ej3aqiwZa26424uqj1Iffv6efQY.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681571540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "databend.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.databend.com/blog/2023/04/13/diving-into-the-future-serverless-data-warehouse-platform-architecture/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?auto=webp&amp;v=enabled&amp;s=91391bd25d84a560236232ee3c0ad4c936cc33a4", "width": 1876, "height": 799}, "resolutions": [{"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5e8ce9a98a80da9bf5ab68a5d868c99632b78b5", "width": 108, "height": 45}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c2f2c585db55edd51e07bee86ac9d4beebfd0692", "width": 216, "height": 91}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7129633690960b716cdd465a6f0b95e07d2b988c", "width": 320, "height": 136}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4816b53b13ace18152784d12856d82c950f651b4", "width": 640, "height": 272}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d64784ed0d0911d035ce370630a05ea285dbc4d", "width": 960, "height": 408}, {"url": "https://external-preview.redd.it/2i4y4ur_w9Zm5GLTTCxPmFxtidjIA37wypTJic1hGA8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bda5ec9483c46ab209ebd0fd852a6c2e306e166f", "width": 1080, "height": 459}], "variants": {}, "id": "rPO5lJ3zPMxPH3wNXtvkYTcOqg6BHOpTKgvVTe_M61k"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12n6shn", "is_robot_indexable": true, "report_reasons": null, "author": "PsiACE", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12n6shn/diving_into_the_future_serverless_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.databend.com/blog/2023/04/13/diving-into-the-future-serverless-data-warehouse-platform-architecture/", "subreddit_subscribers": 99214, "created_utc": 1681571540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI've noticed that there have been a lot of posts discussing Databricks vs Snowflake on this forum, but I'm interested in hearing about your experiences with Redshift. If you've transitioned from Redshift to Snowflake, I would love to hear your reasons for doing so.\n\nI've come across a post that suggests that when properly optimized, Redshift can outperform Snowflake. However, I'm curious to know what advantages Snowflake offers over Redshift.", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redshift Vs Snowflake", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mw1yy", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681549543.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve noticed that there have been a lot of posts discussing Databricks vs Snowflake on this forum, but I&amp;#39;m interested in hearing about your experiences with Redshift. If you&amp;#39;ve transitioned from Redshift to Snowflake, I would love to hear your reasons for doing so.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve come across a post that suggests that when properly optimized, Redshift can outperform Snowflake. However, I&amp;#39;m curious to know what advantages Snowflake offers over Redshift.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12mw1yy", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mw1yy/redshift_vs_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mw1yy/redshift_vs_snowflake/", "subreddit_subscribers": 99214, "created_utc": 1681549543.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I can only think of 1 benefit for federated learning scenarios where sharing a schema with other trusted clients could be beneficial, where machine learning can be carried out on heterogenous datasets that have different formats but require a ML model that solves a specific problem which everyone shares. \n\nThis involves having the data never leaving their internal systems, with a model trained locally. It would require the training set to have the same schema across all clients. \n\nAny other potential uses or benefits for this?", "author_fullname": "t2_bk9ikdyl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Benefits of sharing schemas externally to other trusted parties?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mu721", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681544187.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can only think of 1 benefit for federated learning scenarios where sharing a schema with other trusted clients could be beneficial, where machine learning can be carried out on heterogenous datasets that have different formats but require a ML model that solves a specific problem which everyone shares. &lt;/p&gt;\n\n&lt;p&gt;This involves having the data never leaving their internal systems, with a model trained locally. It would require the training set to have the same schema across all clients. &lt;/p&gt;\n\n&lt;p&gt;Any other potential uses or benefits for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12mu721", "is_robot_indexable": true, "report_reasons": null, "author": "yinyanglanguage", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mu721/benefits_of_sharing_schemas_externally_to_other/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mu721/benefits_of_sharing_schemas_externally_to_other/", "subreddit_subscribers": 99214, "created_utc": 1681544187.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I'm evaluating couchbase for one of my projects. I require some of the features in the enterprise version but as it's just a small personal project I don't want to buy the license.   \nI tried and it seems that they let you download the Couchbase server ee, Couchbase gateway ee and Couchbase lite ee without any license key, so can I just use it without a key forever?\n\nHow would that stop someone from using enterprise edition Couchbase for commercial purposes?", "author_fullname": "t2_99l81eq0h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Couchbase Enterprise license", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12msgxv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681539559.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I&amp;#39;m evaluating couchbase for one of my projects. I require some of the features in the enterprise version but as it&amp;#39;s just a small personal project I don&amp;#39;t want to buy the license.&lt;br/&gt;\nI tried and it seems that they let you download the Couchbase server ee, Couchbase gateway ee and Couchbase lite ee without any license key, so can I just use it without a key forever?&lt;/p&gt;\n\n&lt;p&gt;How would that stop someone from using enterprise edition Couchbase for commercial purposes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12msgxv", "is_robot_indexable": true, "report_reasons": null, "author": "Mysterious-Reading72", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12msgxv/couchbase_enterprise_license/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12msgxv/couchbase_enterprise_license/", "subreddit_subscribers": 99214, "created_utc": 1681539559.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Where do you normally put your business logic in analytical application? Stored Procedures ,view or separate layer? Do you consider Transformation Logic as business layer? In my opinion, all the business logic related to data should remain within data world.   \n\n\nI have seen many data patterns where T are done by python or some other languages. I am not sure why would you want to do that. Why would you pull the data from analytical platform(Usually database and MPP database) which can handle complex query and transformation efficiently and again put it back to database. You are not severely underutilizing the power of these analytical platform. I am not against python but in my opinion there are lots of bad practice going on in the industry because of hype. Python definitely has very specific role in data engineering and I understand that but I have seen many patterns where people have been misusing it severely specially on transformation side.   \n\n\nExample, In one my previous teams, data engineer pull the data from snowflake using python which was running in a VM, wrote a lots of data frames, transformed it and put it back into snowflake. I meant why? It's like you bought a ferrari but you chose drive in busy street which has one lane.", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where do you put your business logic?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mfhhk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681510270.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Where do you normally put your business logic in analytical application? Stored Procedures ,view or separate layer? Do you consider Transformation Logic as business layer? In my opinion, all the business logic related to data should remain within data world.   &lt;/p&gt;\n\n&lt;p&gt;I have seen many data patterns where T are done by python or some other languages. I am not sure why would you want to do that. Why would you pull the data from analytical platform(Usually database and MPP database) which can handle complex query and transformation efficiently and again put it back to database. You are not severely underutilizing the power of these analytical platform. I am not against python but in my opinion there are lots of bad practice going on in the industry because of hype. Python definitely has very specific role in data engineering and I understand that but I have seen many patterns where people have been misusing it severely specially on transformation side.   &lt;/p&gt;\n\n&lt;p&gt;Example, In one my previous teams, data engineer pull the data from snowflake using python which was running in a VM, wrote a lots of data frames, transformed it and put it back into snowflake. I meant why? It&amp;#39;s like you bought a ferrari but you chose drive in busy street which has one lane.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12mfhhk", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mfhhk/where_do_you_put_your_business_logic/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mfhhk/where_do_you_put_your_business_logic/", "subreddit_subscribers": 99214, "created_utc": 1681510270.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "For the folks who are self-hosting Dremio, what are your preferred method of hosting in AWS?\n\nI have deployed via the AWS Market option for the demo but long term will probably want some other option for deployment other than the AWS Market Cloudformation.\n\n&amp;#x200B;\n\nAre folks deploying Dremio via EC2 instances (in which we would define the infra with Terraform)?  Anyone running Dremio on Kubernetes?", "author_fullname": "t2_fwy0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Dremio - Preferred Deployment Method for Self-Hosting in AWS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mcuus", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681505274.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the folks who are self-hosting Dremio, what are your preferred method of hosting in AWS?&lt;/p&gt;\n\n&lt;p&gt;I have deployed via the AWS Market option for the demo but long term will probably want some other option for deployment other than the AWS Market Cloudformation.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Are folks deploying Dremio via EC2 instances (in which we would define the infra with Terraform)?  Anyone running Dremio on Kubernetes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mcuus", "is_robot_indexable": true, "report_reasons": null, "author": "exemaitch", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mcuus/dremio_preferred_deployment_method_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mcuus/dremio_preferred_deployment_method_for/", "subreddit_subscribers": 99214, "created_utc": 1681505274.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi, I am wondering a custom Data Catalog vs  existing solution like data hub. Our data schema is not huge but high skus for an e-commerce mart. Has anyone here seen/done building custom Data Catalog and succeeded/failed?", "author_fullname": "t2_6zaja793", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Custom Data Catalog", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mp3yb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681531253.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681531054.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am wondering a custom Data Catalog vs  existing solution like data hub. Our data schema is not huge but high skus for an e-commerce mart. Has anyone here seen/done building custom Data Catalog and succeeded/failed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12mp3yb", "is_robot_indexable": true, "report_reasons": null, "author": "Ambitious_Cucumber96", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12mp3yb/custom_data_catalog/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12mp3yb/custom_data_catalog/", "subreddit_subscribers": 99214, "created_utc": 1681531054.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}