{"kind": "Listing", "data": {"after": "t3_12me6v3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_fc92z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Over $23,000 Worth of Sabrent SSDs Deliver 168TB at 31 GB/s", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "news", "downs": 0, "thumbnail_height": 122, "top_awarded_type": null, "hide_score": false, "name": "t3_12mmw5k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.96, "author_flair_background_color": null, "ups": 126, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 126, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/-AdYqveeCD382N2dzBN9Vw2BovvcJX8PHl5Kh00iX_A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681525982.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "tomshardware.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.tomshardware.com/news/over-dollar23000-worth-of-sabrent-ssds-deliver-168tb-at-31-gbs", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uwSFSe-2ldBjgZIO_ZKdkUZ9cQrHf3eAquMh3AIlbeA.jpg?auto=webp&amp;v=enabled&amp;s=87bea97df232d6a1350979d0dbca6af62d45f727", "width": 1200, "height": 1048}, "resolutions": [{"url": "https://external-preview.redd.it/uwSFSe-2ldBjgZIO_ZKdkUZ9cQrHf3eAquMh3AIlbeA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=47b60da4f722083a3e93c80afc3a14065b971441", "width": 108, "height": 94}, {"url": "https://external-preview.redd.it/uwSFSe-2ldBjgZIO_ZKdkUZ9cQrHf3eAquMh3AIlbeA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c6a9310dc3b4d2b0b73686e4a440610151cc857", "width": 216, "height": 188}, {"url": "https://external-preview.redd.it/uwSFSe-2ldBjgZIO_ZKdkUZ9cQrHf3eAquMh3AIlbeA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e37cf9242295b9a4483028f064e43d183f9cd6f3", "width": 320, "height": 279}, {"url": "https://external-preview.redd.it/uwSFSe-2ldBjgZIO_ZKdkUZ9cQrHf3eAquMh3AIlbeA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e5e5a142bdd0f6f60b7f7db681ac7b49271cf1de", "width": 640, "height": 558}, {"url": "https://external-preview.redd.it/uwSFSe-2ldBjgZIO_ZKdkUZ9cQrHf3eAquMh3AIlbeA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e246c4f6bc081bcd20c08b6c39a51248685cda83", "width": 960, "height": 838}, {"url": "https://external-preview.redd.it/uwSFSe-2ldBjgZIO_ZKdkUZ9cQrHf3eAquMh3AIlbeA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a828781098812e9db3b2ea8248dbc15970dc40e9", "width": 1080, "height": 943}], "variants": {}, "id": "9OunuC6DvHtE-bljXTDtS8tThCuDwZHWi7Ar0yn-Z04"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "ed4936de-4bf6-11e3-8e8d-12313d184137", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mmw5k", "is_robot_indexable": true, "report_reasons": null, "author": "dstarr3", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mmw5k/over_23000_worth_of_sabrent_ssds_deliver_168tb_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.tomshardware.com/news/over-dollar23000-worth-of-sabrent-ssds-deliver-168tb-at-31-gbs", "subreddit_subscribers": 677959, "created_utc": 1681525982.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_dfgjhry0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I don't have much data (especially compared to some of you guys O_O) but I was wondering if this is a competent backup strategy?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_12modde", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.9, "author_flair_background_color": null, "ups": 47, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 47, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/RY8IXSDQkHFdfVdZBwCFWFzM4KhQ-8to4z_vva_MxYw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681529322.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/39c54kegwyta1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/39c54kegwyta1.png?auto=webp&amp;v=enabled&amp;s=5a25f22a9f5058970fff2cb0e5b7a3392a6ada03", "width": 4000, "height": 2250}, "resolutions": [{"url": "https://preview.redd.it/39c54kegwyta1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5eced2dc69d235483a2d00daf25f057996dfea3", "width": 108, "height": 60}, {"url": "https://preview.redd.it/39c54kegwyta1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=92e313366dbf5df03564efe8980df2d7364a2a58", "width": 216, "height": 121}, {"url": "https://preview.redd.it/39c54kegwyta1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f822103bb9e44fe55d65f2ece13818428ec6d6f2", "width": 320, "height": 180}, {"url": "https://preview.redd.it/39c54kegwyta1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea116338b9bc9f95f6d42a5b607cdea9763840c0", "width": 640, "height": 360}, {"url": "https://preview.redd.it/39c54kegwyta1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b4cd05dc7d6ebfdf2f53cecb3d55f98118f6aa68", "width": 960, "height": 540}, {"url": "https://preview.redd.it/39c54kegwyta1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=48162045d67dcbabef0a88d6f2b01994c181b152", "width": 1080, "height": 607}], "variants": {}, "id": "der-WrnDqesBzp4YZJxfkWXWVcI49I5UiFRB2hgq2gE"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12modde", "is_robot_indexable": true, "report_reasons": null, "author": "Leftover-Waffle", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12modde/i_dont_have_much_data_especially_compared_to_some/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/39c54kegwyta1.png", "subreddit_subscribers": 677959, "created_utc": 1681529322.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Still under development, but I made a way that allows for people to [search content on the Wayback Machine](https://internetarchivesearch.wordpress.com). This was designed to fill some major fundamental flaws the Internet Archive has been lacking in. One major problem is, that the IA, using keyword search on their main site you have to make it go to a specific collection rather than the whole Wayback machine. Another problem is for search, the IA is based out of SF. The US is the most visited country on the IA. So if someone from the east wants to search the IA it takes forever or just quits on them. Mine is based out of Tulsa, OK. So the middle of the country and its built on WP so there are servers all over the US. If you want to visit it, have fun, I'm not going to try to promote it or anything. Just wanted to let you archivists know about it. Visit it if you want, have fun searching! I would also love it if one of you guys help index it, if interested, please contact me. I will not pay you, however.", "author_fullname": "t2_7ycngdvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I created a way to search the Internet Archive's Wayback Machine", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mizf6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681517247.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Still under development, but I made a way that allows for people to &lt;a href=\"https://internetarchivesearch.wordpress.com\"&gt;search content on the Wayback Machine&lt;/a&gt;. This was designed to fill some major fundamental flaws the Internet Archive has been lacking in. One major problem is, that the IA, using keyword search on their main site you have to make it go to a specific collection rather than the whole Wayback machine. Another problem is for search, the IA is based out of SF. The US is the most visited country on the IA. So if someone from the east wants to search the IA it takes forever or just quits on them. Mine is based out of Tulsa, OK. So the middle of the country and its built on WP so there are servers all over the US. If you want to visit it, have fun, I&amp;#39;m not going to try to promote it or anything. Just wanted to let you archivists know about it. Visit it if you want, have fun searching! I would also love it if one of you guys help index it, if interested, please contact me. I will not pay you, however.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/F6AGihc6sjMmnVLm-XWIe0jcP2vPZpBwjOqH2PhZX-c.jpg?auto=webp&amp;v=enabled&amp;s=16976b1a0b8ae7efd842bffba9cb108b7335c3be", "width": 200, "height": 200}, "resolutions": [{"url": "https://external-preview.redd.it/F6AGihc6sjMmnVLm-XWIe0jcP2vPZpBwjOqH2PhZX-c.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15176241ef47b0546ee3b8703cb7e16b18feb504", "width": 108, "height": 108}], "variants": {}, "id": "sAYY86zgdT-GUuuQxQ55XhcVr4wy0uMIajd_UOBdg3Y"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mizf6", "is_robot_indexable": true, "report_reasons": null, "author": "Fortnite_Skin_Leaker", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mizf6/i_created_a_way_to_search_the_internet_archives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mizf6/i_created_a_way_to_search_the_internet_archives/", "subreddit_subscribers": 677959, "created_utc": 1681517247.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I can't imagine what my home storage would grow to with a symmetrical 10g connection. For $200, that is a DEAL. Cheapest I've seen 10gig.\n\n\n\nClip From Article;\n\nGigabitNow today announced service and pricing plans, including a $69.99 2Gbps internet subscription, a 5Gbps Internet plan for $149.99, and a connection up to 10Gig for $199.99", "author_fullname": "t2_76tlc1a2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Bloomington, IN now has 10Gig Fiber for $199.99", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mfvyl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e4444668-b98a-11e2-b419-12313d169640", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "threefive", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681511082.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "bloomington.in.gov", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t imagine what my home storage would grow to with a symmetrical 10g connection. For $200, that is a DEAL. Cheapest I&amp;#39;ve seen 10gig.&lt;/p&gt;\n\n&lt;p&gt;Clip From Article;&lt;/p&gt;\n\n&lt;p&gt;GigabitNow today announced service and pricing plans, including a $69.99 2Gbps internet subscription, a 5Gbps Internet plan for $149.99, and a connection up to 10Gig for $199.99&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://bloomington.in.gov/news/2023/04/04/5572", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "1.44MB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mfvyl", "is_robot_indexable": true, "report_reasons": null, "author": "Vast-Program7060", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12mfvyl/bloomington_in_now_has_10gig_fiber_for_19999/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://bloomington.in.gov/news/2023/04/04/5572", "subreddit_subscribers": 677959, "created_utc": 1681511082.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "HTTRACK? Or something easier.\n\nhttps://www.si.edu/search/collection-images?edan_q=&amp;edan_fq%5B0%5D=media_usage%3ACC0&amp;oa=1", "author_fullname": "t2_5zao0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Smithsonian's 4 million Public Domain Art Images were just added. How do we download?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mew4y", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681509145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;HTTRACK? Or something easier.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.si.edu/search/collection-images?edan_q=&amp;amp;edan_fq%5B0%5D=media_usage%3ACC0&amp;amp;oa=1\"&gt;https://www.si.edu/search/collection-images?edan_q=&amp;amp;edan_fq%5B0%5D=media_usage%3ACC0&amp;amp;oa=1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mew4y", "is_robot_indexable": true, "report_reasons": null, "author": "argusromblei", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mew4y/smithsonians_4_million_public_domain_art_images/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mew4y/smithsonians_4_million_public_domain_art_images/", "subreddit_subscribers": 677959, "created_utc": 1681509145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recently discovered that on my backup SSD a lot of images can't be opened - they have file header error. What's worse, Windows chkdsk reports no error for the disk. The file has the same size, and copied files have the same error. I'm now afraid that this can happen in reverse, with such erroneous files poisoning the backups. Would there be a solid way to ensure/check file integrity **on the source** before copying?\n\n- Versioning is not feasible for this amount of data. \n- ZFS could be a partial solution but I have data that would not be practical to store on a network drive (for Windows)", "author_fullname": "t2_4a8ri", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to avoid bitrotted files being transferred to backups?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mdq5s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681506958.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently discovered that on my backup SSD a lot of images can&amp;#39;t be opened - they have file header error. What&amp;#39;s worse, Windows chkdsk reports no error for the disk. The file has the same size, and copied files have the same error. I&amp;#39;m now afraid that this can happen in reverse, with such erroneous files poisoning the backups. Would there be a solid way to ensure/check file integrity &lt;strong&gt;on the source&lt;/strong&gt; before copying?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Versioning is not feasible for this amount of data. &lt;/li&gt;\n&lt;li&gt;ZFS could be a partial solution but I have data that would not be practical to store on a network drive (for Windows)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mdq5s", "is_robot_indexable": true, "report_reasons": null, "author": "poisonborz", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mdq5s/how_to_avoid_bitrotted_files_being_transferred_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mdq5s/how_to_avoid_bitrotted_files_being_transferred_to/", "subreddit_subscribers": 677959, "created_utc": 1681506958.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hi,   I've been having a lot of issues with the \"save outlinks\" feature for   the past 5 months on some board sites with images (outlink redirecting   not to a thumbnail but a Hi-Res image), in that it overloads the  servers  and it makes it very difficult for me to verify if all the  outlinks  were really saved, also it is extremely slow.\n\nI know there are other online tools that also archive sites like \"[**Archive.today**](https://archive.today/)**\",**   however this only seems to make snapshots and it is impractical to  save  each image individually. How do I save a site with images on sites  like  these or is there another site that does it like [archive.org](https://archive.org/)?\n\nPS: Also [archive.org](https://archive.org/) is amazing but its existence seems to be at risk lately, it's good to think of alternatives.", "author_fullname": "t2_k4bl8tpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Alternative to \"Internet Archive\"?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mhs25", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681514843.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,   I&amp;#39;ve been having a lot of issues with the &amp;quot;save outlinks&amp;quot; feature for   the past 5 months on some board sites with images (outlink redirecting   not to a thumbnail but a Hi-Res image), in that it overloads the  servers  and it makes it very difficult for me to verify if all the  outlinks  were really saved, also it is extremely slow.&lt;/p&gt;\n\n&lt;p&gt;I know there are other online tools that also archive sites like &amp;quot;&lt;a href=\"https://archive.today/\"&gt;&lt;strong&gt;Archive.today&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&amp;quot;,&lt;/strong&gt;   however this only seems to make snapshots and it is impractical to  save  each image individually. How do I save a site with images on sites  like  these or is there another site that does it like &lt;a href=\"https://archive.org/\"&gt;archive.org&lt;/a&gt;?&lt;/p&gt;\n\n&lt;p&gt;PS: Also &lt;a href=\"https://archive.org/\"&gt;archive.org&lt;/a&gt; is amazing but its existence seems to be at risk lately, it&amp;#39;s good to think of alternatives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cMsDEjZcw1yJ2-Oox97YXjr_B80QoYA7KKKBUw8desk.jpg?auto=webp&amp;v=enabled&amp;s=70049ac5e4587eb732d86f2bf3c7f941a6314e91", "width": 144, "height": 144}, "resolutions": [{"url": "https://external-preview.redd.it/cMsDEjZcw1yJ2-Oox97YXjr_B80QoYA7KKKBUw8desk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e81d09b8bf08f6c2e49210b8e709ad2fba21fef5", "width": 108, "height": 108}], "variants": {}, "id": "5WAXcyxu5qzeFIANl2gYNgI5-HT3q1BW7sA5zqtskZE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mhs25", "is_robot_indexable": true, "report_reasons": null, "author": "sylph79", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mhs25/alternative_to_internet_archive/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mhs25/alternative_to_internet_archive/", "subreddit_subscribers": 677959, "created_utc": 1681514843.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Around 100k images", "author_fullname": "t2_80r87i4z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can't download \"thisisnthappiness.com\" with TumblThree. They have some special sort of tumblr page. The archive is complete, anybody know how to download alll images of the archive?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12my30k", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681555200.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Around 100k images&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12my30k", "is_robot_indexable": true, "report_reasons": null, "author": "PalmMallMars", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12my30k/cant_download_thisisnthappinesscom_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12my30k/cant_download_thisisnthappinesscom_with/", "subreddit_subscribers": 677959, "created_utc": 1681555200.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I will be getting my hands on an old DOS scene CD tomorrow, and wanted to know the best software to use to grab the disk - bad sectors, weirdness, gaps and all, so I can try and get the on disk browser menu working again in DosBox / on vintage PCs\n\nOnly really have my windows 10 laptop to work with (I can load 7 if I need)\n\nWhat's my best bet here?", "author_fullname": "t2_4watb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Windows 10 software to exactly archive CD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12nf13p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681585748.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I will be getting my hands on an old DOS scene CD tomorrow, and wanted to know the best software to use to grab the disk - bad sectors, weirdness, gaps and all, so I can try and get the on disk browser menu working again in DosBox / on vintage PCs&lt;/p&gt;\n\n&lt;p&gt;Only really have my windows 10 laptop to work with (I can load 7 if I need)&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s my best bet here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12nf13p", "is_robot_indexable": true, "report_reasons": null, "author": "sapopeonarope", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12nf13p/windows_10_software_to_exactly_archive_cd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12nf13p/windows_10_software_to_exactly_archive_cd/", "subreddit_subscribers": 677959, "created_utc": 1681585748.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've recently started saving website pages, but I've noticed that some websites don't seem to allow this. For reference I am on a mac and just doing right click -&gt; save as -&gt; complete webpage. However, when clicking on the downloaded file it brings up a 404 error page. I've noticed this specifically on Medium and Substack (I've only tried downloading free substacks). I have tried downloading as HTML only and as Single File, but the HTML only also doesn't work, and the Single File looks really bad. Does anyone have any advice? I could just screenshot or copy-paste, but I'd rather preserve the original formatting/look.", "author_fullname": "t2_gbe9x2h7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Saving HTML websites gives 404 Page Not Found Error", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mhebx", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681514097.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently started saving website pages, but I&amp;#39;ve noticed that some websites don&amp;#39;t seem to allow this. For reference I am on a mac and just doing right click -&amp;gt; save as -&amp;gt; complete webpage. However, when clicking on the downloaded file it brings up a 404 error page. I&amp;#39;ve noticed this specifically on Medium and Substack (I&amp;#39;ve only tried downloading free substacks). I have tried downloading as HTML only and as Single File, but the HTML only also doesn&amp;#39;t work, and the Single File looks really bad. Does anyone have any advice? I could just screenshot or copy-paste, but I&amp;#39;d rather preserve the original formatting/look.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mhebx", "is_robot_indexable": true, "report_reasons": null, "author": "minnow-mini", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mhebx/saving_html_websites_gives_404_page_not_found/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mhebx/saving_html_websites_gives_404_page_not_found/", "subreddit_subscribers": 677959, "created_utc": 1681514097.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm looking for a storage service that lets me store a bit under 1TB of data until July. Does anyone have any idea of such place or online service?", "author_fullname": "t2_ty7oey24", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Temporary place to store ~1TB", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mduu4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.64, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681507209.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for a storage service that lets me store a bit under 1TB of data until July. Does anyone have any idea of such place or online service?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mduu4", "is_robot_indexable": true, "report_reasons": null, "author": "Intergalactic_Sesame", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mduu4/temporary_place_to_store_1tb/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mduu4/temporary_place_to_store_1tb/", "subreddit_subscribers": 677959, "created_utc": 1681507209.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "My understanding with USB is that as long as it's capable to move data, you can guarantee it's a good cable -- there's rarely something as \"slightly\" faulty. If it's broken, it won't give either power or data at all.\n\nThen comes my HDD. Suddenly on every transfer, it'd cause A LOT of problem. Speed drops to 0 often, causing hangs midway (and I have to force eject to let my Windows continue to work), and I can't delete a lot of data without causing issues.\n\nAll which are gone when I swap my USB cable (for now).\n\nThe one prompting me to suspect USB is UltraDMA CRC Error Count in SMART, people say it's due to SATA cable issue, but here I'm using USB.\n\nCan anyone clarify on this?", "author_fullname": "t2_tac95", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it possible an External HDD USB cable causing problem to data transfer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mz5e5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681557954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My understanding with USB is that as long as it&amp;#39;s capable to move data, you can guarantee it&amp;#39;s a good cable -- there&amp;#39;s rarely something as &amp;quot;slightly&amp;quot; faulty. If it&amp;#39;s broken, it won&amp;#39;t give either power or data at all.&lt;/p&gt;\n\n&lt;p&gt;Then comes my HDD. Suddenly on every transfer, it&amp;#39;d cause A LOT of problem. Speed drops to 0 often, causing hangs midway (and I have to force eject to let my Windows continue to work), and I can&amp;#39;t delete a lot of data without causing issues.&lt;/p&gt;\n\n&lt;p&gt;All which are gone when I swap my USB cable (for now).&lt;/p&gt;\n\n&lt;p&gt;The one prompting me to suspect USB is UltraDMA CRC Error Count in SMART, people say it&amp;#39;s due to SATA cable issue, but here I&amp;#39;m using USB.&lt;/p&gt;\n\n&lt;p&gt;Can anyone clarify on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mz5e5", "is_robot_indexable": true, "report_reasons": null, "author": "ArsenicBismuth", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mz5e5/is_it_possible_an_external_hdd_usb_cable_causing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mz5e5/is_it_possible_an_external_hdd_usb_cable_causing/", "subreddit_subscribers": 677959, "created_utc": 1681557954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am trying to get some data off a likely bad drive.  I rebooted my W11 desktop recently and when it came back on and connected via one of the 3 drives in a R0 setup is not detected, at first it said drive not connected then it said other issues and currently in the storage aspect of the QNAP.  I have purchased 2 new 14TB drives to swap out with that one and another with low level SMART stats.\n\n&amp;#x200B;\n\nWhat options do I have to restore the data off the 3 drives to transfer to other drives then remove them and combine the 2x14TB drives and rebuild.\n\n&amp;#x200B;\n\nI know that I should have other options and am working towards another NAS to use as secondary and tertiary backup.  I just would like to find a way to restore the data at least as long as possible to copy data off.  At this point there is about 21TB of data across the 3x R0 combined drives.", "author_fullname": "t2_d1f2mdx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Drive issues QNAP 873", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "troubleshooting", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12my5pp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Troubleshooting", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681555400.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to get some data off a likely bad drive.  I rebooted my W11 desktop recently and when it came back on and connected via one of the 3 drives in a R0 setup is not detected, at first it said drive not connected then it said other issues and currently in the storage aspect of the QNAP.  I have purchased 2 new 14TB drives to swap out with that one and another with low level SMART stats.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What options do I have to restore the data off the 3 drives to transfer to other drives then remove them and combine the 2x14TB drives and rebuild.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I know that I should have other options and am working towards another NAS to use as secondary and tertiary backup.  I just would like to find a way to restore the data at least as long as possible to copy data off.  At this point there is about 21TB of data across the 3x R0 combined drives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "5b6d7a04-b94e-11eb-b676-0ed4dfcb172d", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "86TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12my5pp", "is_robot_indexable": true, "report_reasons": null, "author": "kookykrazee", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12my5pp/drive_issues_qnap_873/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12my5pp/drive_issues_qnap_873/", "subreddit_subscribers": 677959, "created_utc": 1681555400.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "", "author_fullname": "t2_h0c8uq0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Digital hoarding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "friday", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mx5vh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.54, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Free-Post Friday!", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "created": 1681552768.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "hobbygenerator.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://hobbygenerator.com/hobby?hobby=Digital%20hoarding", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "82f515be-b94e-11eb-9f8a-0e1030dba663", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "2x8TB+2x4TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mx5vh", "is_robot_indexable": true, "report_reasons": null, "author": "theniwo", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12mx5vh/digital_hoarding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://hobbygenerator.com/hobby?hobby=Digital%20hoarding", "subreddit_subscribers": 677959, "created_utc": 1681552768.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I am posting here in hope someone will guide me in the right direction. I have alread checked some posts, but was unsure if my method is correct.\n\nMy main system in Linux Manjaro, and currently I'm manually syncing files to another system on LAN that is running same OS. Ideally, I would like this to be automated, but I have no experience with any of that. All (or almost all) data is backed up in cloud as well. The problem is, with adding or removing data, I must have some duplicates somewhere, or files that have been deleted from my system on a backup system; which is pointless.\n\nI have 2 HDD's  dedicated for backups that will be stored in a separate location. The drives would be 1:1 copy if one fails. I think that is sufficient enough, basically having 4 copies of each file on different HDDs.\n\nThose HDDs, I'd like to sync about once a month, and write all the changes that happened since last backup. So files that got modified, deleted, added... I don't know what the most convenient way would be.\n\nThe files consist of all types, and maybe having a hash/index of all of them would make sense, since search is really slow sometimes. Is there a program that would manage all of that for me? The drives I have now are 12TB ones - the offline ones are SMR, and will only be used as a backup.", "author_fullname": "t2_1nikfyn2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Indexing / hashing files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mx533", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681552703.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am posting here in hope someone will guide me in the right direction. I have alread checked some posts, but was unsure if my method is correct.&lt;/p&gt;\n\n&lt;p&gt;My main system in Linux Manjaro, and currently I&amp;#39;m manually syncing files to another system on LAN that is running same OS. Ideally, I would like this to be automated, but I have no experience with any of that. All (or almost all) data is backed up in cloud as well. The problem is, with adding or removing data, I must have some duplicates somewhere, or files that have been deleted from my system on a backup system; which is pointless.&lt;/p&gt;\n\n&lt;p&gt;I have 2 HDD&amp;#39;s  dedicated for backups that will be stored in a separate location. The drives would be 1:1 copy if one fails. I think that is sufficient enough, basically having 4 copies of each file on different HDDs.&lt;/p&gt;\n\n&lt;p&gt;Those HDDs, I&amp;#39;d like to sync about once a month, and write all the changes that happened since last backup. So files that got modified, deleted, added... I don&amp;#39;t know what the most convenient way would be.&lt;/p&gt;\n\n&lt;p&gt;The files consist of all types, and maybe having a hash/index of all of them would make sense, since search is really slow sometimes. Is there a program that would manage all of that for me? The drives I have now are 12TB ones - the offline ones are SMR, and will only be used as a backup.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mx533", "is_robot_indexable": true, "report_reasons": null, "author": "StrlA", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mx533/indexing_hashing_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mx533/indexing_hashing_files/", "subreddit_subscribers": 677959, "created_utc": 1681552703.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I have around 10Tb of total data (unique), stored in a variety of cloud service providers across multiple account (OneDrive, Google Drive, Box, Dropbox, etc). I've recently begun thinking about backing up this data, as this is the only copy of data I have.\n\n&amp;#x200B;\n\nDue to the size of the data, browsing reddit has impressed upon me that the best way to back up all this data to backblaze would be to get a large VPS, download, and then rclone to backblaze.\n\n&amp;#x200B;\n\nIs this the best solution, or is there potentially something smarter I'm missing?", "author_fullname": "t2_9ghuw0d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Redundancy Backups of Cloud Data via Backblaze", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12nfsxp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681587243.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have around 10Tb of total data (unique), stored in a variety of cloud service providers across multiple account (OneDrive, Google Drive, Box, Dropbox, etc). I&amp;#39;ve recently begun thinking about backing up this data, as this is the only copy of data I have.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Due to the size of the data, browsing reddit has impressed upon me that the best way to back up all this data to backblaze would be to get a large VPS, download, and then rclone to backblaze.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Is this the best solution, or is there potentially something smarter I&amp;#39;m missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12nfsxp", "is_robot_indexable": true, "report_reasons": null, "author": "SmittyJohnsontheone", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12nfsxp/redundancy_backups_of_cloud_data_via_backblaze/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12nfsxp/redundancy_backups_of_cloud_data_via_backblaze/", "subreddit_subscribers": 677959, "created_utc": 1681587243.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is this a crazy idea for some reason? I am thinking it's not. Zfs copies = 2 plus compression means I dont lose a ton of space plus I get bitrot protection. Snapraid allows me to recover from total drive failures and to add one drive at a time. Mergerfs makes the whole thing seamless.", "author_fullname": "t2_13g9il", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snapraid / Mergerfs with single drives formatted as zfs copies=2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ncbu2", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681580501.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is this a crazy idea for some reason? I am thinking it&amp;#39;s not. Zfs copies = 2 plus compression means I dont lose a ton of space plus I get bitrot protection. Snapraid allows me to recover from total drive failures and to add one drive at a time. Mergerfs makes the whole thing seamless.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12ncbu2", "is_robot_indexable": true, "report_reasons": null, "author": "linuxman1929", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12ncbu2/snapraid_mergerfs_with_single_drives_formatted_as/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12ncbu2/snapraid_mergerfs_with_single_drives_formatted_as/", "subreddit_subscribers": 677959, "created_utc": 1681580501.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I'm sure someone already asked that question, but after reading the FAQs and dozens of threads without any luck, I'll just ask...\n\nScenario:\nI have about 10 different sized, external, encrypted HDDs (1 to 18TB), where I backup all kind of data. To prevent data loss in case of a hardware failure, I have stored pretty much every file on two different HDDs. This worked quite fine for me, but over the years I'm now at a point where I lost track, if really every file has a duplicate somewhere else...\n\nNow it's a mess and in case of a failure I would not even know which files I lost or where to find the backup of the backup.\n\nI know that there are some indexing tools, which would help me keep track of the files, but are there also tools which support for such a backup strategy? Something which would show me an overview of files, where only one copy exists?", "author_fullname": "t2_d2lct", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Backup on multiple external drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "backup", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12nbp3z", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Backup", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681579238.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sure someone already asked that question, but after reading the FAQs and dozens of threads without any luck, I&amp;#39;ll just ask...&lt;/p&gt;\n\n&lt;p&gt;Scenario:\nI have about 10 different sized, external, encrypted HDDs (1 to 18TB), where I backup all kind of data. To prevent data loss in case of a hardware failure, I have stored pretty much every file on two different HDDs. This worked quite fine for me, but over the years I&amp;#39;m now at a point where I lost track, if really every file has a duplicate somewhere else...&lt;/p&gt;\n\n&lt;p&gt;Now it&amp;#39;s a mess and in case of a failure I would not even know which files I lost or where to find the backup of the backup.&lt;/p&gt;\n\n&lt;p&gt;I know that there are some indexing tools, which would help me keep track of the files, but are there also tools which support for such a backup strategy? Something which would show me an overview of files, where only one copy exists?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "7eead6f2-b94e-11eb-af94-0e4c7cd4fb01", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12nbp3z", "is_robot_indexable": true, "report_reasons": null, "author": "hash0", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12nbp3z/backup_on_multiple_external_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12nbp3z/backup_on_multiple_external_drives/", "subreddit_subscribers": 677959, "created_utc": 1681579238.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I recall there being some software that could reliably replicate / copy data from A to B, whilst also performing a check (CRC check??).\n\nTried Google and I get pretty much all the paid-promoted stuff which is clearly being pushed by the algo, and not because the software is necessarily good.\n\nWhat do folks around here use and recommend?\n\nEssentially I have 1TB (yes I know, nothing compare to some of you, but still plenty for me!) that I've copied from one 2TB drive to another 2TB drive.\n\nI'll periodically update files on one drive, and would like to be able to hit a button and 'sync' the recently changed files or new files to the second drive - what's the best way to do this?\n\n**Edit: I've found this interesting article** [**https://kinolios.com/en/blog/cinematography/post-production/backups-et-integrite-des-donnees-partie-i-freefilesync/**](https://kinolios.com/en/blog/cinematography/post-production/backups-et-integrite-des-donnees-partie-i-freefilesync/) **which looks legit. I think it's partially in French, but it looks like someone's actual hands-on experience, which is what I'm after**", "author_fullname": "t2_4d5ehhv2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's the best software for keeping data replicated / in sync when backing up?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n3upo", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681569256.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681567990.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recall there being some software that could reliably replicate / copy data from A to B, whilst also performing a check (CRC check??).&lt;/p&gt;\n\n&lt;p&gt;Tried Google and I get pretty much all the paid-promoted stuff which is clearly being pushed by the algo, and not because the software is necessarily good.&lt;/p&gt;\n\n&lt;p&gt;What do folks around here use and recommend?&lt;/p&gt;\n\n&lt;p&gt;Essentially I have 1TB (yes I know, nothing compare to some of you, but still plenty for me!) that I&amp;#39;ve copied from one 2TB drive to another 2TB drive.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll periodically update files on one drive, and would like to be able to hit a button and &amp;#39;sync&amp;#39; the recently changed files or new files to the second drive - what&amp;#39;s the best way to do this?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit: I&amp;#39;ve found this interesting article&lt;/strong&gt; &lt;a href=\"https://kinolios.com/en/blog/cinematography/post-production/backups-et-integrite-des-donnees-partie-i-freefilesync/\"&gt;&lt;strong&gt;https://kinolios.com/en/blog/cinematography/post-production/backups-et-integrite-des-donnees-partie-i-freefilesync/&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;which looks legit. I think it&amp;#39;s partially in French, but it looks like someone&amp;#39;s actual hands-on experience, which is what I&amp;#39;m after&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7YhI2SiGD2uVXilhB6wby5U6dXSPbVWD-wwSucElV2E.jpg?auto=webp&amp;v=enabled&amp;s=20be7e8ed32871e5b0a9b98fac6535e24fd2ad76", "width": 1500, "height": 762}, "resolutions": [{"url": "https://external-preview.redd.it/7YhI2SiGD2uVXilhB6wby5U6dXSPbVWD-wwSucElV2E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=51ea4c4466a0bf0f940ff42bd0f6f5b94bc53330", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/7YhI2SiGD2uVXilhB6wby5U6dXSPbVWD-wwSucElV2E.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=25167e5aa8ae173b0a4ab70d8b25a33ea5405fb8", "width": 216, "height": 109}, {"url": "https://external-preview.redd.it/7YhI2SiGD2uVXilhB6wby5U6dXSPbVWD-wwSucElV2E.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b52333862e737171f7d1ee781f9b71ec5630aed8", "width": 320, "height": 162}, {"url": "https://external-preview.redd.it/7YhI2SiGD2uVXilhB6wby5U6dXSPbVWD-wwSucElV2E.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d53c0226c8371ca56382db9e4633966b75beb695", "width": 640, "height": 325}, {"url": "https://external-preview.redd.it/7YhI2SiGD2uVXilhB6wby5U6dXSPbVWD-wwSucElV2E.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1488dece49d903dbf698a921da5270a373147db0", "width": 960, "height": 487}, {"url": "https://external-preview.redd.it/7YhI2SiGD2uVXilhB6wby5U6dXSPbVWD-wwSucElV2E.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=236fa4c8611fe7532aaf84788152199512cd330a", "width": 1080, "height": 548}], "variants": {}, "id": "jHqT9gQOb5yPP3xY1QsHs_8DB2Xi4cY0A6ExV9xUBWU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12n3upo", "is_robot_indexable": true, "report_reasons": null, "author": "i-dm", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12n3upo/whats_the_best_software_for_keeping_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12n3upo/whats_the_best_software_for_keeping_data/", "subreddit_subscribers": 677959, "created_utc": 1681567990.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "What's the best way to store, organise and self-transport multiple \"passport\" style external drives, such as WD Elements?   Ideally I'd like a lightweight stackable plastic case containing foam where 10-20 such drives and/or SSDs can be stored edge-on (hence edge- labelled and placed/found easily).  Do such things exist, and if so where?   I've searched lots and not found anything like this.  Currently I use strong reusable carrier bags, which feels poor and leads to lots of rummaging.  What do other people in my situation do (that's better)?", "author_fullname": "t2_v9cgie", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What/where are cases to store/organise/self-transport multiple \"passport\" style external drives?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n29ot", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681564978.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the best way to store, organise and self-transport multiple &amp;quot;passport&amp;quot; style external drives, such as WD Elements?   Ideally I&amp;#39;d like a lightweight stackable plastic case containing foam where 10-20 such drives and/or SSDs can be stored edge-on (hence edge- labelled and placed/found easily).  Do such things exist, and if so where?   I&amp;#39;ve searched lots and not found anything like this.  Currently I use strong reusable carrier bags, which feels poor and leads to lots of rummaging.  What do other people in my situation do (that&amp;#39;s better)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12n29ot", "is_robot_indexable": true, "report_reasons": null, "author": "davidgaryesp", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12n29ot/whatwhere_are_cases_to_storeorganiseselftransport/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12n29ot/whatwhere_are_cases_to_storeorganiseselftransport/", "subreddit_subscribers": 677959, "created_utc": 1681564978.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Is there a program that you can load in the predefined start and end times of pieces of the same video and the program will extract the defined pieces as their own separate files automatically and as a batch process?  \n\n\nIn other words, if a video is 3 minutes long, a user can load in the following:  \n\n\n0:00-1:00 - extracts to file 1  \n1:00-2:00 - extracts to file 2  \n2:00-3:00 - extracts to file 3  \n\n\nand the program automatically creates 3 files based on the above definition?", "author_fullname": "t2_74i2uwce", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is there a program that cut YouTube videos into user-defined start and end times from the video?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "scripts", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12n10m3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Scripts/Software", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681562300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there a program that you can load in the predefined start and end times of pieces of the same video and the program will extract the defined pieces as their own separate files automatically and as a batch process?  &lt;/p&gt;\n\n&lt;p&gt;In other words, if a video is 3 minutes long, a user can load in the following:  &lt;/p&gt;\n\n&lt;p&gt;0:00-1:00 - extracts to file 1&lt;br/&gt;\n1:00-2:00 - extracts to file 2&lt;br/&gt;\n2:00-3:00 - extracts to file 3  &lt;/p&gt;\n\n&lt;p&gt;and the program automatically creates 3 files based on the above definition?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "70ae6ea0-b94e-11eb-af00-0ec434816a2f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12n10m3", "is_robot_indexable": true, "report_reasons": null, "author": "humelectra2000", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12n10m3/is_there_a_program_that_cut_youtube_videos_into/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12n10m3/is_there_a_program_that_cut_youtube_videos_into/", "subreddit_subscribers": 677959, "created_utc": 1681562300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "i live in a relatively small flat so there's no way to move it anywhere else, i have 2x WD Red 12TB and those aren't that loud; been looking at Seagate Exos X20 20TB - wondering how loud would it be if i had 8 of those in a Fractal Node 804? or if there is anything quieter at a similar price point (i'm in germany so the exos is 320 EUR).\n\nthanks in advance", "author_fullname": "t2_ak0pkhdl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "quietest 20TB HDD for a home server?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mzldr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681559039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i live in a relatively small flat so there&amp;#39;s no way to move it anywhere else, i have 2x WD Red 12TB and those aren&amp;#39;t that loud; been looking at Seagate Exos X20 20TB - wondering how loud would it be if i had 8 of those in a Fractal Node 804? or if there is anything quieter at a similar price point (i&amp;#39;m in germany so the exos is 320 EUR).&lt;/p&gt;\n\n&lt;p&gt;thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mzldr", "is_robot_indexable": true, "report_reasons": null, "author": "basedqwq", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mzldr/quietest_20tb_hdd_for_a_home_server/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mzldr/quietest_20tb_hdd_for_a_home_server/", "subreddit_subscribers": 677959, "created_utc": 1681559039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "I've tried a few hash programs to validate my .md5 files, and have yet to find one that is capable of loading all hash files in a folder (including subfolders) for example...\n\nDrive X:\\\\ has a folder named \"TV Series\" and several subfolders for rach season containing individual episodes. I created seperate hash files for each file for every episode. (Not sure if it's better to create a hash for each *completed* season though instead) \n\nUsing programs like TurboSFV, I can only load a single folder and I'd have to load each folder one by one to check my hashes. There is no option for subfolders. \n\nI am on Windows, I have a program to create hashes but need one to validate them.\n\nAny help is appreciated.", "author_fullname": "t2_vc6ecyv0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Hash file program that validates folders or entire drive containing hashes?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12nb8pe", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "e34ee8aa-b988-11e2-9fc1-12313d18884c", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": "hd", "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681578326.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried a few hash programs to validate my .md5 files, and have yet to find one that is capable of loading all hash files in a folder (including subfolders) for example...&lt;/p&gt;\n\n&lt;p&gt;Drive X:\\ has a folder named &amp;quot;TV Series&amp;quot; and several subfolders for rach season containing individual episodes. I created seperate hash files for each file for every episode. (Not sure if it&amp;#39;s better to create a hash for each &lt;em&gt;completed&lt;/em&gt; season though instead) &lt;/p&gt;\n\n&lt;p&gt;Using programs like TurboSFV, I can only load a single folder and I&amp;#39;d have to load each folder one by one to check my hashes. There is no option for subfolders. &lt;/p&gt;\n\n&lt;p&gt;I am on Windows, I have a program to create hashes but need one to validate them.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "36TB", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12nb8pe", "is_robot_indexable": true, "report_reasons": null, "author": "RileyKennels", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/DataHoarder/comments/12nb8pe/hash_file_program_that_validates_folders_or/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12nb8pe/hash_file_program_that_validates_folders_or/", "subreddit_subscribers": 677959, "created_utc": 1681578326.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Hey all; long-time reader, occasional poster here.\n\nWith large capacity hard drive prices coming down, I've decided to add a physical off-site backup to my backup strategy. Specifically, I'm looking at making a local copy of my Windows Server 2016-based NAS to a spare HDD, and then storing that drive off-site.\n\nThe only difficult requirement is that I'd like to have the backup drive encrypted, so that it wouldn't be trivial to access my data if someone else got their hands on the drive. And the encryption needs to *not* be keyed to the host computer, so that I can access the drive from another Windows computer should the worst happen to my NAS.\n\nThere seems to be no shortage of backup software on Windows, most of which is either more powerful or more complex than what I really need (e.g. I don't need a DB-based system with incremental backups or things like that). So I'm at a bit of a loss trying to dig through all the options.\n\nSo what are my best options here?", "author_fullname": "t2_e56kq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Software Options For Offline Encrypted NAS Backups on Windows?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mso17", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681540107.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all; long-time reader, occasional poster here.&lt;/p&gt;\n\n&lt;p&gt;With large capacity hard drive prices coming down, I&amp;#39;ve decided to add a physical off-site backup to my backup strategy. Specifically, I&amp;#39;m looking at making a local copy of my Windows Server 2016-based NAS to a spare HDD, and then storing that drive off-site.&lt;/p&gt;\n\n&lt;p&gt;The only difficult requirement is that I&amp;#39;d like to have the backup drive encrypted, so that it wouldn&amp;#39;t be trivial to access my data if someone else got their hands on the drive. And the encryption needs to &lt;em&gt;not&lt;/em&gt; be keyed to the host computer, so that I can access the drive from another Windows computer should the worst happen to my NAS.&lt;/p&gt;\n\n&lt;p&gt;There seems to be no shortage of backup software on Windows, most of which is either more powerful or more complex than what I really need (e.g. I don&amp;#39;t need a DB-based system with incremental backups or things like that). So I&amp;#39;m at a bit of a loss trying to dig through all the options.&lt;/p&gt;\n\n&lt;p&gt;So what are my best options here?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12mso17", "is_robot_indexable": true, "report_reasons": null, "author": "Verite_Rendition", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12mso17/software_options_for_offline_encrypted_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12mso17/software_options_for_offline_encrypted_nas/", "subreddit_subscribers": 677959, "created_utc": 1681540107.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "DataHoarder", "selftext": "Any one know of how I could get myself a cheap janky NAS? I don't mine if its a Pi Zero with a USB 2.5\" mechanical drive and has split wires hanging out (I was figuring out some dongle to set up a 1TB HDD to my calculator but new nspire OS killed linux boot)\n\nJust looking for a cheap jank setup with at least one terabyte of storage, don't plan on having it out in the open to be seen.", "author_fullname": "t2_3ck6amuu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Cheap NAS", "link_flair_richtext": [], "subreddit_name_prefixed": "r/DataHoarder", "hidden": false, "pwls": 6, "link_flair_css_class": "question", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12me6v3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Question/Advice", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681507834.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.DataHoarder", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any one know of how I could get myself a cheap janky NAS? I don&amp;#39;t mine if its a Pi Zero with a USB 2.5&amp;quot; mechanical drive and has split wires hanging out (I was figuring out some dongle to set up a 1TB HDD to my calculator but new nspire OS killed linux boot)&lt;/p&gt;\n\n&lt;p&gt;Just looking for a cheap jank setup with at least one terabyte of storage, don&amp;#39;t plan on having it out in the open to be seen.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "268afdf0-4bf8-11e3-be37-12313d18f999", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2x7he", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#d3d6da", "id": "12me6v3", "is_robot_indexable": true, "report_reasons": null, "author": "ClutchDutch_Artist", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/DataHoarder/comments/12me6v3/cheap_nas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/DataHoarder/comments/12me6v3/cheap_nas/", "subreddit_subscribers": 677959, "created_utc": 1681507834.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}