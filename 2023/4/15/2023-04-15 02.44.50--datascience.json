{"kind": "Listing", "data": {"after": "t3_12m412q", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I graduated May 2021 and started working as a Data Scientist at General Motors. I was given a project but there was no project manager, customer, deadline or anything. I held myself accountable for 5 months setting my own deadlines etc until I realized that this just wasn't a real project and the company was terribly mismanaged. I took initiative to find other projects and each project I got just kept getting canceled or there wasn't any data.\n\nThen I switched to a Solutions Data Scientist role July 2022 at a larger tech company last year. I spent the first 3 months in training (completely irrelevant to Data Science) and then had absolutely no work. I networked like crazy and got a project on another team last fall which was somewhat exciting but this year there has been absolutely nothing. The other teams seem to have work but they won't let me on to even shadow because they want to prioritize utilization for their employees. My team has no data science work or data to play with. The Solutions Data Scientist role is weird because it is client-facing so if the clients don't want to engage, there is no work. Another challenge is that even when the clients engage, our priority is selling the company's AI products so there is minimal actual data science involved.\n\nI'm kind of concerned that I'm going to end up with years of Data Science experience with nothing to show for it. What should I do? If I applied for Data Science at another company now, it would be hard given the recessionary environment and the fact that I've probably gotten pretty technically soft now.\n\nIs this the reality with Data Science roles? Is it like a fake job? I'm wondering if I should move to a different role, like Product Management, where I might be guaranteed a steady flow of work to do.\n\nI'm also kind of concerned about Chat GPT4. My company sells products in the NLP space and with Chat GPT4, I feel like our products will be pretty obsolete soon.", "author_fullname": "t2_8cg2z0mf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Haven't had any real work for the last 2 years at 2 different companies", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12li3r3", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 116, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 116, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681493295.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681442744.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I graduated May 2021 and started working as a Data Scientist at General Motors. I was given a project but there was no project manager, customer, deadline or anything. I held myself accountable for 5 months setting my own deadlines etc until I realized that this just wasn&amp;#39;t a real project and the company was terribly mismanaged. I took initiative to find other projects and each project I got just kept getting canceled or there wasn&amp;#39;t any data.&lt;/p&gt;\n\n&lt;p&gt;Then I switched to a Solutions Data Scientist role July 2022 at a larger tech company last year. I spent the first 3 months in training (completely irrelevant to Data Science) and then had absolutely no work. I networked like crazy and got a project on another team last fall which was somewhat exciting but this year there has been absolutely nothing. The other teams seem to have work but they won&amp;#39;t let me on to even shadow because they want to prioritize utilization for their employees. My team has no data science work or data to play with. The Solutions Data Scientist role is weird because it is client-facing so if the clients don&amp;#39;t want to engage, there is no work. Another challenge is that even when the clients engage, our priority is selling the company&amp;#39;s AI products so there is minimal actual data science involved.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m kind of concerned that I&amp;#39;m going to end up with years of Data Science experience with nothing to show for it. What should I do? If I applied for Data Science at another company now, it would be hard given the recessionary environment and the fact that I&amp;#39;ve probably gotten pretty technically soft now.&lt;/p&gt;\n\n&lt;p&gt;Is this the reality with Data Science roles? Is it like a fake job? I&amp;#39;m wondering if I should move to a different role, like Product Management, where I might be guaranteed a steady flow of work to do.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also kind of concerned about Chat GPT4. My company sells products in the NLP space and with Chat GPT4, I feel like our products will be pretty obsolete soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12li3r3", "is_robot_indexable": true, "report_reasons": null, "author": "Legitimate_Ebb3623", "discussion_type": null, "num_comments": 56, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12li3r3/havent_had_any_real_work_for_the_last_2_years_at/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12li3r3/havent_had_any_real_work_for_the_last_2_years_at/", "subreddit_subscribers": 873096, "created_utc": 1681442744.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am really struggling to get an interview lately for data science roles. I apply for jobs that my resume matches almost exactly, yet I get an automated email 1 day later saying that I am rejected and they are pursuing \"candidates that are more suited for the role\". How are other candidates more suited for a role when I am literally exactly suited for the role? I even apply for jobs that are in person/hybrid when I live 5 minutes away from their office, and literally meet every job requirement. I have an MS in Data Science. I live 5 minutes away. I have the exact number of years of experience you are asking for in all of the tech stacks you require, I have the exact same salary expectations, I have the exact same industry and domain level expertise, yet I'm still rejected instantly after applying. I even apply for jobs that literally just opened, as in they were posted less than 1 day ago, and I still get instantly rejected saying \"they decided to pursue other more suitable candidates for the role\". How have you already decided to pursue other candidates after the job has only been posted for 3 hours and I live 5 minutes away for an in person role and I meet all the requirements and also all of the \"nice to have's\"? I don't get it.", "author_fullname": "t2_fztbyin", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Automatically rejected for every job I apply for, but I meet literally all of the requirements and \"nice to have\" requirements also", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m9hch", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 95, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 95, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681499548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am really struggling to get an interview lately for data science roles. I apply for jobs that my resume matches almost exactly, yet I get an automated email 1 day later saying that I am rejected and they are pursuing &amp;quot;candidates that are more suited for the role&amp;quot;. How are other candidates more suited for a role when I am literally exactly suited for the role? I even apply for jobs that are in person/hybrid when I live 5 minutes away from their office, and literally meet every job requirement. I have an MS in Data Science. I live 5 minutes away. I have the exact number of years of experience you are asking for in all of the tech stacks you require, I have the exact same salary expectations, I have the exact same industry and domain level expertise, yet I&amp;#39;m still rejected instantly after applying. I even apply for jobs that literally just opened, as in they were posted less than 1 day ago, and I still get instantly rejected saying &amp;quot;they decided to pursue other more suitable candidates for the role&amp;quot;. How have you already decided to pursue other candidates after the job has only been posted for 3 hours and I live 5 minutes away for an in person role and I meet all the requirements and also all of the &amp;quot;nice to have&amp;#39;s&amp;quot;? I don&amp;#39;t get it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m9hch", "is_robot_indexable": true, "report_reasons": null, "author": "Edge779", "discussion_type": null, "num_comments": 62, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m9hch/automatically_rejected_for_every_job_i_apply_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12m9hch/automatically_rejected_for_every_job_i_apply_for/", "subreddit_subscribers": 873096, "created_utc": 1681499548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "In February I started a project in my university about air quality. My team downloaded the data from the Spanish Government web and we started cleaning data from then on.\n\nAlmost two months have passed and we haven't finished the data cleaning process. I have to mention that we had filtered the data through many python scripts (w/ pandas) and we have cleaned a lot. I consider that in one week we'll have finished.\n\nThe point here is that everyone else was already analyzing data like a month ago, but I feel that they do not have so much data like us. Moreover, their data comes from some suspicious websites.\n\nMy question is, is it ok? Or should I have reduced the quantity of data? \n\nPD: sorry if my English is wrong, I'm still learning!", "author_fullname": "t2_ulhyxkkk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I'm in my first project and the data cleaning process is taking sooo long, it is ok?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lnxaw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 48, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 48, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681456497.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In February I started a project in my university about air quality. My team downloaded the data from the Spanish Government web and we started cleaning data from then on.&lt;/p&gt;\n\n&lt;p&gt;Almost two months have passed and we haven&amp;#39;t finished the data cleaning process. I have to mention that we had filtered the data through many python scripts (w/ pandas) and we have cleaned a lot. I consider that in one week we&amp;#39;ll have finished.&lt;/p&gt;\n\n&lt;p&gt;The point here is that everyone else was already analyzing data like a month ago, but I feel that they do not have so much data like us. Moreover, their data comes from some suspicious websites.&lt;/p&gt;\n\n&lt;p&gt;My question is, is it ok? Or should I have reduced the quantity of data? &lt;/p&gt;\n\n&lt;p&gt;PD: sorry if my English is wrong, I&amp;#39;m still learning!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lnxaw", "is_robot_indexable": true, "report_reasons": null, "author": "LaiqianDS", "discussion_type": null, "num_comments": 44, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lnxaw/im_in_my_first_project_and_the_data_cleaning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lnxaw/im_in_my_first_project_and_the_data_cleaning/", "subreddit_subscribers": 873096, "created_utc": 1681456497.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was looking into self-hosting Bloom as an alternative to GPT. Besides concerns about the context window being too small and the overall quality, I do really like it from a privacy and availability perspective.   \n\n\nBut a production machine running it would cost about 280K per year. I am contemplating setting this up as a shared resource and making it publicly available as an alternative to GPT. Would anyone be interested in that?", "author_fullname": "t2_13mxw7nv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Public Bloom Instance?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m27p4", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 27, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 27, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681488484.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was looking into self-hosting Bloom as an alternative to GPT. Besides concerns about the context window being too small and the overall quality, I do really like it from a privacy and availability perspective.   &lt;/p&gt;\n\n&lt;p&gt;But a production machine running it would cost about 280K per year. I am contemplating setting this up as a shared resource and making it publicly available as an alternative to GPT. Would anyone be interested in that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m27p4", "is_robot_indexable": true, "report_reasons": null, "author": "fokke2508", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m27p4/public_bloom_instance/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12m27p4/public_bloom_instance/", "subreddit_subscribers": 873096, "created_utc": 1681488484.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I saw a plot today and for some reason, after over a decade in the profession, thought that the standard axes might not be the norm. I was brought up with the standard X-Y axes, but might not be the case in other countries where left to right is not the norm.\n\nSo for people writing in non-latin scripts, Arabic, Hebrew, Standard Chinese, etc, do you draw your plots the same way?\n\nDo you plot time series plots with time going from left to right?", "author_fullname": "t2_v1t0s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Non left-to-right writers: how do you plot time-series?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12loszb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681458693.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw a plot today and for some reason, after over a decade in the profession, thought that the standard axes might not be the norm. I was brought up with the standard X-Y axes, but might not be the case in other countries where left to right is not the norm.&lt;/p&gt;\n\n&lt;p&gt;So for people writing in non-latin scripts, Arabic, Hebrew, Standard Chinese, etc, do you draw your plots the same way?&lt;/p&gt;\n\n&lt;p&gt;Do you plot time series plots with time going from left to right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12loszb", "is_robot_indexable": true, "report_reasons": null, "author": "philwinder", "discussion_type": null, "num_comments": 15, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12loszb/non_lefttoright_writers_how_do_you_plot_timeseries/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12loszb/non_lefttoright_writers_how_do_you_plot_timeseries/", "subreddit_subscribers": 873096, "created_utc": 1681458693.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi all\n\nJust wondering, in my specific field it is very important for my bosses that subject-experts are involved in the feature selection process. They claim that we cannot fully automate feature selection as some are more prone to overfitting for instance, and subject-experts who understand the business and features should take a look.\n\nPersonally it makes the process very exhausting for me as I'm more dependent in others. \n\nnote: we work with regression and XGBOOST models.\n\n&amp;#x200B;\n\nWhat do you think of this practice, and how does it work in your workplace?", "author_fullname": "t2_5hjxl4ya", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How automatic is your pipeline (or: how much do humans intervene in feature selection)?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m4cwt", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681492586.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all&lt;/p&gt;\n\n&lt;p&gt;Just wondering, in my specific field it is very important for my bosses that subject-experts are involved in the feature selection process. They claim that we cannot fully automate feature selection as some are more prone to overfitting for instance, and subject-experts who understand the business and features should take a look.&lt;/p&gt;\n\n&lt;p&gt;Personally it makes the process very exhausting for me as I&amp;#39;m more dependent in others. &lt;/p&gt;\n\n&lt;p&gt;note: we work with regression and XGBOOST models.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;What do you think of this practice, and how does it work in your workplace?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m4cwt", "is_robot_indexable": true, "report_reasons": null, "author": "PlainPiano9", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m4cwt/how_automatic_is_your_pipeline_or_how_much_do/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12m4cwt/how_automatic_is_your_pipeline_or_how_much_do/", "subreddit_subscribers": 873096, "created_utc": 1681492586.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Picterra &amp; Segment Anything Model (SAM) by Meta AI integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_12m0607", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_9or07", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dut4rnaD6QePyxQacRopSJTipuUS3t7_aEeJff1ijIk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "picterra", "selftext": "", "author_fullname": "t2_9or07", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Picterra &amp; Segment Anything Model (SAM) by Meta AI integration", "link_flair_richtext": [], "subreddit_name_prefixed": "r/picterra", "hidden": false, "pwls": null, "link_flair_css_class": null, "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_12m01yp", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": null, "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/dut4rnaD6QePyxQacRopSJTipuUS3t7_aEeJff1ijIk.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "created": 1681484330.0, "link_flair_type": "text", "wls": null, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/feed/update/urn:li:activity:7052653361631772673", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?auto=webp&amp;v=enabled&amp;s=6c0c8af8576f2ac38fc1186500dae252b30f52a9", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26bdb80dffc3b5d5b7dbbdd0bbafce2b30377b34", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45c6809882717fed47592c987ccaca1789cb0d84", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=585a3f7db351644ade99e89d7a0ab4b6825a9e8e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a60f537c154d67aa3410a9e4c6062a8ca1ea0bbb", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e465a4f6bec066daec448928ad4b32a5d48fd80", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d033dd5c7387956318a42af5725ee22ae5318a9", "width": 1080, "height": 607}], "variants": {}, "id": "G7yGMdsVFtIRCy7I0oG2pO7X_jXOt0gtbPWTC6mNjEI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_62sdxk", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m01yp", "is_robot_indexable": true, "report_reasons": null, "author": "unsaltedrhino", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": null, "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/picterra/comments/12m01yp/picterra_segment_anything_model_sam_by_meta_ai/", "parent_whitelist_status": null, "stickied": false, "url": "https://www.linkedin.com/feed/update/urn:li:activity:7052653361631772673", "subreddit_subscribers": 17, "created_utc": 1681484330.0, "num_crossposts": 4, "media": null, "is_video": false}], "created": 1681484529.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "linkedin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.linkedin.com/feed/update/urn:li:activity:7052653361631772673", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?auto=webp&amp;v=enabled&amp;s=6c0c8af8576f2ac38fc1186500dae252b30f52a9", "width": 1280, "height": 720}, "resolutions": [{"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=26bdb80dffc3b5d5b7dbbdd0bbafce2b30377b34", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=45c6809882717fed47592c987ccaca1789cb0d84", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=585a3f7db351644ade99e89d7a0ab4b6825a9e8e", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a60f537c154d67aa3410a9e4c6062a8ca1ea0bbb", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7e465a4f6bec066daec448928ad4b32a5d48fd80", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/3hbFvMp3E7wOpZd9I5atrYbuPNLW4RFdPaHLq1f5C_8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7d033dd5c7387956318a42af5725ee22ae5318a9", "width": 1080, "height": 607}], "variants": {}, "id": "G7yGMdsVFtIRCy7I0oG2pO7X_jXOt0gtbPWTC6mNjEI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m0607", "is_robot_indexable": true, "report_reasons": null, "author": "unsaltedrhino", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12m01yp", "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m0607/picterra_segment_anything_model_sam_by_meta_ai/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.linkedin.com/feed/update/urn:li:activity:7052653361631772673", "subreddit_subscribers": 873096, "created_utc": 1681484529.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello folks!\n\nI am attempting to perform DBSCAN on a dataset with approximately 2.5 million rows and 23 columns. After reading many places online, I understand that memory allocation is a problem for performing DBSCAN on such a huge dataset. Does anyone know how to do it, and in addition to it, can DBSCAN be used with parallel processing?\n\nThanks!", "author_fullname": "t2_3wr0pzmd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Batch processing for DBSCAN", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12logrh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681457850.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello folks!&lt;/p&gt;\n\n&lt;p&gt;I am attempting to perform DBSCAN on a dataset with approximately 2.5 million rows and 23 columns. After reading many places online, I understand that memory allocation is a problem for performing DBSCAN on such a huge dataset. Does anyone know how to do it, and in addition to it, can DBSCAN be used with parallel processing?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12logrh", "is_robot_indexable": true, "report_reasons": null, "author": "sARUcasm", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12logrh/batch_processing_for_dbscan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12logrh/batch_processing_for_dbscan/", "subreddit_subscribers": 873096, "created_utc": 1681457850.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I think I already know the answer but want to get other opinions.\n\nI have two large data sets that I had access to in the past: 1 was shared with me on Github and is still available on their profile - Its real data but redacted for HIPAA reasons. \n\nAnother Data set I had been given access to for during my Capstone project  - Its also redacted and does not have any direct patient identifiers (Medical recor numbers but this means nothing to me or This is the only thing I'm worried about)\n\n&amp;#x200B;\n\nWould it be appropriate for me to re-use these data sets and put them up on my portfolio with data visualizations and as 'data cleaning' projects?\n\n&amp;#x200B;\n\nAny advice is appreciated", "author_fullname": "t2_k95d913", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it ethical or I guess allowed for me to use a prior data set for practice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mga65", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681511873.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think I already know the answer but want to get other opinions.&lt;/p&gt;\n\n&lt;p&gt;I have two large data sets that I had access to in the past: 1 was shared with me on Github and is still available on their profile - Its real data but redacted for HIPAA reasons. &lt;/p&gt;\n\n&lt;p&gt;Another Data set I had been given access to for during my Capstone project  - Its also redacted and does not have any direct patient identifiers (Medical recor numbers but this means nothing to me or This is the only thing I&amp;#39;m worried about)&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Would it be appropriate for me to re-use these data sets and put them up on my portfolio with data visualizations and as &amp;#39;data cleaning&amp;#39; projects?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Any advice is appreciated&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12mga65", "is_robot_indexable": true, "report_reasons": null, "author": "Potential_Lettuce", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12mga65/is_it_ethical_or_i_guess_allowed_for_me_to_use_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12mga65/is_it_ethical_or_i_guess_allowed_for_me_to_use_a/", "subreddit_subscribers": 873096, "created_utc": 1681511873.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Basically my dataset has hundreds of points of their own error attributed to them. I am fitting the model to the data, finding the derivative of that model at a certain point, then using that value in further calculations. \n\n&amp;#x200B;\n\nI can get an error estimate of the regression model, but what value would I attribute to a point estimated by that model which would also incorporate the inherent error of the dataset?", "author_fullname": "t2_15xfqn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Regression error of a single data point which already has error attributed to it?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m6g2p", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681495532.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically my dataset has hundreds of points of their own error attributed to them. I am fitting the model to the data, finding the derivative of that model at a certain point, then using that value in further calculations. &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;I can get an error estimate of the regression model, but what value would I attribute to a point estimated by that model which would also incorporate the inherent error of the dataset?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m6g2p", "is_robot_indexable": true, "report_reasons": null, "author": "overhollowhills", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m6g2p/regression_error_of_a_single_data_point_which/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12m6g2p/regression_error_of_a_single_data_point_which/", "subreddit_subscribers": 873096, "created_utc": 1681495532.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi everyone,\n\nI'm excited to share my new course \"GCP Machine Learning Engineer Certification Practice Tests\". This course will prepare you for the Google Cloud Professional Machine Learning Engineer Certification exam.\n\nI'm offering free coupons for a limited time to the members of this forum. You can enjoy the promotion code by following the link that I provide below:\n\n[https://www.udemy.com/course/gcp-machine-learning-engineer-certification-practice-tests/?couponCode=401A9DAE7AD1B7A04A9F](https://www.udemy.com/course/gcp-machine-learning-engineer-certification-practice-tests/?couponCode=401A9DAE7AD1B7A04A9F)\n\nThank you for your time, and I look forward to seeing you in the course.\n\nBest regards", "author_fullname": "t2_5mszqi6z", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "GCP Machine Learning Engineer Certification Practice Tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lxy6f", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681480221.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m excited to share my new course &amp;quot;GCP Machine Learning Engineer Certification Practice Tests&amp;quot;. This course will prepare you for the Google Cloud Professional Machine Learning Engineer Certification exam.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m offering free coupons for a limited time to the members of this forum. You can enjoy the promotion code by following the link that I provide below:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.udemy.com/course/gcp-machine-learning-engineer-certification-practice-tests/?couponCode=401A9DAE7AD1B7A04A9F\"&gt;https://www.udemy.com/course/gcp-machine-learning-engineer-certification-practice-tests/?couponCode=401A9DAE7AD1B7A04A9F&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time, and I look forward to seeing you in the course.&lt;/p&gt;\n\n&lt;p&gt;Best regards&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/SkULa4bA-x6rFM2NB9QBvqnw6A8yPis5hoj5bXZp9BY.jpg?auto=webp&amp;v=enabled&amp;s=d1b1e4b558fbd24815a4c40a69f1477abf8c35ec", "width": 480, "height": 270}, "resolutions": [{"url": "https://external-preview.redd.it/SkULa4bA-x6rFM2NB9QBvqnw6A8yPis5hoj5bXZp9BY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ab07e42428cc252d1bbe5af1aa462f16dc2f45cd", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/SkULa4bA-x6rFM2NB9QBvqnw6A8yPis5hoj5bXZp9BY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c24f70bd0ca0ae3cb6ef524f9794373b9c286eb", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/SkULa4bA-x6rFM2NB9QBvqnw6A8yPis5hoj5bXZp9BY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f5fbd95be8c60478b2c0a406540ed973a646a50", "width": 320, "height": 180}], "variants": {}, "id": "GcuI27pv6hgL8kaNonF8CXNN7JikQOIVQhitYMJHsrY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lxy6f", "is_robot_indexable": true, "report_reasons": null, "author": "Entire-Work34", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lxy6f/gcp_machine_learning_engineer_certification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lxy6f/gcp_machine_learning_engineer_certification/", "subreddit_subscribers": 873096, "created_utc": 1681480221.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Craig Newmark of craigslist agreed to match $50,000 for Mozilla\u2019s Responsible AI Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mgeyq", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_47sfqfo", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "MachineLearning", "selftext": "With this donation from Craig Newmark Philanthropies, Mozilla will invest $100,000 into top applications and projects: [https://twitter.com/craignewmark/status/1646904897449902080](https://twitter.com/craignewmark/status/1646904897449902080)", "author_fullname": "t2_47sfqfo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "[N] Craig Newmark, founder of craigslist, has agreed to match cash prizes for Mozilla\u2019s Responsible AI Challenge", "link_flair_richtext": [], "subreddit_name_prefixed": "r/MachineLearning", "hidden": false, "pwls": 6, "link_flair_css_class": "two", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mf1t6", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "News", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681509449.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With this donation from Craig Newmark Philanthropies, Mozilla will invest $100,000 into top applications and projects: &lt;a href=\"https://twitter.com/craignewmark/status/1646904897449902080\"&gt;https://twitter.com/craignewmark/status/1646904897449902080&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t7OR3BzacQGEvL5gLiQZmZ7Ogbg82q83B7_s6_RkU-E.jpg?auto=webp&amp;v=enabled&amp;s=981d13dfbdf3537f46558b0d5f5d9ae17bd7e58c", "width": 140, "height": 140}, "resolutions": [{"url": "https://external-preview.redd.it/t7OR3BzacQGEvL5gLiQZmZ7Ogbg82q83B7_s6_RkU-E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae27b8c80e574226ecea84dbcd9cbb23fef6f511", "width": 108, "height": 108}], "variants": {}, "id": "j1GXf9fVJlSzEzzC1nnXzAIq2qDN4aWXEay5o-Tb9ww"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2r3gv", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12mf1t6", "is_robot_indexable": true, "report_reasons": null, "author": "joodfish", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/MachineLearning/comments/12mf1t6/n_craig_newmark_founder_of_craigslist_has_agreed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/MachineLearning/comments/12mf1t6/n_craig_newmark_founder_of_craigslist_has_agreed/", "subreddit_subscribers": 2628904, "created_utc": 1681509449.0, "num_crossposts": 2, "media": null, "is_video": false}], "created": 1681512142.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.MachineLearning", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/MachineLearning/comments/12mf1t6/n_craig_newmark_founder_of_craigslist_has_agreed/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/t7OR3BzacQGEvL5gLiQZmZ7Ogbg82q83B7_s6_RkU-E.jpg?auto=webp&amp;v=enabled&amp;s=981d13dfbdf3537f46558b0d5f5d9ae17bd7e58c", "width": 140, "height": 140}, "resolutions": [{"url": "https://external-preview.redd.it/t7OR3BzacQGEvL5gLiQZmZ7Ogbg82q83B7_s6_RkU-E.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ae27b8c80e574226ecea84dbcd9cbb23fef6f511", "width": 108, "height": 108}], "variants": {}, "id": "j1GXf9fVJlSzEzzC1nnXzAIq2qDN4aWXEay5o-Tb9ww"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12mgeyq", "is_robot_indexable": true, "report_reasons": null, "author": "joodfish", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12mf1t6", "author_flair_text_color": null, "permalink": "/r/datascience/comments/12mgeyq/craig_newmark_of_craigslist_agreed_to_match_50000/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/MachineLearning/comments/12mf1t6/n_craig_newmark_founder_of_craigslist_has_agreed/", "subreddit_subscribers": 873096, "created_utc": 1681512142.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Original Monty Hall Problem.\n\nSuppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\n\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n\nWe know that in the original problem, switching has the higher expected value.\n\n\nNow let\u2019s suppose that there are still 3 doors, but there are 2 players this time. Each player picks a unique door, and assuming that they didn\u2019t both pick the doors with the goat, the unpicked door is revealed to show a goat. \n\nUnder the original problem, it would be profitable for both players to switch their selection. But this just feels so counter intuitive?", "author_fullname": "t2_15wxtiq7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Monty Hall Problem with 2 players", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m8q4g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681498487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Original Monty Hall Problem.&lt;/p&gt;\n\n&lt;p&gt;Suppose you&amp;#39;re on a game show, and you&amp;#39;re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what&amp;#39;s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, &amp;quot;Do you want to pick door No. 2?&amp;quot; Is it to your advantage to switch your choice?&lt;/p&gt;\n\n&lt;p&gt;\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014&lt;/p&gt;\n\n&lt;p&gt;We know that in the original problem, switching has the higher expected value.&lt;/p&gt;\n\n&lt;p&gt;Now let\u2019s suppose that there are still 3 doors, but there are 2 players this time. Each player picks a unique door, and assuming that they didn\u2019t both pick the doors with the goat, the unpicked door is revealed to show a goat. &lt;/p&gt;\n\n&lt;p&gt;Under the original problem, it would be profitable for both players to switch their selection. But this just feels so counter intuitive?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m8q4g", "is_robot_indexable": true, "report_reasons": null, "author": "ThreeToInfinity", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m8q4g/monty_hall_problem_with_2_players/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12m8q4g/monty_hall_problem_with_2_players/", "subreddit_subscribers": 873096, "created_utc": 1681498487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_qqy6or6h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is the advantage of using Machine learning in Azure?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12luj28", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681472838.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12luj28", "is_robot_indexable": true, "report_reasons": null, "author": "star-lord-98", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12luj28/what_is_the_advantage_of_using_machine_learning/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12luj28/what_is_the_advantage_of_using_machine_learning/", "subreddit_subscribers": 873096, "created_utc": 1681472838.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've been working as radiological imaging technologist for past two years without any increments and it doesn't add any new experience at all. Now the company planning to laid me off soon and to regret they informed that they won't provide any experience certificates too.\n\nNow I'm intend to start from the scratch again, I'm highly interested in data science, I did few internships in data science. But it is really hard to land into the job. I constantly applying for job but didn't even got single interview.\n\nI did few own projects but I have no idea how to build a portfolio.\n\nWhat should I do?", "author_fullname": "t2_tesccx8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "is it possible for a medical Imaging Tech to be a data scientist", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lkaej", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681447583.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as radiological imaging technologist for past two years without any increments and it doesn&amp;#39;t add any new experience at all. Now the company planning to laid me off soon and to regret they informed that they won&amp;#39;t provide any experience certificates too.&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m intend to start from the scratch again, I&amp;#39;m highly interested in data science, I did few internships in data science. But it is really hard to land into the job. I constantly applying for job but didn&amp;#39;t even got single interview.&lt;/p&gt;\n\n&lt;p&gt;I did few own projects but I have no idea how to build a portfolio.&lt;/p&gt;\n\n&lt;p&gt;What should I do?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lkaej", "is_robot_indexable": true, "report_reasons": null, "author": "Dilly_03", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lkaej/is_it_possible_for_a_medical_imaging_tech_to_be_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lkaej/is_it_possible_for_a_medical_imaging_tech_to_be_a/", "subreddit_subscribers": 873096, "created_utc": 1681447583.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "The title is provocative but it was meant to give an idea about my current doubt.\n\nLet's suppose to have the current scenario:\n\n* Consumer app with monthly subscriptions\n* Active users: pay for at least 12 months\n* Non Active users: pay for less than 12 months\n* Binary classification problem: Active (A), Non Active (B)\n* Model based on historical usage of new customers over their first month\n* Objective: predict only the number of new users that will be Active (positive), NOT which users will be active\n\nMy question is: would the following (unknown) metric make sense?\n\n x = (TP + FP) / (TP + FN) \n\nIt is meant to represent the ratio of the total number of positive predictions (i.e., Class A) to the total number of actual activations.\n\nIf x is close to 1 it means that my model predicts accurately the number of positive cases.\n\nThe fact that I could potentially have a lot of FP is not important as far as it's close to the number of FN.\n\nI'm sure that I'm missing something trivial but I need help to understand what it is :-)\n\nThanks!", "author_fullname": "t2_islc4wxy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why should I care about TRUE prediction?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12mf5sr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681509656.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title is provocative but it was meant to give an idea about my current doubt.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s suppose to have the current scenario:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Consumer app with monthly subscriptions&lt;/li&gt;\n&lt;li&gt;Active users: pay for at least 12 months&lt;/li&gt;\n&lt;li&gt;Non Active users: pay for less than 12 months&lt;/li&gt;\n&lt;li&gt;Binary classification problem: Active (A), Non Active (B)&lt;/li&gt;\n&lt;li&gt;Model based on historical usage of new customers over their first month&lt;/li&gt;\n&lt;li&gt;Objective: predict only the number of new users that will be Active (positive), NOT which users will be active&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My question is: would the following (unknown) metric make sense?&lt;/p&gt;\n\n&lt;p&gt;x = (TP + FP) / (TP + FN) &lt;/p&gt;\n\n&lt;p&gt;It is meant to represent the ratio of the total number of positive predictions (i.e., Class A) to the total number of actual activations.&lt;/p&gt;\n\n&lt;p&gt;If x is close to 1 it means that my model predicts accurately the number of positive cases.&lt;/p&gt;\n\n&lt;p&gt;The fact that I could potentially have a lot of FP is not important as far as it&amp;#39;s close to the number of FN.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure that I&amp;#39;m missing something trivial but I need help to understand what it is :-)&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12mf5sr", "is_robot_indexable": true, "report_reasons": null, "author": "ExcellentReality3153", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12mf5sr/why_should_i_care_about_true_prediction/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12mf5sr/why_should_i_care_about_true_prediction/", "subreddit_subscribers": 873096, "created_utc": 1681509656.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I know this is quite a subjective question, and so maybe the answers I'm looking for is just guidance on how to approach this, but **how do you determine how you want to roll out a feature following a successful test?**\n\nFor context, we are launching an experiment on a tight timeline, let's say the experiment allocates to 10% of users. \n\nLet's say we find that the test is successful, we're happy with results, and want to globally make the change to all users.\n\n**How do we approach how we want to phase it out? L**et's assume just going from 10% to 100% is out of the question due to minimizing any unforeseen risks.\n\n**How would you determine what %s each \"phase\" should be? do we go 10-25-50-100? how long do you measure each phase for, especially if we're working on a tight deadline?**\n\nThank you for any inputs all.", "author_fullname": "t2_133emd5x", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to properly roll out a successful test/feature", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m6acs", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681495357.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know this is quite a subjective question, and so maybe the answers I&amp;#39;m looking for is just guidance on how to approach this, but &lt;strong&gt;how do you determine how you want to roll out a feature following a successful test?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;For context, we are launching an experiment on a tight timeline, let&amp;#39;s say the experiment allocates to 10% of users. &lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say we find that the test is successful, we&amp;#39;re happy with results, and want to globally make the change to all users.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How do we approach how we want to phase it out? L&lt;/strong&gt;et&amp;#39;s assume just going from 10% to 100% is out of the question due to minimizing any unforeseen risks.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How would you determine what %s each &amp;quot;phase&amp;quot; should be? do we go 10-25-50-100? how long do you measure each phase for, especially if we&amp;#39;re working on a tight deadline?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Thank you for any inputs all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m6acs", "is_robot_indexable": true, "report_reasons": null, "author": "vatom14", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m6acs/how_to_properly_roll_out_a_successful_testfeature/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12m6acs/how_to_properly_roll_out_a_successful_testfeature/", "subreddit_subscribers": 873096, "created_utc": 1681495357.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_jlunblur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Neuropizza: Models for spike train classification and machine learning parameter identifiability", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12lqcjr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gmCBxvCmf3E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Neuropizza: Models for spike train classification and machine learning parameter identifiability\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Neuropizza: Models for spike train classification and machine learning parameter identifiability", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gmCBxvCmf3E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Neuropizza: Models for spike train classification and machine learning parameter identifiability\"&gt;&lt;/iframe&gt;", "author_name": "Alessandro Crimi", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/gmCBxvCmf3E/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@alecrimi"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gmCBxvCmf3E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Neuropizza: Models for spike train classification and machine learning parameter identifiability\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12lqcjr", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/8lNZGkMOprrdB1ghwXatBtA54J4xbLDYB9CzK2c3Czw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1681462593.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=gmCBxvCmf3E", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/L-fyEboM97IOvjzJEFcLUBwj7NiSfQjKykCz78nHuQA.jpg?auto=webp&amp;v=enabled&amp;s=1cf2643d9230ada28a2f582a146b3e5908603f83", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/L-fyEboM97IOvjzJEFcLUBwj7NiSfQjKykCz78nHuQA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d62acf4c880618e6540135b3f3cd3a00fbe2045c", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/L-fyEboM97IOvjzJEFcLUBwj7NiSfQjKykCz78nHuQA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a8feb3843340e170c5bd3fd3553efb7c0186021b", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/L-fyEboM97IOvjzJEFcLUBwj7NiSfQjKykCz78nHuQA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6633094d776054bca9a80602c94d88296fa2224", "width": 320, "height": 240}], "variants": {}, "id": "ISUixBol5tj9cIpOo7ZBV3bQ-qx8V_DTqB4IYfkumJA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lqcjr", "is_robot_indexable": true, "report_reasons": null, "author": "rottoneuro", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lqcjr/neuropizza_models_for_spike_train_classification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=gmCBxvCmf3E", "subreddit_subscribers": 873096, "created_utc": 1681462593.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Neuropizza: Models for spike train classification and machine learning parameter identifiability", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/gmCBxvCmf3E?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Neuropizza: Models for spike train classification and machine learning parameter identifiability\"&gt;&lt;/iframe&gt;", "author_name": "Alessandro Crimi", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/gmCBxvCmf3E/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@alecrimi"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hey, I'm having trouble understanding how to incremental load from multiple source tables that join on each other to a target table. I was wondering if anyone has had to do something similar and has any ideas they could share. I'm using databricks as a platform if that matters.\n\nExample:\n\nLarge Table: basicStats with columns (nameID,name,fatherID)\n\nLarge Table: moreStats with columns (nameID,height)\n\nLooking to implement incremental loading on a new table:\n\nselect sha1(basicStats.nameID) nameHashKey,\n\n[basicStats.name](https://basicstats.name/) name,\n\nbasicStatsFather[.name](https://father.name/) as fatherName,\n\nmoreStats.height as height\n\nfrom basicStats join moreStats\n\non [basicStats.nameID = moreStats.name](https://table1.name%3Dtable2.name/)ID\n\njoin basicStats as basicStatsFather\n\non basicStats.nameID = basicStatsFather.fatherID", "author_fullname": "t2_6pedjzz0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Incremental loading from multiple tables into a target table", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lkw6s", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681448977.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I&amp;#39;m having trouble understanding how to incremental load from multiple source tables that join on each other to a target table. I was wondering if anyone has had to do something similar and has any ideas they could share. I&amp;#39;m using databricks as a platform if that matters.&lt;/p&gt;\n\n&lt;p&gt;Example:&lt;/p&gt;\n\n&lt;p&gt;Large Table: basicStats with columns (nameID,name,fatherID)&lt;/p&gt;\n\n&lt;p&gt;Large Table: moreStats with columns (nameID,height)&lt;/p&gt;\n\n&lt;p&gt;Looking to implement incremental loading on a new table:&lt;/p&gt;\n\n&lt;p&gt;select sha1(basicStats.nameID) nameHashKey,&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://basicstats.name/\"&gt;basicStats.name&lt;/a&gt; name,&lt;/p&gt;\n\n&lt;p&gt;basicStatsFather&lt;a href=\"https://father.name/\"&gt;.name&lt;/a&gt; as fatherName,&lt;/p&gt;\n\n&lt;p&gt;moreStats.height as height&lt;/p&gt;\n\n&lt;p&gt;from basicStats join moreStats&lt;/p&gt;\n\n&lt;p&gt;on &lt;a href=\"https://table1.name%3Dtable2.name/\"&gt;basicStats.nameID = moreStats.name&lt;/a&gt;ID&lt;/p&gt;\n\n&lt;p&gt;join basicStats as basicStatsFather&lt;/p&gt;\n\n&lt;p&gt;on basicStats.nameID = basicStatsFather.fatherID&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lkw6s", "is_robot_indexable": true, "report_reasons": null, "author": "Simp4ABGs", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lkw6s/incremental_loading_from_multiple_tables_into_a/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lkw6s/incremental_loading_from_multiple_tables_into_a/", "subreddit_subscribers": 873096, "created_utc": 1681448977.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hello,\n\nMy coworker and I are planning to attend this year's ICML, but we don't have the personal or institutional funding to go. We were denied funding from our university (we are both staff members,) and plan to fill out the financial aid application.\n\nFor those who've sat at the other side of the table, what are you looking for in the financial aid application? We really, completely and utterly do not have any money (just out of college, living on our own, educational staff budget) and we're both women if that helps (other minority affiliations as well,) but not sure what else to write.\n\nThanks!", "author_fullname": "t2_45bgy9c3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you look for in financial aid applications to conferences?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m6m8v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681495726.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;My coworker and I are planning to attend this year&amp;#39;s ICML, but we don&amp;#39;t have the personal or institutional funding to go. We were denied funding from our university (we are both staff members,) and plan to fill out the financial aid application.&lt;/p&gt;\n\n&lt;p&gt;For those who&amp;#39;ve sat at the other side of the table, what are you looking for in the financial aid application? We really, completely and utterly do not have any money (just out of college, living on our own, educational staff budget) and we&amp;#39;re both women if that helps (other minority affiliations as well,) but not sure what else to write.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m6m8v", "is_robot_indexable": true, "report_reasons": null, "author": "EasternStuff5015", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m6m8v/what_do_you_look_for_in_financial_aid/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12m6m8v/what_do_you_look_for_in_financial_aid/", "subreddit_subscribers": 873096, "created_utc": 1681495726.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Graphing Progress over time", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m0npb", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_3z3rz92v", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "default", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "excel", "selftext": "I have an excel book - each line represents a job application and is time-stamped with the date the app was submitted. I\u2019m trying to figure out how to create a line graph that shows the accumulation of apps over time. EG if 35 people applied on April 1, and 50 people applied on April 2, the graph should indicate 85 apps on April 2. \n\nHow do I get there?", "author_fullname": "t2_3z3rz92v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Graphing Progress over time", "link_flair_richtext": [{"e": "text", "t": "Waiting on OP"}], "subreddit_name_prefixed": "r/excel", "hidden": false, "pwls": 6, "link_flair_css_class": "waitingonop", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m0jjl", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Waiting on OP", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681485246.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.excel", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an excel book - each line represents a job application and is time-stamped with the date the app was submitted. I\u2019m trying to figure out how to create a line graph that shows the accumulation of apps over time. EG if 35 people applied on April 1, and 50 people applied on April 2, the graph should indicate 85 apps on April 2. &lt;/p&gt;\n\n&lt;p&gt;How do I get there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "top", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "fb6ebbf6-05f3-11e3-9e5e-12313d21c6e5", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qur2", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#78bafe", "id": "12m0jjl", "is_robot_indexable": true, "report_reasons": null, "author": "gingerbreadman1242", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/excel/comments/12m0jjl/graphing_progress_over_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/excel/comments/12m0jjl/graphing_progress_over_time/", "subreddit_subscribers": 617820, "created_utc": 1681485246.0, "num_crossposts": 1, "media": null, "is_video": false}], "created": 1681485467.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.excel", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/excel/comments/12m0jjl/graphing_progress_over_time/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m0npb", "is_robot_indexable": true, "report_reasons": null, "author": "gingerbreadman1242", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_12m0jjl", "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m0npb/graphing_progress_over_time/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/excel/comments/12m0jjl/graphing_progress_over_time/", "subreddit_subscribers": 873096, "created_utc": 1681485467.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Everyone company now post ChatGPT is pushing for some sort of position within an AI realm. For the most part us in the field have been relegated to research or BI work within a small group of people. When if ever do you all think we will see a jobs boom coming for our sector like we see for backend devs?", "author_fullname": "t2_7xce854y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "A boom in Data Science/Data Engineering/ML jobs coming?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lxrln", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681479852.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone company now post ChatGPT is pushing for some sort of position within an AI realm. For the most part us in the field have been relegated to research or BI work within a small group of people. When if ever do you all think we will see a jobs boom coming for our sector like we see for backend devs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lxrln", "is_robot_indexable": true, "report_reasons": null, "author": "Professional-Humor-8", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lxrln/a_boom_in_data_sciencedata_engineeringml_jobs/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lxrln/a_boom_in_data_sciencedata_engineeringml_jobs/", "subreddit_subscribers": 873096, "created_utc": 1681479852.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Apologies for cross-posting with /r/Python, but this is particularly relevant to many data science practitioners.\n\nWe're excited to announce the release of a new data visualization toolkit: Highcharts for Python!\n\nIt's a collection of Python libraries designed to provide turn-key comprehensive support for Highcharts data visualizations in Python, including native integrations with Pandas, Jupyter, PySpark, GeoPandas, ESRI, and more. If you're unfamiliar with Highcharts, it is one of the leading Javascript data visualization solutions, used to easily create highly interactive, beautifully styled data visualizations with over 150 different chart types. Using the Highcharts for Python toolkit, you can easily integrate this rich set of visualization capabilities into your Notebooks and analytical apps.\n\nThe toolkit includes full support for the Highcharts suite of data visualization libraries, including Highcharts Core, Highcharts Stock, Highcharts Maps, and Highcharts Gantt. Full disclosure, the libraries are free to try, but like all of the Highcharts visualization products, using them commercially requires a license with extensive (human) support.\n\nYou can find the Github repos for the full toolkit at [https://github.com/highcharts-for-python](https://github.com/highcharts-for-python), more details about what the toolkit can do at [https://www.highcharts.com/blog/integrations/python/](https://www.highcharts.com/blog/integrations/python/), and extensive documentation at [https://highchartspython.com/get-help](https://highchartspython.com/get-help).\n\nGiven that this is v.1.0 of our Python toolkit, it's obviously early days, so we'd love your thoughts and perspectives on the library!", "author_fullname": "t2_slz15ug2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Highcharts for Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lo5c9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1681471868.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681457064.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apologies for cross-posting with &lt;a href=\"/r/Python\"&gt;/r/Python&lt;/a&gt;, but this is particularly relevant to many data science practitioners.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re excited to announce the release of a new data visualization toolkit: Highcharts for Python!&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a collection of Python libraries designed to provide turn-key comprehensive support for Highcharts data visualizations in Python, including native integrations with Pandas, Jupyter, PySpark, GeoPandas, ESRI, and more. If you&amp;#39;re unfamiliar with Highcharts, it is one of the leading Javascript data visualization solutions, used to easily create highly interactive, beautifully styled data visualizations with over 150 different chart types. Using the Highcharts for Python toolkit, you can easily integrate this rich set of visualization capabilities into your Notebooks and analytical apps.&lt;/p&gt;\n\n&lt;p&gt;The toolkit includes full support for the Highcharts suite of data visualization libraries, including Highcharts Core, Highcharts Stock, Highcharts Maps, and Highcharts Gantt. Full disclosure, the libraries are free to try, but like all of the Highcharts visualization products, using them commercially requires a license with extensive (human) support.&lt;/p&gt;\n\n&lt;p&gt;You can find the Github repos for the full toolkit at &lt;a href=\"https://github.com/highcharts-for-python\"&gt;https://github.com/highcharts-for-python&lt;/a&gt;, more details about what the toolkit can do at &lt;a href=\"https://www.highcharts.com/blog/integrations/python/\"&gt;https://www.highcharts.com/blog/integrations/python/&lt;/a&gt;, and extensive documentation at &lt;a href=\"https://highchartspython.com/get-help\"&gt;https://highchartspython.com/get-help&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Given that this is v.1.0 of our Python toolkit, it&amp;#39;s obviously early days, so we&amp;#39;d love your thoughts and perspectives on the library!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/_3LlpUUp6HRq5Z8CqXOcf3_dCusLNtl5ctelt09hDJk.jpg?auto=webp&amp;v=enabled&amp;s=ca049026258518f8138fd7240933069a6a3be7fd", "width": 280, "height": 280}, "resolutions": [{"url": "https://external-preview.redd.it/_3LlpUUp6HRq5Z8CqXOcf3_dCusLNtl5ctelt09hDJk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=68a16a84c27767e5de0f2d9819019a88d82bdead", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/_3LlpUUp6HRq5Z8CqXOcf3_dCusLNtl5ctelt09hDJk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e77aeb240f13fef2e0772cc3baf5ca57bdbddf50", "width": 216, "height": 216}], "variants": {}, "id": "TR-WAEGmKFLYlEzFxgs36Pc3Ai1bg-nSDR-ZU89qSIs"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lo5c9", "is_robot_indexable": true, "report_reasons": null, "author": "highcharts", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lo5c9/highcharts_for_python/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lo5c9/highcharts_for_python/", "subreddit_subscribers": 873096, "created_utc": 1681457064.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "CPA that works at a old culture company that not as tech savvy. I\u2019m trying to get more skills that will help me for my next job.\n\nI think we have some pretty rich data and I really want to learn and use python. Is it worth it for me to learn plotty/dash to create dashboards for comparisons?\n\nI thought about some use cases where it could add value.\n\n1. Balance Sheet account analysis: Where we look at balance sheet accounts and compare them across periods. (I could see having a stacked bar graph showing values of certain line items like Accounts receivable and using the visualization to quickly find which accounts are outliers)\n\n2. OPEX Analysis (similar to above for quick identification of movers) \n\n3. Impairment Analysis(we have a large portfolio of assets so maybe it possible to map out their profitability and using figure out which assets could qualify for an asset writedown)\n\nI\u2019m sure there more, but i\u2019m just curious if it becomes a valuable set of skills.", "author_fullname": "t2_4m8xm989", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accountant here, should I learn Plotty and Dash?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12lm76g", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1681452163.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;CPA that works at a old culture company that not as tech savvy. I\u2019m trying to get more skills that will help me for my next job.&lt;/p&gt;\n\n&lt;p&gt;I think we have some pretty rich data and I really want to learn and use python. Is it worth it for me to learn plotty/dash to create dashboards for comparisons?&lt;/p&gt;\n\n&lt;p&gt;I thought about some use cases where it could add value.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Balance Sheet account analysis: Where we look at balance sheet accounts and compare them across periods. (I could see having a stacked bar graph showing values of certain line items like Accounts receivable and using the visualization to quickly find which accounts are outliers)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;OPEX Analysis (similar to above for quick identification of movers) &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Impairment Analysis(we have a large portfolio of assets so maybe it possible to map out their profitability and using figure out which assets could qualify for an asset writedown)&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I\u2019m sure there more, but i\u2019m just curious if it becomes a valuable set of skills.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12lm76g", "is_robot_indexable": true, "report_reasons": null, "author": "hsidbjakoxj", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12lm76g/accountant_here_should_i_learn_plotty_and_dash/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12lm76g/accountant_here_should_i_learn_plotty_and_dash/", "subreddit_subscribers": 873096, "created_utc": 1681452163.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As generative AI matures and expands, we can expect to see a diverse ecosystem of professions and skills emerging such as **Control Models Specialists**.\n\n**Ensuring compliance with regulations**, such as the upcoming [European AI Act](https://artificialintelligenceact.eu/), will become increasingly important. Control models specialists will develop systems to validate human-AI interactions, detect biases, and ensure adherence to privacy and security regulations.  \n\n\nFor more information on how this disruptive Technology can shape the future workforce, [read this medium article](https://medium.com/@maximehaegeman/shaping-the-future-workforce-with-generative-ai-roles-56cabe0168bb) \ud83d\udc48", "author_fullname": "t2_983tug55s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Generative AI reshaping the future Workforce", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12m412q", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.17, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1681491951.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As generative AI matures and expands, we can expect to see a diverse ecosystem of professions and skills emerging such as &lt;strong&gt;Control Models Specialists&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Ensuring compliance with regulations&lt;/strong&gt;, such as the upcoming &lt;a href=\"https://artificialintelligenceact.eu/\"&gt;European AI Act&lt;/a&gt;, will become increasingly important. Control models specialists will develop systems to validate human-AI interactions, detect biases, and ensure adherence to privacy and security regulations.  &lt;/p&gt;\n\n&lt;p&gt;For more information on how this disruptive Technology can shape the future workforce, &lt;a href=\"https://medium.com/@maximehaegeman/shaping-the-future-workforce-with-generative-ai-roles-56cabe0168bb\"&gt;read this medium article&lt;/a&gt; \ud83d\udc48&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XrnsTXBAzXjU3J_xRg28Cakfohdga0BLYWD0MOzo9OI.jpg?auto=webp&amp;v=enabled&amp;s=8e87c451ef348bb11d19fdc6a29f9696a9cdf31a", "width": 700, "height": 700}, "resolutions": [{"url": "https://external-preview.redd.it/XrnsTXBAzXjU3J_xRg28Cakfohdga0BLYWD0MOzo9OI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8bfc6d6930df0c06c69aaafbfecd03929f060d58", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/XrnsTXBAzXjU3J_xRg28Cakfohdga0BLYWD0MOzo9OI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d10fd307f305e524ef4d88ca010b7f63d45359bb", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/XrnsTXBAzXjU3J_xRg28Cakfohdga0BLYWD0MOzo9OI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f56ad7b94ddc6e2657c439d8827c747add2c360a", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/XrnsTXBAzXjU3J_xRg28Cakfohdga0BLYWD0MOzo9OI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16e9b1c58fbbf429e3a6d3e49d189dab55ce249e", "width": 640, "height": 640}], "variants": {}, "id": "A83_FVtL_M8DC_8TOXZ0adGfrLJusi1N5EjkBgIRMb8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12m412q", "is_robot_indexable": true, "report_reasons": null, "author": "Maximum247", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12m412q/generative_ai_reshaping_the_future_workforce/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12m412q/generative_ai_reshaping_the_future_workforce/", "subreddit_subscribers": 873096, "created_utc": 1681491951.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}