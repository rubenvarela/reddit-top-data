{"kind": "Listing", "data": {"after": "t3_12dy8rr", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I was sitting there yesterday on a video call interviewing for a senior role. She was telling me about how excited everyone is for the company mission. Telling me about all their backers and partners including Amazon, MSFT, governments etc.\n\nAnd I'm sitting there thinking....the mission of what, exactly? To receive a wage in exchange for helping to extract more wealth from the general population and push it toward the top few %? \n\nIsn't that what nearly all models and algorithms are doing? More efficiently transferring wealth to the top few % of people and we get a relatively tiny cut of that in return? At some point, as housing, education and healthcare costs takes up a higher and higher % of everyone's paycheck (from 20% to 50%, eventually 85%) there will be so little wealth left to extract that our \"relatively\" tiny cut of 100-200k per year will become an absolutely tiny cut as well.\n\nIsn't that what your real mission is? Even in healthcare, \"We are improving patient lives!\" you mean by lowering everyone's salaries because premiums and healthcare prices have to go up to help pay for this extremely expensive \"high tech\" proprietary medical thing that a few people benefit from? But you were able to rub elbows with (essentially bribe) enough \"key opinion leaders\" who got this thing to be covered by insurance and taxpayers?", "author_fullname": "t2_w71g3v97", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ever disassociate during job interviews because you feel like everything the company, and what you'll be doing, is just quickening the return to the feudal age?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dhmus", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 577, "total_awards_received": 2, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 577, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680781957.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680781526.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was sitting there yesterday on a video call interviewing for a senior role. She was telling me about how excited everyone is for the company mission. Telling me about all their backers and partners including Amazon, MSFT, governments etc.&lt;/p&gt;\n\n&lt;p&gt;And I&amp;#39;m sitting there thinking....the mission of what, exactly? To receive a wage in exchange for helping to extract more wealth from the general population and push it toward the top few %? &lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t that what nearly all models and algorithms are doing? More efficiently transferring wealth to the top few % of people and we get a relatively tiny cut of that in return? At some point, as housing, education and healthcare costs takes up a higher and higher % of everyone&amp;#39;s paycheck (from 20% to 50%, eventually 85%) there will be so little wealth left to extract that our &amp;quot;relatively&amp;quot; tiny cut of 100-200k per year will become an absolutely tiny cut as well.&lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t that what your real mission is? Even in healthcare, &amp;quot;We are improving patient lives!&amp;quot; you mean by lowering everyone&amp;#39;s salaries because premiums and healthcare prices have to go up to help pay for this extremely expensive &amp;quot;high tech&amp;quot; proprietary medical thing that a few people benefit from? But you were able to rub elbows with (essentially bribe) enough &amp;quot;key opinion leaders&amp;quot; who got this thing to be covered by insurance and taxpayers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [{"giver_coin_reward": null, "subreddit_id": null, "is_new": false, "days_of_drip_extension": null, "coin_price": 50, "id": "award_02d9ab2c-162e-4c01-8438-317a016ed3d9", "penny_donate": null, "award_sub_type": "GLOBAL", "coin_reward": 0, "icon_url": "https://i.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png", "days_of_premium": null, "tiers_by_required_awardings": null, "resized_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=32add54efce28cc8ce035c5e2bc89a27286a815e", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=dfb00ece05340570177df7cfa1af6d2737c0910b", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=e8b0b87b868f6cd6313e2c90975dac636e4a0412", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=2a3ad7ec2ccc57b6c65b17e2b57647a81f335039", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/p4yzxkaed5f61_oldtakemyenergy.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=d4a8ca64b391e8b057408067d77f503752c29b7e", "width": 128, "height": 128}], "icon_width": 2048, "static_icon_width": 2048, "start_date": null, "is_enabled": true, "awardings_required_to_grant_benefits": null, "description": "I'm in this with you.", "end_date": null, "sticky_duration_seconds": null, "subreddit_coin_reward": 0, "count": 2, "static_icon_height": 2048, "name": "Take My Energy", "resized_static_icons": [{"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=16&amp;height=16&amp;auto=webp&amp;v=enabled&amp;s=4efb20a46b5cee58042da74830ee914d1547236c", "width": 16, "height": 16}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=32&amp;height=32&amp;auto=webp&amp;v=enabled&amp;s=83e8bea70baef2140842017e967f163a9f530a9d", "width": 32, "height": 32}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=48&amp;height=48&amp;auto=webp&amp;v=enabled&amp;s=14fb29ce140b35a21a7cc7ee1c4d212ce0b1179d", "width": 48, "height": 48}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=64&amp;height=64&amp;auto=webp&amp;v=enabled&amp;s=533b05085677b48f15004bd7f9ff19ec5b29099f", "width": 64, "height": 64}, {"url": "https://preview.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png?width=128&amp;height=128&amp;auto=webp&amp;v=enabled&amp;s=6f767b3c289e5cb2a733b24da5f4c46d9c079bc7", "width": 128, "height": 128}], "icon_format": "PNG", "icon_height": 2048, "penny_price": 0, "award_type": "global", "static_icon_url": "https://i.redd.it/award_images/t5_q0gj4/jtw7x06j68361_TakeMyEnergyElf.png"}], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dhmus", "is_robot_indexable": true, "report_reasons": null, "author": "SnowceanDiving", "discussion_type": null, "num_comments": 187, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dhmus/ever_disassociate_during_job_interviews_because/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dhmus/ever_disassociate_during_job_interviews_because/", "subreddit_subscribers": 868788, "created_utc": 1680781526.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "With Pandas 2.0, no existing code should break and everything will work as is. However, the primary update that is subtle is the use of Apache Arrow API vs. Numpy for managing and ingesting data (using methods like read\\_csv, read\\_sql, read\\_parquet, etc). This new integration is hope to increase efficiency in terms of memory use and improving the usage of data types such string,  datatime, and categories.  \n\n\n&gt;Python data structures (lists, dictionaries, tuples, etc) are very slow and can't be used. So the data representation is not Python and is not standard, and an implementation needs to happen via Python extensions, usually implemented in C (also in C++, Rust and others). For many years, the main extension to represent arrays and perform operations on them in a fast way has been NumPy. And this is what pandas was initially built on.  \n&gt;  \n&gt;While NumPy has been good enough to make pandas the popular library it is, it was never built as a backend for dataframe libraries, and it has some important limitations.\n\n**Summary of improvements include:**\n\n* **Managing missing values:** By using Arrow, pandas is able to deal with missing values without having to implement its own version for each data type. Instead, the Apache Arrow in-memory data representation includes an equivalent representation as part of its specification\n* **Speed:** Given an example of a dataframe with 2.5 million rows running in the author's laptop, running the `endswith` function is 31.6x fasters using Apache Arrow vs. Numpy (14.9ms vs. 471ms, respectively)\n* **Interoperability:** Ingesting a data in one format and outputting it in a different format should not be challenging. For example, moving from SAS data to Latex, using Pandas &lt;2.0 would require:\n   * Load the data from SAS into a pandas dataframe\n   * Export the dataframe to a parquet file\n   * Load the parquet file from Polars\n   * Make the transformations in Polars\n   * Export the Polars dataframe into a second parquet file\n   * Load the Parquet into pandas\n   * Export the data to the final LATEX file  \nHowever, with PyArrow, the operation can be as simple as such (after Polars bug fixes and using Pandas 2.0):\n\n&amp;#8203;\n\n    loaded_pandas_data = pandas.read_sas(fname) \n    \n    polars_data = polars.from_pandas(loaded_pandas_data) \n    # perform operations with pandas polars \n    \n    to_export_pandas_data = polars.to_pandas(use_pyarrow_extension_array=True) to_export_pandas_data.to_latex()\n\n* **Expanding Data Type Support:**\n\n&gt;Arrow types are broader and better when used outside of a numerical tool like NumPy. It has better support for dates and time, including types for date-only or time-only data, different precision (e.g. seconds, milliseconds, etc.), different sizes (32 bits, 63 bits, etc.). The boolean type in Arrow uses a single bit per value, consuming one eighth of memory. It also supports other types, like decimals, or binary data, as well as complex types (for example a column where each value is a list). There is [a table](https://pandas.pydata.org/docs/dev/reference/arrays.html?highlight=arrowdtype#pyarrow) in the pandas documentation mapping Arrow to NumPy types.\n\n[https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i)", "author_fullname": "t2_48648", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Pandas 2.0 is going live, and Apache Arrow will replace Numpy, and that's a great thing!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dbhsg", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 552, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 552, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680763026.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With Pandas 2.0, no existing code should break and everything will work as is. However, the primary update that is subtle is the use of Apache Arrow API vs. Numpy for managing and ingesting data (using methods like read_csv, read_sql, read_parquet, etc). This new integration is hope to increase efficiency in terms of memory use and improving the usage of data types such string,  datatime, and categories.  &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Python data structures (lists, dictionaries, tuples, etc) are very slow and can&amp;#39;t be used. So the data representation is not Python and is not standard, and an implementation needs to happen via Python extensions, usually implemented in C (also in C++, Rust and others). For many years, the main extension to represent arrays and perform operations on them in a fast way has been NumPy. And this is what pandas was initially built on.  &lt;/p&gt;\n\n&lt;p&gt;While NumPy has been good enough to make pandas the popular library it is, it was never built as a backend for dataframe libraries, and it has some important limitations.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Summary of improvements include:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Managing missing values:&lt;/strong&gt; By using Arrow, pandas is able to deal with missing values without having to implement its own version for each data type. Instead, the Apache Arrow in-memory data representation includes an equivalent representation as part of its specification&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Given an example of a dataframe with 2.5 million rows running in the author&amp;#39;s laptop, running the &lt;code&gt;endswith&lt;/code&gt; function is 31.6x fasters using Apache Arrow vs. Numpy (14.9ms vs. 471ms, respectively)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Interoperability:&lt;/strong&gt; Ingesting a data in one format and outputting it in a different format should not be challenging. For example, moving from SAS data to Latex, using Pandas &amp;lt;2.0 would require:\n\n&lt;ul&gt;\n&lt;li&gt;Load the data from SAS into a pandas dataframe&lt;/li&gt;\n&lt;li&gt;Export the dataframe to a parquet file&lt;/li&gt;\n&lt;li&gt;Load the parquet file from Polars&lt;/li&gt;\n&lt;li&gt;Make the transformations in Polars&lt;/li&gt;\n&lt;li&gt;Export the Polars dataframe into a second parquet file&lt;/li&gt;\n&lt;li&gt;Load the Parquet into pandas&lt;/li&gt;\n&lt;li&gt;Export the data to the final LATEX file&lt;br/&gt;\nHowever, with PyArrow, the operation can be as simple as such (after Polars bug fixes and using Pandas 2.0):&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;loaded_pandas_data = pandas.read_sas(fname) \n\npolars_data = polars.from_pandas(loaded_pandas_data) \n# perform operations with pandas polars \n\nto_export_pandas_data = polars.to_pandas(use_pyarrow_extension_array=True) to_export_pandas_data.to_latex()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Expanding Data Type Support:&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Arrow types are broader and better when used outside of a numerical tool like NumPy. It has better support for dates and time, including types for date-only or time-only data, different precision (e.g. seconds, milliseconds, etc.), different sizes (32 bits, 63 bits, etc.). The boolean type in Arrow uses a single bit per value, consuming one eighth of memory. It also supports other types, like decimals, or binary data, as well as complex types (for example a column where each value is a list). There is &lt;a href=\"https://pandas.pydata.org/docs/dev/reference/arrays.html?highlight=arrowdtype#pyarrow\"&gt;a table&lt;/a&gt; in the pandas documentation mapping Arrow to NumPy types.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i\"&gt;https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dbhsg", "is_robot_indexable": true, "report_reasons": null, "author": "forbiscuit", "discussion_type": null, "num_comments": 50, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dbhsg/pandas_20_is_going_live_and_apache_arrow_will/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dbhsg/pandas_20_is_going_live_and_apache_arrow_will/", "subreddit_subscribers": 868788, "created_utc": 1680763026.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm currently pursing a BS in Data Science and currently exploring future career options. I am interested in working in a social/gov or health related industry as a data scientist or analyst. \n\nTo those of you in these industries, I am very curious about your skill set, education path, job satisfaction, and work-to-life balance. Do you feel like you are making a positive impact? Thank you in advance!", "author_fullname": "t2_5keqevvl", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are Data Scientists in health or social work related fields in this sub?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dvu9v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680810808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m currently pursing a BS in Data Science and currently exploring future career options. I am interested in working in a social/gov or health related industry as a data scientist or analyst. &lt;/p&gt;\n\n&lt;p&gt;To those of you in these industries, I am very curious about your skill set, education path, job satisfaction, and work-to-life balance. Do you feel like you are making a positive impact? Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dvu9v", "is_robot_indexable": true, "report_reasons": null, "author": "lana0415", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dvu9v/are_data_scientists_in_health_or_social_work/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dvu9v/are_data_scientists_in_health_or_social_work/", "subreddit_subscribers": 868788, "created_utc": 1680810808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have some time series data where I've noticed that the error/accuracy of my predictions from a large ensemble of models has a moderate autocorrelation to past predictions. If a model was accurate yesterday then it has a decent probability of being accurate today. I was wondering if anyone has any ideas on how to capitalize on this.\n\nMy current strategy has been to simply weight each model in the ensemble based on a rolling 3 day correlation to the target live variable. Models with a high 3 day average would have a higher weight and vice versa. I've had decent success with this strategy so far, but I'm trying to brainstorm ways to take this to the next level.\n\nOther ideas I had were to train a LSTM model on the historical performance of each model in attempt to predict the current days performance.\n\nCurios to hear if anyone has any ideas on how improve on this.\n\n[Autocorrelation of model predictions](https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e5e89e0e35a7380bea579449cedf93d7f8bb577)", "author_fullname": "t2_d43cut1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are ways to capitalize on autocorrelation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "media_metadata": {"5fae6kxce6sa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 81, "x": 108, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55809e12059b4d9a55bcf068b56fc33329a56d65"}, {"y": 162, "x": 216, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=84c3fa6c7341b66e9fa108232da6b434c8d83738"}, {"y": 240, "x": 320, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ee9d9bf6e03381bbca907adf7e5a547b868b77c"}, {"y": 480, "x": 640, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=95022d6d89919b09f8ec27d3d4f512e97f82aa0f"}], "s": {"y": 480, "x": 640, "u": "https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3e5e89e0e35a7380bea579449cedf93d7f8bb577"}, "id": "5fae6kxce6sa1"}}, "name": "t3_12d79g9", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/URbWOLYXKtJSLomT80r1NC-7LDMcnD_rgTfKZQ3EilE.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680750540.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have some time series data where I&amp;#39;ve noticed that the error/accuracy of my predictions from a large ensemble of models has a moderate autocorrelation to past predictions. If a model was accurate yesterday then it has a decent probability of being accurate today. I was wondering if anyone has any ideas on how to capitalize on this.&lt;/p&gt;\n\n&lt;p&gt;My current strategy has been to simply weight each model in the ensemble based on a rolling 3 day correlation to the target live variable. Models with a high 3 day average would have a higher weight and vice versa. I&amp;#39;ve had decent success with this strategy so far, but I&amp;#39;m trying to brainstorm ways to take this to the next level.&lt;/p&gt;\n\n&lt;p&gt;Other ideas I had were to train a LSTM model on the historical performance of each model in attempt to predict the current days performance.&lt;/p&gt;\n\n&lt;p&gt;Curios to hear if anyone has any ideas on how improve on this.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5fae6kxce6sa1.png?width=640&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=3e5e89e0e35a7380bea579449cedf93d7f8bb577\"&gt;Autocorrelation of model predictions&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12d79g9", "is_robot_indexable": true, "report_reasons": null, "author": "_McFuggin_", "discussion_type": null, "num_comments": 1, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12d79g9/what_are_ways_to_capitalize_on_autocorrelation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12d79g9/what_are_ways_to_capitalize_on_autocorrelation/", "subreddit_subscribers": 868788, "created_utc": 1680750540.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am currently building some forecasting models using a lightgbm regressor. \n\nIt is in an auto platform that I'm evaluating for work, and unfortunately it just auto creates the train / test split.  This is done by randomly splitting and shuffling the data. So the training set may include the very first day and the very last day. It also auto-chooses the model so I have no choice in that either.\n\nIt auto creates features from the date column, e.g. day of year, week of year etc. And uses those as predictors.\n\nI've read comments that this training split is a bad idea, and a walk forward method should be used. \n\nI agree that walk forward makes more sense, and i use that for when I'm doing time series in python. \n\nBUT... i haven't seen an explanation as to why randomly shuffling and splitting the data is not recommended. \n\nCan somebody explain?", "author_fullname": "t2_vtpzvynk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Using tree based methods for forecasting. Why is it a bad idea to shuffle the data for training and testing?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dcoi5", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680767062.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently building some forecasting models using a lightgbm regressor. &lt;/p&gt;\n\n&lt;p&gt;It is in an auto platform that I&amp;#39;m evaluating for work, and unfortunately it just auto creates the train / test split.  This is done by randomly splitting and shuffling the data. So the training set may include the very first day and the very last day. It also auto-chooses the model so I have no choice in that either.&lt;/p&gt;\n\n&lt;p&gt;It auto creates features from the date column, e.g. day of year, week of year etc. And uses those as predictors.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve read comments that this training split is a bad idea, and a walk forward method should be used. &lt;/p&gt;\n\n&lt;p&gt;I agree that walk forward makes more sense, and i use that for when I&amp;#39;m doing time series in python. &lt;/p&gt;\n\n&lt;p&gt;BUT... i haven&amp;#39;t seen an explanation as to why randomly shuffling and splitting the data is not recommended. &lt;/p&gt;\n\n&lt;p&gt;Can somebody explain?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dcoi5", "is_robot_indexable": true, "report_reasons": null, "author": "Minimum-Lemon-402", "discussion_type": null, "num_comments": 22, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dcoi5/using_tree_based_methods_for_forecasting_why_is/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dcoi5/using_tree_based_methods_for_forecasting_why_is/", "subreddit_subscribers": 868788, "created_utc": 1680767062.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm wondering what I can do to be ahead of my peers. I know the market is \"saturated\" with a lot of hype in the data science field. But what are things you wish you knew? For example, paying attention in class or taking a course online... \n\nThere's so much out there nowadays, but knowing all of it isn't necessary.\n\n I am putting more time into Math, SQL, and Python. Any recommendations you can give me would be cool.", "author_fullname": "t2_vngkp0a1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What recommendations would you give to an undergraduate or things you would've done differently while in school?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dx054", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680813132.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m wondering what I can do to be ahead of my peers. I know the market is &amp;quot;saturated&amp;quot; with a lot of hype in the data science field. But what are things you wish you knew? For example, paying attention in class or taking a course online... &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s so much out there nowadays, but knowing all of it isn&amp;#39;t necessary.&lt;/p&gt;\n\n&lt;p&gt;I am putting more time into Math, SQL, and Python. Any recommendations you can give me would be cool.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dx054", "is_robot_indexable": true, "report_reasons": null, "author": "kaxziss", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dx054/what_recommendations_would_you_give_to_an/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dx054/what_recommendations_would_you_give_to_an/", "subreddit_subscribers": 868788, "created_utc": 1680813132.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm having trouble visualizing datasets with correlations that are non-spherical like the famous moons example. In my own time, I've played around with KNN and DBSCAN and it feels like KNN outperforms. Researching further, it looks like DBSCAN is especially good with image and geographical data. Does anyone else have any more examples they could share?", "author_fullname": "t2_45bgy9c3", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What are some real life use cases where DBSCAN outperforms KNN?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dsiom", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680804271.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m having trouble visualizing datasets with correlations that are non-spherical like the famous moons example. In my own time, I&amp;#39;ve played around with KNN and DBSCAN and it feels like KNN outperforms. Researching further, it looks like DBSCAN is especially good with image and geographical data. Does anyone else have any more examples they could share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dsiom", "is_robot_indexable": true, "report_reasons": null, "author": "EasternStuff5015", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dsiom/what_are_some_real_life_use_cases_where_dbscan/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dsiom/what_are_some_real_life_use_cases_where_dbscan/", "subreddit_subscribers": 868788, "created_utc": 1680804271.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Hi there :) \n\nI am currently evaluating to educate myself in the field of AI and Data Analysis. \n\nWhat I am curious now, is how realistic it is to find jobs as a freelancer, if I want to a) work with F#, and b) work on my own codebases.\n\nUsually in software jobs, you work on the existing codebases of previous colleagues, in this field it kinda seems this could be different.\n\nAs far as I understand, do I get simply get data from a database, that I have to access, evaluate and present.\n\nSo I thought if that's the case, I could choose the stack on my own anyway, and I would be not so bound to Python, Excel and the common SQL DSL. \n\nHow independent can I choose my technology as a freelancer?", "author_fullname": "t2_78je0kwv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "F# Freelancing", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12d94qr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680761228.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680755704.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there :) &lt;/p&gt;\n\n&lt;p&gt;I am currently evaluating to educate myself in the field of AI and Data Analysis. &lt;/p&gt;\n\n&lt;p&gt;What I am curious now, is how realistic it is to find jobs as a freelancer, if I want to a) work with F#, and b) work on my own codebases.&lt;/p&gt;\n\n&lt;p&gt;Usually in software jobs, you work on the existing codebases of previous colleagues, in this field it kinda seems this could be different.&lt;/p&gt;\n\n&lt;p&gt;As far as I understand, do I get simply get data from a database, that I have to access, evaluate and present.&lt;/p&gt;\n\n&lt;p&gt;So I thought if that&amp;#39;s the case, I could choose the stack on my own anyway, and I would be not so bound to Python, Excel and the common SQL DSL. &lt;/p&gt;\n\n&lt;p&gt;How independent can I choose my technology as a freelancer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12d94qr", "is_robot_indexable": true, "report_reasons": null, "author": "Outrageous_Stomach_8", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12d94qr/f_freelancing/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12d94qr/f_freelancing/", "subreddit_subscribers": 868788, "created_utc": 1680755704.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Good Evening,\n\nI am a mathematics and science high school teacher, and I just completed a data science bootcamp from SMU. I am wondering if joining professional data science organizations would be a value, and if so, which organizations would be the best one to join. I live in Fort Worth, TX.  \n\n\nI am choosing from among these:\n\n[https://www.datascienceassn.org/](https://www.datascienceassn.org/)\n\n[https://adasci.org/](https://adasci.org/)\n\n[https://www.informs.org/](https://www.informs.org/)\n\n[https://www.asist.org/](https://www.asist.org/)\n\nMany of them require dues and they come with courses to further my skills. If you have any recommendations about these, it would be helpful for me.", "author_fullname": "t2_8pawae23u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Science Professional Organizations", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12e7drd", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680836106.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Good Evening,&lt;/p&gt;\n\n&lt;p&gt;I am a mathematics and science high school teacher, and I just completed a data science bootcamp from SMU. I am wondering if joining professional data science organizations would be a value, and if so, which organizations would be the best one to join. I live in Fort Worth, TX.  &lt;/p&gt;\n\n&lt;p&gt;I am choosing from among these:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.datascienceassn.org/\"&gt;https://www.datascienceassn.org/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://adasci.org/\"&gt;https://adasci.org/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.informs.org/\"&gt;https://www.informs.org/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.asist.org/\"&gt;https://www.asist.org/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Many of them require dues and they come with courses to further my skills. If you have any recommendations about these, it would be helpful for me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12e7drd", "is_robot_indexable": true, "report_reasons": null, "author": "sajid1760", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12e7drd/data_science_professional_organizations/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12e7drd/data_science_professional_organizations/", "subreddit_subscribers": 868788, "created_utc": 1680836106.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "As a soon to be graduate in masters in big data analytics, I'd like to know what certificates could boost my job search. I don't have any experience in the industry yet so it's difficult to score much interviews. Any suggestions?", "author_fullname": "t2_ddtcnl2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Certifications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12e64mw", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680833297.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a soon to be graduate in masters in big data analytics, I&amp;#39;d like to know what certificates could boost my job search. I don&amp;#39;t have any experience in the industry yet so it&amp;#39;s difficult to score much interviews. Any suggestions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12e64mw", "is_robot_indexable": true, "report_reasons": null, "author": "Sukhman18", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12e64mw/certifications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12e64mw/certifications/", "subreddit_subscribers": 868788, "created_utc": 1680833297.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": " Hello everyone! This is my first time using Reddit, but my only way to get some insight on what exactly data science is. Or rather if it would be the right fit for me. I have a joint honours in philosophy and biological anthropology that I completed in 2021. My goal has always been to go to medical school and I got so close but ultimately did not get in. My whole life has been geared around the life sciences and health/ethics/well-being so I never considered this field or even knew about it. But I don\u2019t want to keep trying my luck at that when I could potentially be happy doing something else that has a greater than 6% chance of success (Canadian med school admission is ridiculous).  \n\nIn my last year of my undergrad I took a statistics course and ended up getting an A+ but what was most surprising is how much I enjoyed it. Particularly, understanding the real life significance of stats. I also have taken symbolic logic courses and did very well with those too and i\u2019m thinking I might be well suited to the field of DS. It just feels like such a big leap for me and I am child of immigrants who never even finished high school so they can\u2019t really help me navigate this stuff but I LOVE learning and want to do this but want a practical career as well.   \n\nSo my **questions** are: what is working in DS actually like? Can this career give me a good lifestyle. I.e. could it potentially sustain a family one day? And how do I make this shift (undergrad, certificate, masters)?  \nbonus: will I only have the option to work for businesses or is there the ability to help people too?\n\ntia!", "author_fullname": "t2_8p55lrtpu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is a career switch to DS a good choice?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e2qo0", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680825568.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! This is my first time using Reddit, but my only way to get some insight on what exactly data science is. Or rather if it would be the right fit for me. I have a joint honours in philosophy and biological anthropology that I completed in 2021. My goal has always been to go to medical school and I got so close but ultimately did not get in. My whole life has been geared around the life sciences and health/ethics/well-being so I never considered this field or even knew about it. But I don\u2019t want to keep trying my luck at that when I could potentially be happy doing something else that has a greater than 6% chance of success (Canadian med school admission is ridiculous).  &lt;/p&gt;\n\n&lt;p&gt;In my last year of my undergrad I took a statistics course and ended up getting an A+ but what was most surprising is how much I enjoyed it. Particularly, understanding the real life significance of stats. I also have taken symbolic logic courses and did very well with those too and i\u2019m thinking I might be well suited to the field of DS. It just feels like such a big leap for me and I am child of immigrants who never even finished high school so they can\u2019t really help me navigate this stuff but I LOVE learning and want to do this but want a practical career as well.   &lt;/p&gt;\n\n&lt;p&gt;So my &lt;strong&gt;questions&lt;/strong&gt; are: what is working in DS actually like? Can this career give me a good lifestyle. I.e. could it potentially sustain a family one day? And how do I make this shift (undergrad, certificate, masters)?&lt;br/&gt;\nbonus: will I only have the option to work for businesses or is there the ability to help people too?&lt;/p&gt;\n\n&lt;p&gt;tia!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12e2qo0", "is_robot_indexable": true, "report_reasons": null, "author": "Kant-B-Bothered", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12e2qo0/is_a_career_switch_to_ds_a_good_choice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12e2qo0/is_a_career_switch_to_ds_a_good_choice/", "subreddit_subscribers": 868788, "created_utc": 1680825568.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I have a terrible time committing basic functions to memory and often times find myself googling something as basic as instantiating an X-by-Y dataframe despite doing repeatedly. Here's [this](https://pandas-dataframe-generator.glitch.me) if anyone else is as lazy as I am.\n\nTell me I'm not alone?", "author_fullname": "t2_8qld5k9d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I made a simple webpage for generating Pandas dataframes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dz6xy", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680817669.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a terrible time committing basic functions to memory and often times find myself googling something as basic as instantiating an X-by-Y dataframe despite doing repeatedly. Here&amp;#39;s &lt;a href=\"https://pandas-dataframe-generator.glitch.me\"&gt;this&lt;/a&gt; if anyone else is as lazy as I am.&lt;/p&gt;\n\n&lt;p&gt;Tell me I&amp;#39;m not alone?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dz6xy", "is_robot_indexable": true, "report_reasons": null, "author": "additional_pyl0ns", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dz6xy/i_made_a_simple_webpage_for_generating_pandas/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dz6xy/i_made_a_simple_webpage_for_generating_pandas/", "subreddit_subscribers": 868788, "created_utc": 1680817669.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "So in SF, Order and Order Products are two separate tables, and when we use it for analysis, we pull both tables, and join them.\n\nin ERP, transaction and transaction lines are also separate tables, and we join them when we need to use them.\n\nSo my question is if i do transformation and then sending them to the Data Warehouse, should i still transform them separately? the Kimball book suggests joining the Transaction and Transaction Line inside the data warehouse as one big table. but this is from a 2007 article...\n\nwhat's the best way now in 2023? or db is Snowflake.\n\nforgot to add our setup. We use Fivetran to bring over SF and ERP data into Snowflake warehouse (without any changes), and then plan to use DBT to do some transformation (getting rid of a lot of useless columns, and customizations) and then bring it back into snowflake as Final table for endpoint analysts to use. My question is regarding the DBT steps.", "author_fullname": "t2_cooe1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Question about Data Warehouse for ERP/CRM data (Specifically Transaction Data)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dv3un", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680810319.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680809378.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So in SF, Order and Order Products are two separate tables, and when we use it for analysis, we pull both tables, and join them.&lt;/p&gt;\n\n&lt;p&gt;in ERP, transaction and transaction lines are also separate tables, and we join them when we need to use them.&lt;/p&gt;\n\n&lt;p&gt;So my question is if i do transformation and then sending them to the Data Warehouse, should i still transform them separately? the Kimball book suggests joining the Transaction and Transaction Line inside the data warehouse as one big table. but this is from a 2007 article...&lt;/p&gt;\n\n&lt;p&gt;what&amp;#39;s the best way now in 2023? or db is Snowflake.&lt;/p&gt;\n\n&lt;p&gt;forgot to add our setup. We use Fivetran to bring over SF and ERP data into Snowflake warehouse (without any changes), and then plan to use DBT to do some transformation (getting rid of a lot of useless columns, and customizations) and then bring it back into snowflake as Final table for endpoint analysts to use. My question is regarding the DBT steps.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dv3un", "is_robot_indexable": true, "report_reasons": null, "author": "tyw214", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dv3un/question_about_data_warehouse_for_erpcrm_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dv3un/question_about_data_warehouse_for_erpcrm_data/", "subreddit_subscribers": 868788, "created_utc": 1680809378.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I am thrilled to announce to all of you who are looking for real-world project to show in portfolio, i am hosting a 2 months local chapter in Omdena \"Developing a forest fire detection system in Algeria using satellite imagery and machine learning\", beginners and experts are all welcome to join \n\nhttps://omdena.com/projects/developing-a-forest-fire-detection-and-monitoring-system-for-algeria/", "author_fullname": "t2_3n9hivan", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "real-world AI project looking for collaborators(beginners or experts?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dtb34", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.57, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680805826.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am thrilled to announce to all of you who are looking for real-world project to show in portfolio, i am hosting a 2 months local chapter in Omdena &amp;quot;Developing a forest fire detection system in Algeria using satellite imagery and machine learning&amp;quot;, beginners and experts are all welcome to join &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://omdena.com/projects/developing-a-forest-fire-detection-and-monitoring-system-for-algeria/\"&gt;https://omdena.com/projects/developing-a-forest-fire-detection-and-monitoring-system-for-algeria/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/zGfIM0Ozo1Bxy5vziT62CCYvKY749u7qMPGqoTZBGhg.jpg?auto=webp&amp;v=enabled&amp;s=24340473ae28813dadf53122fa03f75c4f4a5c5a", "width": 2560, "height": 1700}, "resolutions": [{"url": "https://external-preview.redd.it/zGfIM0Ozo1Bxy5vziT62CCYvKY749u7qMPGqoTZBGhg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9d3f824d145b1cdd427774d56402167c17b31de3", "width": 108, "height": 71}, {"url": "https://external-preview.redd.it/zGfIM0Ozo1Bxy5vziT62CCYvKY749u7qMPGqoTZBGhg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36185c9f84bb07da964ab5a2d86b3926f44937b7", "width": 216, "height": 143}, {"url": "https://external-preview.redd.it/zGfIM0Ozo1Bxy5vziT62CCYvKY749u7qMPGqoTZBGhg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d10b096d171aec406e37626ec3a4ec6afc86fcdb", "width": 320, "height": 212}, {"url": "https://external-preview.redd.it/zGfIM0Ozo1Bxy5vziT62CCYvKY749u7qMPGqoTZBGhg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=716af971639fc440785b1e3fcde3c0fb57c94c2c", "width": 640, "height": 425}, {"url": "https://external-preview.redd.it/zGfIM0Ozo1Bxy5vziT62CCYvKY749u7qMPGqoTZBGhg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16e3f9e6a5cc56f4d6b63946bbfce771741c9b33", "width": 960, "height": 637}, {"url": "https://external-preview.redd.it/zGfIM0Ozo1Bxy5vziT62CCYvKY749u7qMPGqoTZBGhg.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f80eb88d1243f4a82171a0a7c3e3383810c04fd", "width": 1080, "height": 717}], "variants": {}, "id": "971COXhfyQiSUBEjaXtCVhdLOMGXNKteAFNJ15t_Dvo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dtb34", "is_robot_indexable": true, "report_reasons": null, "author": "how_why123", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dtb34/realworld_ai_project_looking_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dtb34/realworld_ai_project_looking_for/", "subreddit_subscribers": 868788, "created_utc": 1680805826.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Is there an established framework that categorizes different types of synthetic data based on utility?  Example: How do I discern between  basic code-checking data and realistic predictive data?", "author_fullname": "t2_8jclstavf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Categories of synthetic data based on usage", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dopqj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680796814.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an established framework that categorizes different types of synthetic data based on utility?  Example: How do I discern between  basic code-checking data and realistic predictive data?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dopqj", "is_robot_indexable": true, "report_reasons": null, "author": "Head_Confusion_3036", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dopqj/categories_of_synthetic_data_based_on_usage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dopqj/categories_of_synthetic_data_based_on_usage/", "subreddit_subscribers": 868788, "created_utc": 1680796814.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "P.s- Will share additional information as we discuss as I am also exploring the same.\n\nEdit 1-  I've got high level customer info about age, investor type, province and their type of account.\n\nI have also got customer transaction information and how long they've stayed invested also the redemption date.", "author_fullname": "t2_du1e9k98", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Working on a customer redemption classification problem with a very limited amount of data. With the aim to predict redemptions in the next 3 months. I've got customer info and transaction info. What are your ideas to go about the same.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12da863", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.62, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680760982.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680758948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;P.s- Will share additional information as we discuss as I am also exploring the same.&lt;/p&gt;\n\n&lt;p&gt;Edit 1-  I&amp;#39;ve got high level customer info about age, investor type, province and their type of account.&lt;/p&gt;\n\n&lt;p&gt;I have also got customer transaction information and how long they&amp;#39;ve stayed invested also the redemption date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12da863", "is_robot_indexable": true, "report_reasons": null, "author": "Lazzy_Engineer", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12da863/working_on_a_customer_redemption_classification/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12da863/working_on_a_customer_redemption_classification/", "subreddit_subscribers": 868788, "created_utc": 1680758948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Arrow String improvements in Pandas/Dask DataFrames", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "tooling", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_12dk7v7", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Tooling", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6uwjML35z-FxsE_pRboam-GDwq291V8TrPtVWQof1WA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680787351.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?auto=webp&amp;v=enabled&amp;s=6859023178db536f94b55bb63382a7a09109032b", "width": 792, "height": 490}, "resolutions": [{"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9618b07d38c42c9b44e92644148b22822523cd9e", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e340c81948a4fba6552f26b728af8711788bc35", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ac1c66e5bf7be22194afbe2e088a490ce2f46dd", "width": 320, "height": 197}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4cd149bae3ae18eb7da5790f10a0c5f43a190ed", "width": 640, "height": 395}], "variants": {}, "id": "V5FocKkZS3WgTL0zjz6iujrafykJdPf8Om_OTahpOug"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "aaf5d8cc-d780-11e7-a4a5-0e68d01eab56", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dk7v7", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dk7v7/arrow_string_improvements_in_pandasdask_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "subreddit_subscribers": 868788, "created_utc": 1680787351.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_4aj5kedr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Copilot for Data Analysts and Scientists", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 83, "top_awarded_type": null, "hide_score": false, "name": "t3_12dj3p1", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/S4NvIz2Yu4cw-LbC6opvLpZmOuxYNBzIAuO2mwv5PVs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680784876.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "probeai.app", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "http://probeai.app", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?auto=webp&amp;v=enabled&amp;s=39c96244d01c619f072638ed1b135771ffdc1f9a", "width": 1512, "height": 902}, "resolutions": [{"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e7ea3edcfc856d16386fdc3023b6e1e968622acc", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b2eb779a93aa7c0f078292864a2070b5e53b9d5a", "width": 216, "height": 128}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cdc2434ca938edc67a4307496eb2357296c079f8", "width": 320, "height": 190}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=72f8af6a5256d2e7718adb0976776b37cfe36921", "width": 640, "height": 381}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=30ce965e82973f4408e202bcdb741236d619b520", "width": 960, "height": 572}, {"url": "https://external-preview.redd.it/bqKJsfDBnUu9agyZpaXF0gqsDWAeq7MbenGEIICYRYk.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8d4b63fc799e44bc3cac33118f5bcbc65319f4e4", "width": 1080, "height": 644}], "variants": {}, "id": "X96x_BIF_c3FBKbM5vL0UruDpZtfeCA_t9_iFhoPIKY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dj3p1", "is_robot_indexable": true, "report_reasons": null, "author": "atharvakharbade", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dj3p1/copilot_for_data_analysts_and_scientists/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "http://probeai.app", "subreddit_subscribers": 868788, "created_utc": 1680784876.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "user_reports": [], "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My new project with Arduino: Ultra-Tiny Solution of Daily Activities \u200b\u200bRecognition. Can you repeat? Please, leave your comments.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "projects", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "name": "t3_12dge9v", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "author_fullname": "t2_8sds8", "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Projects", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sYUBvdJXNIEroAyPdmrxhzxZG_3FeN7XJhUNm2cF35U.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": false, "mod_note": null, "crosspost_parent_list": [{"approved_at_utc": null, "subreddit": "arduino", "selftext": "Prototyping: a Hand Tracker with Nicla Sense ME and Bosch sensors\n\n## Things used in this project:\n\n**Hardware components**\n\n[Arduino Nicla Sense ME](https://referral.element14.com/OrderCodeView?fsku=3874817&amp;nsku=99AJ4432&amp;COM=e14c-noscript&amp;CMP=e14c-noscript&amp;osetc=e14-noscript-tracking-loss) \u00d71  \n[Bosch BHI260 sensor](https://www.bosch-sensortec.com/news/new-generation-of-smart-sensor-hubs-bhi260-and-bha260.html)\n\n## Software apps and online services\n\n[Neuton Tiny ML](https://neuton.ai/)  \n[Arduino IDE](https://www.arduino.cc/en/software)\n\nIn my project, I want to show an example of how you can significantly increase the complexity of intelligence functionality, and determine what a wearable device can recognize with a very small and accurate neural network. Also, you will find a detailed guide on how to implement Multiple Daily Activities recognition using the Nicla Sense ME. You also can\u00a0[**participate in my practical webinar**](https://us06web.zoom.us/webinar/register/3416800160594/WN_X0wH_gaqSiylUGehTFXlSQ)\u00a0on\u00a0**April 11 at 5 pm CET**. Where you can watch how to create this solution from leaders from Neuton.AI, Arduino and Bosch. Don't miss this opportunity to learn from experts directly.\n\n[https://youtu.be/3nkoJZD\\_Lh8](https://youtu.be/3nkoJZD_Lh8)\n\nIf you have Nicla Sense ME, you can try my pre-trained model in your experiment/project/classes.\n\nOur test results show that the model perfectly recognizes the movements of 30 participants whose data was not used for training the model.\n\nFollowing this tutorial, you also can easily make your own experiment and train your neural network on your data to recognize the same or different movements.\n\nWe believe that the ability to recognize an extremely complex and similar action using small neural networks opens up a new era for always-on devices.\n\nBelow, you will find a full tutorial on how to reproduce this experiment, including data collection, model training, model embedding and inference. In addition, we have created a public repository with a pre-trained model that has demonstrated good generalization on new users. It is a precompiled model archive that you may just download and embed into your Nicla Sense ME with a few clicks of a mouse and start testing.\n\n[**https://github.com/NeutonTinyML/hand-activity-recognition**](https://github.com/NeutonTinyML/hand-activity-recognition)\n\n## Data Collection with Nicla Sense ME\n\nFor this case, I\u2019ve selected 5 types of activities (for your experiment, you may choose other activities):\n\n* Washing hands\n* Brushing teeth\n* Clapping\n* Brushing hair\n* Random activity (negative class)\n\nFor each activity type, I collected sensor data with Nicla Sense ME for 10 minutes non-stop. Data was collected from 7 different people from 7 different locations. It was quite a challenging, yet fun process because, for proper data collection, it was undesirable to interrupt. Here\u2019s some advice for those who plan to re-conduct my experiment: before collecting data for another activity, get some rest! However, if 10 minutes of constant hair brushing is too much, you can do two 5-minute data collection streams and then concatenate the two files.\n\nFor the negative class, we collected 60 minutes of data. There\u2019s no need to do anything special this time; just get on with your everyday activities. Type on the keyboard, operate a mouse, answer calls, drink tea, or do whatever you want. Just be sure you don\u2019t sit completely idle with your hands still during negative class data collection. The negative class is a subset of everything else excluding the 4 classes outlined above.\n\n## Steps:\n\nNow let me provide you with detailed guidelines on how to tune Arduino\u2019s board, Nicla Sense ME to collect data.\n\n**1.**\u00a0Visit\u00a0[**https://www.arduino.cc/en/software**](https://www.arduino.cc/en/software)[,](https://www.arduino.cc/en/software)\u00a0download Arduino IDE 2.x.x for your OS, and install a package.\n\n**2.**\u00a0Open Arduino IDE, click on the\u00a0**Boards Manager**\u00a0icon, type \u201cnicla sense me\u201d and install the\u00a0**Arduino Mbed OS Nicla Boards**\u00a0package.\n\nClick on the\u00a0**Library Manager icon**, type \u201cnicla sense me\u201d in the search box and install two libraries:\u00a0**Arduino\\_BHY2**\u00a0and\u00a0**ArduinoBLE**.\n\nType \u201cprintf\u201d in the search box and install\u00a0**LibPrintf**.\n\n**3.**\u00a0In the main menu, select\u00a0**File-&gt;Examples-&gt;Arduino\\_BHY2&gt;BHYFirmwareUpdate**.\n\n**4.**\u00a0Connect your\u00a0**Nicla Sense ME**\u00a0board to the USB and select the port in the dropdown menu.\n\n**5.**\u00a0Click on the Upload button and wait until the uploading process is finished.\n\n**6.**\u00a0Click on the\u00a0**Serial Monitor**\u00a0icon and select\u00a0**115200 baud**, then click the RESET button on your Nicla board.\n\n**7.**\u00a0After 5-10 seconds you\u2019ll see that the BHY firmware is uploaded.\n\n**8.** Download the Arduino precompiled library for sensor data collection using a Nicla Sense ME from [**the Neuton repository.**](https://github.com/NeutonTinyML/hand-activity-recognition)\n\nmodel -&gt; Arduino\\_Neuton.zip (do not uncompress the archive)\n\n**9.** Install the \u2018Arduino\\_Neuton.zip\u2019 model into the Arduino IDE:\n\nSketch -&gt; Include Library -&gt; Add .ZIP Library\u2026 (point to the downloaded \u2018Arduino\\_Neuton.zip\u2019 archive)\n\nhttps://preview.redd.it/dt1grc4jg8sa1.jpg?width=1840&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1e386acb758209202ac65e6407f3611f045e4d27\n\n**10.** Quit &amp; restart the Arduino IDE (it will take up to a minute for the Arduino IDE to index all of it\u2019s examples, including the freshly installed \u2018Arduino\\_Neuton\u2019)\n\n**11.** Open the installed example: File -&gt; Examples -&gt; Arduino\\_Neuton -&gt; Inertial\\_Sensor\\_Data\\_Collection (a new Arduino IDE window will pop up, close the previous Arduino IDE window)\n\nhttps://preview.redd.it/4b39v85og8sa1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=97e3e32ce2dbfe134d3fb3531f8e195d5f9883df\n\n**12.** Connect your Nicla Sense ME to your computer\u2019s USB port.\n\n**13.** In the IDE select your MCU and upload the firmware.\n\n**14.**\u00a0After the firmware is uploaded (1-2 minutes), open the Serial Monitor - you should see the sensors readings (In case you will see the connection error \u2013 restart the Arduino IDE and open the Serial Monitor once again)\n\nNow you can unplug the board from the USB port.\n\n**15.**\u00a0Attach the board firmly to the watch or bracelet. The Micro-USB port should be in the bottom-right corner.\n\nhttps://preview.redd.it/u3q15z2er3ra1.png?width=612&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e7734fafef3cfa398d6a23d99c8f7ad5a7f20a8a\n\n**16.**\u00a0Put a watch or a bracelet with a circuit board on your right hand, and connect the cable to it. Make sure that the cable near the connector is not taut so as not to damage the board.\n\n**17.**\u00a0Depending on your OS, open the serial port and try to log data. A serial port can be seen in the hint.\n\nhttps://preview.redd.it/504en08gr3ra1.png?width=1306&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=03994ba652b0ea9c154c699a2aa6fe4af274d5d2\n\nUse this command in the terminal if you\u2019re on macOS:\n\n**stty -f /dev/cu.usbmodem85EB3A0F2 115200 | cat /dev/cu.usbmodem85EB3A0F2 | tee 4\\_brushing\\_hair.csv**\n\nThis will set the port speed to 115200 baud. Print out data from the serial port as text and save it to a CSV file.\n\nhttps://preview.redd.it/e5oi0ljir3ra1.png?width=1596&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2c686bee67e11d127b9d7e4cbcfb3370271a95d8\n\nPress Control-C to stop recording. There will be a CSV file with data from sensors (accelerometer, linear accelerometer, and gyroscope) and timestamp.\n\nhttps://preview.redd.it/bks9n7hjr3ra1.png?width=1592&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f452d539b15cfbfadeb7296bd1e62fa99fe4a5bd\n\n**18.**\u00a0Start recording the movement. Press the Reset button on the Nicla board and restart logging to the file.\n\nFor example, clap for 10 minutes (use the stopwatch application in your smartphone). During this process, change the position of your hands to make the data more diverse. Do not stop moving while the recording is in progress.\n\nThe recording will start in \\~7 seconds after pushing the Reset button.\n\nOnce 10 minutes are finished, continue the movement for 10-20 seconds and stop recording.\n\n**19.**\u00a0Repeat step 18 for each activity type (use different file names):\n\nhttps://preview.redd.it/ccm1r5eor3ra1.png?width=288&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a3b0606ae26446d326daf001fa936823bf06a05\n\nReset the board before recording each movement.\n\n**20.**\u00a0Repeat step 18 for the negative class and start recording for 1 hour.\n\n[**Here\u2019s the video**](https://drive.google.com/file/d/106bVkRNZPmXwlctTsF9wtmeEbS9FMAgg/view?usp=share_link) on how the data should be collected (make sure to do the actual activity: i.e. brushing hair should be done with an actual hairbrush on your head, not in the air).\n\n## Model training\n\nAfter you have collected data on all activities and combined them into one dataset, proceed to the\u00a0[**Neuton.ai**](https://login.neuton.ai/?utm_source=reddit&amp;utm_medium=project&amp;utm_campaign=Ultra-Tiny+Solution+of+Daily+Activities+%E2%80%8B%E2%80%8BRecognition)\u00a0platform to train your model.\n\n**1.**\u00a0Create a new solution\n\nhttps://preview.redd.it/gr3pw96tr3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=77506bbc9f1fa56bc07a6883c27af09dcb9b747c\n\n**2.**\u00a0Select the data type and upload the data\n\nhttps://preview.redd.it/3ththa1ur3ra1.jpg?width=892&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=58ce6f80240f2c8cbb66cf9c4f13e8382ba77eaa\n\nhttps://preview.redd.it/tvqrx1nur3ra1.jpg?width=1426&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=504dcaa6ce7ed6622c3b7cc57de045e218cd3c70\n\n**3.**\u00a0In the left field remove variables lacc\\_X, lacc\\_Y, lacc\\_Z (linear accelerometer is not needed for this model); in the right field select the target variable and click next.\n\nhttps://preview.redd.it/18m11zryr3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1b25cc45b4de00892e38514166178285320db1b2\n\n**4.**\u00a0Select the task type.\n\nhttps://preview.redd.it/kw9gxc60s3ra1.jpg?width=1306&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=95fd5bb4735430323398988b3298f16c5a38a550\n\n**5.**\u00a0Select input data type as INT16.\n\nhttps://preview.redd.it/k306wz61s3ra1.jpg?width=1260&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a86d3d099a80a5f5d32a7039f5ff20eec9341ff0\n\n**6.**\u00a0Enable Digital Signal Preprocessing, select Window size 200 and Sliding shift 5.\n\nhttps://preview.redd.it/mxac6t72s3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=86cee3a36680436d9344c67a6b555da490eb5ae1\n\n**7.**\u00a0For each of the features (acc\\_X, acc\\_Y, acc\\_Z, gyro\\_X, gyro\\_Y, gyro\\_Z) repeat the following steps:\n\na. Click \u2018Edit\u2019\n\nhttps://preview.redd.it/txmla2e3s3ra1.jpg?width=1824&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0d231efc02565ec396ddffa797531d38d69df85a\n\nb. Select \u2018Remove all\u2019\n\nhttps://preview.redd.it/2214pk94s3ra1.jpg?width=1788&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b446cf5ad18da0ac95ef4af9ff110ea77466a7b6\n\nc. Select \u2018Statistical\u2019 features and check the following features: Mean, Root mean square, Mean Absolute Deviation, Standard Deviation, Mean-crossing Rate, Zero-crossing Rate\n\nhttps://preview.redd.it/ai8e1825s3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6ea60af3a3f2f7bc12ee8ef63260a95f46bdfd42\n\n**8.**\u00a0Select the microcontroller bit depth: 8 bit and start training.\n\nhttps://preview.redd.it/51gk2076s3ra1.jpg?width=1324&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=891901581f7ba2d1656e4c43bc1be28575e1f588\n\nAfter the model is trained, proceed to the Prediction tab, check out model quality metrics and download the archive with the model for embedding.\n\nhttps://preview.redd.it/u35z3987s3ra1.jpg?width=1022&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=541bcb13e7c5f9e3b4db0e3294c351b4b0a1e0d7\n\nThe downloaded model source code will look like this:\n\nhttps://preview.redd.it/5yvqpe99s3ra1.jpg?width=1460&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a541f9a703d38b30cb34d642453a359af80c5b2b\n\nThe archive contains everything required for embedding the model into any MCU. Use the archive contents to compile an Arduino Sketch, flush your Nicla Sense ME with this sketch and start inference.\n\nIn case you would like to inference our pretrained model, just download the precompiled sketch for Arduino Nicla Sense ME in our repository:\u00a0[**https://github.com/NeutonTinyML/hand-activity-recognition**](https://github.com/NeutonTinyML/hand-activity-recognition)**.**", "author_fullname": "t2_8sds8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "My new project with Arduino: Ultra-Tiny Solution of Daily Activities \u200b\u200bRecognition. Can you repeat? Please, leave your comments.", "link_flair_richtext": [{"e": "text", "t": "Look what I made!"}], "subreddit_name_prefixed": "r/arduino", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 50, "top_awarded_type": null, "hide_score": false, "media_metadata": {"tvqrx1nur3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 61, "x": 108, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fbeff0c98296eb76f4069e0990a17604905e6c45"}, {"y": 122, "x": 216, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e91bc53f8e0135fb2ceefc2a54365089d4bb37f3"}, {"y": 182, "x": 320, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c24845b32d2b3c6d33855375122f768e9587994c"}, {"y": 364, "x": 640, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c0965a77b839926c684460b7b419790e62065f9"}, {"y": 546, "x": 960, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c0c865b2744a92e67269034aac4313447138ad8"}, {"y": 614, "x": 1080, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4a83cf717ad602ab63dabcdc6757dd0905ad70fc"}], "s": {"y": 812, "x": 1426, "u": "https://preview.redd.it/tvqrx1nur3ra1.jpg?width=1426&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=504dcaa6ce7ed6622c3b7cc57de045e218cd3c70"}, "id": "tvqrx1nur3ra1"}, "2214pk94s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 64, "x": 108, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=107f42b08257ebbf00982bc916298d6e4859f8d4"}, {"y": 129, "x": 216, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a9e0e078627214072998f2b16c108c2f7161518"}, {"y": 192, "x": 320, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50a777899aafcf66a031fb4f7a84387528d1c167"}, {"y": 384, "x": 640, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7cd671c46c3ee176fb39e95c48b3cd8f6dd09000"}, {"y": 576, "x": 960, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a55c9ef2702ac12c13927b15bc7c646b1a7e8fc1"}, {"y": 648, "x": 1080, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50519ea09214d6b9027699522b138c34b65b7875"}], "s": {"y": 1074, "x": 1788, "u": "https://preview.redd.it/2214pk94s3ra1.jpg?width=1788&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=b446cf5ad18da0ac95ef4af9ff110ea77466a7b6"}, "id": "2214pk94s3ra1"}, "4b39v85og8sa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 95, "x": 108, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d3112f5152d63b434ec88fa3b09e5d7d25648b46"}, {"y": 190, "x": 216, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b62e6a47bdaae868b25c082991e9016146c02f11"}, {"y": 282, "x": 320, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6894c0afaa09e45965154725e4307ba0279ee3e"}, {"y": 564, "x": 640, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=939566b13b16f63d9b12068192ec44d654f23792"}, {"y": 847, "x": 960, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15227ec06cacc3980c2436fd0fca04e586825e5f"}, {"y": 952, "x": 1080, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ed349ee91fdc25ef31a2f91b0eab90efebb17e77"}], "s": {"y": 1807, "x": 2048, "u": "https://preview.redd.it/4b39v85og8sa1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=97e3e32ce2dbfe134d3fb3531f8e195d5f9883df"}, "id": "4b39v85og8sa1"}, "3ththa1ur3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 129, "x": 108, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e2bee0e8b8ab1eeaf114e338c3a48899327548f"}, {"y": 259, "x": 216, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f224deb366044988c9e067b4cce1678f4af214ba"}, {"y": 384, "x": 320, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9f9e17bb9f83785c77bf91b48cfaef15444f3081"}, {"y": 769, "x": 640, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0cdbf9f4c2b46b348cc45d2f1963436157673b72"}], "s": {"y": 1072, "x": 892, "u": "https://preview.redd.it/3ththa1ur3ra1.jpg?width=892&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=58ce6f80240f2c8cbb66cf9c4f13e8382ba77eaa"}, "id": "3ththa1ur3ra1"}, "txmla2e3s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 50, "x": 108, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=57b7eb2306adb71eb044cfe40d9bb42a873c2256"}, {"y": 100, "x": 216, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=343aa46729c29bc5cf39a0e0f577a05f8d6fa801"}, {"y": 148, "x": 320, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39436f3621d0080ce382d995c755ec89bf2cccf0"}, {"y": 296, "x": 640, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bb579e716247a4a45ea5b8dde97dac640b94ab3c"}, {"y": 445, "x": 960, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=af250f3de945e1a718e1ffe1150b936953307fde"}, {"y": 500, "x": 1080, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eae228c76a24b022ce8a582a2ac03c5c20647ec4"}], "s": {"y": 846, "x": 1824, "u": "https://preview.redd.it/txmla2e3s3ra1.jpg?width=1824&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=0d231efc02565ec396ddffa797531d38d69df85a"}, "id": "txmla2e3s3ra1"}, "18m11zryr3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7614754e11e2f9f116deb0fbb4c6b5496e01fb8f"}, {"y": 135, "x": 216, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=02442a576ce14af510bc4bbc34f93a41817a3214"}, {"y": 200, "x": 320, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3cb0d67ef86eb4babc67b755f7f95997152f5ad6"}, {"y": 400, "x": 640, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b06b04c18cf3ff5098dda415d8caf9c71996d356"}, {"y": 600, "x": 960, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=77e795f4e104f2fae0edec23c451847bab91c999"}, {"y": 676, "x": 1080, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6fbe62bbcaaae20e5aa91916c695e421e5dfb6d"}], "s": {"y": 1282, "x": 2048, "u": "https://preview.redd.it/18m11zryr3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1b25cc45b4de00892e38514166178285320db1b2"}, "id": "18m11zryr3ra1"}, "504en08gr3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 49, "x": 108, "u": "https://preview.redd.it/504en08gr3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=abbd91cc6f99845493f07d6e638174386eae18be"}, {"y": 99, "x": 216, "u": "https://preview.redd.it/504en08gr3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f55dd9d212ab7a1066bb50e462d4540264ab4f3e"}, {"y": 147, "x": 320, "u": "https://preview.redd.it/504en08gr3ra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ac2a6d92a1d389e49504256cb2f18ade7f9f306"}, {"y": 295, "x": 640, "u": "https://preview.redd.it/504en08gr3ra1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b6bea0e977faf9df3af9e980945bbd406fe3ba5"}, {"y": 443, "x": 960, "u": "https://preview.redd.it/504en08gr3ra1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=58571b4d567799da42e5a0d482c27db5425d77a5"}, {"y": 499, "x": 1080, "u": "https://preview.redd.it/504en08gr3ra1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=777c561347d1c64b738537b3464b6030553736cb"}], "s": {"y": 604, "x": 1306, "u": "https://preview.redd.it/504en08gr3ra1.png?width=1306&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=03994ba652b0ea9c154c699a2aa6fe4af274d5d2"}, "id": "504en08gr3ra1"}, "bks9n7hjr3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 72, "x": 108, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=181ec25aabf3c130b80f2b444d82e497771393c0"}, {"y": 144, "x": 216, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8d9cd4b98f63ecdbac1329c43cbf5610c9d5bb8"}, {"y": 213, "x": 320, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b18d6c0e50f6409652bc0ceefce10621a7399ee"}, {"y": 426, "x": 640, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3c271b262c4e6337cd84a2002b25fa6e63c84299"}, {"y": 640, "x": 960, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2582f30b09d73aaaf5a4ef93caa5eecfdc6d34d0"}, {"y": 720, "x": 1080, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=70560ba59935b0c2a4835541b30a1328e5c1c0ff"}], "s": {"y": 1062, "x": 1592, "u": "https://preview.redd.it/bks9n7hjr3ra1.png?width=1592&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f452d539b15cfbfadeb7296bd1e62fa99fe4a5bd"}, "id": "bks9n7hjr3ra1"}, "u3q15z2er3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 105, "x": 108, "u": "https://preview.redd.it/u3q15z2er3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf11b6464ef4b8639e26a94b3d50c8ae6ab386d9"}, {"y": 210, "x": 216, "u": "https://preview.redd.it/u3q15z2er3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bf8a0f55c0b826869f2723e6481bda1a94a2fc99"}, {"y": 311, "x": 320, "u": "https://preview.redd.it/u3q15z2er3ra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16ab60ddfb27f78f0f66b12ea99d2fdd5e81e804"}], "s": {"y": 596, "x": 612, "u": "https://preview.redd.it/u3q15z2er3ra1.png?width=612&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e7734fafef3cfa398d6a23d99c8f7ad5a7f20a8a"}, "id": "u3q15z2er3ra1"}, "51gk2076s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 33, "x": 108, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c0a1e48c78cb0a420f44d4c61b57de133a523c8d"}, {"y": 67, "x": 216, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca42b4e16aa3b4e31eca75965fbcf00dd65b1267"}, {"y": 100, "x": 320, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4da05cd09ef87bdb28cfc46cbb4895afc607d4c1"}, {"y": 200, "x": 640, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d77b715049e62f9cfb0b98ea7ae796fcc0f45b1"}, {"y": 300, "x": 960, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6f7bcf0f9b71ad0764f112066787a7d6ee2ad821"}, {"y": 337, "x": 1080, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16362df5384fcf79b261bb28b7c24d19d3ac2016"}], "s": {"y": 414, "x": 1324, "u": "https://preview.redd.it/51gk2076s3ra1.jpg?width=1324&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=891901581f7ba2d1656e4c43bc1be28575e1f588"}, "id": "51gk2076s3ra1"}, "ai8e1825s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 53, "x": 108, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1cfd8ddc942169637496c8808f4c460fea445d18"}, {"y": 107, "x": 216, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c42b320894871760efa443abe0d34e785841820d"}, {"y": 159, "x": 320, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c17e944c231fdf22c439a37efda051a4c13214ff"}, {"y": 318, "x": 640, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f059b1afd3fa5c55ac1c77fef211068d036dc3ec"}, {"y": 477, "x": 960, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50cb6ce02486783c907069021c5938f9d1ca39b4"}, {"y": 536, "x": 1080, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe0bcd79bd56dd022242a98bed43a15dcc311d12"}], "s": {"y": 1018, "x": 2048, "u": "https://preview.redd.it/ai8e1825s3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=6ea60af3a3f2f7bc12ee8ef63260a95f46bdfd42"}, "id": "ai8e1825s3ra1"}, "ccm1r5eor3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 54, "x": 108, "u": "https://preview.redd.it/ccm1r5eor3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=be8bc2d3bcf9c2557e62fa7f2360695bf4d07928"}, {"y": 109, "x": 216, "u": "https://preview.redd.it/ccm1r5eor3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=33dcc2e827a96d7b2729223a08c783b5e1607e36"}], "s": {"y": 146, "x": 288, "u": "https://preview.redd.it/ccm1r5eor3ra1.png?width=288&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0a3b0606ae26446d326daf001fa936823bf06a05"}, "id": "ccm1r5eor3ra1"}, "mxac6t72s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 83, "x": 108, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ee46a1fed439d94736e0ac0deb0dcfe039ae35e8"}, {"y": 167, "x": 216, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1a3d50cd3f563b618f404c4fa86fb9b6db8fa21e"}, {"y": 248, "x": 320, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ce2af96bbc76ca199d3ed793393a7980380bbdca"}, {"y": 496, "x": 640, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1f8d487c3b3d9bf4887c25785442f7b287173c2b"}, {"y": 744, "x": 960, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7597cd11429ed528724dfc2bb8fd036b9b5809f2"}, {"y": 837, "x": 1080, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=44a0848d590b5df0643b37d25f4e8d5ce2fabe4f"}], "s": {"y": 1588, "x": 2048, "u": "https://preview.redd.it/mxac6t72s3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=86cee3a36680436d9344c67a6b555da490eb5ae1"}, "id": "mxac6t72s3ra1"}, "kw9gxc60s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 67, "x": 108, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f7083b14bb0aa141306b79d21b3b690d3a3dcdb6"}, {"y": 134, "x": 216, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2d65399333fe90133857be130050dabf5b3a3729"}, {"y": 199, "x": 320, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0572baa66ec2433a82ea2dd9ee24fff38c8a6ea7"}, {"y": 398, "x": 640, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d80d74d6e40f6d2ebcfb63b6039c87ab09d4fcc"}, {"y": 598, "x": 960, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=868d4290d98ceb8df1256aed44e3dae62a482bbb"}, {"y": 673, "x": 1080, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5349f8fd61e97711e4e04fc067e992c4d2ddbb5e"}], "s": {"y": 814, "x": 1306, "u": "https://preview.redd.it/kw9gxc60s3ra1.jpg?width=1306&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=95fd5bb4735430323398988b3298f16c5a38a550"}, "id": "kw9gxc60s3ra1"}, "k306wz61s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 32, "x": 108, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b915a2cb218c64464c76f49f30136b576a9b6385"}, {"y": 65, "x": 216, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=05a619469cc2e69f220412506cb041d20c7dc086"}, {"y": 97, "x": 320, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b122618a47025005851a41d2327a1b4b38ee0a09"}, {"y": 195, "x": 640, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=22bca2d47bedf3801c71bbabb029020a1d808d1a"}, {"y": 292, "x": 960, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd8cc819463a3c50c08d58f0398c42a2042b503b"}, {"y": 329, "x": 1080, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=081d91f4946b32002c904142dad842e8e56f9cc0"}], "s": {"y": 384, "x": 1260, "u": "https://preview.redd.it/k306wz61s3ra1.jpg?width=1260&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a86d3d099a80a5f5d32a7039f5ff20eec9341ff0"}, "id": "k306wz61s3ra1"}, "e5oi0ljir3ra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 70, "x": 108, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6c8276234e46ed277d986a108737bf0c35c207f2"}, {"y": 141, "x": 216, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e1a847585509cbc61cc9296ee16230f83faadb3e"}, {"y": 210, "x": 320, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=acebe45286c3556db6b6319ede5419586efd6464"}, {"y": 420, "x": 640, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a925566995c91605bafce879972aecebbe0d3938"}, {"y": 630, "x": 960, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=146f5b39517e3ac77cf92028583bd3dc2155f135"}, {"y": 709, "x": 1080, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4c8e0f2aaf0571eba204262e2921b90c17a1fe35"}], "s": {"y": 1048, "x": 1596, "u": "https://preview.redd.it/e5oi0ljir3ra1.png?width=1596&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2c686bee67e11d127b9d7e4cbcfb3370271a95d8"}, "id": "e5oi0ljir3ra1"}, "dt1grc4jg8sa1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 38, "x": 108, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=076e69b1ad3aa912f5f605a7d154e4817a81834f"}, {"y": 77, "x": 216, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8b8fe0cf56a4469540490cb428bf02970c0cff0b"}, {"y": 114, "x": 320, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f5fe9a191e5ab5ece9a50b3145b5cc6aa0d0e830"}, {"y": 228, "x": 640, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7895360955b5177f791fb2a5f8db90533ab7db9b"}, {"y": 343, "x": 960, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d33826234bf4eba3e91fc99056a75e72e8945b2"}, {"y": 386, "x": 1080, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=07f9e665bd622c1254ee8931a4a50f1d69ae8e47"}], "s": {"y": 658, "x": 1840, "u": "https://preview.redd.it/dt1grc4jg8sa1.jpg?width=1840&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1e386acb758209202ac65e6407f3611f045e4d27"}, "id": "dt1grc4jg8sa1"}, "u35z3987s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 82, "x": 108, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85ee6b3a39e556632aa556a0906e67a84a83bbfd"}, {"y": 165, "x": 216, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=766ff34db8432742f042e6653b0da91f15165e0b"}, {"y": 245, "x": 320, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3946fa4d099a5b89de0fee8ce6d2096dbc944626"}, {"y": 491, "x": 640, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e9cb400a5249f6db85ad35e5606480f2df344c41"}, {"y": 737, "x": 960, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56fa5055ea03c641a752a2caaa1c8ae135d01b6c"}], "s": {"y": 785, "x": 1022, "u": "https://preview.redd.it/u35z3987s3ra1.jpg?width=1022&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=541bcb13e7c5f9e3b4db0e3294c351b4b0a1e0d7"}, "id": "u35z3987s3ra1"}, "gr3pw96tr3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 5, "x": 108, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=396def3eb7358ced864682723e3e9b9dea9feca3"}, {"y": 10, "x": 216, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df0bfed1fef00918e8f2bb5b3ead1ece721d5da0"}, {"y": 15, "x": 320, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=53d407ea84b4cf0c1d063eadcfd66ec8e0c9239f"}, {"y": 30, "x": 640, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eba30de0893bbfeffd46c25412551f88ff8d4878"}, {"y": 45, "x": 960, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a07e7263584f08616ddb887a8b41dc0b091f4db7"}, {"y": 50, "x": 1080, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa6cd16184187dc0a6314843e0797130060dd9b0"}], "s": {"y": 96, "x": 2048, "u": "https://preview.redd.it/gr3pw96tr3ra1.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=77506bbc9f1fa56bc07a6883c27af09dcb9b747c"}, "id": "gr3pw96tr3ra1"}, "5yvqpe99s3ra1": {"status": "valid", "e": "Image", "m": "image/jpg", "p": [{"y": 57, "x": 108, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a684a29396d7e1ddcf08d4fff41d450f1fb59e0c"}, {"y": 114, "x": 216, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=180aeb2962885d3308cd563962cf752070ade8d7"}, {"y": 170, "x": 320, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2e11f9f54b8e0d3b0af74b6423c6012875661d43"}, {"y": 340, "x": 640, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d447701dc9ea05ff5253c24afb7c48130adeb5d"}, {"y": 510, "x": 960, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a16c121d7cb04649a79692e213c8fb923d2cf1a9"}, {"y": 574, "x": 1080, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a78d5bf52b1bfb60bb5b763663fcbd480b8a54b1"}], "s": {"y": 776, "x": 1460, "u": "https://preview.redd.it/5yvqpe99s3ra1.jpg?width=1460&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a541f9a703d38b30cb34d642453a359af80c5b2b"}, "id": "5yvqpe99s3ra1"}}, "name": "t3_127rccj", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Look what I made!", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/sYUBvdJXNIEroAyPdmrxhzxZG_3FeN7XJhUNm2cF35U.jpg", "edited": 1680773684.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680281088.0, "link_flair_type": "richtext", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.arduino", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Prototyping: a Hand Tracker with Nicla Sense ME and Bosch sensors&lt;/p&gt;\n\n&lt;h2&gt;Things used in this project:&lt;/h2&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware components&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://referral.element14.com/OrderCodeView?fsku=3874817&amp;amp;nsku=99AJ4432&amp;amp;COM=e14c-noscript&amp;amp;CMP=e14c-noscript&amp;amp;osetc=e14-noscript-tracking-loss\"&gt;Arduino Nicla Sense ME&lt;/a&gt; \u00d71&lt;br/&gt;\n&lt;a href=\"https://www.bosch-sensortec.com/news/new-generation-of-smart-sensor-hubs-bhi260-and-bha260.html\"&gt;Bosch BHI260 sensor&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Software apps and online services&lt;/h2&gt;\n\n&lt;p&gt;&lt;a href=\"https://neuton.ai/\"&gt;Neuton Tiny ML&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://www.arduino.cc/en/software\"&gt;Arduino IDE&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In my project, I want to show an example of how you can significantly increase the complexity of intelligence functionality, and determine what a wearable device can recognize with a very small and accurate neural network. Also, you will find a detailed guide on how to implement Multiple Daily Activities recognition using the Nicla Sense ME. You also can\u00a0&lt;a href=\"https://us06web.zoom.us/webinar/register/3416800160594/WN_X0wH_gaqSiylUGehTFXlSQ\"&gt;&lt;strong&gt;participate in my practical webinar&lt;/strong&gt;&lt;/a&gt;\u00a0on\u00a0&lt;strong&gt;April 11 at 5 pm CET&lt;/strong&gt;. Where you can watch how to create this solution from leaders from Neuton.AI, Arduino and Bosch. Don&amp;#39;t miss this opportunity to learn from experts directly.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/3nkoJZD_Lh8\"&gt;https://youtu.be/3nkoJZD_Lh8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you have Nicla Sense ME, you can try my pre-trained model in your experiment/project/classes.&lt;/p&gt;\n\n&lt;p&gt;Our test results show that the model perfectly recognizes the movements of 30 participants whose data was not used for training the model.&lt;/p&gt;\n\n&lt;p&gt;Following this tutorial, you also can easily make your own experiment and train your neural network on your data to recognize the same or different movements.&lt;/p&gt;\n\n&lt;p&gt;We believe that the ability to recognize an extremely complex and similar action using small neural networks opens up a new era for always-on devices.&lt;/p&gt;\n\n&lt;p&gt;Below, you will find a full tutorial on how to reproduce this experiment, including data collection, model training, model embedding and inference. In addition, we have created a public repository with a pre-trained model that has demonstrated good generalization on new users. It is a precompiled model archive that you may just download and embed into your Nicla Sense ME with a few clicks of a mouse and start testing.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/NeutonTinyML/hand-activity-recognition\"&gt;&lt;strong&gt;https://github.com/NeutonTinyML/hand-activity-recognition&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;h2&gt;Data Collection with Nicla Sense ME&lt;/h2&gt;\n\n&lt;p&gt;For this case, I\u2019ve selected 5 types of activities (for your experiment, you may choose other activities):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Washing hands&lt;/li&gt;\n&lt;li&gt;Brushing teeth&lt;/li&gt;\n&lt;li&gt;Clapping&lt;/li&gt;\n&lt;li&gt;Brushing hair&lt;/li&gt;\n&lt;li&gt;Random activity (negative class)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For each activity type, I collected sensor data with Nicla Sense ME for 10 minutes non-stop. Data was collected from 7 different people from 7 different locations. It was quite a challenging, yet fun process because, for proper data collection, it was undesirable to interrupt. Here\u2019s some advice for those who plan to re-conduct my experiment: before collecting data for another activity, get some rest! However, if 10 minutes of constant hair brushing is too much, you can do two 5-minute data collection streams and then concatenate the two files.&lt;/p&gt;\n\n&lt;p&gt;For the negative class, we collected 60 minutes of data. There\u2019s no need to do anything special this time; just get on with your everyday activities. Type on the keyboard, operate a mouse, answer calls, drink tea, or do whatever you want. Just be sure you don\u2019t sit completely idle with your hands still during negative class data collection. The negative class is a subset of everything else excluding the 4 classes outlined above.&lt;/p&gt;\n\n&lt;h2&gt;Steps:&lt;/h2&gt;\n\n&lt;p&gt;Now let me provide you with detailed guidelines on how to tune Arduino\u2019s board, Nicla Sense ME to collect data.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt;\u00a0Visit\u00a0&lt;a href=\"https://www.arduino.cc/en/software\"&gt;&lt;strong&gt;https://www.arduino.cc/en/software&lt;/strong&gt;&lt;/a&gt;&lt;a href=\"https://www.arduino.cc/en/software\"&gt;,&lt;/a&gt;\u00a0download Arduino IDE 2.x.x for your OS, and install a package.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;\u00a0Open Arduino IDE, click on the\u00a0&lt;strong&gt;Boards Manager&lt;/strong&gt;\u00a0icon, type \u201cnicla sense me\u201d and install the\u00a0&lt;strong&gt;Arduino Mbed OS Nicla Boards&lt;/strong&gt;\u00a0package.&lt;/p&gt;\n\n&lt;p&gt;Click on the\u00a0&lt;strong&gt;Library Manager icon&lt;/strong&gt;, type \u201cnicla sense me\u201d in the search box and install two libraries:\u00a0&lt;strong&gt;Arduino_BHY2&lt;/strong&gt;\u00a0and\u00a0&lt;strong&gt;ArduinoBLE&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Type \u201cprintf\u201d in the search box and install\u00a0&lt;strong&gt;LibPrintf&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt;\u00a0In the main menu, select\u00a0&lt;strong&gt;File-&amp;gt;Examples-&amp;gt;Arduino_BHY2&amp;gt;BHYFirmwareUpdate&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt;\u00a0Connect your\u00a0&lt;strong&gt;Nicla Sense ME&lt;/strong&gt;\u00a0board to the USB and select the port in the dropdown menu.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt;\u00a0Click on the Upload button and wait until the uploading process is finished.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt;\u00a0Click on the\u00a0&lt;strong&gt;Serial Monitor&lt;/strong&gt;\u00a0icon and select\u00a0&lt;strong&gt;115200 baud&lt;/strong&gt;, then click the RESET button on your Nicla board.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt;\u00a0After 5-10 seconds you\u2019ll see that the BHY firmware is uploaded.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;8.&lt;/strong&gt; Download the Arduino precompiled library for sensor data collection using a Nicla Sense ME from &lt;a href=\"https://github.com/NeutonTinyML/hand-activity-recognition\"&gt;&lt;strong&gt;the Neuton repository.&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;model -&amp;gt; Arduino_Neuton.zip (do not uncompress the archive)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;9.&lt;/strong&gt; Install the \u2018Arduino_Neuton.zip\u2019 model into the Arduino IDE:&lt;/p&gt;\n\n&lt;p&gt;Sketch -&amp;gt; Include Library -&amp;gt; Add .ZIP Library\u2026 (point to the downloaded \u2018Arduino_Neuton.zip\u2019 archive)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/dt1grc4jg8sa1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1e386acb758209202ac65e6407f3611f045e4d27\"&gt;https://preview.redd.it/dt1grc4jg8sa1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1e386acb758209202ac65e6407f3611f045e4d27&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;10.&lt;/strong&gt; Quit &amp;amp; restart the Arduino IDE (it will take up to a minute for the Arduino IDE to index all of it\u2019s examples, including the freshly installed \u2018Arduino_Neuton\u2019)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;11.&lt;/strong&gt; Open the installed example: File -&amp;gt; Examples -&amp;gt; Arduino_Neuton -&amp;gt; Inertial_Sensor_Data_Collection (a new Arduino IDE window will pop up, close the previous Arduino IDE window)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4b39v85og8sa1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=97e3e32ce2dbfe134d3fb3531f8e195d5f9883df\"&gt;https://preview.redd.it/4b39v85og8sa1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=97e3e32ce2dbfe134d3fb3531f8e195d5f9883df&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;12.&lt;/strong&gt; Connect your Nicla Sense ME to your computer\u2019s USB port.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;13.&lt;/strong&gt; In the IDE select your MCU and upload the firmware.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;14.&lt;/strong&gt;\u00a0After the firmware is uploaded (1-2 minutes), open the Serial Monitor - you should see the sensors readings (In case you will see the connection error \u2013 restart the Arduino IDE and open the Serial Monitor once again)&lt;/p&gt;\n\n&lt;p&gt;Now you can unplug the board from the USB port.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;15.&lt;/strong&gt;\u00a0Attach the board firmly to the watch or bracelet. The Micro-USB port should be in the bottom-right corner.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u3q15z2er3ra1.png?width=612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e7734fafef3cfa398d6a23d99c8f7ad5a7f20a8a\"&gt;https://preview.redd.it/u3q15z2er3ra1.png?width=612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=e7734fafef3cfa398d6a23d99c8f7ad5a7f20a8a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;16.&lt;/strong&gt;\u00a0Put a watch or a bracelet with a circuit board on your right hand, and connect the cable to it. Make sure that the cable near the connector is not taut so as not to damage the board.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;17.&lt;/strong&gt;\u00a0Depending on your OS, open the serial port and try to log data. A serial port can be seen in the hint.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/504en08gr3ra1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=03994ba652b0ea9c154c699a2aa6fe4af274d5d2\"&gt;https://preview.redd.it/504en08gr3ra1.png?width=1306&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=03994ba652b0ea9c154c699a2aa6fe4af274d5d2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Use this command in the terminal if you\u2019re on macOS:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;stty -f /dev/cu.usbmodem85EB3A0F2 115200 | cat /dev/cu.usbmodem85EB3A0F2 | tee 4_brushing_hair.csv&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This will set the port speed to 115200 baud. Print out data from the serial port as text and save it to a CSV file.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e5oi0ljir3ra1.png?width=1596&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2c686bee67e11d127b9d7e4cbcfb3370271a95d8\"&gt;https://preview.redd.it/e5oi0ljir3ra1.png?width=1596&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=2c686bee67e11d127b9d7e4cbcfb3370271a95d8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Press Control-C to stop recording. There will be a CSV file with data from sensors (accelerometer, linear accelerometer, and gyroscope) and timestamp.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bks9n7hjr3ra1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=f452d539b15cfbfadeb7296bd1e62fa99fe4a5bd\"&gt;https://preview.redd.it/bks9n7hjr3ra1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=f452d539b15cfbfadeb7296bd1e62fa99fe4a5bd&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;18.&lt;/strong&gt;\u00a0Start recording the movement. Press the Reset button on the Nicla board and restart logging to the file.&lt;/p&gt;\n\n&lt;p&gt;For example, clap for 10 minutes (use the stopwatch application in your smartphone). During this process, change the position of your hands to make the data more diverse. Do not stop moving while the recording is in progress.&lt;/p&gt;\n\n&lt;p&gt;The recording will start in ~7 seconds after pushing the Reset button.&lt;/p&gt;\n\n&lt;p&gt;Once 10 minutes are finished, continue the movement for 10-20 seconds and stop recording.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;19.&lt;/strong&gt;\u00a0Repeat step 18 for each activity type (use different file names):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ccm1r5eor3ra1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a3b0606ae26446d326daf001fa936823bf06a05\"&gt;https://preview.redd.it/ccm1r5eor3ra1.png?width=288&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0a3b0606ae26446d326daf001fa936823bf06a05&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Reset the board before recording each movement.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;20.&lt;/strong&gt;\u00a0Repeat step 18 for the negative class and start recording for 1 hour.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://drive.google.com/file/d/106bVkRNZPmXwlctTsF9wtmeEbS9FMAgg/view?usp=share_link\"&gt;&lt;strong&gt;Here\u2019s the video&lt;/strong&gt;&lt;/a&gt; on how the data should be collected (make sure to do the actual activity: i.e. brushing hair should be done with an actual hairbrush on your head, not in the air).&lt;/p&gt;\n\n&lt;h2&gt;Model training&lt;/h2&gt;\n\n&lt;p&gt;After you have collected data on all activities and combined them into one dataset, proceed to the\u00a0&lt;a href=\"https://login.neuton.ai/?utm_source=reddit&amp;amp;utm_medium=project&amp;amp;utm_campaign=Ultra-Tiny+Solution+of+Daily+Activities+%E2%80%8B%E2%80%8BRecognition\"&gt;&lt;strong&gt;Neuton.ai&lt;/strong&gt;&lt;/a&gt;\u00a0platform to train your model.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt;\u00a0Create a new solution&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gr3pw96tr3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=77506bbc9f1fa56bc07a6883c27af09dcb9b747c\"&gt;https://preview.redd.it/gr3pw96tr3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=77506bbc9f1fa56bc07a6883c27af09dcb9b747c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;\u00a0Select the data type and upload the data&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3ththa1ur3ra1.jpg?width=892&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=58ce6f80240f2c8cbb66cf9c4f13e8382ba77eaa\"&gt;https://preview.redd.it/3ththa1ur3ra1.jpg?width=892&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=58ce6f80240f2c8cbb66cf9c4f13e8382ba77eaa&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tvqrx1nur3ra1.jpg?width=1426&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=504dcaa6ce7ed6622c3b7cc57de045e218cd3c70\"&gt;https://preview.redd.it/tvqrx1nur3ra1.jpg?width=1426&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=504dcaa6ce7ed6622c3b7cc57de045e218cd3c70&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt;\u00a0In the left field remove variables lacc_X, lacc_Y, lacc_Z (linear accelerometer is not needed for this model); in the right field select the target variable and click next.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/18m11zryr3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1b25cc45b4de00892e38514166178285320db1b2\"&gt;https://preview.redd.it/18m11zryr3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=1b25cc45b4de00892e38514166178285320db1b2&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt;\u00a0Select the task type.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kw9gxc60s3ra1.jpg?width=1306&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=95fd5bb4735430323398988b3298f16c5a38a550\"&gt;https://preview.redd.it/kw9gxc60s3ra1.jpg?width=1306&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=95fd5bb4735430323398988b3298f16c5a38a550&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt;\u00a0Select input data type as INT16.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/k306wz61s3ra1.jpg?width=1260&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a86d3d099a80a5f5d32a7039f5ff20eec9341ff0\"&gt;https://preview.redd.it/k306wz61s3ra1.jpg?width=1260&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a86d3d099a80a5f5d32a7039f5ff20eec9341ff0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt;\u00a0Enable Digital Signal Preprocessing, select Window size 200 and Sliding shift 5.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mxac6t72s3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=86cee3a36680436d9344c67a6b555da490eb5ae1\"&gt;https://preview.redd.it/mxac6t72s3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=86cee3a36680436d9344c67a6b555da490eb5ae1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;7.&lt;/strong&gt;\u00a0For each of the features (acc_X, acc_Y, acc_Z, gyro_X, gyro_Y, gyro_Z) repeat the following steps:&lt;/p&gt;\n\n&lt;p&gt;a. Click \u2018Edit\u2019&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/txmla2e3s3ra1.jpg?width=1824&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0d231efc02565ec396ddffa797531d38d69df85a\"&gt;https://preview.redd.it/txmla2e3s3ra1.jpg?width=1824&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=0d231efc02565ec396ddffa797531d38d69df85a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;b. Select \u2018Remove all\u2019&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/2214pk94s3ra1.jpg?width=1788&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b446cf5ad18da0ac95ef4af9ff110ea77466a7b6\"&gt;https://preview.redd.it/2214pk94s3ra1.jpg?width=1788&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b446cf5ad18da0ac95ef4af9ff110ea77466a7b6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;c. Select \u2018Statistical\u2019 features and check the following features: Mean, Root mean square, Mean Absolute Deviation, Standard Deviation, Mean-crossing Rate, Zero-crossing Rate&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ai8e1825s3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6ea60af3a3f2f7bc12ee8ef63260a95f46bdfd42\"&gt;https://preview.redd.it/ai8e1825s3ra1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=6ea60af3a3f2f7bc12ee8ef63260a95f46bdfd42&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;8.&lt;/strong&gt;\u00a0Select the microcontroller bit depth: 8 bit and start training.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/51gk2076s3ra1.jpg?width=1324&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=891901581f7ba2d1656e4c43bc1be28575e1f588\"&gt;https://preview.redd.it/51gk2076s3ra1.jpg?width=1324&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=891901581f7ba2d1656e4c43bc1be28575e1f588&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;After the model is trained, proceed to the Prediction tab, check out model quality metrics and download the archive with the model for embedding.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/u35z3987s3ra1.jpg?width=1022&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=541bcb13e7c5f9e3b4db0e3294c351b4b0a1e0d7\"&gt;https://preview.redd.it/u35z3987s3ra1.jpg?width=1022&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=541bcb13e7c5f9e3b4db0e3294c351b4b0a1e0d7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The downloaded model source code will look like this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5yvqpe99s3ra1.jpg?width=1460&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a541f9a703d38b30cb34d642453a359af80c5b2b\"&gt;https://preview.redd.it/5yvqpe99s3ra1.jpg?width=1460&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=a541f9a703d38b30cb34d642453a359af80c5b2b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The archive contains everything required for embedding the model into any MCU. Use the archive contents to compile an Arduino Sketch, flush your Nicla Sense ME with this sketch and start inference.&lt;/p&gt;\n\n&lt;p&gt;In case you would like to inference our pretrained model, just download the precompiled sketch for Arduino Nicla Sense ME in our repository:\u00a0&lt;a href=\"https://github.com/NeutonTinyML/hand-activity-recognition\"&gt;&lt;strong&gt;https://github.com/NeutonTinyML/hand-activity-recognition&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "8a993096-51ce-11e9-84d9-0e29268ab306", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2qknj", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#94e044", "id": "127rccj", "is_robot_indexable": true, "report_reasons": null, "author": "wasteguru", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/arduino/comments/127rccj/my_new_project_with_arduino_ultratiny_solution_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/arduino/comments/127rccj/my_new_project_with_arduino_ultratiny_solution_of/", "subreddit_subscribers": 562424, "created_utc": 1680281088.0, "num_crossposts": 16, "media": null, "is_video": false}], "created": 1680778514.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.arduino", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "/r/arduino/comments/127rccj/my_new_project_with_arduino_ultratiny_solution_of/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "937a6f50-d780-11e7-826d-0ed1beddcc82", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dge9v", "is_robot_indexable": true, "report_reasons": null, "author": "wasteguru", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "crosspost_parent": "t3_127rccj", "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dge9v/my_new_project_with_arduino_ultratiny_solution_of/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "/r/arduino/comments/127rccj/my_new_project_with_arduino_ultratiny_solution_of/", "subreddit_subscribers": 868788, "created_utc": 1680778514.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Are you a data whiz, language expert, or someone who is interested in pushing the boundaries of large language models? We\u2019re looking to revolutionize the way we understand and interact with financial data and want to have a little fun by launching a challenge with a chance to win $1000.\n\nWe are doing something like this on our GPInvestBot Twitter, but think we can make it even more interesting by engaging with others.\n\nMost liked headline/tweet created automatically summarizing macroeconomic and financial time series data wins. More details here: https://www.globalpredictions.com/posts/unlocking-insights-in-time-series-data-the-generative-ai-challenge", "author_fullname": "t2_p9otw1al", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking to push the boundaries? New challenge to turn macro data into interesting insights", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "fun", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dqjha", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Fun/Trivia", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680800420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are you a data whiz, language expert, or someone who is interested in pushing the boundaries of large language models? We\u2019re looking to revolutionize the way we understand and interact with financial data and want to have a little fun by launching a challenge with a chance to win $1000.&lt;/p&gt;\n\n&lt;p&gt;We are doing something like this on our GPInvestBot Twitter, but think we can make it even more interesting by engaging with others.&lt;/p&gt;\n\n&lt;p&gt;Most liked headline/tweet created automatically summarizing macroeconomic and financial time series data wins. More details here: &lt;a href=\"https://www.globalpredictions.com/posts/unlocking-insights-in-time-series-data-the-generative-ai-challenge\"&gt;https://www.globalpredictions.com/posts/unlocking-insights-in-time-series-data-the-generative-ai-challenge&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/7AuEb9DaOBM5SbScEjNQg-AzfC9e52u2PO2WN7GkotY.jpg?auto=webp&amp;v=enabled&amp;s=0fba279202c7fd132a1b2474de83d7459578e681", "width": 1910, "height": 1000}, "resolutions": [{"url": "https://external-preview.redd.it/7AuEb9DaOBM5SbScEjNQg-AzfC9e52u2PO2WN7GkotY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dfc5e4751d806349c96a3bc6a2fd69c835382a49", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/7AuEb9DaOBM5SbScEjNQg-AzfC9e52u2PO2WN7GkotY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20e12f6970ed8eb7015de7a14f060737ef0f42e7", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/7AuEb9DaOBM5SbScEjNQg-AzfC9e52u2PO2WN7GkotY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9e0ef79987d80b02b2438ad474238253cdbf667f", "width": 320, "height": 167}, {"url": "https://external-preview.redd.it/7AuEb9DaOBM5SbScEjNQg-AzfC9e52u2PO2WN7GkotY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=368cd28d8fa82501c9782f47d6191e246a9754d6", "width": 640, "height": 335}, {"url": "https://external-preview.redd.it/7AuEb9DaOBM5SbScEjNQg-AzfC9e52u2PO2WN7GkotY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f1a437ec26ce30a0cac93c282a033b5d25231d25", "width": 960, "height": 502}, {"url": "https://external-preview.redd.it/7AuEb9DaOBM5SbScEjNQg-AzfC9e52u2PO2WN7GkotY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1ae250158e104c18068aafec508feefe5472d56", "width": 1080, "height": 565}], "variants": {}, "id": "m6ZIV4lMAqK8horuOkbW85X4N-4BOMbbhvGxVqlIZkU"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "b026063c-d780-11e7-aba9-0e030591a4b4", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dqjha", "is_robot_indexable": true, "report_reasons": null, "author": "BenGlobalPredictions", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dqjha/looking_to_push_the_boundaries_new_challenge_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dqjha/looking_to_push_the_boundaries_new_challenge_to/", "subreddit_subscribers": 868788, "created_utc": 1680800420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I've found out today my company has signed up to a marketing association that requires audits. We haven't been told how often or the standard. The thing is we do market research, not marketing so I don't get what they would audit us for.\n\nShould I be worried? I had no consultation on this before it happened.", "author_fullname": "t2_r614hsw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Should I be worried we are under a new audit system?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dojgh", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680796461.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found out today my company has signed up to a marketing association that requires audits. We haven&amp;#39;t been told how often or the standard. The thing is we do market research, not marketing so I don&amp;#39;t get what they would audit us for.&lt;/p&gt;\n\n&lt;p&gt;Should I be worried? I had no consultation on this before it happened.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dojgh", "is_robot_indexable": true, "report_reasons": null, "author": "psychmancer", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dojgh/should_i_be_worried_we_are_under_a_new_audit/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dojgh/should_i_be_worried_we_are_under_a_new_audit/", "subreddit_subscribers": 868788, "created_utc": 1680796461.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I'm looking to further my education in data science and looking to see if any current data scientists are able to answer some industry questions 1  on 1. This would be a prequisite for acceptance into the cohort.  Any current data professionals be able to help?", "author_fullname": "t2_70ffsxmu", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Informational Interview", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12djc5e", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680785420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking to further my education in data science and looking to see if any current data scientists are able to answer some industry questions 1  on 1. This would be a prequisite for acceptance into the cohort.  Any current data professionals be able to help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12djc5e", "is_robot_indexable": true, "report_reasons": null, "author": "Sea-Complex2759", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12djc5e/informational_interview/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12djc5e/informational_interview/", "subreddit_subscribers": 868788, "created_utc": 1680785420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "", "author_fullname": "t2_c8af2ty7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Why AI Struggles in Generating Human Hands: A Deep Dive into the Complexities", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "education", "downs": 0, "thumbnail_height": 140, "top_awarded_type": null, "hide_score": false, "name": "t3_12dayeu", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Education", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JQZsL6lzjZ8cePTEqT18L6JjMNVzZwRJa6HiUvC2h-A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680761225.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/@ramaditya.missula/why-ai-struggles-in-generating-human-hands-a-deep-dive-into-the-complexities-24403e3478f3", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?auto=webp&amp;v=enabled&amp;s=a115ef4591c48733972d3794c780fc2c1e515044", "width": 1200, "height": 1200}, "resolutions": [{"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=15183f486bd3fb807f52fd7fc4879f14e5af8417", "width": 108, "height": 108}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ddf2a272e1d9c3ea0c891fbe20f10e904d1e4b38", "width": 216, "height": 216}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c9af96d97291044a94af8fd652279e3151264586", "width": 320, "height": 320}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5405a2f84efbb1a7a6b38d7163f08151870491ff", "width": 640, "height": 640}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=78116c2541048b0585b87c55631b258e026fefbb", "width": 960, "height": 960}, {"url": "https://external-preview.redd.it/AYbfFHOFntp7KrpZvR1DzZ3N611oI-o79vw-tY87xaw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=cc33d30a9277e10082ad9aa0cc1a5f26e5b9e411", "width": 1080, "height": 1080}], "variants": {}, "id": "FI8ZsgjZPgi9bOy6JnRCXfJyAFOkliNf0FqKVXVNmHw"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "99f9652a-d780-11e7-b558-0e52cdd59ace", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dayeu", "is_robot_indexable": true, "report_reasons": null, "author": "WeakTry9804", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dayeu/why_ai_struggles_in_generating_human_hands_a_deep/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/@ramaditya.missula/why-ai-struggles-in-generating-human-hands-a-deep-dive-into-the-complexities-24403e3478f3", "subreddit_subscribers": 868788, "created_utc": 1680761225.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "Which is better to pursue now with the worry of AI taking over jobs possibly in the near future? Which has a better work/life balance?", "author_fullname": "t2_30nvs816", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Analyst vs. Software Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "discussion", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dx49a", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680813360.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which is better to pursue now with the worry of AI taking over jobs possibly in the near future? Which has a better work/life balance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "4fad7108-d77d-11e7-b0c6-0ee69f155af2", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dx49a", "is_robot_indexable": true, "report_reasons": null, "author": "navywife223", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dx49a/data_analyst_vs_software_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dx49a/data_analyst_vs_software_engineer/", "subreddit_subscribers": 868788, "created_utc": 1680813360.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "datascience", "selftext": "I want to make a career in data science. I have 4 months to study whatever I can. I don't want to get a degree but still work in this domain based on the skills I acquire in these four months. \nFirst of all is it possible?\nSecondly can someone please provide what to study in these 4 months (in order) and relevant links considering I know nothing as of now.", "author_fullname": "t2_slgwqeo0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Very Urgent - Data Science Advice", "link_flair_richtext": [], "subreddit_name_prefixed": "r/datascience", "hidden": false, "pwls": 6, "link_flair_css_class": "career", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dy8rr", "quarantine": false, "link_flair_text_color": "dark", "upvote_ratio": 0.2, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680815659.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.datascience", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to make a career in data science. I have 4 months to study whatever I can. I don&amp;#39;t want to get a degree but still work in this domain based on the skills I acquire in these four months. \nFirst of all is it possible?\nSecondly can someone please provide what to study in these 4 months (in order) and relevant links considering I know nothing as of now.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": "confidence", "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "a6ee6fa0-d780-11e7-b6d0-0e0bd8823a7e", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_2sptq", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "", "id": "12dy8rr", "is_robot_indexable": true, "report_reasons": null, "author": "Ok-Combination4623", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/datascience/comments/12dy8rr/very_urgent_data_science_advice/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/datascience/comments/12dy8rr/very_urgent_data_science_advice/", "subreddit_subscribers": 868788, "created_utc": 1680815659.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}