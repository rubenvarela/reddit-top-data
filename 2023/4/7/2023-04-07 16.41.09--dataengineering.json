{"kind": "Listing", "data": {"after": "t3_12e65p3", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a junior engineer for 9 months now at a consultancy, and I have to say, it's been a real struggle. Up to now, I've worked on 3 projects and I've struggled on each one. I struggle with the expectation that my company has for me to deliver. I understand I need to deliver, but it feels like I'm stuck in a vicious circle.\n\nAll of the cloud stuff, sql, and pipelines are new to me. Each project has lasted 2 or 3 months and by the time I'm comfortable and started to deliver, the project is over. In my spare time, I do plenty of tutorials to get myself up to speed, but it seems like all the personal work I do is not relevant to the project work that I'm doing because it's s so specific. Majority of the time I'm depending on my fellow engineers to help me out. I can honestly say, that I've only completed a handful of tickets by myself and I feel ashamed of it. They expect me to deliver when I simply don't know. I honestly feel like a burden on every team that I'm on.\n\nToday I had a 1-1 with my manager and he said that people on my previous project complained about my lack of contribution and how they weren't able to bill for their work because they were helping me. This was a shock because none of my colleagues expressed this to me while I was there.\n\nMy manager decided to do a 'PIP' (Performance Improvement Plan) to get me to improve. I've told my manager that I need to be on a project that is 6 months to a year-long but he keeps saying there isn't one. I feel kinda sad that he's put me on an informal disciplinary thing. He kept on saying that it was nothing, but I know it will be used against me if I don't improve. \n\nI feel like I'm alone and don't know what to do. Am I just kidding myself that I'm an engineer?", "author_fullname": "t2_81mnb4wb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shall I accept that I'm a useless engineer and look somewhere else or is this feeling part of the journey?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e0lti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 58, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 58, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680820742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a junior engineer for 9 months now at a consultancy, and I have to say, it&amp;#39;s been a real struggle. Up to now, I&amp;#39;ve worked on 3 projects and I&amp;#39;ve struggled on each one. I struggle with the expectation that my company has for me to deliver. I understand I need to deliver, but it feels like I&amp;#39;m stuck in a vicious circle.&lt;/p&gt;\n\n&lt;p&gt;All of the cloud stuff, sql, and pipelines are new to me. Each project has lasted 2 or 3 months and by the time I&amp;#39;m comfortable and started to deliver, the project is over. In my spare time, I do plenty of tutorials to get myself up to speed, but it seems like all the personal work I do is not relevant to the project work that I&amp;#39;m doing because it&amp;#39;s s so specific. Majority of the time I&amp;#39;m depending on my fellow engineers to help me out. I can honestly say, that I&amp;#39;ve only completed a handful of tickets by myself and I feel ashamed of it. They expect me to deliver when I simply don&amp;#39;t know. I honestly feel like a burden on every team that I&amp;#39;m on.&lt;/p&gt;\n\n&lt;p&gt;Today I had a 1-1 with my manager and he said that people on my previous project complained about my lack of contribution and how they weren&amp;#39;t able to bill for their work because they were helping me. This was a shock because none of my colleagues expressed this to me while I was there.&lt;/p&gt;\n\n&lt;p&gt;My manager decided to do a &amp;#39;PIP&amp;#39; (Performance Improvement Plan) to get me to improve. I&amp;#39;ve told my manager that I need to be on a project that is 6 months to a year-long but he keeps saying there isn&amp;#39;t one. I feel kinda sad that he&amp;#39;s put me on an informal disciplinary thing. He kept on saying that it was nothing, but I know it will be used against me if I don&amp;#39;t improve. &lt;/p&gt;\n\n&lt;p&gt;I feel like I&amp;#39;m alone and don&amp;#39;t know what to do. Am I just kidding myself that I&amp;#39;m an engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12e0lti", "is_robot_indexable": true, "report_reasons": null, "author": "Taurusamazing92", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e0lti/shall_i_accept_that_im_a_useless_engineer_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e0lti/shall_i_accept_that_im_a_useless_engineer_and/", "subreddit_subscribers": 96604, "created_utc": 1680820742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hf9ddayj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineers processing data access requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": false, "name": "t3_12ekdv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "ups": 65, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 65, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PwQFpnI6ohAOk8a2oHExRt5VPPNiNCh0lB3wSKRXgPM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680872041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/fvhpekkylgsa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/fvhpekkylgsa1.png?auto=webp&amp;v=enabled&amp;s=3d488123b490adb09b02b89bf0062491807eb3a7", "width": 1566, "height": 1096}, "resolutions": [{"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e723ffde2298b53be5040ca50cd3fa857d8cddd6", "width": 108, "height": 75}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dad4b60ee7905425f32c05d87fc14462894faa9b", "width": 216, "height": 151}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4acec0efae2a9d17c5d774ba1e561901f29f158e", "width": 320, "height": 223}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2933de1a74c8bd7130551f9d2351a29612fe36d", "width": 640, "height": 447}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c972504627d1b0844729ccf54b5b5183bcc45c04", "width": 960, "height": 671}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba3a61733592d6c7ee91b71476a7fd4e9fb8c9aa", "width": 1080, "height": 755}], "variants": {}, "id": "DY6enitmFwWz8hoYEQaE_nDciegvG14Wmu3A3m0BEUU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12ekdv2", "is_robot_indexable": true, "report_reasons": null, "author": "Bart_Vee", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ekdv2/data_engineers_processing_data_access_requests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/fvhpekkylgsa1.png", "subreddit_subscribers": 96604, "created_utc": 1680872041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're already accustomed to hearing people say \"this is how YOU should use chatGPT\", but I rarely see people say \"This is how I use chat gpt.\"  \n\n\nIf you've got first hand experience using chat GPT to add legitimate value as a data engineer, I'd love to hear about it!", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How has ChatGPT helped you in your DE job? First hand experience only, plz.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dwoe3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680812476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re already accustomed to hearing people say &amp;quot;this is how YOU should use chatGPT&amp;quot;, but I rarely see people say &amp;quot;This is how I use chat gpt.&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve got first hand experience using chat GPT to add legitimate value as a data engineer, I&amp;#39;d love to hear about it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dwoe3", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 104, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dwoe3/how_has_chatgpt_helped_you_in_your_de_job_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dwoe3/how_has_chatgpt_helped_you_in_your_de_job_first/", "subreddit_subscribers": 96604, "created_utc": 1680812476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI have seen a lot of posts regarding the use cases of Docker but not really any on how it actually works on a functional level. Many mention that it is great for dependency issues (one version of Python required for one process vs another) but how does Docker actually solve for this?", "author_fullname": "t2_jr18wyyz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker - Magic or Hype?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dteg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 19, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 19, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680814420.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680806003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have seen a lot of posts regarding the use cases of Docker but not really any on how it actually works on a functional level. Many mention that it is great for dependency issues (one version of Python required for one process vs another) but how does Docker actually solve for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dteg3", "is_robot_indexable": true, "report_reasons": null, "author": "Fintechie__", "discussion_type": null, "num_comments": 27, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dteg3/docker_magic_or_hype/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dteg3/docker_magic_or_hype/", "subreddit_subscribers": 96604, "created_utc": 1680806003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I haven\u2019t done too many technical interviews for DE (&lt;20). Normally I get questions that i can do pretty easily. Things like: modeling, \u201chow would you build this pipeline?\u201d, basic SQL/Python, cloud services/data warehousing. \n\nToday I got put into a live coding interview unexpectedly, and was asked to get data from an API and do some transformations against it. The solution involved using requests library and doing some json loading/parsing. For the transformation it involved using the pivot function in python.\n\nI was completely caught off guard, as I haven\u2019t done the above in a while and my mind sort of blanked. \n\nIs this something that I should have known how to do easily?", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you be able to do this on the spot during an interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e589j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680831292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven\u2019t done too many technical interviews for DE (&amp;lt;20). Normally I get questions that i can do pretty easily. Things like: modeling, \u201chow would you build this pipeline?\u201d, basic SQL/Python, cloud services/data warehousing. &lt;/p&gt;\n\n&lt;p&gt;Today I got put into a live coding interview unexpectedly, and was asked to get data from an API and do some transformations against it. The solution involved using requests library and doing some json loading/parsing. For the transformation it involved using the pivot function in python.&lt;/p&gt;\n\n&lt;p&gt;I was completely caught off guard, as I haven\u2019t done the above in a while and my mind sort of blanked. &lt;/p&gt;\n\n&lt;p&gt;Is this something that I should have known how to do easily?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12e589j", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12e589j/would_you_be_able_to_do_this_on_the_spot_during/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e589j/would_you_be_able_to_do_this_on_the_spot_during/", "subreddit_subscribers": 96604, "created_utc": 1680831292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to gauge the viability of running Python/Go/Dask data lake transformations (raw data in many files -&gt; cleaned data in many parquet files). \n\n- Spark might intro a lot of overhead and complexity\n- We don't want to buy into Databricks\n- We're running on K8s with a solid platform team for support\n- Right now, Python scripts + Fivetran drop raw data files into data lake\n- First goal is to transform these raw files into silver and gold parquet-based tables in the data lake\n- After gold, teams are allowed to load into a DWH, query directly with some BI tool, etc but that's not the focus right now.\n\nWe know our infra will change over the next couple of years as this data operation gets going so writing simple, low cost scripts in languages that we're handy with would be great.\n\nMy worry in this case is performance. Can these languages/frameworks work well when needing to transform ~50GBs of data spread across many (hourly partitioned probably) files and transform them into the silver and gold. \n\nAny experience with something like this? Thanks.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any teams building data lakes without Spark? And specifically with vanilla Python/Go or with something like Dask?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dps2u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680798898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to gauge the viability of running Python/Go/Dask data lake transformations (raw data in many files -&amp;gt; cleaned data in many parquet files). &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Spark might intro a lot of overhead and complexity&lt;/li&gt;\n&lt;li&gt;We don&amp;#39;t want to buy into Databricks&lt;/li&gt;\n&lt;li&gt;We&amp;#39;re running on K8s with a solid platform team for support&lt;/li&gt;\n&lt;li&gt;Right now, Python scripts + Fivetran drop raw data files into data lake&lt;/li&gt;\n&lt;li&gt;First goal is to transform these raw files into silver and gold parquet-based tables in the data lake&lt;/li&gt;\n&lt;li&gt;After gold, teams are allowed to load into a DWH, query directly with some BI tool, etc but that&amp;#39;s not the focus right now.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We know our infra will change over the next couple of years as this data operation gets going so writing simple, low cost scripts in languages that we&amp;#39;re handy with would be great.&lt;/p&gt;\n\n&lt;p&gt;My worry in this case is performance. Can these languages/frameworks work well when needing to transform ~50GBs of data spread across many (hourly partitioned probably) files and transform them into the silver and gold. &lt;/p&gt;\n\n&lt;p&gt;Any experience with something like this? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dps2u", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 46, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dps2u/any_teams_building_data_lakes_without_spark_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dps2u/any_teams_building_data_lakes_without_spark_and/", "subreddit_subscribers": 96604, "created_utc": 1680798898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The title says everything.Just find Apache Airflow fascinating but I cannot run it locally. Any idea where I can experiment it on the cloud? Almost every other post in this sub is on Airflow/Dagster.   \n\n\nFor example - We have replit to experiment on Python or other programming languages, so is there a similar infra for Airflow/Dagster?", "author_fullname": "t2_6hp3gp78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where can I experiment with Apache Airflow without running it locally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ecrzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680850948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title says everything.Just find Apache Airflow fascinating but I cannot run it locally. Any idea where I can experiment it on the cloud? Almost every other post in this sub is on Airflow/Dagster.   &lt;/p&gt;\n\n&lt;p&gt;For example - We have replit to experiment on Python or other programming languages, so is there a similar infra for Airflow/Dagster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ecrzm", "is_robot_indexable": true, "report_reasons": null, "author": "rohetoric", "discussion_type": null, "num_comments": 24, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ecrzm/where_can_i_experiment_with_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ecrzm/where_can_i_experiment_with_apache_airflow/", "subreddit_subscribers": 96604, "created_utc": 1680850948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since joining a Bay Area tech company, I have realized the grind to keep up with all the new tools in software engineering is alot. I am 35 and realizing it takes too much time commitment to keep up after work. Even as a data engineering manager, I am expected to know all the new skills and tools. What other roles align to a data engineering background? Product roles for data platforms? I like working with the business.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Change?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e6iqp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680834164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since joining a Bay Area tech company, I have realized the grind to keep up with all the new tools in software engineering is alot. I am 35 and realizing it takes too much time commitment to keep up after work. Even as a data engineering manager, I am expected to know all the new skills and tools. What other roles align to a data engineering background? Product roles for data platforms? I like working with the business.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12e6iqp", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e6iqp/career_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e6iqp/career_change/", "subreddit_subscribers": 96604, "created_utc": 1680834164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm just an average (perhaps below average) data engineer. I have read many posts in this subreddit daily (other sources also) and have seen many visions from you guys. I want to contribute something, but have nothing to share. So I share this article (I have just read that minutes ago):  \n[https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c](https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c)  \nAt first I was mind-blown, and thought of creating a repo built on top ChatGPT to extract, transform data. My dream collapsed right afterwards, after knowing the existence of AWS Aurora.[https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/](https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/)  \nHowever, i'm still get excited due to this. If the data is relatively simply structured, this would be extremely handy. Otherwise, ETL/ELT pipelines are still needed. Other pros and cons of zero-ETL are also mentioned in the above towardsdatascience article.  \nWhat are your thoughts about this?", "author_fullname": "t2_4exoz3kf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The future of less ETL-data-pipeline building", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12eg4ty", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680862065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just an average (perhaps below average) data engineer. I have read many posts in this subreddit daily (other sources also) and have seen many visions from you guys. I want to contribute something, but have nothing to share. So I share this article (I have just read that minutes ago):&lt;br/&gt;\n&lt;a href=\"https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c\"&gt;https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&lt;/a&gt;&lt;br/&gt;\nAt first I was mind-blown, and thought of creating a repo built on top ChatGPT to extract, transform data. My dream collapsed right afterwards, after knowing the existence of AWS Aurora.&lt;a href=\"https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/\"&gt;https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/&lt;/a&gt;&lt;br/&gt;\nHowever, i&amp;#39;m still get excited due to this. If the data is relatively simply structured, this would be extremely handy. Otherwise, ETL/ELT pipelines are still needed. Other pros and cons of zero-ETL are also mentioned in the above towardsdatascience article.&lt;br/&gt;\nWhat are your thoughts about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?auto=webp&amp;v=enabled&amp;s=ec51b72c1edf8ba4f8dc0eb8bbd0887c5a10bbdd", "width": 1200, "height": 543}, "resolutions": [{"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1e8c9fd48223781443d8c46be48ab920179e2b7", "width": 108, "height": 48}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=382e0f4f2c49bf8cf0e3b3aa4d813e621e21cb1b", "width": 216, "height": 97}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ddedb3f7dfc408418a63076bae0756ac336c924", "width": 320, "height": 144}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e8423f80288e2ce8684080fb040aba5768a2f39", "width": 640, "height": 289}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db543297d3c099db0127a0b2f21eaec818bce132", "width": 960, "height": 434}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3176ee720d2a0710d0cbc971054c316ad001141", "width": 1080, "height": 488}], "variants": {}, "id": "NUV7jTZprOTDG09SohaAm32t7mHuSpgMRI6ybLu37-E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12eg4ty", "is_robot_indexable": true, "report_reasons": null, "author": "duohd", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12eg4ty/the_future_of_less_etldatapipeline_building/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eg4ty/the_future_of_less_etldatapipeline_building/", "subreddit_subscribers": 96604, "created_utc": 1680862065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In AWS Glue (PySpark) I'm reading a 29GB CSV file from S3 then repartitioning to 204 partitions and finally writing out to S3. The Spark log for the FileScanRDD stage shows that a single executor is processing the entire 29GB and spilling a huge amount of data. To me this seems like Spark is using a  single executor to first read the entire CSV. I was under the impression that a  splittable format such as CSV could be read in chunks by each executor rather than going into a single executor. Is the only way to avoid the single-executor read to break up the input CSV into smaller files before reading? Also, why are  the 101GB disk and 40GB memory spill larger than the input size of 29GB? I'm  generally just trying to understand how the data transfer is happening  with respect to executors/partitions.\n\n&amp;#x200B;\n\n[FileScan Executor Metrics](https://preview.redd.it/s8gseh4mjasa1.png?width=1871&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fdb7fe4668506e2ece55a36057df3cd0208960b0)\n\n&amp;#x200B;\n\n[FileScan DAG](https://preview.redd.it/nung0qcojasa1.png?width=565&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=70ed7073d4608c93117de8fb4767a3cb9f57fe2b)\n\n&amp;#x200B;\n\n    import sys\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext, SparkConf\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    import time\n    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n    \n    sc = SparkContext()\n    glueContext = GlueContext(sc)\n    spark = glueContext.spark_session\n    \n    \n    df = spark.read.format('csv').option('header',True)\\\n                                .option('multiline',True)\\\n                                .option(\"escape\", \"\\\"\")\\\n                                .option(\"delimiter\", \",\")\\\n                                .load(f's3://bucket/folder')\n    \n    df = df.repartition(numPartitions=204)\n    \n    \n    db = 'athena-db-name'\n    path = 's3://target_bucket/target_folder'\n    table = 'target_table_name'\n    folder = table.replace('_','-')\n    target_path = path + folder\n    \n    \n    df.write.saveAsTable(f\"`{db}`.{table}\", format='parquet', mode='overwrite', path=target_path)", "author_fullname": "t2_1evp2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark (AWS Glue) spill when reading large CSV file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 20, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nung0qcojasa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 132, "x": 108, "u": "https://preview.redd.it/nung0qcojasa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca92d05ca7ae22413fedb452ca6bdef04f34997d"}, {"y": 265, "x": 216, "u": "https://preview.redd.it/nung0qcojasa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de956270a3400fa6e6283e50d500906975001db1"}, {"y": 393, "x": 320, "u": "https://preview.redd.it/nung0qcojasa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8859cfb230ddf501ddf0044fd189fad47589abe"}], "s": {"y": 695, "x": 565, "u": "https://preview.redd.it/nung0qcojasa1.png?width=565&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=70ed7073d4608c93117de8fb4767a3cb9f57fe2b"}, "id": "nung0qcojasa1"}, "s8gseh4mjasa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 16, "x": 108, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7e753437a67bae601dd2e16fece2b648cc0f2ff"}, {"y": 32, "x": 216, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bda3090b9168e845d89555e379289c18be255c63"}, {"y": 47, "x": 320, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc98e1e06dea0ec99d3ec622e4217d074cf3959b"}, {"y": 95, "x": 640, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d70bfc4858eca2a3bf43c275b01c1762a6150d1"}, {"y": 142, "x": 960, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bc6879429dfbb8293c22095bc630b1b7988daba"}, {"y": 160, "x": 1080, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=481f83db3e97408cfb64fcf36606cd5770a9fea7"}], "s": {"y": 278, "x": 1871, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=1871&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fdb7fe4668506e2ece55a36057df3cd0208960b0"}, "id": "s8gseh4mjasa1"}}, "name": "t3_12dpot8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ffkA9zEVfvOCOc37wATyi335c6BIwRvXsujeDYMRxL4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680798718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In AWS Glue (PySpark) I&amp;#39;m reading a 29GB CSV file from S3 then repartitioning to 204 partitions and finally writing out to S3. The Spark log for the FileScanRDD stage shows that a single executor is processing the entire 29GB and spilling a huge amount of data. To me this seems like Spark is using a  single executor to first read the entire CSV. I was under the impression that a  splittable format such as CSV could be read in chunks by each executor rather than going into a single executor. Is the only way to avoid the single-executor read to break up the input CSV into smaller files before reading? Also, why are  the 101GB disk and 40GB memory spill larger than the input size of 29GB? I&amp;#39;m  generally just trying to understand how the data transfer is happening  with respect to executors/partitions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s8gseh4mjasa1.png?width=1871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fdb7fe4668506e2ece55a36057df3cd0208960b0\"&gt;FileScan Executor Metrics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nung0qcojasa1.png?width=565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=70ed7073d4608c93117de8fb4767a3cb9f57fe2b\"&gt;FileScan DAG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext, SparkConf\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport time\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\n\ndf = spark.read.format(&amp;#39;csv&amp;#39;).option(&amp;#39;header&amp;#39;,True)\\\n                            .option(&amp;#39;multiline&amp;#39;,True)\\\n                            .option(&amp;quot;escape&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;)\\\n                            .option(&amp;quot;delimiter&amp;quot;, &amp;quot;,&amp;quot;)\\\n                            .load(f&amp;#39;s3://bucket/folder&amp;#39;)\n\ndf = df.repartition(numPartitions=204)\n\n\ndb = &amp;#39;athena-db-name&amp;#39;\npath = &amp;#39;s3://target_bucket/target_folder&amp;#39;\ntable = &amp;#39;target_table_name&amp;#39;\nfolder = table.replace(&amp;#39;_&amp;#39;,&amp;#39;-&amp;#39;)\ntarget_path = path + folder\n\n\ndf.write.saveAsTable(f&amp;quot;`{db}`.{table}&amp;quot;, format=&amp;#39;parquet&amp;#39;, mode=&amp;#39;overwrite&amp;#39;, path=target_path)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dpot8", "is_robot_indexable": true, "report_reasons": null, "author": "LaminatedMisanthropy", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dpot8/pyspark_aws_glue_spill_when_reading_large_csv_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dpot8/pyspark_aws_glue_spill_when_reading_large_csv_file/", "subreddit_subscribers": 96604, "created_utc": 1680798718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering, \n\nI been tasked with setting up a new environment for my teams processing. We have quite a few restrictions which makes it hard to narrow in on a good plan for our use case.\n\nCurrently my team uses on-prem SAS that can no longer handle the load and the discussions with IT seem to indicate that we will never get a new server so we need to piggyback off an existing environment used by other teams, that is basically locked down (no internet) windows VMs (64GB 8 cores can maybe get bigger) on the cloud with databricks connected to them.\n\nthe environment limitations / processing details:\n\n* Must be python, dont have permission to use another language except maybe sql\n* Solution that is available on PYPI repository only and will be installing it offline with anaconda. \n* cant install external executables\n* We do mostly small batch processing jobs usually small &lt;10GB but occasionally some large 100-200GB batch processing. \n* We need to output as csvs.\n\nSo far I am weighing between the following options and am happy to hear other thoughts and opinions:\n\n* Spark, (can use local spark for small jobs and use databricks for any large jobs) \n* Pandas (I worry it will die with the larger datasets)\n* Polars (I havent used this before but heard it is a better replacement for pandas)\n* Dask (I assume it will eliminate the problems of pandas?)\n\nI would like to see something with pipe lining options, maybe a local dagster?\n\n&amp;#x200B;\n\nAnyway thanks for reading this far, I hope someone has some suggestions, right now I am thinking that I am fucked with all the limitations put on me.", "author_fullname": "t2_3x579bn9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with deciding data-eng stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e43ql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680828737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;, &lt;/p&gt;\n\n&lt;p&gt;I been tasked with setting up a new environment for my teams processing. We have quite a few restrictions which makes it hard to narrow in on a good plan for our use case.&lt;/p&gt;\n\n&lt;p&gt;Currently my team uses on-prem SAS that can no longer handle the load and the discussions with IT seem to indicate that we will never get a new server so we need to piggyback off an existing environment used by other teams, that is basically locked down (no internet) windows VMs (64GB 8 cores can maybe get bigger) on the cloud with databricks connected to them.&lt;/p&gt;\n\n&lt;p&gt;the environment limitations / processing details:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Must be python, dont have permission to use another language except maybe sql&lt;/li&gt;\n&lt;li&gt;Solution that is available on PYPI repository only and will be installing it offline with anaconda. &lt;/li&gt;\n&lt;li&gt;cant install external executables&lt;/li&gt;\n&lt;li&gt;We do mostly small batch processing jobs usually small &amp;lt;10GB but occasionally some large 100-200GB batch processing. &lt;/li&gt;\n&lt;li&gt;We need to output as csvs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So far I am weighing between the following options and am happy to hear other thoughts and opinions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Spark, (can use local spark for small jobs and use databricks for any large jobs) &lt;/li&gt;\n&lt;li&gt;Pandas (I worry it will die with the larger datasets)&lt;/li&gt;\n&lt;li&gt;Polars (I havent used this before but heard it is a better replacement for pandas)&lt;/li&gt;\n&lt;li&gt;Dask (I assume it will eliminate the problems of pandas?)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would like to see something with pipe lining options, maybe a local dagster?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyway thanks for reading this far, I hope someone has some suggestions, right now I am thinking that I am fucked with all the limitations put on me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12e43ql", "is_robot_indexable": true, "report_reasons": null, "author": "Mclovine_aus", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e43ql/help_with_deciding_dataeng_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e43ql/help_with_deciding_dataeng_stack/", "subreddit_subscribers": 96604, "created_utc": 1680828737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title says it all", "author_fullname": "t2_vm5kbtxr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is dbt + athena/presto a viable stack? how do you orchestrate creation and maintenance of your athena/presto tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ee6bw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680855655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title says it all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ee6bw", "is_robot_indexable": true, "report_reasons": null, "author": "srevolve", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ee6bw/is_dbt_athenapresto_a_viable_stack_how_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ee6bw/is_dbt_athenapresto_a_viable_stack_how_do_you/", "subreddit_subscribers": 96604, "created_utc": 1680855655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_txvugrht", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What I Don't Want To See In The Data World In 5 Years", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 61, "top_awarded_type": null, "hide_score": false, "name": "t3_12el758", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/9_HjsHTlAulDskKkZGvs_BmLnOsisdmtkTax-6ZqajA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680873784.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "seattledataguy.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://seattledataguy.substack.com/p/what-i-dont-want-to-see-in-the-data", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?auto=webp&amp;v=enabled&amp;s=a6131fd83abd457f466d76cc4cfb5c2cbdcb8757", "width": 1200, "height": 530}, "resolutions": [{"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a405b03b1770f1ed5165c561229970ed093f6ab6", "width": 108, "height": 47}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0de727292d78f7c60a0b11a9371f6d7e43037c6b", "width": 216, "height": 95}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4f3280f08c8675e6c2e2443ac8c9ee0635a28776", "width": 320, "height": 141}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a59a467376400f564838cdcab402c593a327bf6e", "width": 640, "height": 282}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7b4a9a8ffb25ae89a2d7142f3204946bcd630b26", "width": 960, "height": 424}, {"url": "https://external-preview.redd.it/R-x1Ur-J9SX_VG7MbOlf1rIFTHwH6_OP5n7jTD2Iu_0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3891ef64193dd18a9f484a17b1b0ce7bbc27bf26", "width": 1080, "height": 477}], "variants": {}, "id": "uA_trWBusPbOF2d9apwPVAwzE_UX45bICkDcvh2Z0Vc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineering Company", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12el758", "is_robot_indexable": true, "report_reasons": null, "author": "prequel_co", "discussion_type": null, "num_comments": 2, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12el758/what_i_dont_want_to_see_in_the_data_world_in_5/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://seattledataguy.substack.com/p/what-i-dont-want-to-see-in-the-data", "subreddit_subscribers": 96604, "created_utc": 1680873784.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jkhpl2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to run dynamic Github Action workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": false, "name": "t3_12ejuno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/FZ-EQeupBO47ozfZ4Yhsp-mzoSYWcEYoFigcz_6ap38.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680870879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/35692957ef94", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?auto=webp&amp;v=enabled&amp;s=eef5caba0c98e8021ccceef2330b82bb0c4606f5", "width": 1200, "height": 750}, "resolutions": [{"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a850d3475130a02050eaace31f251748b905d792", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adef17c44871e980d9b4e4473cdd9d8ef5f4a484", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b74f8c19297e4c723220debde72294891846b15b", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd5f85dd858cd8d54a191d6499ade97cfa1dac48", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=242170b34d9b9b44c47ac539d4498063edcfd29e", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=770b2e3cc0ccc9d1a5c21f271a323605672be27d", "width": 1080, "height": 675}], "variants": {}, "id": "corEyKiyBN7xmaYdlI-4JDM9ATZVUIIp5xW9IRgtJB8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ejuno", "is_robot_indexable": true, "report_reasons": null, "author": "Luxi36", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ejuno/how_to_run_dynamic_github_action_workflows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/35692957ef94", "subreddit_subscribers": 96604, "created_utc": 1680870879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category. The category is not distinct thus other row may refer to any of the 3 categories.\n\ne.g. Dataset A\n\n&amp;#x200B;\n\n|Date|Value|Category|\n|:-|:-|:-|\n|1/1/2010|1.11111|Alpha|\n|2/1/2010|2.11111|Beta|\n|3/1/2010|2.00009|Alpha|\n|4/1/2010|0.00000|Charlie|\n\nBut the 10 datasets have different volume of data. E.g.  dataset A may have 10K rows, dataset B around 100K, Dataset C 1 million, etc.\n\nI couldn't process all the data as its too large.\n\nWhat would be the best way to sample each dataset? I'd like the sample containing a fair representative of the 3 categories.", "author_fullname": "t2_3z6gqvrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to represent large categorical data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12egjxl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680863159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category. The category is not distinct thus other row may refer to any of the 3 categories.&lt;/p&gt;\n\n&lt;p&gt;e.g. Dataset A&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Date&lt;/th&gt;\n&lt;th align=\"left\"&gt;Value&lt;/th&gt;\n&lt;th align=\"left\"&gt;Category&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.11111&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alpha&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.11111&lt;/td&gt;\n&lt;td align=\"left\"&gt;Beta&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.00009&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alpha&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.00000&lt;/td&gt;\n&lt;td align=\"left\"&gt;Charlie&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;But the 10 datasets have different volume of data. E.g.  dataset A may have 10K rows, dataset B around 100K, Dataset C 1 million, etc.&lt;/p&gt;\n\n&lt;p&gt;I couldn&amp;#39;t process all the data as its too large.&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to sample each dataset? I&amp;#39;d like the sample containing a fair representative of the 3 categories.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12egjxl", "is_robot_indexable": true, "report_reasons": null, "author": "runnersgo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12egjxl/how_to_represent_large_categorical_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12egjxl/how_to_represent_large_categorical_data/", "subreddit_subscribers": 96604, "created_utc": 1680863159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all - does using Delta Sharing cost anything as a Databricks customer? I would want to know the answer for multiple use cases (DBX-to-DBX internal, DBX-to-DBX external or another organization but same VPC, and DBX-to-DBX external different VPC or CSP altogether).   \n\n\nThanks so much.", "author_fullname": "t2_4oqqusfb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Delta Sharing Cost", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12epuyd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680883088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all - does using Delta Sharing cost anything as a Databricks customer? I would want to know the answer for multiple use cases (DBX-to-DBX internal, DBX-to-DBX external or another organization but same VPC, and DBX-to-DBX external different VPC or CSP altogether).   &lt;/p&gt;\n\n&lt;p&gt;Thanks so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12epuyd", "is_robot_indexable": true, "report_reasons": null, "author": "Foreign_Magician_429", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12epuyd/delta_sharing_cost/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12epuyd/delta_sharing_cost/", "subreddit_subscribers": 96604, "created_utc": 1680883088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_vuozxz2n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Workflow orchestrators: Comparing Metaflow, Kedro, Luigi, Airflow, Flyte, Prefect and others", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 82, "top_awarded_type": null, "hide_score": true, "name": "t3_12epsbd", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/7O2Raqq31tVgokW1i_9qxmlOv59uD82kjm35PDTk6ug.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680882946.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "dsdaily.substack.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://dsdaily.substack.com/p/workflow-orchestrators-metaflow-kedro", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?auto=webp&amp;v=enabled&amp;s=a6a802bb2d7efde54ff0ef2559e66e363bf49a5a", "width": 1018, "height": 598}, "resolutions": [{"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a90eb229993f8276fb3f03b5a2de1277134e270", "width": 108, "height": 63}, {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c7481dd837331da965c18dc15b5976422a51163e", "width": 216, "height": 126}, {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fe6d2538e603e9ff23432a09fc2833ac7a295d1b", "width": 320, "height": 187}, {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9afb7d19e43b264aa71a446da5da02ffb669588a", "width": 640, "height": 375}, {"url": "https://external-preview.redd.it/B7967E5kGkGqxs4Lda43PVqY-eNs7YQ6ZZ3CzhQUvlU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e0c11dd6aa271307b19698104a9c2ca7db2cc184", "width": 960, "height": 563}], "variants": {}, "id": "k_OJwzSNhRaOxxkROZWgnREqC7dSqVJTRM4y-8YAGys"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12epsbd", "is_robot_indexable": true, "report_reasons": null, "author": "RAFisherman", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12epsbd/workflow_orchestrators_comparing_metaflow_kedro/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://dsdaily.substack.com/p/workflow-orchestrators-metaflow-kedro", "subreddit_subscribers": 96604, "created_utc": 1680882946.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey r/dataengineering! I've been contributing to an open-source project in the data lineage domain, and I'd really like to know more about the challenges you face in your day-to-day work.\n\nSo, let's discuss! What do you find most troublesome about data lineage? Are you struggling with tracking data dependencies, compliance, testing, or maybe something else entirely?", "author_fullname": "t2_i0qyphvw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Fellow Data Engineers, Share Your Data Lineage Struggles!", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12ep3pl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680881592.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! I&amp;#39;ve been contributing to an open-source project in the data lineage domain, and I&amp;#39;d really like to know more about the challenges you face in your day-to-day work.&lt;/p&gt;\n\n&lt;p&gt;So, let&amp;#39;s discuss! What do you find most troublesome about data lineage? Are you struggling with tracking data dependencies, compliance, testing, or maybe something else entirely?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ep3pl", "is_robot_indexable": true, "report_reasons": null, "author": "ProfessionalHorse707", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ep3pl/fellow_data_engineers_share_your_data_lineage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ep3pl/fellow_data_engineers_share_your_data_lineage/", "subreddit_subscribers": 96604, "created_utc": 1680881592.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm a data scientist within a small data science team. We're currently in the midst of a transition to cloud, but due to regulatory mess we cannot actually switch within the next three years. We're kind of an oddball out of the bunch, as the other IT teams focus in making microservices for the organization and don't care much about what we do. 90% of the organization is not IT, so it's not like they hand us the best tools.\n\nMeanwhile, we kind of need a robust development environment that data scientists can use to make more 'production-ready' products. Right now, DEs kind of have to make major adjustments to fit everything in docker containers and have to deal with the issue of debugging things in different environments than it was developed in (Windows Server 2019 vs Linux in the container). Also there's an issue where if someone is running a model (let's say XGB) it literally freezes the session of other colleagues logged into the remote desktop session (yikes!). \n\nWhat's currently a 'good' practice in terms of setting up a development environment? I've looked into Hyper-V or something like Proxmox  and starting up a simple CLI debian per project and using remote explorer and/or dev containers using VSCode. It's a major requirement to have RBAC (Role based access control) per project. If it's indeed not a crazy solution, how would it work so that a DBA can quickly give access to tables for that project in that environment?", "author_fullname": "t2_607hu6ywi", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "On premise development environment for data scientists. Best practices?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12eozr9", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680881394.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a data scientist within a small data science team. We&amp;#39;re currently in the midst of a transition to cloud, but due to regulatory mess we cannot actually switch within the next three years. We&amp;#39;re kind of an oddball out of the bunch, as the other IT teams focus in making microservices for the organization and don&amp;#39;t care much about what we do. 90% of the organization is not IT, so it&amp;#39;s not like they hand us the best tools.&lt;/p&gt;\n\n&lt;p&gt;Meanwhile, we kind of need a robust development environment that data scientists can use to make more &amp;#39;production-ready&amp;#39; products. Right now, DEs kind of have to make major adjustments to fit everything in docker containers and have to deal with the issue of debugging things in different environments than it was developed in (Windows Server 2019 vs Linux in the container). Also there&amp;#39;s an issue where if someone is running a model (let&amp;#39;s say XGB) it literally freezes the session of other colleagues logged into the remote desktop session (yikes!). &lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s currently a &amp;#39;good&amp;#39; practice in terms of setting up a development environment? I&amp;#39;ve looked into Hyper-V or something like Proxmox  and starting up a simple CLI debian per project and using remote explorer and/or dev containers using VSCode. It&amp;#39;s a major requirement to have RBAC (Role based access control) per project. If it&amp;#39;s indeed not a crazy solution, how would it work so that a DBA can quickly give access to tables for that project in that environment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12eozr9", "is_robot_indexable": true, "report_reasons": null, "author": "Substantial_Score757", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12eozr9/on_premise_development_environment_for_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eozr9/on_premise_development_environment_for_data/", "subreddit_subscribers": 96604, "created_utc": 1680881394.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I wanted to try out `pyarrow`'s `S3FileSystem` by converting a `boto3` snippet to read gzipped jsonline files (a dump from DynamoDB). Compared to the original, the `pyarrow` one is terribly slow *well before data starts reading JSON into memory*.\n\nThe snippet core logic is this:\n\n```python\nfrom pyarrow import fs\ns3 = fs.S3FileSystem()\n\nfrom mymodule import yield_as_json\n\nmyfile = \"bucket/path/to/dump.json.gz\"\n\nwith s3.open_input_stream(myfile) as stream:\n    data = list(yield_as_json(stream))\n```\n\nWhat's inside `yield_as_json` does not really matter - I believe it works, because the second snippet (below) works. My suspicion is that `pyarrow` reads the whole data in memory, decompresses it and then starts iterating over lines.\n\nBy comparison, the `boto3` snippet is:\n\n```python\nimport boto3\nimport gzip\n\nfrom mymodule import yield_as_json\n\ns3 = boto3.session.Session(**session_kwargs).client(\"s3\")\n\nbucket = \"bucket\"\nkey = \"path/to/dump.json.gz\"\nstream = s3.get_object(Bucket=bucket, Key=key).get(\"Body\")\n\nwith gzip.open(stream, \"rb\") as zipped:\n    data = list(yield_as_json(zipped))\n```\n\nDoes anyone know what's happening, or how I could improve the code?", "author_fullname": "t2_329zuj1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "pyarrow S3FS: reading zipped files is slow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12eoc5a", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "245217ea-ac9d-11eb-a81a-0e03519a5d4b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680880088.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to try out &lt;code&gt;pyarrow&lt;/code&gt;&amp;#39;s &lt;code&gt;S3FileSystem&lt;/code&gt; by converting a &lt;code&gt;boto3&lt;/code&gt; snippet to read gzipped jsonline files (a dump from DynamoDB). Compared to the original, the &lt;code&gt;pyarrow&lt;/code&gt; one is terribly slow &lt;em&gt;well before data starts reading JSON into memory&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;The snippet core logic is this:&lt;/p&gt;\n\n&lt;p&gt;```python\nfrom pyarrow import fs\ns3 = fs.S3FileSystem()&lt;/p&gt;\n\n&lt;p&gt;from mymodule import yield_as_json&lt;/p&gt;\n\n&lt;p&gt;myfile = &amp;quot;bucket/path/to/dump.json.gz&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;with s3.open_input_stream(myfile) as stream:\n    data = list(yield_as_json(stream))\n```&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s inside &lt;code&gt;yield_as_json&lt;/code&gt; does not really matter - I believe it works, because the second snippet (below) works. My suspicion is that &lt;code&gt;pyarrow&lt;/code&gt; reads the whole data in memory, decompresses it and then starts iterating over lines.&lt;/p&gt;\n\n&lt;p&gt;By comparison, the &lt;code&gt;boto3&lt;/code&gt; snippet is:&lt;/p&gt;\n\n&lt;p&gt;```python\nimport boto3\nimport gzip&lt;/p&gt;\n\n&lt;p&gt;from mymodule import yield_as_json&lt;/p&gt;\n\n&lt;p&gt;s3 = boto3.session.Session(**session_kwargs).client(&amp;quot;s3&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;bucket = &amp;quot;bucket&amp;quot;\nkey = &amp;quot;path/to/dump.json.gz&amp;quot;\nstream = s3.get_object(Bucket=bucket, Key=key).get(&amp;quot;Body&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;with gzip.open(stream, &amp;quot;rb&amp;quot;) as zipped:\n    data = list(yield_as_json(zipped))\n```&lt;/p&gt;\n\n&lt;p&gt;Does anyone know what&amp;#39;s happening, or how I could improve the code?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Scientist", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12eoc5a", "is_robot_indexable": true, "report_reasons": null, "author": "BaggiPonte", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12eoc5a/pyarrow_s3fs_reading_zipped_files_is_slow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eoc5a/pyarrow_s3fs_reading_zipped_files_is_slow/", "subreddit_subscribers": 96604, "created_utc": 1680880088.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys,\n\nI have a heroku app, which runs an api server, and I wanted to run also an airflow server on the same heroku app, meaning that I want a dag to run daily.\n\nI have the following structure, the repo contains an airflow folder which is set as path for AIRFLOW\\_HOME. The folder contains a dags folder where i have the script.\n\nNow my question would be what commands should the Procfile contains?\n\nCause at the moment it contains: web: gunicorn -w 4 -k uvicorn.workers.UvicornWorker api.main:app &amp;&amp; cd airflow\\_folder &amp;&amp; airflow webserver -p 8080 &amp;&amp; airflow schedule.\n\nThe first command before &amp;&amp; is to start the api server, and the others are to run the airflow server. Not sure if that's the best, or if i should do it in a different way. The thing is on my heroku logs there's no error / complain, but the airflow dag doesn't run either and I am not sure what I am doing wrong and how I should make it work as expected.", "author_fullname": "t2_b4ypm8ew", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Integrate airflow in my github app running on heroku", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12eo3z0", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680879663.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys,&lt;/p&gt;\n\n&lt;p&gt;I have a heroku app, which runs an api server, and I wanted to run also an airflow server on the same heroku app, meaning that I want a dag to run daily.&lt;/p&gt;\n\n&lt;p&gt;I have the following structure, the repo contains an airflow folder which is set as path for AIRFLOW_HOME. The folder contains a dags folder where i have the script.&lt;/p&gt;\n\n&lt;p&gt;Now my question would be what commands should the Procfile contains?&lt;/p&gt;\n\n&lt;p&gt;Cause at the moment it contains: web: gunicorn -w 4 -k uvicorn.workers.UvicornWorker api.main:app &amp;amp;&amp;amp; cd airflow_folder &amp;amp;&amp;amp; airflow webserver -p 8080 &amp;amp;&amp;amp; airflow schedule.&lt;/p&gt;\n\n&lt;p&gt;The first command before &amp;amp;&amp;amp; is to start the api server, and the others are to run the airflow server. Not sure if that&amp;#39;s the best, or if i should do it in a different way. The thing is on my heroku logs there&amp;#39;s no error / complain, but the airflow dag doesn&amp;#39;t run either and I am not sure what I am doing wrong and how I should make it work as expected.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12eo3z0", "is_robot_indexable": true, "report_reasons": null, "author": "Koxinfster", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12eo3z0/integrate_airflow_in_my_github_app_running_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eo3z0/integrate_airflow_in_my_github_app_running_on/", "subreddit_subscribers": 96604, "created_utc": 1680879663.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We used to use traditional syntax in all our Airflow dags, however, some of the tasks would be much easier to write with the TaskFlow API but on the other hand that would mean mixing two different styles. \n\nWhat is your view on this? Would you go with easier path to develop (mixing both syntax) or cleaner (use exactly one flavor)?", "author_fullname": "t2_12fc4o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Combining TaskFlow API and Traditional syntax in Airflow", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12emkq6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680876642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We used to use traditional syntax in all our Airflow dags, however, some of the tasks would be much easier to write with the TaskFlow API but on the other hand that would mean mixing two different styles. &lt;/p&gt;\n\n&lt;p&gt;What is your view on this? Would you go with easier path to develop (mixing both syntax) or cleaner (use exactly one flavor)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12emkq6", "is_robot_indexable": true, "report_reasons": null, "author": "romanzdk", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12emkq6/combining_taskflow_api_and_traditional_syntax_in/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12emkq6/combining_taskflow_api_and_traditional_syntax_in/", "subreddit_subscribers": 96604, "created_utc": 1680876642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello  \nI am working at consulting company as BI developer (mainly PowerBI but also a Qliksense), however recently I have been roll off from the project due to project freeze and now siting on a bench. I've been told that I'll be having an interview next week Tue/Wed on data engineering role with AWS. My cloud \"stack\" is Azure - AZ-900, DP-900, PL-300. I used a little bit of Azure Data Studio and Synapse. Thats pretty much it. I know SQL to the extend I can freely make a view myself with some transformations or to answer some business question but never been \"deeper\" and I am afraid not that I'll not land in that position - it is guaranteed, but that I'll make fool of myself. I know core concepts of DE, but never really used the tools na AWS itself.   \n\n\nAny protips for me? What to prepare if we're talking about bare minimum, to not make a fool out of myself?", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "help me in not making fool of myself", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ekb6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680871879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;br/&gt;\nI am working at consulting company as BI developer (mainly PowerBI but also a Qliksense), however recently I have been roll off from the project due to project freeze and now siting on a bench. I&amp;#39;ve been told that I&amp;#39;ll be having an interview next week Tue/Wed on data engineering role with AWS. My cloud &amp;quot;stack&amp;quot; is Azure - AZ-900, DP-900, PL-300. I used a little bit of Azure Data Studio and Synapse. Thats pretty much it. I know SQL to the extend I can freely make a view myself with some transformations or to answer some business question but never been &amp;quot;deeper&amp;quot; and I am afraid not that I&amp;#39;ll not land in that position - it is guaranteed, but that I&amp;#39;ll make fool of myself. I know core concepts of DE, but never really used the tools na AWS itself.   &lt;/p&gt;\n\n&lt;p&gt;Any protips for me? What to prepare if we&amp;#39;re talking about bare minimum, to not make a fool out of myself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ekb6i", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ekb6i/help_me_in_not_making_fool_of_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ekb6i/help_me_in_not_making_fool_of_myself/", "subreddit_subscribers": 96604, "created_utc": 1680871879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone!\n\nI've been searching for a while an installer for SpectX (the log analytics tool). Since the company was bought by Dynatrace, it has become impossible to find it online.\n\nI was wondering if anyone has an installer, or even an installed version from which we can extract the specx.jar.\n\nI happen to have a license for it, but no way to use it.", "author_fullname": "t2_8q939c1mp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SpectX", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12eg82n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680862305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been searching for a while an installer for SpectX (the log analytics tool). Since the company was bought by Dynatrace, it has become impossible to find it online.&lt;/p&gt;\n\n&lt;p&gt;I was wondering if anyone has an installer, or even an installed version from which we can extract the specx.jar.&lt;/p&gt;\n\n&lt;p&gt;I happen to have a license for it, but no way to use it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12eg82n", "is_robot_indexable": true, "report_reasons": null, "author": "AdditionalDLL", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12eg82n/spectx/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eg82n/spectx/", "subreddit_subscribers": 96604, "created_utc": 1680862305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, a junior here and would appreciate your help. I came across this binary encoding  v.s. textual encoding concept in the book \"Designing Data-Intensive App\". It talks about how binary encoding can save more space than textual JSON encoding. My questions are: \n\n1. Since everything is stored as 0/1, wouldn't textual JSON encoding actually stores as binary as well? \n\n2. How could binary encoding actually saves space? Assuming we have this JSON {\"userName\": \"Martin\"}. \n\n* If we do binary encoding, we'd need to store (a) #of fields in this object, (b) field type and length, in this case, string as field type and length = 8. (c) the string userName itself in binary, (d) value type and length, in this case, string as value type and length = 6, and (e) the string Martin itselt in binary.  \n* However, if we do textual JSON encoding, won't we need to do (a)-(e) just like above too? So how would binary encoding actually save space?\n\n&amp;#x200B;\n\nThanks all for your help.", "author_fullname": "t2_szomhuik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Binary Encoding &amp; Textual Encoding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e65p3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680833367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, a junior here and would appreciate your help. I came across this binary encoding  v.s. textual encoding concept in the book &amp;quot;Designing Data-Intensive App&amp;quot;. It talks about how binary encoding can save more space than textual JSON encoding. My questions are: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Since everything is stored as 0/1, wouldn&amp;#39;t textual JSON encoding actually stores as binary as well? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How could binary encoding actually saves space? Assuming we have this JSON {&amp;quot;userName&amp;quot;: &amp;quot;Martin&amp;quot;}. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If we do binary encoding, we&amp;#39;d need to store (a) #of fields in this object, (b) field type and length, in this case, string as field type and length = 8. (c) the string userName itself in binary, (d) value type and length, in this case, string as value type and length = 6, and (e) the string Martin itselt in binary.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;However, if we do textual JSON encoding, won&amp;#39;t we need to do (a)-(e) just like above too? So how would binary encoding actually save space?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks all for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12e65p3", "is_robot_indexable": true, "report_reasons": null, "author": "TendMyOwnGarden", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e65p3/binary_encoding_textual_encoding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e65p3/binary_encoding_textual_encoding/", "subreddit_subscribers": 96604, "created_utc": 1680833367.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}