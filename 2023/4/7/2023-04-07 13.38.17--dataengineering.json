{"kind": "Listing", "data": {"after": "t3_12dopeu", "dist": 25, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been working as a junior engineer for 9 months now at a consultancy, and I have to say, it's been a real struggle. Up to now, I've worked on 3 projects and I've struggled on each one. I struggle with the expectation that my company has for me to deliver. I understand I need to deliver, but it feels like I'm stuck in a vicious circle.\n\nAll of the cloud stuff, sql, and pipelines are new to me. Each project has lasted 2 or 3 months and by the time I'm comfortable and started to deliver, the project is over. In my spare time, I do plenty of tutorials to get myself up to speed, but it seems like all the personal work I do is not relevant to the project work that I'm doing because it's s so specific. Majority of the time I'm depending on my fellow engineers to help me out. I can honestly say, that I've only completed a handful of tickets by myself and I feel ashamed of it. They expect me to deliver when I simply don't know. I honestly feel like a burden on every team that I'm on.\n\nToday I had a 1-1 with my manager and he said that people on my previous project complained about my lack of contribution and how they weren't able to bill for their work because they were helping me. This was a shock because none of my colleagues expressed this to me while I was there.\n\nMy manager decided to do a 'PIP' (Performance Improvement Plan) to get me to improve. I've told my manager that I need to be on a project that is 6 months to a year-long but he keeps saying there isn't one. I feel kinda sad that he's put me on an informal disciplinary thing. He kept on saying that it was nothing, but I know it will be used against me if I don't improve. \n\nI feel like I'm alone and don't know what to do. Am I just kidding myself that I'm an engineer?", "author_fullname": "t2_81mnb4wb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Shall I accept that I'm a useless engineer and look somewhere else or is this feeling part of the journey?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e0lti", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 54, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 54, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680820742.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working as a junior engineer for 9 months now at a consultancy, and I have to say, it&amp;#39;s been a real struggle. Up to now, I&amp;#39;ve worked on 3 projects and I&amp;#39;ve struggled on each one. I struggle with the expectation that my company has for me to deliver. I understand I need to deliver, but it feels like I&amp;#39;m stuck in a vicious circle.&lt;/p&gt;\n\n&lt;p&gt;All of the cloud stuff, sql, and pipelines are new to me. Each project has lasted 2 or 3 months and by the time I&amp;#39;m comfortable and started to deliver, the project is over. In my spare time, I do plenty of tutorials to get myself up to speed, but it seems like all the personal work I do is not relevant to the project work that I&amp;#39;m doing because it&amp;#39;s s so specific. Majority of the time I&amp;#39;m depending on my fellow engineers to help me out. I can honestly say, that I&amp;#39;ve only completed a handful of tickets by myself and I feel ashamed of it. They expect me to deliver when I simply don&amp;#39;t know. I honestly feel like a burden on every team that I&amp;#39;m on.&lt;/p&gt;\n\n&lt;p&gt;Today I had a 1-1 with my manager and he said that people on my previous project complained about my lack of contribution and how they weren&amp;#39;t able to bill for their work because they were helping me. This was a shock because none of my colleagues expressed this to me while I was there.&lt;/p&gt;\n\n&lt;p&gt;My manager decided to do a &amp;#39;PIP&amp;#39; (Performance Improvement Plan) to get me to improve. I&amp;#39;ve told my manager that I need to be on a project that is 6 months to a year-long but he keeps saying there isn&amp;#39;t one. I feel kinda sad that he&amp;#39;s put me on an informal disciplinary thing. He kept on saying that it was nothing, but I know it will be used against me if I don&amp;#39;t improve. &lt;/p&gt;\n\n&lt;p&gt;I feel like I&amp;#39;m alone and don&amp;#39;t know what to do. Am I just kidding myself that I&amp;#39;m an engineer?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12e0lti", "is_robot_indexable": true, "report_reasons": null, "author": "Taurusamazing92", "discussion_type": null, "num_comments": 36, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e0lti/shall_i_accept_that_im_a_useless_engineer_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e0lti/shall_i_accept_that_im_a_useless_engineer_and/", "subreddit_subscribers": 96563, "created_utc": 1680820742.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're already accustomed to hearing people say \"this is how YOU should use chatGPT\", but I rarely see people say \"This is how I use chat gpt.\"  \n\n\nIf you've got first hand experience using chat GPT to add legitimate value as a data engineer, I'd love to hear about it!", "author_fullname": "t2_nkrhcqia", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How has ChatGPT helped you in your DE job? First hand experience only, plz.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dwoe3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.7, "author_flair_background_color": null, "subreddit_type": "public", "ups": 30, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 30, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680812476.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re already accustomed to hearing people say &amp;quot;this is how YOU should use chatGPT&amp;quot;, but I rarely see people say &amp;quot;This is how I use chat gpt.&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;ve got first hand experience using chat GPT to add legitimate value as a data engineer, I&amp;#39;d love to hear about it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dwoe3", "is_robot_indexable": true, "report_reasons": null, "author": "JParkerRogers", "discussion_type": null, "num_comments": 89, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dwoe3/how_has_chatgpt_helped_you_in_your_de_job_first/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dwoe3/how_has_chatgpt_helped_you_in_your_de_job_first/", "subreddit_subscribers": 96563, "created_utc": 1680812476.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you are writing complex SQLs it is most likely the systems aren\u2019t designed well and you are working on a bad data model.", "author_fullname": "t2_ui4m14ke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Here is a controversial statement", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e8vxo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680839695.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you are writing complex SQLs it is most likely the systems aren\u2019t designed well and you are working on a bad data model.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12e8vxo", "is_robot_indexable": true, "report_reasons": null, "author": "Alarmed-Sock4915", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e8vxo/here_is_a_controversial_statement/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e8vxo/here_is_a_controversial_statement/", "subreddit_subscribers": 96563, "created_utc": 1680839695.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Trying to gauge the viability of running Python/Go/Dask data lake transformations (raw data in many files -&gt; cleaned data in many parquet files). \n\n- Spark might intro a lot of overhead and complexity\n- We don't want to buy into Databricks\n- We're running on K8s with a solid platform team for support\n- Right now, Python scripts + Fivetran drop raw data files into data lake\n- First goal is to transform these raw files into silver and gold parquet-based tables in the data lake\n- After gold, teams are allowed to load into a DWH, query directly with some BI tool, etc but that's not the focus right now.\n\nWe know our infra will change over the next couple of years as this data operation gets going so writing simple, low cost scripts in languages that we're handy with would be great.\n\nMy worry in this case is performance. Can these languages/frameworks work well when needing to transform ~50GBs of data spread across many (hourly partitioned probably) files and transform them into the silver and gold. \n\nAny experience with something like this? Thanks.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Any teams building data lakes without Spark? And specifically with vanilla Python/Go or with something like Dask?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dps2u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680798898.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to gauge the viability of running Python/Go/Dask data lake transformations (raw data in many files -&amp;gt; cleaned data in many parquet files). &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Spark might intro a lot of overhead and complexity&lt;/li&gt;\n&lt;li&gt;We don&amp;#39;t want to buy into Databricks&lt;/li&gt;\n&lt;li&gt;We&amp;#39;re running on K8s with a solid platform team for support&lt;/li&gt;\n&lt;li&gt;Right now, Python scripts + Fivetran drop raw data files into data lake&lt;/li&gt;\n&lt;li&gt;First goal is to transform these raw files into silver and gold parquet-based tables in the data lake&lt;/li&gt;\n&lt;li&gt;After gold, teams are allowed to load into a DWH, query directly with some BI tool, etc but that&amp;#39;s not the focus right now.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We know our infra will change over the next couple of years as this data operation gets going so writing simple, low cost scripts in languages that we&amp;#39;re handy with would be great.&lt;/p&gt;\n\n&lt;p&gt;My worry in this case is performance. Can these languages/frameworks work well when needing to transform ~50GBs of data spread across many (hourly partitioned probably) files and transform them into the silver and gold. &lt;/p&gt;\n\n&lt;p&gt;Any experience with something like this? Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dps2u", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 43, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dps2u/any_teams_building_data_lakes_without_spark_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dps2u/any_teams_building_data_lakes_without_spark_and/", "subreddit_subscribers": 96563, "created_utc": 1680798898.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI have seen a lot of posts regarding the use cases of Docker but not really any on how it actually works on a functional level. Many mention that it is great for dependency issues (one version of Python required for one process vs another) but how does Docker actually solve for this?", "author_fullname": "t2_jr18wyyz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Docker - Magic or Hype?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dteg3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680814420.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680806003.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I have seen a lot of posts regarding the use cases of Docker but not really any on how it actually works on a functional level. Many mention that it is great for dependency issues (one version of Python required for one process vs another) but how does Docker actually solve for this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dteg3", "is_robot_indexable": true, "report_reasons": null, "author": "Fintechie__", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dteg3/docker_magic_or_hype/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dteg3/docker_magic_or_hype/", "subreddit_subscribers": 96563, "created_utc": 1680806003.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I haven\u2019t done too many technical interviews for DE (&lt;20). Normally I get questions that i can do pretty easily. Things like: modeling, \u201chow would you build this pipeline?\u201d, basic SQL/Python, cloud services/data warehousing. \n\nToday I got put into a live coding interview unexpectedly, and was asked to get data from an API and do some transformations against it. The solution involved using requests library and doing some json loading/parsing. For the transformation it involved using the pivot function in python.\n\nI was completely caught off guard, as I haven\u2019t done the above in a while and my mind sort of blanked. \n\nIs this something that I should have known how to do easily?", "author_fullname": "t2_8x16rrzg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Would you be able to do this on the spot during an interview?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e589j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": "transparent", "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": "fbc7d3e6-ac9c-11eb-adda-0e0b12e4a59b", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680831292.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I haven\u2019t done too many technical interviews for DE (&amp;lt;20). Normally I get questions that i can do pretty easily. Things like: modeling, \u201chow would you build this pipeline?\u201d, basic SQL/Python, cloud services/data warehousing. &lt;/p&gt;\n\n&lt;p&gt;Today I got put into a live coding interview unexpectedly, and was asked to get data from an API and do some transformations against it. The solution involved using requests library and doing some json loading/parsing. For the transformation it involved using the pivot function in python.&lt;/p&gt;\n\n&lt;p&gt;I was completely caught off guard, as I haven\u2019t done the above in a while and my mind sort of blanked. &lt;/p&gt;\n\n&lt;p&gt;Is this something that I should have known how to do easily?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12e589j", "is_robot_indexable": true, "report_reasons": null, "author": "Justanotherguy2022", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12e589j/would_you_be_able_to_do_this_on_the_spot_during/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e589j/would_you_be_able_to_do_this_on_the_spot_during/", "subreddit_subscribers": 96563, "created_utc": 1680831292.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Since joining a Bay Area tech company, I have realized the grind to keep up with all the new tools in software engineering is alot. I am 35 and realizing it takes too much time commitment to keep up after work. Even as a data engineering manager, I am expected to know all the new skills and tools. What other roles align to a data engineering background? Product roles for data platforms? I like working with the business.", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career Change?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e6iqp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.83, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680834164.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since joining a Bay Area tech company, I have realized the grind to keep up with all the new tools in software engineering is alot. I am 35 and realizing it takes too much time commitment to keep up after work. Even as a data engineering manager, I am expected to know all the new skills and tools. What other roles align to a data engineering background? Product roles for data platforms? I like working with the business.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12e6iqp", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e6iqp/career_change/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e6iqp/career_change/", "subreddit_subscribers": 96563, "created_utc": 1680834164.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm just an average (perhaps below average) data engineer. I have read many posts in this subreddit daily (other sources also) and have seen many visions from you guys. I want to contribute something, but have nothing to share. So I share this article (I have just read that minutes ago):  \n[https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c](https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c)  \nAt first I was mind-blown, and thought of creating a repo built on top ChatGPT to extract, transform data. My dream collapsed right afterwards, after knowing the existence of AWS Aurora.[https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/](https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/)  \nHowever, i'm still get excited due to this. If the data is relatively simply structured, this would be extremely handy. Otherwise, ETL/ELT pipelines are still needed. Other pros and cons of zero-ETL are also mentioned in the above towardsdatascience article.  \nWhat are your thoughts about this?", "author_fullname": "t2_4exoz3kf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "The future of less ETL-data-pipeline building", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12eg4ty", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1680862065.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m just an average (perhaps below average) data engineer. I have read many posts in this subreddit daily (other sources also) and have seen many visions from you guys. I want to contribute something, but have nothing to share. So I share this article (I have just read that minutes ago):&lt;br/&gt;\n&lt;a href=\"https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c\"&gt;https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&lt;/a&gt;&lt;br/&gt;\nAt first I was mind-blown, and thought of creating a repo built on top ChatGPT to extract, transform data. My dream collapsed right afterwards, after knowing the existence of AWS Aurora.&lt;a href=\"https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/\"&gt;https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-aurora-zero-etl-integration-redshift/&lt;/a&gt;&lt;br/&gt;\nHowever, i&amp;#39;m still get excited due to this. If the data is relatively simply structured, this would be extremely handy. Otherwise, ETL/ELT pipelines are still needed. Other pros and cons of zero-ETL are also mentioned in the above towardsdatascience article.&lt;br/&gt;\nWhat are your thoughts about this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?auto=webp&amp;v=enabled&amp;s=ec51b72c1edf8ba4f8dc0eb8bbd0887c5a10bbdd", "width": 1200, "height": 543}, "resolutions": [{"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c1e8c9fd48223781443d8c46be48ab920179e2b7", "width": 108, "height": 48}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=382e0f4f2c49bf8cf0e3b3aa4d813e621e21cb1b", "width": 216, "height": 97}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8ddedb3f7dfc408418a63076bae0756ac336c924", "width": 320, "height": 144}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e8423f80288e2ce8684080fb040aba5768a2f39", "width": 640, "height": 289}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=db543297d3c099db0127a0b2f21eaec818bce132", "width": 960, "height": 434}, {"url": "https://external-preview.redd.it/xSB-VRAG8K0EqeEJx3raBuXgYpw50BJqz4xHJE_81c8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f3176ee720d2a0710d0cbc971054c316ad001141", "width": 1080, "height": 488}], "variants": {}, "id": "NUV7jTZprOTDG09SohaAm32t7mHuSpgMRI6ybLu37-E"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12eg4ty", "is_robot_indexable": true, "report_reasons": null, "author": "duohd", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12eg4ty/the_future_of_less_etldatapipeline_building/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12eg4ty/the_future_of_less_etldatapipeline_building/", "subreddit_subscribers": 96563, "created_utc": 1680862065.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In AWS Glue (PySpark) I'm reading a 29GB CSV file from S3 then repartitioning to 204 partitions and finally writing out to S3. The Spark log for the FileScanRDD stage shows that a single executor is processing the entire 29GB and spilling a huge amount of data. To me this seems like Spark is using a  single executor to first read the entire CSV. I was under the impression that a  splittable format such as CSV could be read in chunks by each executor rather than going into a single executor. Is the only way to avoid the single-executor read to break up the input CSV into smaller files before reading? Also, why are  the 101GB disk and 40GB memory spill larger than the input size of 29GB? I'm  generally just trying to understand how the data transfer is happening  with respect to executors/partitions.\n\n&amp;#x200B;\n\n[FileScan Executor Metrics](https://preview.redd.it/s8gseh4mjasa1.png?width=1871&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fdb7fe4668506e2ece55a36057df3cd0208960b0)\n\n&amp;#x200B;\n\n[FileScan DAG](https://preview.redd.it/nung0qcojasa1.png?width=565&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=70ed7073d4608c93117de8fb4767a3cb9f57fe2b)\n\n&amp;#x200B;\n\n    import sys\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext, SparkConf\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    import time\n    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n    \n    sc = SparkContext()\n    glueContext = GlueContext(sc)\n    spark = glueContext.spark_session\n    \n    \n    df = spark.read.format('csv').option('header',True)\\\n                                .option('multiline',True)\\\n                                .option(\"escape\", \"\\\"\")\\\n                                .option(\"delimiter\", \",\")\\\n                                .load(f's3://bucket/folder')\n    \n    df = df.repartition(numPartitions=204)\n    \n    \n    db = 'athena-db-name'\n    path = 's3://target_bucket/target_folder'\n    table = 'target_table_name'\n    folder = table.replace('_','-')\n    target_path = path + folder\n    \n    \n    df.write.saveAsTable(f\"`{db}`.{table}\", format='parquet', mode='overwrite', path=target_path)", "author_fullname": "t2_1evp2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark (AWS Glue) spill when reading large CSV file", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 20, "top_awarded_type": null, "hide_score": false, "media_metadata": {"nung0qcojasa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 132, "x": 108, "u": "https://preview.redd.it/nung0qcojasa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca92d05ca7ae22413fedb452ca6bdef04f34997d"}, {"y": 265, "x": 216, "u": "https://preview.redd.it/nung0qcojasa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=de956270a3400fa6e6283e50d500906975001db1"}, {"y": 393, "x": 320, "u": "https://preview.redd.it/nung0qcojasa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b8859cfb230ddf501ddf0044fd189fad47589abe"}], "s": {"y": 695, "x": 565, "u": "https://preview.redd.it/nung0qcojasa1.png?width=565&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=70ed7073d4608c93117de8fb4767a3cb9f57fe2b"}, "id": "nung0qcojasa1"}, "s8gseh4mjasa1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 16, "x": 108, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a7e753437a67bae601dd2e16fece2b648cc0f2ff"}, {"y": 32, "x": 216, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=bda3090b9168e845d89555e379289c18be255c63"}, {"y": 47, "x": 320, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dc98e1e06dea0ec99d3ec622e4217d074cf3959b"}, {"y": 95, "x": 640, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=0d70bfc4858eca2a3bf43c275b01c1762a6150d1"}, {"y": 142, "x": 960, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3bc6879429dfbb8293c22095bc630b1b7988daba"}, {"y": 160, "x": 1080, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=481f83db3e97408cfb64fcf36606cd5770a9fea7"}], "s": {"y": 278, "x": 1871, "u": "https://preview.redd.it/s8gseh4mjasa1.png?width=1871&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=fdb7fe4668506e2ece55a36057df3cd0208960b0"}, "id": "s8gseh4mjasa1"}}, "name": "t3_12dpot8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/ffkA9zEVfvOCOc37wATyi335c6BIwRvXsujeDYMRxL4.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680798718.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In AWS Glue (PySpark) I&amp;#39;m reading a 29GB CSV file from S3 then repartitioning to 204 partitions and finally writing out to S3. The Spark log for the FileScanRDD stage shows that a single executor is processing the entire 29GB and spilling a huge amount of data. To me this seems like Spark is using a  single executor to first read the entire CSV. I was under the impression that a  splittable format such as CSV could be read in chunks by each executor rather than going into a single executor. Is the only way to avoid the single-executor read to break up the input CSV into smaller files before reading? Also, why are  the 101GB disk and 40GB memory spill larger than the input size of 29GB? I&amp;#39;m  generally just trying to understand how the data transfer is happening  with respect to executors/partitions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s8gseh4mjasa1.png?width=1871&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=fdb7fe4668506e2ece55a36057df3cd0208960b0\"&gt;FileScan Executor Metrics&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nung0qcojasa1.png?width=565&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=70ed7073d4608c93117de8fb4767a3cb9f57fe2b\"&gt;FileScan DAG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext, SparkConf\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport time\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\n\n\ndf = spark.read.format(&amp;#39;csv&amp;#39;).option(&amp;#39;header&amp;#39;,True)\\\n                            .option(&amp;#39;multiline&amp;#39;,True)\\\n                            .option(&amp;quot;escape&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;)\\\n                            .option(&amp;quot;delimiter&amp;quot;, &amp;quot;,&amp;quot;)\\\n                            .load(f&amp;#39;s3://bucket/folder&amp;#39;)\n\ndf = df.repartition(numPartitions=204)\n\n\ndb = &amp;#39;athena-db-name&amp;#39;\npath = &amp;#39;s3://target_bucket/target_folder&amp;#39;\ntable = &amp;#39;target_table_name&amp;#39;\nfolder = table.replace(&amp;#39;_&amp;#39;,&amp;#39;-&amp;#39;)\ntarget_path = path + folder\n\n\ndf.write.saveAsTable(f&amp;quot;`{db}`.{table}&amp;quot;, format=&amp;#39;parquet&amp;#39;, mode=&amp;#39;overwrite&amp;#39;, path=target_path)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dpot8", "is_robot_indexable": true, "report_reasons": null, "author": "LaminatedMisanthropy", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dpot8/pyspark_aws_glue_spill_when_reading_large_csv_file/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dpot8/pyspark_aws_glue_spill_when_reading_large_csv_file/", "subreddit_subscribers": 96563, "created_utc": 1680798718.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_meq7wkla", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Snowflake vs Redshift: a comprehensive guide on choosing your cloud data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 73, "top_awarded_type": null, "hide_score": false, "name": "t3_12dkst1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": "transparent", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": "02917a1a-ac9d-11eb-beee-0ed0a94b470d", "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/E7S3Y4PGDYtRiG7Qx7vB_-NFu3PcyJ9ljzm69C6Ws3A.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680788599.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "airbyte.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://airbyte.com/blog/snowflake-vs-redshift", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?auto=webp&amp;v=enabled&amp;s=47a86384027e35d5146c9faa79795835d7a8d550", "width": 1800, "height": 946}, "resolutions": [{"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=789e3825d5a199333c23ac750d8dd304cc73a331", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=978c798f81ef9e752dee778def8ecb74e3ce938f", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f6e45f6c05585fce3fb85f409d71d7e1e156c645", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4e3f92c8c124d0017104970b7f52c32604059077", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2dcfe0591ca1ac1a3859ed799e9a0336c4bca90b", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/uPK2BpY6nPGO0-wl1JTUYBE81zabtWqEZJJC2o5GgQc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=026bcd859fd79edaf025c8df1554c501a2585eac", "width": 1080, "height": 567}], "variants": {}, "id": "FcKPoXUUPVO80Peja_IMgPPk998zQAeEIgojWPAIXeE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "Senior Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12dkst1", "is_robot_indexable": true, "report_reasons": null, "author": "thabarrera", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "dark", "permalink": "/r/dataengineering/comments/12dkst1/snowflake_vs_redshift_a_comprehensive_guide_on/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://airbyte.com/blog/snowflake-vs-redshift", "subreddit_subscribers": 96563, "created_utc": 1680788599.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The title says everything.Just find Apache Airflow fascinating but I cannot run it locally. Any idea where I can experiment it on the cloud? Almost every other post in this sub is on Airflow/Dagster.   \n\n\nFor example - We have replit to experiment on Python or other programming languages, so is there a similar infra for Airflow/Dagster?", "author_fullname": "t2_6hp3gp78", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where can I experiment with Apache Airflow without running it locally?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ecrzm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680850948.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The title says everything.Just find Apache Airflow fascinating but I cannot run it locally. Any idea where I can experiment it on the cloud? Almost every other post in this sub is on Airflow/Dagster.   &lt;/p&gt;\n\n&lt;p&gt;For example - We have replit to experiment on Python or other programming languages, so is there a similar infra for Airflow/Dagster?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ecrzm", "is_robot_indexable": true, "report_reasons": null, "author": "rohetoric", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ecrzm/where_can_i_experiment_with_apache_airflow/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ecrzm/where_can_i_experiment_with_apache_airflow/", "subreddit_subscribers": 96563, "created_utc": 1680850948.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering, \n\nI been tasked with setting up a new environment for my teams processing. We have quite a few restrictions which makes it hard to narrow in on a good plan for our use case.\n\nCurrently my team uses on-prem SAS that can no longer handle the load and the discussions with IT seem to indicate that we will never get a new server so we need to piggyback off an existing environment used by other teams, that is basically locked down (no internet) windows VMs (64GB 8 cores can maybe get bigger) on the cloud with databricks connected to them.\n\nthe environment limitations / processing details:\n\n* Must be python, dont have permission to use another language except maybe sql\n* Solution that is available on PYPI repository only and will be installing it offline with anaconda. \n* cant install external executables\n* We do mostly small batch processing jobs usually small &lt;10GB but occasionally some large 100-200GB batch processing. \n* We need to output as csvs.\n\nSo far I am weighing between the following options and am happy to hear other thoughts and opinions:\n\n* Spark, (can use local spark for small jobs and use databricks for any large jobs) \n* Pandas (I worry it will die with the larger datasets)\n* Polars (I havent used this before but heard it is a better replacement for pandas)\n* Dask (I assume it will eliminate the problems of pandas?)\n\nI would like to see something with pipe lining options, maybe a local dagster?\n\n&amp;#x200B;\n\nAnyway thanks for reading this far, I hope someone has some suggestions, right now I am thinking that I am fucked with all the limitations put on me.", "author_fullname": "t2_3x579bn9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Help with deciding data-eng stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e43ql", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680828737.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;, &lt;/p&gt;\n\n&lt;p&gt;I been tasked with setting up a new environment for my teams processing. We have quite a few restrictions which makes it hard to narrow in on a good plan for our use case.&lt;/p&gt;\n\n&lt;p&gt;Currently my team uses on-prem SAS that can no longer handle the load and the discussions with IT seem to indicate that we will never get a new server so we need to piggyback off an existing environment used by other teams, that is basically locked down (no internet) windows VMs (64GB 8 cores can maybe get bigger) on the cloud with databricks connected to them.&lt;/p&gt;\n\n&lt;p&gt;the environment limitations / processing details:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Must be python, dont have permission to use another language except maybe sql&lt;/li&gt;\n&lt;li&gt;Solution that is available on PYPI repository only and will be installing it offline with anaconda. &lt;/li&gt;\n&lt;li&gt;cant install external executables&lt;/li&gt;\n&lt;li&gt;We do mostly small batch processing jobs usually small &amp;lt;10GB but occasionally some large 100-200GB batch processing. &lt;/li&gt;\n&lt;li&gt;We need to output as csvs.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So far I am weighing between the following options and am happy to hear other thoughts and opinions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Spark, (can use local spark for small jobs and use databricks for any large jobs) &lt;/li&gt;\n&lt;li&gt;Pandas (I worry it will die with the larger datasets)&lt;/li&gt;\n&lt;li&gt;Polars (I havent used this before but heard it is a better replacement for pandas)&lt;/li&gt;\n&lt;li&gt;Dask (I assume it will eliminate the problems of pandas?)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would like to see something with pipe lining options, maybe a local dagster?&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Anyway thanks for reading this far, I hope someone has some suggestions, right now I am thinking that I am fucked with all the limitations put on me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12e43ql", "is_robot_indexable": true, "report_reasons": null, "author": "Mclovine_aus", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e43ql/help_with_deciding_dataeng_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e43ql/help_with_deciding_dataeng_stack/", "subreddit_subscribers": 96563, "created_utc": 1680828737.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_hf9ddayj", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data engineers processing data access requests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 97, "top_awarded_type": null, "hide_score": true, "name": "t3_12ekdv2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": true, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Meme", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/PwQFpnI6ohAOk8a2oHExRt5VPPNiNCh0lB3wSKRXgPM.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "image", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680872041.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "i.redd.it", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://i.redd.it/fvhpekkylgsa1.png", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://preview.redd.it/fvhpekkylgsa1.png?auto=webp&amp;v=enabled&amp;s=3d488123b490adb09b02b89bf0062491807eb3a7", "width": 1566, "height": 1096}, "resolutions": [{"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e723ffde2298b53be5040ca50cd3fa857d8cddd6", "width": 108, "height": 75}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=dad4b60ee7905425f32c05d87fc14462894faa9b", "width": 216, "height": 151}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4acec0efae2a9d17c5d774ba1e561901f29f158e", "width": 320, "height": 223}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2933de1a74c8bd7130551f9d2351a29612fe36d", "width": 640, "height": 447}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c972504627d1b0844729ccf54b5b5183bcc45c04", "width": 960, "height": 671}, {"url": "https://preview.redd.it/fvhpekkylgsa1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ba3a61733592d6c7ee91b71476a7fd4e9fb8c9aa", "width": 1080, "height": 755}], "variants": {}, "id": "DY6enitmFwWz8hoYEQaE_nDciegvG14Wmu3A3m0BEUU"}], "enabled": true}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "dc9742bc-a7de-11eb-a2e7-0e0348c8a4c1", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff66ac", "id": "12ekdv2", "is_robot_indexable": true, "report_reasons": null, "author": "Bart_Vee", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ekdv2/data_engineers_processing_data_access_requests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://i.redd.it/fvhpekkylgsa1.png", "subreddit_subscribers": 96563, "created_utc": 1680872041.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "title says it all", "author_fullname": "t2_vm5kbtxr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is dbt + athena/presto a viable stack? how do you orchestrate creation and maintenance of your athena/presto tables?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ee6bw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680855655.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;title says it all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ee6bw", "is_robot_indexable": true, "report_reasons": null, "author": "srevolve", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ee6bw/is_dbt_athenapresto_a_viable_stack_how_do_you/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ee6bw/is_dbt_athenapresto_a_viable_stack_how_do_you/", "subreddit_subscribers": 96563, "created_utc": 1680855655.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all,\n\nI am working on designing the architecture of a web app that is going to ask a bunch of input parameters from the user and perform joins, filters and aggregations on BigQuery.\n\nMy first idea was to have a flask app for the web part. Request comes in from the user, flask runs the function that pulls data from BQ and we process the data locally.\n\nHowever, this does not seem to be very performant. I would like to avoid having to read/write large tables into BigQuery so I am looking for a SQL-based solution. \n\nI am already using dbt for other projects but I am wondering if it really suits this use-case. \n\nI did stumble upon dbt server that would be able to run dbt operations in response to API Requests, but don't know how it would scale.\n\nAny thoughts ?", "author_fullname": "t2_kqzh0kz", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data processing and web app", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dl8qj", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680789584.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I am working on designing the architecture of a web app that is going to ask a bunch of input parameters from the user and perform joins, filters and aggregations on BigQuery.&lt;/p&gt;\n\n&lt;p&gt;My first idea was to have a flask app for the web part. Request comes in from the user, flask runs the function that pulls data from BQ and we process the data locally.&lt;/p&gt;\n\n&lt;p&gt;However, this does not seem to be very performant. I would like to avoid having to read/write large tables into BigQuery so I am looking for a SQL-based solution. &lt;/p&gt;\n\n&lt;p&gt;I am already using dbt for other projects but I am wondering if it really suits this use-case. &lt;/p&gt;\n\n&lt;p&gt;I did stumble upon dbt server that would be able to run dbt operations in response to API Requests, but don&amp;#39;t know how it would scale.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dl8qj", "is_robot_indexable": true, "report_reasons": null, "author": "JamieA28", "discussion_type": null, "num_comments": 6, "send_replies": false, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dl8qj/data_processing_and_web_app/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dl8qj/data_processing_and_web_app/", "subreddit_subscribers": 96563, "created_utc": 1680789584.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello  \nI am working at consulting company as BI developer (mainly PowerBI but also a Qliksense), however recently I have been roll off from the project due to project freeze and now siting on a bench. I've been told that I'll be having an interview next week Tue/Wed on data engineering role with AWS. My cloud \"stack\" is Azure - AZ-900, DP-900, PL-300. I used a little bit of Azure Data Studio and Synapse. Thats pretty much it. I know SQL to the extend I can freely make a view myself with some transformations or to answer some business question but never been \"deeper\" and I am afraid not that I'll not land in that position - it is guaranteed, but that I'll make fool of myself. I know core concepts of DE, but never really used the tools na AWS itself.   \n\n\nAny protips for me? What to prepare if we're talking about bare minimum, to not make a fool out of myself?", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "help me in not making fool of myself", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12ekb6i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680871879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello&lt;br/&gt;\nI am working at consulting company as BI developer (mainly PowerBI but also a Qliksense), however recently I have been roll off from the project due to project freeze and now siting on a bench. I&amp;#39;ve been told that I&amp;#39;ll be having an interview next week Tue/Wed on data engineering role with AWS. My cloud &amp;quot;stack&amp;quot; is Azure - AZ-900, DP-900, PL-300. I used a little bit of Azure Data Studio and Synapse. Thats pretty much it. I know SQL to the extend I can freely make a view myself with some transformations or to answer some business question but never been &amp;quot;deeper&amp;quot; and I am afraid not that I&amp;#39;ll not land in that position - it is guaranteed, but that I&amp;#39;ll make fool of myself. I know core concepts of DE, but never really used the tools na AWS itself.   &lt;/p&gt;\n\n&lt;p&gt;Any protips for me? What to prepare if we&amp;#39;re talking about bare minimum, to not make a fool out of myself?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ekb6i", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ekb6i/help_me_in_not_making_fool_of_myself/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ekb6i/help_me_in_not_making_fool_of_myself/", "subreddit_subscribers": 96563, "created_utc": 1680871879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_1jkhpl2f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to run dynamic Github Action workflows", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 87, "top_awarded_type": null, "hide_score": true, "name": "t3_12ejuno", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://a.thumbs.redditmedia.com/FZ-EQeupBO47ozfZ4Yhsp-mzoSYWcEYoFigcz_6ap38.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680870879.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/p/35692957ef94", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?auto=webp&amp;v=enabled&amp;s=eef5caba0c98e8021ccceef2330b82bb0c4606f5", "width": 1200, "height": 750}, "resolutions": [{"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a850d3475130a02050eaace31f251748b905d792", "width": 108, "height": 67}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adef17c44871e980d9b4e4473cdd9d8ef5f4a484", "width": 216, "height": 135}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b74f8c19297e4c723220debde72294891846b15b", "width": 320, "height": 200}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fd5f85dd858cd8d54a191d6499ade97cfa1dac48", "width": 640, "height": 400}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=242170b34d9b9b44c47ac539d4498063edcfd29e", "width": 960, "height": 600}, {"url": "https://external-preview.redd.it/vGEmG2mmaakKcxnXDQY3ZlcJrqrVPcNl_RptOf4OtC0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=770b2e3cc0ccc9d1a5c21f271a323605672be27d", "width": 1080, "height": 675}], "variants": {}, "id": "corEyKiyBN7xmaYdlI-4JDM9ATZVUIIp5xW9IRgtJB8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ejuno", "is_robot_indexable": true, "report_reasons": null, "author": "Luxi36", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ejuno/how_to_run_dynamic_github_action_workflows/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/p/35692957ef94", "subreddit_subscribers": 96563, "created_utc": 1680870879.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have to tokenize some sensitive columns in my data warehouse. However, the tokenization needs to be format preserving so that business users can still carry our analysis without any issues. Are there any standard format preserving tokenization algorithm that I can use out of box or should I write my own custom tokenization algorithm?", "author_fullname": "t2_virernyk", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Are there any standard format preserving tokenization algorithm that we can use out of box?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dnhpn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680794354.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have to tokenize some sensitive columns in my data warehouse. However, the tokenization needs to be format preserving so that business users can still carry our analysis without any issues. Are there any standard format preserving tokenization algorithm that I can use out of box or should I write my own custom tokenization algorithm?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dnhpn", "is_robot_indexable": true, "report_reasons": null, "author": "Hitoxi", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dnhpn/are_there_any_standard_format_preserving/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dnhpn/are_there_any_standard_format_preserving/", "subreddit_subscribers": 96563, "created_utc": 1680794354.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category. The category is not distinct thus other row may refer to any of the 3 categories.\n\ne.g. Dataset A\n\n&amp;#x200B;\n\n|Date|Value|Category|\n|:-|:-|:-|\n|1/1/2010|1.11111|Alpha|\n|2/1/2010|2.11111|Beta|\n|3/1/2010|2.00009|Alpha|\n|4/1/2010|0.00000|Charlie|\n\nBut the 10 datasets have different volume of data. E.g.  dataset A may have 10K rows, dataset B around 100K, Dataset C 1 million, etc.\n\nI couldn't process all the data as its too large.\n\nWhat would be the best way to sample each dataset? I'd like the sample containing a fair representative of the 3 categories.", "author_fullname": "t2_3z6gqvrh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to represent large categorical data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12egjxl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680863159.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category. The category is not distinct thus other row may refer to any of the 3 categories.&lt;/p&gt;\n\n&lt;p&gt;e.g. Dataset A&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Date&lt;/th&gt;\n&lt;th align=\"left\"&gt;Value&lt;/th&gt;\n&lt;th align=\"left\"&gt;Category&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;1/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;1.11111&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alpha&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;2/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.11111&lt;/td&gt;\n&lt;td align=\"left\"&gt;Beta&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;3/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;2.00009&lt;/td&gt;\n&lt;td align=\"left\"&gt;Alpha&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;4/1/2010&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.00000&lt;/td&gt;\n&lt;td align=\"left\"&gt;Charlie&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;But the 10 datasets have different volume of data. E.g.  dataset A may have 10K rows, dataset B around 100K, Dataset C 1 million, etc.&lt;/p&gt;\n\n&lt;p&gt;I couldn&amp;#39;t process all the data as its too large.&lt;/p&gt;\n\n&lt;p&gt;What would be the best way to sample each dataset? I&amp;#39;d like the sample containing a fair representative of the 3 categories.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12egjxl", "is_robot_indexable": true, "report_reasons": null, "author": "runnersgo", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12egjxl/how_to_represent_large_categorical_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12egjxl/how_to_represent_large_categorical_data/", "subreddit_subscribers": 96563, "created_utc": 1680863159.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone, a junior here and would appreciate your help. I came across this binary encoding  v.s. textual encoding concept in the book \"Designing Data-Intensive App\". It talks about how binary encoding can save more space than textual JSON encoding. My questions are: \n\n1. Since everything is stored as 0/1, wouldn't textual JSON encoding actually stores as binary as well? \n\n2. How could binary encoding actually saves space? Assuming we have this JSON {\"userName\": \"Martin\"}. \n\n* If we do binary encoding, we'd need to store (a) #of fields in this object, (b) field type and length, in this case, string as field type and length = 8. (c) the string userName itself in binary, (d) value type and length, in this case, string as value type and length = 6, and (e) the string Martin itselt in binary.  \n* However, if we do textual JSON encoding, won't we need to do (a)-(e) just like above too? So how would binary encoding actually save space?\n\n&amp;#x200B;\n\nThanks all for your help.", "author_fullname": "t2_szomhuik", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Binary Encoding &amp; Textual Encoding", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e65p3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680833367.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, a junior here and would appreciate your help. I came across this binary encoding  v.s. textual encoding concept in the book &amp;quot;Designing Data-Intensive App&amp;quot;. It talks about how binary encoding can save more space than textual JSON encoding. My questions are: &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Since everything is stored as 0/1, wouldn&amp;#39;t textual JSON encoding actually stores as binary as well? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How could binary encoding actually saves space? Assuming we have this JSON {&amp;quot;userName&amp;quot;: &amp;quot;Martin&amp;quot;}. &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If we do binary encoding, we&amp;#39;d need to store (a) #of fields in this object, (b) field type and length, in this case, string as field type and length = 8. (c) the string userName itself in binary, (d) value type and length, in this case, string as value type and length = 6, and (e) the string Martin itselt in binary.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;However, if we do textual JSON encoding, won&amp;#39;t we need to do (a)-(e) just like above too? So how would binary encoding actually save space?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks all for your help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12e65p3", "is_robot_indexable": true, "report_reasons": null, "author": "TendMyOwnGarden", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e65p3/binary_encoding_textual_encoding/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e65p3/binary_encoding_textual_encoding/", "subreddit_subscribers": 96563, "created_utc": 1680833367.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Is there anyway to set file sizes on write to 128mb without using coalesce() or repartition()?\n\nBtw, I\u2019m also interested in answers that use glue syntax aswell as pyspark.", "author_fullname": "t2_5fmit0v9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Spark, Glue and Small/Big files", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dot4u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680796980.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there anyway to set file sizes on write to 128mb without using coalesce() or repartition()?&lt;/p&gt;\n\n&lt;p&gt;Btw, I\u2019m also interested in answers that use glue syntax aswell as pyspark.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dot4u", "is_robot_indexable": true, "report_reasons": null, "author": "gabbom_XCII", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dot4u/spark_glue_and_smallbig_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dot4u/spark_glue_and_smallbig_files/", "subreddit_subscribers": 96563, "created_utc": 1680796980.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey all, colorfulskull here. I'm a 3y swe, now moving into a de path in a mid-size startup. So, I've been working with several data scientists and analysts who love using Jupyter notebooks. I mean, I get it \u2013 they're great for prototyping, and it's so fast and easy to get results. But as someone who prefers working in a proper IDE and dev environment, I'm finding it hard when we move towards production. It's like going from smooth sailing to navigating a minefield!\n\nSo, I'm reaching out to the r/dataengineering ecosystem for guidance. What are some best practices you've found for working with notebooks, especially when moving towards production? Someone recommended me projects like nbdev, but not sure if that is the right path the follow. How do you strike a balance between the flexibility of notebooks and the structure of a traditional development environment?  Want to make sure I'm not taking the wrong steps, what are some blogs/individuals/newsletters that I should follow?\n\nI dumped all the questions I have in one post, hope it's not a bother. I'd love to hear your thoughts and experiences \ud83d\ude04\ud83d\ude80\ud83d\udcda", "author_fullname": "t2_43r4n5ss", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "tough time with notebooks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dn7ej", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680793767.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all, colorfulskull here. I&amp;#39;m a 3y swe, now moving into a de path in a mid-size startup. So, I&amp;#39;ve been working with several data scientists and analysts who love using Jupyter notebooks. I mean, I get it \u2013 they&amp;#39;re great for prototyping, and it&amp;#39;s so fast and easy to get results. But as someone who prefers working in a proper IDE and dev environment, I&amp;#39;m finding it hard when we move towards production. It&amp;#39;s like going from smooth sailing to navigating a minefield!&lt;/p&gt;\n\n&lt;p&gt;So, I&amp;#39;m reaching out to the &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt; ecosystem for guidance. What are some best practices you&amp;#39;ve found for working with notebooks, especially when moving towards production? Someone recommended me projects like nbdev, but not sure if that is the right path the follow. How do you strike a balance between the flexibility of notebooks and the structure of a traditional development environment?  Want to make sure I&amp;#39;m not taking the wrong steps, what are some blogs/individuals/newsletters that I should follow?&lt;/p&gt;\n\n&lt;p&gt;I dumped all the questions I have in one post, hope it&amp;#39;s not a bother. I&amp;#39;d love to hear your thoughts and experiences \ud83d\ude04\ud83d\ude80\ud83d\udcda&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12dn7ej", "is_robot_indexable": true, "report_reasons": null, "author": "colorfulskull", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dn7ej/tough_time_with_notebooks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dn7ej/tough_time_with_notebooks/", "subreddit_subscribers": 96563, "created_utc": 1680793767.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_ay1q1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Arrow String improvements in Pandas/Dask DataFrames", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 86, "top_awarded_type": null, "hide_score": false, "name": "t3_12djy0h", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/6uwjML35z-FxsE_pRboam-GDwq291V8TrPtVWQof1WA.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1680786747.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?auto=webp&amp;v=enabled&amp;s=6859023178db536f94b55bb63382a7a09109032b", "width": 792, "height": 490}, "resolutions": [{"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9618b07d38c42c9b44e92644148b22822523cd9e", "width": 108, "height": 66}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3e340c81948a4fba6552f26b728af8711788bc35", "width": 216, "height": 133}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ac1c66e5bf7be22194afbe2e088a490ce2f46dd", "width": 320, "height": 197}, {"url": "https://external-preview.redd.it/Ze7rRKl1l71HThxnJOHJpIfrgdSKrEysX-TeVWvy3aU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d4cd149bae3ae18eb7da5790f10a0c5f43a190ed", "width": 640, "height": 395}], "variants": {}, "id": "V5FocKkZS3WgTL0zjz6iujrafykJdPf8Om_OTahpOug"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "12djy0h", "is_robot_indexable": true, "report_reasons": null, "author": "mrocklin", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12djy0h/arrow_string_improvements_in_pandasdask_dataframes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/coiled-hq/pyarrow-strings-in-dask-dataframes-55a0c4871586", "subreddit_subscribers": 96563, "created_utc": 1680786747.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "If you consider yourself to be on a \u2018high code\u2019 data engineering team what is your tech stack like?", "author_fullname": "t2_1n3qfa0v", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "High Code DE Teams Tech Stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12e9jat", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680841256.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you consider yourself to be on a \u2018high code\u2019 data engineering team what is your tech stack like?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12e9jat", "is_robot_indexable": true, "report_reasons": null, "author": "Culpgrant21", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12e9jat/high_code_de_teams_tech_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12e9jat/high_code_de_teams_tech_stack/", "subreddit_subscribers": 96563, "created_utc": 1680841256.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi,\n\nI am trying to find the optimal program setup for machine learning training and forecasting large amount of financial time series data on a single computer.\n\nComputer: amd 5900x, 128gb ram, 4tb nvme, rtx 3070\n\nData :  Data size is over 1tb  and have individual daily files that needs to be read. over 500 millions of rows of data.\n\nWhat would you use?\n\npyspark, polars, any other alternatives?\n\nSpeed is important, can't load data to memory since it is more than 1tb.\n\nI am not sure about SAS or Alteryx.\n\nThank you.", "author_fullname": "t2_400w8tx5", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Optimal setup for big data training on single computer/node.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12dopeu", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680796798.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am trying to find the optimal program setup for machine learning training and forecasting large amount of financial time series data on a single computer.&lt;/p&gt;\n\n&lt;p&gt;Computer: amd 5900x, 128gb ram, 4tb nvme, rtx 3070&lt;/p&gt;\n\n&lt;p&gt;Data :  Data size is over 1tb  and have individual daily files that needs to be read. over 500 millions of rows of data.&lt;/p&gt;\n\n&lt;p&gt;What would you use?&lt;/p&gt;\n\n&lt;p&gt;pyspark, polars, any other alternatives?&lt;/p&gt;\n\n&lt;p&gt;Speed is important, can&amp;#39;t load data to memory since it is more than 1tb.&lt;/p&gt;\n\n&lt;p&gt;I am not sure about SAS or Alteryx.&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12dopeu", "is_robot_indexable": true, "report_reasons": null, "author": "ozioh19", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12dopeu/optimal_setup_for_big_data_training_on_single/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12dopeu/optimal_setup_for_big_data_training_on_single/", "subreddit_subscribers": 96563, "created_utc": 1680796798.0, "num_crossposts": 1, "media": null, "is_video": false}}], "before": null}}