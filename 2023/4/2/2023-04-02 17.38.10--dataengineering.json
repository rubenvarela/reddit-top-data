{"kind": "Listing", "data": {"after": null, "dist": 9, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hey everyone, I have my final interview for a company I\u2019m in a loop for and it\u2019s a PySpark coding interview.  I\u2019ve never used PySpark before and I let the director know that and he said it\u2019s fine.  It\u2019s a 2 part interview (one part take home 2nd part is this week on zoom) for the take home part I\u2019ve been asked to join a few .csv files together in a Jupyter notebook with pyspark, which wasn\u2019t too bad with the help of google, and I achieved everything they asked for in terms of formatting etc.  the instructions say that the 2nd part will be related to my final table I made in the take home part.  I\u2019m curious if anyone has any insight on what I might expect this week in my 2nd part. I\u2019m familiar with pandas but the instructions specifically said to use Pyspark.  I would go through a PySpark book but I\u2019m limited in time as the interview is so soon.  Any suggestions on what I could cram to study would be really appreciated.", "author_fullname": "t2_dqlps", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "PySpark Interview Questions", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1293ctg", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.95, "author_flair_background_color": null, "subreddit_type": "public", "ups": 37, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 37, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680406921.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680393289.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I have my final interview for a company I\u2019m in a loop for and it\u2019s a PySpark coding interview.  I\u2019ve never used PySpark before and I let the director know that and he said it\u2019s fine.  It\u2019s a 2 part interview (one part take home 2nd part is this week on zoom) for the take home part I\u2019ve been asked to join a few .csv files together in a Jupyter notebook with pyspark, which wasn\u2019t too bad with the help of google, and I achieved everything they asked for in terms of formatting etc.  the instructions say that the 2nd part will be related to my final table I made in the take home part.  I\u2019m curious if anyone has any insight on what I might expect this week in my 2nd part. I\u2019m familiar with pandas but the instructions specifically said to use Pyspark.  I would go through a PySpark book but I\u2019m limited in time as the interview is so soon.  Any suggestions on what I could cram to study would be really appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "1293ctg", "is_robot_indexable": true, "report_reasons": null, "author": "dynamex1097", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1293ctg/pyspark_interview_questions/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1293ctg/pyspark_interview_questions/", "subreddit_subscribers": 95442, "created_utc": 1680393289.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How you will design the cdc mechanism for the schema which is very dynamic in nature. For instance, you have dynamodb where many different keys are being added and removed often. Now if you have to design the pipeline, how will you:\n\n1. capture/manage schema change \n2. How you will store it. What will be the schema\n3. Later down the stream how you will query it as we cannot have definite columns due to schema changing? \n\nThanks in advance", "author_fullname": "t2_7r5xenrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CDC with dynamic schema", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12932fn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680393150.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680392614.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How you will design the cdc mechanism for the schema which is very dynamic in nature. For instance, you have dynamodb where many different keys are being added and removed often. Now if you have to design the pipeline, how will you:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;capture/manage schema change &lt;/li&gt;\n&lt;li&gt;How you will store it. What will be the schema&lt;/li&gt;\n&lt;li&gt;Later down the stream how you will query it as we cannot have definite columns due to schema changing? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12932fn", "is_robot_indexable": true, "report_reasons": null, "author": "Heavy_End_2971", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12932fn/cdc_with_dynamic_schema/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12932fn/cdc_with_dynamic_schema/", "subreddit_subscribers": 95442, "created_utc": 1680392614.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nI developed a python package to build ETL flows/dags. Each flow is defined as class. Its good for visualizing and running your flows and is notebook friendly.\n\n    # example.py\n    from flowrunner import BaseFlow, step, start, end\n    \n    class ExampleFlow(BaseFlow):\n        @start\n        @step(next=['method2', 'method3'])\n        def method1(self):\n            self.a = 1\n    \n        @step(next=['method4'])\n        def method2(self):\n            self.a += 1\n    \n        @step(next=['method4'])\n        def method3(self):\n            self.a += 2\n    \n        @end\n        @step\n        def method4(self):\n            self.a += 3\n            print(\"output of flow is:\", self.a)\n\nRunning the following display command method gives this output\n\n    ExampleFlow().display()\n\nhttps://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=051b993d2f97627fded2d5bb60ad143be662a540\n\n&amp;#x200B;\n\nRepo link: [https://github.com/prithvijitguha/flowrunner](https://github.com/prithvijitguha/flowrunner)\n\nPyPI link: [https://pypi.org/project/flowrunner/](https://pypi.org/project/flowrunner/)\n\nDocumentation link: [https://flowrunner.readthedocs.io/en/latest/](https://flowrunner.readthedocs.io/en/latest/)\n\nLet me know what you think!\n\nFeedback is welcome :)", "author_fullname": "t2_7u46udxa", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python Package to build ETL flows/dags", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "media_metadata": {"bgfx0yu1bgra1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 97, "x": 108, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=710a4eb6b0556220d6d2ee356f3e4e0a010187f4"}, {"y": 195, "x": 216, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=91f11849aa746c67ff61c5632b4855e1a408ff05"}, {"y": 289, "x": 320, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=20897286cb7faad832c2a442c1ab6d079b1247d6"}], "s": {"y": 378, "x": 418, "u": "https://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=051b993d2f97627fded2d5bb60ad143be662a540"}, "id": "bgfx0yu1bgra1"}}, "name": "t3_129f92i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Open Source", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/x5YGC3CyaESrvCzhSudgubSeF1q0qVZQGGI59D1J_1g.jpg", "edited": 1680432501.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1680424992.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I developed a python package to build ETL flows/dags. Each flow is defined as class. Its good for visualizing and running your flows and is notebook friendly.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# example.py\nfrom flowrunner import BaseFlow, step, start, end\n\nclass ExampleFlow(BaseFlow):\n    @start\n    @step(next=[&amp;#39;method2&amp;#39;, &amp;#39;method3&amp;#39;])\n    def method1(self):\n        self.a = 1\n\n    @step(next=[&amp;#39;method4&amp;#39;])\n    def method2(self):\n        self.a += 1\n\n    @step(next=[&amp;#39;method4&amp;#39;])\n    def method3(self):\n        self.a += 2\n\n    @end\n    @step\n    def method4(self):\n        self.a += 3\n        print(&amp;quot;output of flow is:&amp;quot;, self.a)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Running the following display command method gives this output&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ExampleFlow().display()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=051b993d2f97627fded2d5bb60ad143be662a540\"&gt;https://preview.redd.it/bgfx0yu1bgra1.png?width=418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=051b993d2f97627fded2d5bb60ad143be662a540&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Repo link: &lt;a href=\"https://github.com/prithvijitguha/flowrunner\"&gt;https://github.com/prithvijitguha/flowrunner&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;PyPI link: &lt;a href=\"https://pypi.org/project/flowrunner/\"&gt;https://pypi.org/project/flowrunner/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Documentation link: &lt;a href=\"https://flowrunner.readthedocs.io/en/latest/\"&gt;https://flowrunner.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know what you think!&lt;/p&gt;\n\n&lt;p&gt;Feedback is welcome :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?auto=webp&amp;v=enabled&amp;s=7acd1b010da49901af2ff523e0c4c4b74734a776", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d567a311847d745ac71d4672b5dbffa8f4ff9e70", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5a4e0d1259d13b7930932de40e3af8d141f7827c", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=35acbea26dca43d4e12b054cf4a2fcd1444ac1a3", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aeba398e72ae3196a8c56622b201503608d8a208", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f70d407971a59a3400ec9f306fd15bfb01535639", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/rFJe9wc36youBs8KFwafOm-4SAhRu_jNZu8oyRtZtxM.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6ddace866d62d51118274d2d0e1f711a9a94de10", "width": 1080, "height": 540}], "variants": {}, "id": "ILxhIEm5fbc5DVSA_rDAPFSK-cgoksPDe143i61rHoA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "3957ca64-3440-11ed-8329-2aa6ad243a59", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#005ba1", "id": "129f92i", "is_robot_indexable": true, "report_reasons": null, "author": "Revolutionary-Bat176", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129f92i/python_package_to_build_etl_flowsdags/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129f92i/python_package_to_build_etl_flowsdags/", "subreddit_subscribers": 95442, "created_utc": 1680424992.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've a 6 + 9 month experience in internship &amp; FTE in a popular heath care company which mostly oriented around analytics development through reports for clients. Is it worthy to get a GCP certification to advance on the career or would the experience alone can make a difference in interviews?", "author_fullname": "t2_76srr1mpd", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Professional Data Engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128wol5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.74, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680378558.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve a 6 + 9 month experience in internship &amp;amp; FTE in a popular heath care company which mostly oriented around analytics development through reports for clients. Is it worthy to get a GCP certification to advance on the career or would the experience alone can make a difference in interviews?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "128wol5", "is_robot_indexable": true, "report_reasons": null, "author": "pr6g_head", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/128wol5/professional_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/128wol5/professional_data_engineer/", "subreddit_subscribers": 95442, "created_utc": 1680378558.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Sorry to sound amateur but has anyone created an event driven data pipeline? From what I've used in Airflow, it's orchestrating it everyday at a set given time than when a file hits a S3 bucket for example. \n\nIs there any way to create a data pipeline without having an EC2 permanently on when listening to events?", "author_fullname": "t2_wqszb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating an event driven data pipeline", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_128vpba", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680376397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry to sound amateur but has anyone created an event driven data pipeline? From what I&amp;#39;ve used in Airflow, it&amp;#39;s orchestrating it everyday at a set given time than when a file hits a S3 bucket for example. &lt;/p&gt;\n\n&lt;p&gt;Is there any way to create a data pipeline without having an EC2 permanently on when listening to events?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "128vpba", "is_robot_indexable": true, "report_reasons": null, "author": "miridian19", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/128vpba/creating_an_event_driven_data_pipeline/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/128vpba/creating_an_event_driven_data_pipeline/", "subreddit_subscribers": 95442, "created_utc": 1680376397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nFor nearly three years, a colleague and I have been serving as data engineers on a contractual basis(we are from a south asian country) for a US healthcare system. Our workload has been extensive, including the use of SSIS, Tableau, MDS, and PowerShell from the beginning. Over time, we also took on backend development responsibilities for specific use cases. Currently, we are exclusively handling the pipeline migration from SSIS to Prefect, Spark, and DBT because we are the only individuals who are familiar with the \"modern stack\" (let me make it very clear we are in no way experts but we have been learning and applying a lot of stuff on the fly)\n\nIn summary, we have been working tirelessly for a relatively low salary of approximately $700 per month. Do you believe that we could apply directly to the healthcare system and be hired? Are non-US citizens allowed to work with healthcare data and do you think it would be a good idea if I talk with my manager (US) about it?", "author_fullname": "t2_vgi0we9u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Can Data Engineers with Experience in a US Healthcare System Secure Direct Employment?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_1298yz5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.78, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680407145.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;For nearly three years, a colleague and I have been serving as data engineers on a contractual basis(we are from a south asian country) for a US healthcare system. Our workload has been extensive, including the use of SSIS, Tableau, MDS, and PowerShell from the beginning. Over time, we also took on backend development responsibilities for specific use cases. Currently, we are exclusively handling the pipeline migration from SSIS to Prefect, Spark, and DBT because we are the only individuals who are familiar with the &amp;quot;modern stack&amp;quot; (let me make it very clear we are in no way experts but we have been learning and applying a lot of stuff on the fly)&lt;/p&gt;\n\n&lt;p&gt;In summary, we have been working tirelessly for a relatively low salary of approximately $700 per month. Do you believe that we could apply directly to the healthcare system and be hired? Are non-US citizens allowed to work with healthcare data and do you think it would be a good idea if I talk with my manager (US) about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "1298yz5", "is_robot_indexable": true, "report_reasons": null, "author": "cosmic_lurker", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/1298yz5/can_data_engineers_with_experience_in_a_us/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/1298yz5/can_data_engineers_with_experience_in_a_us/", "subreddit_subscribers": 95442, "created_utc": 1680407145.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "The current stack i am working on is mainly databricks.\nRecently i got two interesting setups regarding the parsing of raw text files. \n\n- For the first project, we got a DB dump of all tables combined altogether in a single .csv file, with the table name as the first field. The trick was that some DB rows, due to their length, landed in multiples rows in the text file dump. Using `awk` and an healthy dose of chatgpt, i could get the parsing done pretty quickly without running into an \"Out of Memory\" error.\n\n\n- For the second project,  the text file DB dump consisted in a single line text file with fixed-width fields. There, the `fold` unix command just made the things easier.\n\nThus my question : how often do you use those unix packages to get the stuff done, knowing that spark is of no help in those contexts ? I'm just curious  what are the alternatives with regard to performance.\n\nThanks a lot in advance for your kind feedback.", "author_fullname": "t2_wvt8wjs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Handling messy text file as raw data", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_129ouy7", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1680450782.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680450039.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The current stack i am working on is mainly databricks.\nRecently i got two interesting setups regarding the parsing of raw text files. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;For the first project, we got a DB dump of all tables combined altogether in a single .csv file, with the table name as the first field. The trick was that some DB rows, due to their length, landed in multiples rows in the text file dump. Using &lt;code&gt;awk&lt;/code&gt; and an healthy dose of chatgpt, i could get the parsing done pretty quickly without running into an &amp;quot;Out of Memory&amp;quot; error.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For the second project,  the text file DB dump consisted in a single line text file with fixed-width fields. There, the &lt;code&gt;fold&lt;/code&gt; unix command just made the things easier.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thus my question : how often do you use those unix packages to get the stuff done, knowing that spark is of no help in those contexts ? I&amp;#39;m just curious  what are the alternatives with regard to performance.&lt;/p&gt;\n\n&lt;p&gt;Thanks a lot in advance for your kind feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "129ouy7", "is_robot_indexable": true, "report_reasons": null, "author": "clementalweddingaoui", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129ouy7/handling_messy_text_file_as_raw_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129ouy7/handling_messy_text_file_as_raw_data/", "subreddit_subscribers": 95442, "created_utc": 1680450039.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Cog in a wheel of IT for a huge bank in a third world country where IT projects are grossly mismanaged. \n\n\nJust finished migrating 200TB of CSVs into ADLS. Now have azure databricks to implement medallion architecture and lakehouse dimensional model. \n\nTalking to vendors, what RoRs should I be looking for?\n\nOur main business unit is an enterprise analytics team and they don\u2019t straightforwardly let us know their measures/KPIs. They just what ALL bank data organized into accounts, customers and transactions and I dont think its just that simple..also feels like its boiling the ocean.", "author_fullname": "t2_5j62j6h7", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Roles and Responsibilities for a lakehouse project", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129k4of", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680439239.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cog in a wheel of IT for a huge bank in a third world country where IT projects are grossly mismanaged. &lt;/p&gt;\n\n&lt;p&gt;Just finished migrating 200TB of CSVs into ADLS. Now have azure databricks to implement medallion architecture and lakehouse dimensional model. &lt;/p&gt;\n\n&lt;p&gt;Talking to vendors, what RoRs should I be looking for?&lt;/p&gt;\n\n&lt;p&gt;Our main business unit is an enterprise analytics team and they don\u2019t straightforwardly let us know their measures/KPIs. They just what ALL bank data organized into accounts, customers and transactions and I dont think its just that simple..also feels like its boiling the ocean.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "129k4of", "is_robot_indexable": true, "report_reasons": null, "author": "Snoo-88760", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129k4of/roles_and_responsibilities_for_a_lakehouse_project/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129k4of/roles_and_responsibilities_for_a_lakehouse_project/", "subreddit_subscribers": 95442, "created_utc": 1680439239.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello friends, I've recently graduated from university with a Bachelor's of Science degree in IT. From my university studies I've practiced Python, C, C++, .NET, Java and HTML/CSS while earning a certification in CCNA. I'm very interested in becoming a data engineer please share advice on how I could develop into one.", "author_fullname": "t2_7j180wf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to develop into a data engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_129nx1f", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1680447995.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello friends, I&amp;#39;ve recently graduated from university with a Bachelor&amp;#39;s of Science degree in IT. From my university studies I&amp;#39;ve practiced Python, C, C++, .NET, Java and HTML/CSS while earning a certification in CCNA. I&amp;#39;m very interested in becoming a data engineer please share advice on how I could develop into one.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "129nx1f", "is_robot_indexable": true, "report_reasons": null, "author": "PublicBunch749", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/129nx1f/how_to_develop_into_a_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/129nx1f/how_to_develop_into_a_data_engineer/", "subreddit_subscribers": 95442, "created_utc": 1680447995.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}