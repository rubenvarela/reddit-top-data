{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I may be getting an offer soon for a Lead Data Engineer position, and the more I dwell on it, the more daunting it seems. I have 5 years of building pipelines with everything from Sqoop, Nifi, Python, Informatica, and MySQL but I'm not sure I'm up to the task of what they need. On the other hand, every Lead Data Engineer started somewhere, I guess.\n\nHere's the job description if you're morbidly curious:\n\n[https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/](https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/)\n\nI've always wanted to work with Azure stuff, but my coding is pretty meh and my proficiency with Spark/Databricks is lacking. I would hate to sign on and then get canned because I can't keep up.", "author_fullname": "t2_1663zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I ready to be a \"Lead\" Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vl2p5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 49, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 49, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682201014.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I may be getting an offer soon for a Lead Data Engineer position, and the more I dwell on it, the more daunting it seems. I have 5 years of building pipelines with everything from Sqoop, Nifi, Python, Informatica, and MySQL but I&amp;#39;m not sure I&amp;#39;m up to the task of what they need. On the other hand, every Lead Data Engineer started somewhere, I guess.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the job description if you&amp;#39;re morbidly curious:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/\"&gt;https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always wanted to work with Azure stuff, but my coding is pretty meh and my proficiency with Spark/Databricks is lacking. I would hate to sign on and then get canned because I can&amp;#39;t keep up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12vl2p5", "is_robot_indexable": true, "report_reasons": null, "author": "lengthy_preamble", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vl2p5/am_i_ready_to_be_a_lead_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vl2p5/am_i_ready_to_be_a_lead_data_engineer/", "subreddit_subscribers": 101824, "created_utc": 1682201014.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background\n\nI am a 2 YOE data analyst from a finance background but I am doing lots of data engineering work, so I want to make a move on to a data engineer role. \n\nI am certified with AWS SAA and CCP, and I am going to get the Data Analytics Specialty certification. I have built some data pipelines with AWS services with python scripting.\n\nMy question is: Is it a data structure and algorithm a must for data engineering roles?  \nI had a few interviews and there were coding interviews, but I failed so badly due to the data structure and algorithm questions. It sometimes feels like a software engineer role - perhaps data engineers are really software engineers who focus on data ?\n\nAdditionally, some role require ML knowledge in the coding interview, and again I did it not very well. \n\nI am not so sure if I should slightly leave AWS certifications and focus more on data structure and algorithm knowledge.   \nIt would be highly appreciated if anyone could share your experience, please?\n\np.s. I am seeking ways to improve myself or succeed in the interview.   \nThank you so much", "author_fullname": "t2_pd2piq1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a data structure and algorithm a must for data engineering roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w49y1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 44, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 44, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682246220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background&lt;/p&gt;\n\n&lt;p&gt;I am a 2 YOE data analyst from a finance background but I am doing lots of data engineering work, so I want to make a move on to a data engineer role. &lt;/p&gt;\n\n&lt;p&gt;I am certified with AWS SAA and CCP, and I am going to get the Data Analytics Specialty certification. I have built some data pipelines with AWS services with python scripting.&lt;/p&gt;\n\n&lt;p&gt;My question is: Is it a data structure and algorithm a must for data engineering roles?&lt;br/&gt;\nI had a few interviews and there were coding interviews, but I failed so badly due to the data structure and algorithm questions. It sometimes feels like a software engineer role - perhaps data engineers are really software engineers who focus on data ?&lt;/p&gt;\n\n&lt;p&gt;Additionally, some role require ML knowledge in the coding interview, and again I did it not very well. &lt;/p&gt;\n\n&lt;p&gt;I am not so sure if I should slightly leave AWS certifications and focus more on data structure and algorithm knowledge.&lt;br/&gt;\nIt would be highly appreciated if anyone could share your experience, please?&lt;/p&gt;\n\n&lt;p&gt;p.s. I am seeking ways to improve myself or succeed in the interview.&lt;br/&gt;\nThank you so much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12w49y1", "is_robot_indexable": true, "report_reasons": null, "author": "uk_dataguy", "discussion_type": null, "num_comments": 61, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w49y1/is_it_a_data_structure_and_algorithm_a_must_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w49y1/is_it_a_data_structure_and_algorithm_a_must_for/", "subreddit_subscribers": 101824, "created_utc": 1682246220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,\n\nI'm new to dbt and I'm looking for some guidance on implementing Kimball-style Slowly Changing Dimension (SCD) Type 2 dimensions and Time Span Accumulating Facts using dbt. I have come across some examples that use the snapshot feature to track the history of source tables, but I'm specifically interested in pure SCD2 or time span accumulating facts.\n\n&gt;In the dbt best practices, it's recommended to snapshot the source data in its raw form and use downstream models to clean up the data.   \n\"Snapshot source data.[\u200b](https://docs.getdbt.com/docs/build/snapshots#snapshot-source-data)  \n&gt;  \n&gt;Your models should then select from these snapshots, treating them like regular data sources. As much as possible, snapshot your source data in its raw form and use downstream models to clean up the data\"\n\n[https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices](https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices)\n\nI'm hoping someone can point me in the right direction for a pure SCD2 or time span accumulating facts implementation.\n\nThank you in advance for your help!", "author_fullname": "t2_1v4h09lt", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dbt SCD type2", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w5kn1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 12, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 12, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682249823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m new to dbt and I&amp;#39;m looking for some guidance on implementing Kimball-style Slowly Changing Dimension (SCD) Type 2 dimensions and Time Span Accumulating Facts using dbt. I have come across some examples that use the snapshot feature to track the history of source tables, but I&amp;#39;m specifically interested in pure SCD2 or time span accumulating facts.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;In the dbt best practices, it&amp;#39;s recommended to snapshot the source data in its raw form and use downstream models to clean up the data.&lt;br/&gt;\n&amp;quot;Snapshot source data.&lt;a href=\"https://docs.getdbt.com/docs/build/snapshots#snapshot-source-data\"&gt;\u200b&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Your models should then select from these snapshots, treating them like regular data sources. As much as possible, snapshot your source data in its raw form and use downstream models to clean up the data&amp;quot;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices\"&gt;https://docs.getdbt.com/docs/build/snapshots#snapshot-query-best-practices&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m hoping someone can point me in the right direction for a pure SCD2 or time span accumulating facts implementation.&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?auto=webp&amp;v=enabled&amp;s=bfe5e9b2927d016e953dc1100d04aa7edae028b8", "width": 1200, "height": 630}, "resolutions": [{"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac2470355dc3f9626c6f35140e0ec423549da50e", "width": 108, "height": 56}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6800fda7fcc0abb4bd00f0af3c485a21b9befd60", "width": 216, "height": 113}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7ff6e290ecb9d197e6339baf74f37ad4bcf47da0", "width": 320, "height": 168}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=59c6c188d4255d45db867f741fbf4a6fe1161f4a", "width": 640, "height": 336}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56be5b52655ecb694651e6bf1bf5a025673b7cc3", "width": 960, "height": 504}, {"url": "https://external-preview.redd.it/Jc7Bwo70Vspr8swTKLEwUZGoroiGSARihT_F4cWI5DU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f8a7d892a9bafd02790b2d76b5fbbe76e9d0857f", "width": 1080, "height": 567}], "variants": {}, "id": "KBohsdqrfvkRxfqADmI_uqtotFtqgZjYu8NQbRpJlaE"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12w5kn1", "is_robot_indexable": true, "report_reasons": null, "author": "mrcool444", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w5kn1/dbt_scd_type2/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w5kn1/dbt_scd_type2/", "subreddit_subscribers": 101824, "created_utc": 1682249823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.makingmeaning.info/post/llms-generative-ai-comes-to-databricks", "author_fullname": "t2_a1z6eog1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLMs and Generative AI comes to Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w2gww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682241642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.makingmeaning.info/post/llms-generative-ai-comes-to-databricks\"&gt;https://www.makingmeaning.info/post/llms-generative-ai-comes-to-databricks&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?auto=webp&amp;v=enabled&amp;s=c6c011d5cad9c522dcddd45aa1e7d61f56bb04e8", "width": 1000, "height": 606}, "resolutions": [{"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=42979bf884023d945787bca37677f7c39ed6579c", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c36d10dba0aaea2fc85ddf22624574ee953ee6a3", "width": 216, "height": 130}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=123348506a8c3dbfdf495ff63edbc5298080a89e", "width": 320, "height": 193}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ce7df8f8208c50a669d3d9d0cc715fb3ebd95d0", "width": 640, "height": 387}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa5f0d78536edc3a4d47d2a7687e8b13a858abff", "width": 960, "height": 581}], "variants": {}, "id": "-oI9nzQ8WeRrnbIoYsG4Ofr6SE3w0haEM2R3dsV27-Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12w2gww", "is_robot_indexable": true, "report_reasons": null, "author": "DataAnalyticsDude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w2gww/llms_and_generative_ai_comes_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w2gww/llms_and_generative_ai_comes_to_databricks/", "subreddit_subscribers": 101824, "created_utc": 1682241642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a task to generate a daily sales report for each of our 100k products. Currently, we're using an SQL database to store the data. However, we're running into issues with the database being a bottleneck for our reporting workflow on Kubernetes, as it can only support a few hundred concurrent connections.\n\nWe can't scale up the database as it's expensive and out of my control. So I'm exploring alternative solutions to increase our read and write throughput while reducing database costs. One idea is to save the sales data to blob storage and partition it by product ID, with each product having its own parquet file.\n\nTo read the data, I can use DuckDB or even Pandas since each file will be very small (in KBs when saved in parquet). This can allows us to scale up to thousands of concurrent reads without putting pressure on the database.\n\nHowever, updating or inserting new sales data will be a challenge, as it's not efficient to make frequent small changes to parquet files. To address this, we have to use a temporary file to save new data and changes. Then, every night, we'll run a cronjob to merge the new data and overwrite existing parquet files.\n\nI've heard of Hive or Impala, which support SQL syntax and can save data to parquet files. But I'm not sure if switching to big data solutions is necessary, as our dataset is only a few hundred GBs.\n\nWhat do you guys think of this approach? Do you have any better ideas to increase our read and write throughput while reducing costs? Thank you!", "author_fullname": "t2_3g19ujvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving data from SQL database to blob storage to increase read and write throughput", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vw320", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682225420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a task to generate a daily sales report for each of our 100k products. Currently, we&amp;#39;re using an SQL database to store the data. However, we&amp;#39;re running into issues with the database being a bottleneck for our reporting workflow on Kubernetes, as it can only support a few hundred concurrent connections.&lt;/p&gt;\n\n&lt;p&gt;We can&amp;#39;t scale up the database as it&amp;#39;s expensive and out of my control. So I&amp;#39;m exploring alternative solutions to increase our read and write throughput while reducing database costs. One idea is to save the sales data to blob storage and partition it by product ID, with each product having its own parquet file.&lt;/p&gt;\n\n&lt;p&gt;To read the data, I can use DuckDB or even Pandas since each file will be very small (in KBs when saved in parquet). This can allows us to scale up to thousands of concurrent reads without putting pressure on the database.&lt;/p&gt;\n\n&lt;p&gt;However, updating or inserting new sales data will be a challenge, as it&amp;#39;s not efficient to make frequent small changes to parquet files. To address this, we have to use a temporary file to save new data and changes. Then, every night, we&amp;#39;ll run a cronjob to merge the new data and overwrite existing parquet files.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard of Hive or Impala, which support SQL syntax and can save data to parquet files. But I&amp;#39;m not sure if switching to big data solutions is necessary, as our dataset is only a few hundred GBs.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think of this approach? Do you have any better ideas to increase our read and write throughput while reducing costs? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vw320", "is_robot_indexable": true, "report_reasons": null, "author": "harrisonjjhh", "discussion_type": null, "num_comments": 14, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vw320/moving_data_from_sql_database_to_blob_storage_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vw320/moving_data_from_sql_database_to_blob_storage_to/", "subreddit_subscribers": 101824, "created_utc": 1682225420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my current job, I often need to work with backend team where turnover ratio is pretty high due to various reasons. The main problem, as a data engineer, I could see in the team lacks the structure and responsibility for data engineers because almost 95% of the codes are written in .NET for the batch jobs and OLTP kind of workload. The modeling is all over the place with no structure, sql queries with complex joins all over the places in c# code and biggest issue is the guy running the show do not want to offload those workload into data engineer and strictly against creating stored procedures or views. Because of these lots of bugs are piling up and everyone is rushing to try to fix the bug rather than fixing the underlying root issue. It has been happening for quite some time. I have raised this issue multiple different times even with proposed solution but pure denial.  I am thinking to quit, what do you think I should do in this scenario?", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I handle this situation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vwv1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682227311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my current job, I often need to work with backend team where turnover ratio is pretty high due to various reasons. The main problem, as a data engineer, I could see in the team lacks the structure and responsibility for data engineers because almost 95% of the codes are written in .NET for the batch jobs and OLTP kind of workload. The modeling is all over the place with no structure, sql queries with complex joins all over the places in c# code and biggest issue is the guy running the show do not want to offload those workload into data engineer and strictly against creating stored procedures or views. Because of these lots of bugs are piling up and everyone is rushing to try to fix the bug rather than fixing the underlying root issue. It has been happening for quite some time. I have raised this issue multiple different times even with proposed solution but pure denial.  I am thinking to quit, what do you think I should do in this scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vwv1r", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vwv1r/how_do_i_handle_this_situation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vwv1r/how_do_i_handle_this_situation/", "subreddit_subscribers": 101824, "created_utc": 1682227311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am starting to learn IaC and found out to options for AWS infra. wanted to get opinion from the community what they think about it", "author_fullname": "t2_i8mtb986", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AWS CDK vs Terraform ? Any thoughts on pros and cons of each ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w8q38", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682256580.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am starting to learn IaC and found out to options for AWS infra. wanted to get opinion from the community what they think about it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12w8q38", "is_robot_indexable": true, "report_reasons": null, "author": "BhauNibba", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w8q38/aws_cdk_vs_terraform_any_thoughts_on_pros_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w8q38/aws_cdk_vs_terraform_any_thoughts_on_pros_and/", "subreddit_subscribers": 101824, "created_utc": 1682256580.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently decided that I'd like to continue with Data Engineering as my long-term career path. I graduated with a bachelor's degree in CS at the end of 2022.\n\nI've come to understand that New Grad/Junior/Entry Level Data Engineering positions are almost non-existent. Finding a new grad position in software engineering is also becoming increasingly difficult due to competition and layoffs.\n\nI know (or at least seem to think) that the work of Data Engineers and the work of Software Engineers, Data Scientists, and Data Analysts occasionally overlap.\n\nShould I continue to aim for a career in Software Engineering, or should I go for a career in Data Analytics/Data Science? If I should go for data science/data analytics, is one title more valuable than the other for moving into data engineering?", "author_fullname": "t2_hgait", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career path decisions for future data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vnd5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682209194.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682205740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently decided that I&amp;#39;d like to continue with Data Engineering as my long-term career path. I graduated with a bachelor&amp;#39;s degree in CS at the end of 2022.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve come to understand that New Grad/Junior/Entry Level Data Engineering positions are almost non-existent. Finding a new grad position in software engineering is also becoming increasingly difficult due to competition and layoffs.&lt;/p&gt;\n\n&lt;p&gt;I know (or at least seem to think) that the work of Data Engineers and the work of Software Engineers, Data Scientists, and Data Analysts occasionally overlap.&lt;/p&gt;\n\n&lt;p&gt;Should I continue to aim for a career in Software Engineering, or should I go for a career in Data Analytics/Data Science? If I should go for data science/data analytics, is one title more valuable than the other for moving into data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vnd5g", "is_robot_indexable": true, "report_reasons": null, "author": "AsteroidFive", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vnd5g/career_path_decisions_for_future_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vnd5g/career_path_decisions_for_future_data_engineer/", "subreddit_subscribers": 101824, "created_utc": 1682205740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How does everyone handle upstream data changes that break your data models? For example, we use DMS to replicate our data real time into Redshift raw schema. The upstream teams always make changes and then it breaks our downstream fact tables and staging views. Some of the changes are like removing columns, deprecating an existing column, and adding a new column. I have seen some people say create dbt tests on your sources and make sure everything is build from the staging schema so there is only one location to fix a downstream schema instead of many views. Right now, one small upstream change requires us to fix tons of dbt models. This isn\u2019t really scalable. Any advice?", "author_fullname": "t2_9izf3j1a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Upstream schema changes", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wdcfl", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682262550.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does everyone handle upstream data changes that break your data models? For example, we use DMS to replicate our data real time into Redshift raw schema. The upstream teams always make changes and then it breaks our downstream fact tables and staging views. Some of the changes are like removing columns, deprecating an existing column, and adding a new column. I have seen some people say create dbt tests on your sources and make sure everything is build from the staging schema so there is only one location to fix a downstream schema instead of many views. Right now, one small upstream change requires us to fix tons of dbt models. This isn\u2019t really scalable. Any advice?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12wdcfl", "is_robot_indexable": true, "report_reasons": null, "author": "Used_Ad_2628", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wdcfl/upstream_schema_changes/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wdcfl/upstream_schema_changes/", "subreddit_subscribers": 101824, "created_utc": 1682262550.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Currently we are using a Postgres database where all of the data for our web application is stored. Analytics has become a big part of our product offering and currently we are running analytics from the same database.\n\nThis is working perfectly fine since the main table for the analytics is heavily indexed but it takes ages to refresh that table and we can currently only refresh our data once per day.\n\nI would like to implement an OLAP database for our analytics that basically contains a live copy of our public schema from our OLTP database (The data is mutable) and then join those tables in the OLAP db to create our flat analytics table in as little time as possible (preferably refreshed every 5min).\n\nI have tried using clickhouse and their experimental materialized postgressql database engine to create a replica of my public schema, this has worked great but clickhouse is having a lot of trouble joining our large tables (constant running out of memory errors)\n\nCan anyone recommend a better way to implement what I\u2019m trying to achieve ?\n\nFeel free to recommend other tools or DB\u2019s that i can use. (Bonus points if its open source).\n\nThanks in advance.\nA junior Data Engineer", "author_fullname": "t2_crdnm9xe", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "OLAP Database advise needed", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w5t7y", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682254109.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682250408.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently we are using a Postgres database where all of the data for our web application is stored. Analytics has become a big part of our product offering and currently we are running analytics from the same database.&lt;/p&gt;\n\n&lt;p&gt;This is working perfectly fine since the main table for the analytics is heavily indexed but it takes ages to refresh that table and we can currently only refresh our data once per day.&lt;/p&gt;\n\n&lt;p&gt;I would like to implement an OLAP database for our analytics that basically contains a live copy of our public schema from our OLTP database (The data is mutable) and then join those tables in the OLAP db to create our flat analytics table in as little time as possible (preferably refreshed every 5min).&lt;/p&gt;\n\n&lt;p&gt;I have tried using clickhouse and their experimental materialized postgressql database engine to create a replica of my public schema, this has worked great but clickhouse is having a lot of trouble joining our large tables (constant running out of memory errors)&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a better way to implement what I\u2019m trying to achieve ?&lt;/p&gt;\n\n&lt;p&gt;Feel free to recommend other tools or DB\u2019s that i can use. (Bonus points if its open source).&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.\nA junior Data Engineer&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12w5t7y", "is_robot_indexable": true, "report_reasons": null, "author": "Agile_Trash_470", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w5t7y/olap_database_advise_needed/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w5t7y/olap_database_advise_needed/", "subreddit_subscribers": 101824, "created_utc": 1682250408.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi all, \n\nI'm working to create a data pipeline that is near real-time but also scalable and cost effective. If I can have the data move from source to BigQuery in under 60 seconds that will be fine. Pipeline is in GCP.\n\nRight now, I have a Cloud Run instance that is listening for HTTP requests and then sending the data to GCS. I'm wondering the most effective, scalable, and easy way to move that data from GCS to BigQuery. Two options \n\n* When new data is loaded into GCS, I'll have a pub/sub notification sent out and that will trigger a Cloud Function to move the data. Once it's in a BigQuery staging dataset, I can do some transformations and create some reporting tables. \n* When new data is loaded into GCS, I'll I can use the pull pub/sub message with a DAG and use airflow to move the data from GCS to BigQuery\n\nThinking through the pro's cons of each. First option seems easiest, should be fairly scalable, will be cheap. I'm probably looking at 1k events / hour so nothing crazy. Second option allows me to kick off a DAG that could incorporate other transformation logic but not sure how scalable it will be and airflow on GCP can get expensive. \n\nThanks for the help!", "author_fullname": "t2_9u8negv8h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Most effective way to move data from GCS to BigQuery?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12wip0o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682270182.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working to create a data pipeline that is near real-time but also scalable and cost effective. If I can have the data move from source to BigQuery in under 60 seconds that will be fine. Pipeline is in GCP.&lt;/p&gt;\n\n&lt;p&gt;Right now, I have a Cloud Run instance that is listening for HTTP requests and then sending the data to GCS. I&amp;#39;m wondering the most effective, scalable, and easy way to move that data from GCS to BigQuery. Two options &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When new data is loaded into GCS, I&amp;#39;ll have a pub/sub notification sent out and that will trigger a Cloud Function to move the data. Once it&amp;#39;s in a BigQuery staging dataset, I can do some transformations and create some reporting tables. &lt;/li&gt;\n&lt;li&gt;When new data is loaded into GCS, I&amp;#39;ll I can use the pull pub/sub message with a DAG and use airflow to move the data from GCS to BigQuery&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thinking through the pro&amp;#39;s cons of each. First option seems easiest, should be fairly scalable, will be cheap. I&amp;#39;m probably looking at 1k events / hour so nothing crazy. Second option allows me to kick off a DAG that could incorporate other transformation logic but not sure how scalable it will be and airflow on GCP can get expensive. &lt;/p&gt;\n\n&lt;p&gt;Thanks for the help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12wip0o", "is_robot_indexable": true, "report_reasons": null, "author": "Proper_Ad_7836", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12wip0o/most_effective_way_to_move_data_from_gcs_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12wip0o/most_effective_way_to_move_data_from_gcs_to/", "subreddit_subscribers": 101824, "created_utc": 1682270182.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Here is a dumb question:\n\n**How should I compare Prometheus-like DBs with Snowflake when choosing a target storage time-series data?**\n\nLet me elaborate.\n\nI have a lot of experience in the analytics field for different types of data. That's not the case for time-series data where my \"bat-suite\" is not well equipped.\n\nWe have this use case where servers are generating tons of time-series data. This data has its use for operational purposes. That is not where I come in. I come in for the business analytics goals. We want to make this data available to people that will make business decisions on where to improve, what to explore, etc, based on these server metrics.\n\nToday, Snowflake is the analytics interface in the company. However, if the data is time-series, why would I send them to Snowflake? Considering the company already has a Phrometheus infrastructure in place, with Logstash and Graphana, wouldn't be more interesting to have this specific data ingested to a time-series DB like Phrometheus?\n\nWhen thinking about why not sending to Phrometheus I find the following idea popping up in my head:\n\n1. Better to have only one interface for analytics in the company (cultural reasons).\n2. All data in one place = easier joins (technical reason).\n3. Snowflake is SQL-queryable (that's a cultural and a technical reason).\n\nReasons 1 and 2 by themselves would be enough to make the decision. So I am trying to think without them (Also, maybe the other data to be joined is also time-series and should be moved to a time-series DB? Who knows...).\n\nPlease share your toughs. :)", "author_fullname": "t2_vd6ewwka", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Time-series data: Prometheus-like DB or Snowflake?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w71xc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682253178.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is a dumb question:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How should I compare Prometheus-like DBs with Snowflake when choosing a target storage time-series data?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me elaborate.&lt;/p&gt;\n\n&lt;p&gt;I have a lot of experience in the analytics field for different types of data. That&amp;#39;s not the case for time-series data where my &amp;quot;bat-suite&amp;quot; is not well equipped.&lt;/p&gt;\n\n&lt;p&gt;We have this use case where servers are generating tons of time-series data. This data has its use for operational purposes. That is not where I come in. I come in for the business analytics goals. We want to make this data available to people that will make business decisions on where to improve, what to explore, etc, based on these server metrics.&lt;/p&gt;\n\n&lt;p&gt;Today, Snowflake is the analytics interface in the company. However, if the data is time-series, why would I send them to Snowflake? Considering the company already has a Phrometheus infrastructure in place, with Logstash and Graphana, wouldn&amp;#39;t be more interesting to have this specific data ingested to a time-series DB like Phrometheus?&lt;/p&gt;\n\n&lt;p&gt;When thinking about why not sending to Phrometheus I find the following idea popping up in my head:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Better to have only one interface for analytics in the company (cultural reasons).&lt;/li&gt;\n&lt;li&gt;All data in one place = easier joins (technical reason).&lt;/li&gt;\n&lt;li&gt;Snowflake is SQL-queryable (that&amp;#39;s a cultural and a technical reason).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Reasons 1 and 2 by themselves would be enough to make the decision. So I am trying to think without them (Also, maybe the other data to be joined is also time-series and should be moved to a time-series DB? Who knows...).&lt;/p&gt;\n\n&lt;p&gt;Please share your toughs. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12w71xc", "is_robot_indexable": true, "report_reasons": null, "author": "yfeltz", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w71xc/timeseries_data_prometheuslike_db_or_snowflake/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w71xc/timeseries_data_prometheuslike_db_or_snowflake/", "subreddit_subscribers": 101824, "created_utc": 1682253178.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pandas and NumPy take advantage of more advanced python features to construct their APIs. For example, most container types overload equality so that `df['A'] == 2` produces a sequence of booleans rather than a boolean. Therefore `if df['A'] == 2:` is almost never the thing you want to do. I understand what is going on and why the API was designed like this, but I am in a position where I need to teach biologists and chemists who want to deal with advanced programming concepts as little as possible. I quite often have to make the awkward monologue along the lines of \"Remember how I told you how equality/indexing/callables/etc work? Well, I lied. It's actually way more complicated...\" which is very demotivating for everyone involved. \n\nDo you have any advice on how to navigate these situations? I need my students to be effective and not lose interest", "author_fullname": "t2_xftal", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to teach Pandas and NumPy to python non-experts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vox59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682209249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pandas and NumPy take advantage of more advanced python features to construct their APIs. For example, most container types overload equality so that &lt;code&gt;df[&amp;#39;A&amp;#39;] == 2&lt;/code&gt; produces a sequence of booleans rather than a boolean. Therefore &lt;code&gt;if df[&amp;#39;A&amp;#39;] == 2:&lt;/code&gt; is almost never the thing you want to do. I understand what is going on and why the API was designed like this, but I am in a position where I need to teach biologists and chemists who want to deal with advanced programming concepts as little as possible. I quite often have to make the awkward monologue along the lines of &amp;quot;Remember how I told you how equality/indexing/callables/etc work? Well, I lied. It&amp;#39;s actually way more complicated...&amp;quot; which is very demotivating for everyone involved. &lt;/p&gt;\n\n&lt;p&gt;Do you have any advice on how to navigate these situations? I need my students to be effective and not lose interest&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12vox59", "is_robot_indexable": true, "report_reasons": null, "author": "drninjabatman", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vox59/how_to_teach_pandas_and_numpy_to_python_nonexperts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vox59/how_to_teach_pandas_and_numpy_to_python_nonexperts/", "subreddit_subscribers": 101824, "created_utc": 1682209249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to load a nested json file with into mysql with proper table structure. Json is nested and one json itself can be loaded into multiple rows. Any suggestions would be great\n\nhttps://preview.redd.it/7d58igdwzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507\n\n[For the above structure, how to load JSON into MySQL database](https://preview.redd.it/gupdaj2vzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=34be46932bea0ec4858325075f9501264daa00fd)", "author_fullname": "t2_gzg2l4p2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions to load nested JSON to the database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 55, "top_awarded_type": null, "hide_score": false, "media_metadata": {"7d58igdwzhva1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d623346cf1302d34e87c882bcd778762cdcac3c9"}, {"y": 85, "x": 216, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e76666d51934c93cf0defa5737a91240ff5ec482"}, {"y": 127, "x": 320, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb8eeadd83e5934de70d854533e598e9f8912ee3"}, {"y": 254, "x": 640, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2094286440e9f73d6e6d79c8539a66f1186b37a4"}], "s": {"y": 372, "x": 936, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507"}, "id": "7d58igdwzhva1"}, "gupdaj2vzhva1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38f397291a37cc778a6bf89322e7e55fa3ca2fbf"}, {"y": 75, "x": 216, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea724296814bf6e02d734ad30f47142edaf53095"}, {"y": 111, "x": 320, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f8347635d15a2e0c892a1ec694030b85d98279c"}, {"y": 222, "x": 640, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f2495fa7b26c87f0b72fd7a559be602b5ec67cc"}], "s": {"y": 326, "x": 936, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=34be46932bea0ec4858325075f9501264daa00fd"}, "id": "gupdaj2vzhva1"}}, "name": "t3_12vipxn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jguMdnolWl6JjWHBCQnrACbVgBq4-42bYM7oodXZKck.jpg", "edited": 1682196841.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682196386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to load a nested json file with into mysql with proper table structure. Json is nested and one json itself can be loaded into multiple rows. Any suggestions would be great&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7d58igdwzhva1.png?width=936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507\"&gt;https://preview.redd.it/7d58igdwzhva1.png?width=936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gupdaj2vzhva1.png?width=936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=34be46932bea0ec4858325075f9501264daa00fd\"&gt;For the above structure, how to load JSON into MySQL database&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vipxn", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden-Pitch6371", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vipxn/need_suggestions_to_load_nested_json_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vipxn/need_suggestions_to_load_nested_json_to_the/", "subreddit_subscribers": 101824, "created_utc": 1682196386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nNew to all of this. I've been working on a project where I aggregate streaming data using Confluent's ksqlDB in Confluent CLI. \n\nI would like to pipe my tables into an s3 bucket or a relational database but have been struggling to figure out how exactly I can create a connector. \n\nI followed this guide provided by Confluent - [https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector](https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector) \\- but it seems to be outdated because \"confluent connect cluster create --config-file &lt;file-name&gt;.json\"  shows as 'deprecated' for me in CLI.\n\nAny thoughts how I can work around this? \n\nThanks!", "author_fullname": "t2_96memylv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling to create a S3 Sink Connector with Confluent CLI (Kafka)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vo2mm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682207305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;New to all of this. I&amp;#39;ve been working on a project where I aggregate streaming data using Confluent&amp;#39;s ksqlDB in Confluent CLI. &lt;/p&gt;\n\n&lt;p&gt;I would like to pipe my tables into an s3 bucket or a relational database but have been struggling to figure out how exactly I can create a connector. &lt;/p&gt;\n\n&lt;p&gt;I followed this guide provided by Confluent - &lt;a href=\"https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector\"&gt;https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector&lt;/a&gt; - but it seems to be outdated because &amp;quot;confluent connect cluster create --config-file &amp;lt;file-name&amp;gt;.json&amp;quot;  shows as &amp;#39;deprecated&amp;#39; for me in CLI.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts how I can work around this? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vo2mm", "is_robot_indexable": true, "report_reasons": null, "author": "jazzopardi203", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vo2mm/struggling_to_create_a_s3_sink_connector_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vo2mm/struggling_to_create_a_s3_sink_connector_with/", "subreddit_subscribers": 101824, "created_utc": 1682207305.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}