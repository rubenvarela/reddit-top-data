{"kind": "Listing", "data": {"after": null, "dist": 17, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I use Pandas pretty much daily and except from the usual head(), keys(), dtypes etc, I always have to Google things like groupby to remember the syntax. I know how to use them all but does this syndrome disappear as you get more experienced or does everyone Google these things too? SQL commands I remember a lot as it's plain English but Pandas, no.", "author_fullname": "t2_wqszb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal to not remember Pandas commands and need to constantly Google them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v9d3v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 110, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 110, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682177321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use Pandas pretty much daily and except from the usual head(), keys(), dtypes etc, I always have to Google things like groupby to remember the syntax. I know how to use them all but does this syndrome disappear as you get more experienced or does everyone Google these things too? SQL commands I remember a lot as it&amp;#39;s plain English but Pandas, no.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12v9d3v", "is_robot_indexable": true, "report_reasons": null, "author": "miridian19", "discussion_type": null, "num_comments": 60, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v9d3v/is_it_normal_to_not_remember_pandas_commands_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v9d3v/is_it_normal_to_not_remember_pandas_commands_and/", "subreddit_subscribers": 101600, "created_utc": 1682177321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Over the years people have started realising you don't need a distributed framework if you're not operating on that scale. SQL-first tooling such as DBT and others have also improved SQL-based workflows.\n\nHowever as much as I like SQL before I start a project I always reflect on whether or not it's a good fit. Yes you can do everything with SQL but *should* you? There's times where queries are so far removed from intentions which is a no-go in most other places in software. Sometimes imperative paradigms are a better fit. \n\nDo you go for Python in these cases or does your shop stick to SQL for all tabular data? What are your opinions?", "author_fullname": "t2_8rjci796o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you feel about the return to SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v2lcx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 97, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 97, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682161281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the years people have started realising you don&amp;#39;t need a distributed framework if you&amp;#39;re not operating on that scale. SQL-first tooling such as DBT and others have also improved SQL-based workflows.&lt;/p&gt;\n\n&lt;p&gt;However as much as I like SQL before I start a project I always reflect on whether or not it&amp;#39;s a good fit. Yes you can do everything with SQL but &lt;em&gt;should&lt;/em&gt; you? There&amp;#39;s times where queries are so far removed from intentions which is a no-go in most other places in software. Sometimes imperative paradigms are a better fit. &lt;/p&gt;\n\n&lt;p&gt;Do you go for Python in these cases or does your shop stick to SQL for all tabular data? What are your opinions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12v2lcx", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-One8023", "discussion_type": null, "num_comments": 70, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v2lcx/how_do_you_feel_about_the_return_to_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v2lcx/how_do_you_feel_about_the_return_to_sql/", "subreddit_subscribers": 101600, "created_utc": 1682161281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm talking about the trendy AI firms that are clearly doing mad stuff at the moment. OpenAI, DeepMind, FAIR, Anthropic, I'm sure there's tons of others.\n\nI've always had the feeling they must need data engineers, both low level to do the massive amounts of data processing before we get to the model training part, and the higher level stuff, about analysing results, doing measurements, making research reproducible and so on.\n\nBut I'm not really aware of DE roles in those spaces, wondering if anyone here has any experience with it or is working at one of those places.", "author_fullname": "t2_wphrs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do the AI research companies hire DEs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ux14g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.89, "author_flair_background_color": null, "subreddit_type": "public", "ups": 26, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 26, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682144571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m talking about the trendy AI firms that are clearly doing mad stuff at the moment. OpenAI, DeepMind, FAIR, Anthropic, I&amp;#39;m sure there&amp;#39;s tons of others.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always had the feeling they must need data engineers, both low level to do the massive amounts of data processing before we get to the model training part, and the higher level stuff, about analysing results, doing measurements, making research reproducible and so on.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not really aware of DE roles in those spaces, wondering if anyone here has any experience with it or is working at one of those places.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ux14g", "is_robot_indexable": true, "report_reasons": null, "author": "nesh34", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ux14g/do_the_ai_research_companies_hire_des/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ux14g/do_the_ai_research_companies_hire_des/", "subreddit_subscribers": 101600, "created_utc": 1682144571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a Data Analyst that writes MS SQL views and reports in Power BI. \n\nMy company is at the final stage of rolling out a data warehouse where we will be using for reporting (right now it\u2019s off the prod DB). \n\nAnyway I was asked the other day if I was interested in moving roles from reporting to learning Data Factory and maintaining and creating new ETL\u2019s and no longer work on reports. \n\nI guess my question is, is there a demand for people with Data Factory knowledge? Not that I\u2019m thinking about leaving my company but should anything happen my skill set will be needed.", "author_fullname": "t2_j52ax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ur5oi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 20, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 20, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682131913.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682129386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a Data Analyst that writes MS SQL views and reports in Power BI. &lt;/p&gt;\n\n&lt;p&gt;My company is at the final stage of rolling out a data warehouse where we will be using for reporting (right now it\u2019s off the prod DB). &lt;/p&gt;\n\n&lt;p&gt;Anyway I was asked the other day if I was interested in moving roles from reporting to learning Data Factory and maintaining and creating new ETL\u2019s and no longer work on reports. &lt;/p&gt;\n\n&lt;p&gt;I guess my question is, is there a demand for people with Data Factory knowledge? Not that I\u2019m thinking about leaving my company but should anything happen my skill set will be needed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ur5oi", "is_robot_indexable": true, "report_reasons": null, "author": "lez_s", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ur5oi/data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ur5oi/data_factory/", "subreddit_subscribers": 101600, "created_utc": 1682129386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I may be getting an offer soon for a Lead Data Engineer position, and the more I dwell on it, the more daunting it seems. I have 5 years of building pipelines with everything from Sqoop, Nifi, Python, Informatica, and MySQL but I'm not sure I'm up to the task of what they need. On the other hand, every Lead Data Engineer started somewhere, I guess.\n\nHere's the job description if you're morbidly curious:\n\n[https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/](https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/)\n\nI've always wanted to work with Azure stuff, but my coding is pretty meh and my proficiency with Spark/Databricks is lacking. I would hate to sign on and then get canned because I can't keep up.", "author_fullname": "t2_1663zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I ready to be a \"Lead\" Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vl2p5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682201014.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I may be getting an offer soon for a Lead Data Engineer position, and the more I dwell on it, the more daunting it seems. I have 5 years of building pipelines with everything from Sqoop, Nifi, Python, Informatica, and MySQL but I&amp;#39;m not sure I&amp;#39;m up to the task of what they need. On the other hand, every Lead Data Engineer started somewhere, I guess.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the job description if you&amp;#39;re morbidly curious:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/\"&gt;https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always wanted to work with Azure stuff, but my coding is pretty meh and my proficiency with Spark/Databricks is lacking. I would hate to sign on and then get canned because I can&amp;#39;t keep up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12vl2p5", "is_robot_indexable": true, "report_reasons": null, "author": "lengthy_preamble", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vl2p5/am_i_ready_to_be_a_lead_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vl2p5/am_i_ready_to_be_a_lead_data_engineer/", "subreddit_subscribers": 101600, "created_utc": 1682201014.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_io93l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage database schemas with Terraform in plain SQL | Atlas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_12uywhq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1s8UyyjnFQq02Em7LWQi2oi3R2KXeW2LZx8BKtFqjDI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682150162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "atlasgo.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://atlasgo.io/blog/2023/04/21/terraform-v050", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?auto=webp&amp;v=enabled&amp;s=bdfe189fcf3539f968ba1ad6d6560385b5df6863", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36f51c01b871d14cca0ecde7182dc7586bee6934", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65853ab6a9b7990eb10f3bc7861b98b4a37c9901", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a036214f5d2f95cb7529b9b46f217802b73184b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ca75ab823edc8985fc45494413ffda88d0073c2", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82be9967d84d4e8f251aaeaecdedaf43789adb6f", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adcd8a82f8cbdc8eace9b65b74fedcc700cefda8", "width": 1080, "height": 607}], "variants": {}, "id": "GGnS-T6QHI8DM_NzmZLXRxvo00Kg2yFySzJfHJqnodA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12uywhq", "is_robot_indexable": true, "report_reasons": null, "author": "rotemtam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12uywhq/manage_database_schemas_with_terraform_in_plain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://atlasgo.io/blog/2023/04/21/terraform-v050", "subreddit_subscribers": 101600, "created_utc": 1682150162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ive been chipping away at a CI/CD pipeline for snowflake. Ive been using schemachange and works as an MVP but would not be manageable at scale and there is no rollback feature i have found that would retain data from other pipelines. \n\nId love for people to share their experience with CI/CD for stateful data warehouses, even better if its a large enterprise client. Would like to write in VS code, deploy to a branch, and have that deployment roll out changes. This is happening now but want to hear about more robust solutions that have worked in the past, it seems like theres not much standardization in this space. \n\nP.S. mainly working on changes to DDL on snowflake internal objects", "author_fullname": "t2_3j972yz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD for data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12uzsui", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682153169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ive been chipping away at a CI/CD pipeline for snowflake. Ive been using schemachange and works as an MVP but would not be manageable at scale and there is no rollback feature i have found that would retain data from other pipelines. &lt;/p&gt;\n\n&lt;p&gt;Id love for people to share their experience with CI/CD for stateful data warehouses, even better if its a large enterprise client. Would like to write in VS code, deploy to a branch, and have that deployment roll out changes. This is happening now but want to hear about more robust solutions that have worked in the past, it seems like theres not much standardization in this space. &lt;/p&gt;\n\n&lt;p&gt;P.S. mainly working on changes to DDL on snowflake internal objects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12uzsui", "is_robot_indexable": true, "report_reasons": null, "author": "lturanski", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12uzsui/cicd_for_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12uzsui/cicd_for_data_warehouse/", "subreddit_subscribers": 101600, "created_utc": 1682153169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Data Engineering peeps, if you move data around (whether it be Modern Data Stack or something else), can you let me know.\n\nDo you use ETL or ELT logic?\n\nWhat tools do you for each of the E, L &amp; T steps?\n\n\\#datafam #bigdata #dataengineering", "author_fullname": "t2_353ucr1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12uvhve", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.6, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682140223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data Engineering peeps, if you move data around (whether it be Modern Data Stack or something else), can you let me know.&lt;/p&gt;\n\n&lt;p&gt;Do you use ETL or ELT logic?&lt;/p&gt;\n\n&lt;p&gt;What tools do you for each of the E, L &amp;amp; T steps?&lt;/p&gt;\n\n&lt;p&gt;#datafam #bigdata #dataengineering&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12uvhve", "is_robot_indexable": true, "report_reasons": null, "author": "cmcau", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12uvhve/what_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12uvhve/what_do_you_use/", "subreddit_subscribers": 101600, "created_utc": 1682140223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,\n\nJust trying to build an EDW and looking for advice on design as this is my first time. There are multiple data sources involved. Here is what I am thinking and want to validate if this is a good design/practice.\n\n1. Download all sources into staging landing tables as-is.\n2. Move data from staging to core tables which will have nearly identical table design. During this process, clean, transform data etc..\n3. Move data from core tables to final fact/dimension tables\n4. Build views on these fact/dimension tables to allow for Power BI reporting\n5. Clear staging tables before next load \n\nI know this is high level, but my main confusion is whether I need \"core tables\" or simply keep the data in staging indefinitely and not clear staging tables. Transfer between staging -&gt; core seems redundant.\n\nIs this how people typically do EDW process?\n\nThanks in advance.", "author_fullname": "t2_8kbtrdm4s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on EDW", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vaprs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682179961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;/p&gt;\n\n&lt;p&gt;Just trying to build an EDW and looking for advice on design as this is my first time. There are multiple data sources involved. Here is what I am thinking and want to validate if this is a good design/practice.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download all sources into staging landing tables as-is.&lt;/li&gt;\n&lt;li&gt;Move data from staging to core tables which will have nearly identical table design. During this process, clean, transform data etc..&lt;/li&gt;\n&lt;li&gt;Move data from core tables to final fact/dimension tables&lt;/li&gt;\n&lt;li&gt;Build views on these fact/dimension tables to allow for Power BI reporting&lt;/li&gt;\n&lt;li&gt;Clear staging tables before next load &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I know this is high level, but my main confusion is whether I need &amp;quot;core tables&amp;quot; or simply keep the data in staging indefinitely and not clear staging tables. Transfer between staging -&amp;gt; core seems redundant.&lt;/p&gt;\n\n&lt;p&gt;Is this how people typically do EDW process?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vaprs", "is_robot_indexable": true, "report_reasons": null, "author": "alphaqu22vice", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vaprs/looking_for_advice_on_edw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vaprs/looking_for_advice_on_edw/", "subreddit_subscribers": 101600, "created_utc": 1682179961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is exploring auditing options for our BI datamarts so that our BI and DA teams can use them directly for building user facing dashboards and ad hoc visualisations.\n\nOnly thing is that we've been told that we would need to have some kind of auditing ability where the \"purpose/usecase\" behind a query is logged.\n\nBut, if we end up needing devs to build out some kind of REST API and interface then I have no idea how we would incorporate those into Tableau and our other BI tools.\n\nMy idea for Tableau users is creating a 'published data source's fed by a Prep flow which includes a SQL query and a Tableau Parameter that's within a single or multiline comment (these are all Impala tables which support comments). Our BI creators would then add in the ticket number for their request which would subsequently add that in to every query made on the datamart through that dashboard. Any auditing on the logs could then be pulled from Impala if needed.\n\nOnly issue is security concerns. User inputted parameters are disabled by default for Tableau servers, I guess to prevent injecting unauthorized database commands from a user. \n\nHowever, my proposal here would be only to put these user inputted Parameters within a comment block (either after '--' or strictly in-between '*/', not sure pros/cons for which yet). Hard to find anything on Google because it's bringing up general SQL injection attacks but nothing about whether there's a known SQL injection attack that can escape a SQL comment block to run unauthorized commands. \n\nSo my question here is whether or not this is a dumb idea? Are there any known SQL injection attacks that could use an entry point that's *within* a \"hard-coded\" (i.e. SQL SELECT * FROM table */ &lt;Parameter&gt; */ )  comment block where it could escape the comment block and run unauthorized commands? Note this would be in an on-prem Tableau server only used by on-boarded company users (maybe a few hundred?). \n\nOtherwise I fear the devs will be forced to go the API route which will then mean either developing a Tableau web connector for it, or maybe using the same Parameter concept with a TabPy script call to an API.\n\nThe alternative to user inputted parameters would be a dropdown list of parameters (these are enabled by default on Tableau).\n\n Users would select from some kind of list of codes representing a 'purpose', but I fear going down this path would be modelling this list to be granular enough for compliance's liking.\n\nOtherwise should we consider an entirely different route here for BI data warehouses?   How do people typically approach auditing requirements in DWH environments which could let end users use self serve dashboards while still enabling this kind of auditing requirement.", "author_fullname": "t2_51nsnxi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Comments Security Concerns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v5w9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682170001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is exploring auditing options for our BI datamarts so that our BI and DA teams can use them directly for building user facing dashboards and ad hoc visualisations.&lt;/p&gt;\n\n&lt;p&gt;Only thing is that we&amp;#39;ve been told that we would need to have some kind of auditing ability where the &amp;quot;purpose/usecase&amp;quot; behind a query is logged.&lt;/p&gt;\n\n&lt;p&gt;But, if we end up needing devs to build out some kind of REST API and interface then I have no idea how we would incorporate those into Tableau and our other BI tools.&lt;/p&gt;\n\n&lt;p&gt;My idea for Tableau users is creating a &amp;#39;published data source&amp;#39;s fed by a Prep flow which includes a SQL query and a Tableau Parameter that&amp;#39;s within a single or multiline comment (these are all Impala tables which support comments). Our BI creators would then add in the ticket number for their request which would subsequently add that in to every query made on the datamart through that dashboard. Any auditing on the logs could then be pulled from Impala if needed.&lt;/p&gt;\n\n&lt;p&gt;Only issue is security concerns. User inputted parameters are disabled by default for Tableau servers, I guess to prevent injecting unauthorized database commands from a user. &lt;/p&gt;\n\n&lt;p&gt;However, my proposal here would be only to put these user inputted Parameters within a comment block (either after &amp;#39;--&amp;#39; or strictly in-between &amp;#39;*/&amp;#39;, not sure pros/cons for which yet). Hard to find anything on Google because it&amp;#39;s bringing up general SQL injection attacks but nothing about whether there&amp;#39;s a known SQL injection attack that can escape a SQL comment block to run unauthorized commands. &lt;/p&gt;\n\n&lt;p&gt;So my question here is whether or not this is a dumb idea? Are there any known SQL injection attacks that could use an entry point that&amp;#39;s &lt;em&gt;within&lt;/em&gt; a &amp;quot;hard-coded&amp;quot; (i.e. SQL SELECT * FROM table */ &amp;lt;Parameter&amp;gt; */ )  comment block where it could escape the comment block and run unauthorized commands? Note this would be in an on-prem Tableau server only used by on-boarded company users (maybe a few hundred?). &lt;/p&gt;\n\n&lt;p&gt;Otherwise I fear the devs will be forced to go the API route which will then mean either developing a Tableau web connector for it, or maybe using the same Parameter concept with a TabPy script call to an API.&lt;/p&gt;\n\n&lt;p&gt;The alternative to user inputted parameters would be a dropdown list of parameters (these are enabled by default on Tableau).&lt;/p&gt;\n\n&lt;p&gt;Users would select from some kind of list of codes representing a &amp;#39;purpose&amp;#39;, but I fear going down this path would be modelling this list to be granular enough for compliance&amp;#39;s liking.&lt;/p&gt;\n\n&lt;p&gt;Otherwise should we consider an entirely different route here for BI data warehouses?   How do people typically approach auditing requirements in DWH environments which could let end users use self serve dashboards while still enabling this kind of auditing requirement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12v5w9z", "is_robot_indexable": true, "report_reasons": null, "author": "VersatileGuru", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v5w9z/sql_comments_security_concerns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v5w9z/sql_comments_security_concerns/", "subreddit_subscribers": 101600, "created_utc": 1682170001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pandas and NumPy take advantage of more advanced python features to construct their APIs. For example, most container types overload equality so that `df['A'] == 2` produces a sequence of booleans rather than a boolean. Therefore `if df['A'] == 2:` is almost never the thing you want to do. I understand what is going on and why the API was designed like this, but I am in a position where I need to teach biologists and chemists who want to deal with advanced programming concepts as little as possible. I quite often have to make the awkward monologue along the lines of \"Remember how I told you how equality/indexing/callables/etc work? Well, I lied. It's actually way more complicated...\" which is very demotivating for everyone involved. \n\nDo you have any advice on how to navigate these situations? I need my students to be effective and not lose interest", "author_fullname": "t2_xftal", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to teach Pandas and NumPy to python non-experts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12vox59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682209249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pandas and NumPy take advantage of more advanced python features to construct their APIs. For example, most container types overload equality so that &lt;code&gt;df[&amp;#39;A&amp;#39;] == 2&lt;/code&gt; produces a sequence of booleans rather than a boolean. Therefore &lt;code&gt;if df[&amp;#39;A&amp;#39;] == 2:&lt;/code&gt; is almost never the thing you want to do. I understand what is going on and why the API was designed like this, but I am in a position where I need to teach biologists and chemists who want to deal with advanced programming concepts as little as possible. I quite often have to make the awkward monologue along the lines of &amp;quot;Remember how I told you how equality/indexing/callables/etc work? Well, I lied. It&amp;#39;s actually way more complicated...&amp;quot; which is very demotivating for everyone involved. &lt;/p&gt;\n\n&lt;p&gt;Do you have any advice on how to navigate these situations? I need my students to be effective and not lose interest&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12vox59", "is_robot_indexable": true, "report_reasons": null, "author": "drninjabatman", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vox59/how_to_teach_pandas_and_numpy_to_python_nonexperts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vox59/how_to_teach_pandas_and_numpy_to_python_nonexperts/", "subreddit_subscribers": 101600, "created_utc": 1682209249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am a soon to be 4th year cs student who has recently met with their summer internship manager to discuss projects. \n\nAs it was briefly described to me, I would creating \u201ccheckpoints\u201d within our data system and displaying results with PowerBI. the only tools explicitly mentioned were: SQL, Snowflake, Azure DevOps, and PowerBI. \n\nI was curious about how this very general description of a project sounds to you all. This is my only relevant internship experience before I graduate, and I was really hoping for more of a development/engineering role. I\u2019m just not sure about this project, particularly because of the PowerBI aspect. Does this closer to the analytics realm? or DE?\n\nThank you for any reassurance", "author_fullname": "t2_466tn173", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is my role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v813x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682174661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am a soon to be 4th year cs student who has recently met with their summer internship manager to discuss projects. &lt;/p&gt;\n\n&lt;p&gt;As it was briefly described to me, I would creating \u201ccheckpoints\u201d within our data system and displaying results with PowerBI. the only tools explicitly mentioned were: SQL, Snowflake, Azure DevOps, and PowerBI. &lt;/p&gt;\n\n&lt;p&gt;I was curious about how this very general description of a project sounds to you all. This is my only relevant internship experience before I graduate, and I was really hoping for more of a development/engineering role. I\u2019m just not sure about this project, particularly because of the PowerBI aspect. Does this closer to the analytics realm? or DE?&lt;/p&gt;\n\n&lt;p&gt;Thank you for any reassurance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12v813x", "is_robot_indexable": true, "report_reasons": null, "author": "slurpadurpblurp", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v813x/what_is_my_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v813x/what_is_my_role/", "subreddit_subscribers": 101600, "created_utc": 1682174661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was watching a bunch of conferences and videos about Kappa architecture and Confluent when I learned that Kafka had tiered storage. Did anyone actually used this in production ?\n\nIn one of the talks I've seen, Netflix was talking about how they cooled down every events they received for backfilling just in case. Cooling was required in order to decrease storage costs and avoid long retention periods in Kafka. Then they'd have Flink apps to switch between streaming and batch mode depending on the need. Pretty clever.\n\nAnyway, my question is : now can't they just use Kafka tiered storage to solve the issue of backfilling for their streaming apps ? Has anyone here ever used that ? Any pitfalls ?", "author_fullname": "t2_bja0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone tried Kafka tiered storage ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v1yf3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682159555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was watching a bunch of conferences and videos about Kappa architecture and Confluent when I learned that Kafka had tiered storage. Did anyone actually used this in production ?&lt;/p&gt;\n\n&lt;p&gt;In one of the talks I&amp;#39;ve seen, Netflix was talking about how they cooled down every events they received for backfilling just in case. Cooling was required in order to decrease storage costs and avoid long retention periods in Kafka. Then they&amp;#39;d have Flink apps to switch between streaming and batch mode depending on the need. Pretty clever.&lt;/p&gt;\n\n&lt;p&gt;Anyway, my question is : now can&amp;#39;t they just use Kafka tiered storage to solve the issue of backfilling for their streaming apps ? Has anyone here ever used that ? Any pitfalls ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12v1yf3", "is_robot_indexable": true, "report_reasons": null, "author": "Shinosha", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v1yf3/has_anyone_tried_kafka_tiered_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v1yf3/has_anyone_tried_kafka_tiered_storage/", "subreddit_subscribers": 101600, "created_utc": 1682159555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently decided that I'd like to continue with Data Engineering as my long-term career path. I graduated with a bachelor's degree in CS at the end of 2022.\n\nI've come to understand that New Grad/Junior/Entry Level Data Engineering positions are almost non-existent. Finding a new grad position in software engineering is also becoming increasingly difficult due to competition and layoffs.\n\nI know (or at least seem to think) that the work of Data Engineers and the work of Software Engineers, Data Scientists, and Data Analysts occasionally overlap.\n\nShould I continue to aim for a career in Software Engineering, or should I go for a career in Data Analytics/Data Science? If I should go for data science/data analytics, is one title more valuable than the other for moving into data engineering?", "author_fullname": "t2_hgait", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career path decisions for future data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vnd5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682209194.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682205740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently decided that I&amp;#39;d like to continue with Data Engineering as my long-term career path. I graduated with a bachelor&amp;#39;s degree in CS at the end of 2022.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve come to understand that New Grad/Junior/Entry Level Data Engineering positions are almost non-existent. Finding a new grad position in software engineering is also becoming increasingly difficult due to competition and layoffs.&lt;/p&gt;\n\n&lt;p&gt;I know (or at least seem to think) that the work of Data Engineers and the work of Software Engineers, Data Scientists, and Data Analysts occasionally overlap.&lt;/p&gt;\n\n&lt;p&gt;Should I continue to aim for a career in Software Engineering, or should I go for a career in Data Analytics/Data Science? If I should go for data science/data analytics, is one title more valuable than the other for moving into data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vnd5g", "is_robot_indexable": true, "report_reasons": null, "author": "AsteroidFive", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vnd5g/career_path_decisions_for_future_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vnd5g/career_path_decisions_for_future_data_engineer/", "subreddit_subscribers": 101600, "created_utc": 1682205740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking for examples/implementations of sampling algos (mcmc/posterior sampling) in combination with apache flink.\n\nIs there a way to implement for example JAX/ mcmcs sampling into the streaming processing without having to pull the data in a seperate script?\n\nGoal would be either to have a continous sampling happening or sample when a certain threshold is reached (think temperature of a pc part -&gt;posterior of probability of failure)\n\nI'm assuming the only way to make it near real time would be to throw GPU's or CPU's at it?\n\nI've also considered Kalmann and variational bayes, but both would either be inappropriate for the priors or adjust (vb) them in a way that would skew the posterior to be unreliable.\n\nThank you for your time", "author_fullname": "t2_5dri898p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mcmc/Posterior sampling on streaming data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ven5l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682187954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for examples/implementations of sampling algos (mcmc/posterior sampling) in combination with apache flink.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to implement for example JAX/ mcmcs sampling into the streaming processing without having to pull the data in a seperate script?&lt;/p&gt;\n\n&lt;p&gt;Goal would be either to have a continous sampling happening or sample when a certain threshold is reached (think temperature of a pc part -&amp;gt;posterior of probability of failure)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m assuming the only way to make it near real time would be to throw GPU&amp;#39;s or CPU&amp;#39;s at it?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also considered Kalmann and variational bayes, but both would either be inappropriate for the priors or adjust (vb) them in a way that would skew the posterior to be unreliable.&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ven5l", "is_robot_indexable": true, "report_reasons": null, "author": "Nokita_is_Back", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ven5l/mcmcposterior_sampling_on_streaming_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ven5l/mcmcposterior_sampling_on_streaming_data/", "subreddit_subscribers": 101600, "created_utc": 1682187954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nNew to all of this. I've been working on a project where I aggregate streaming data using Confluent's ksqlDB in Confluent CLI. \n\nI would like to pipe my tables into an s3 bucket or a relational database but have been struggling to figure out how exactly I can create a connector. \n\nI followed this guide provided by Confluent - [https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector](https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector) \\- but it seems to be outdated because \"confluent connect cluster create --config-file &lt;file-name&gt;.json\"  shows as 'deprecated' for me in CLI.\n\nAny thoughts how I can work around this? \n\nThanks!", "author_fullname": "t2_96memylv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling to create a S3 Sink Connector with Confluent CLI (Kafka)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vo2mm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682207305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;New to all of this. I&amp;#39;ve been working on a project where I aggregate streaming data using Confluent&amp;#39;s ksqlDB in Confluent CLI. &lt;/p&gt;\n\n&lt;p&gt;I would like to pipe my tables into an s3 bucket or a relational database but have been struggling to figure out how exactly I can create a connector. &lt;/p&gt;\n\n&lt;p&gt;I followed this guide provided by Confluent - &lt;a href=\"https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector\"&gt;https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector&lt;/a&gt; - but it seems to be outdated because &amp;quot;confluent connect cluster create --config-file &amp;lt;file-name&amp;gt;.json&amp;quot;  shows as &amp;#39;deprecated&amp;#39; for me in CLI.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts how I can work around this? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vo2mm", "is_robot_indexable": true, "report_reasons": null, "author": "jazzopardi203", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vo2mm/struggling_to_create_a_s3_sink_connector_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vo2mm/struggling_to_create_a_s3_sink_connector_with/", "subreddit_subscribers": 101600, "created_utc": 1682207305.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to load a nested json file with into mysql with proper table structure. Json is nested and one json itself can be loaded into multiple rows. Any suggestions would be great\n\nhttps://preview.redd.it/7d58igdwzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507\n\n[For the above structure, how to load JSON into MySQL database](https://preview.redd.it/gupdaj2vzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=34be46932bea0ec4858325075f9501264daa00fd)", "author_fullname": "t2_gzg2l4p2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions to load nested JSON to the database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 55, "top_awarded_type": null, "hide_score": false, "media_metadata": {"7d58igdwzhva1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d623346cf1302d34e87c882bcd778762cdcac3c9"}, {"y": 85, "x": 216, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e76666d51934c93cf0defa5737a91240ff5ec482"}, {"y": 127, "x": 320, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb8eeadd83e5934de70d854533e598e9f8912ee3"}, {"y": 254, "x": 640, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2094286440e9f73d6e6d79c8539a66f1186b37a4"}], "s": {"y": 372, "x": 936, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507"}, "id": "7d58igdwzhva1"}, "gupdaj2vzhva1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38f397291a37cc778a6bf89322e7e55fa3ca2fbf"}, {"y": 75, "x": 216, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea724296814bf6e02d734ad30f47142edaf53095"}, {"y": 111, "x": 320, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f8347635d15a2e0c892a1ec694030b85d98279c"}, {"y": 222, "x": 640, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f2495fa7b26c87f0b72fd7a559be602b5ec67cc"}], "s": {"y": 326, "x": 936, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=34be46932bea0ec4858325075f9501264daa00fd"}, "id": "gupdaj2vzhva1"}}, "name": "t3_12vipxn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jguMdnolWl6JjWHBCQnrACbVgBq4-42bYM7oodXZKck.jpg", "edited": 1682196841.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682196386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to load a nested json file with into mysql with proper table structure. Json is nested and one json itself can be loaded into multiple rows. Any suggestions would be great&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7d58igdwzhva1.png?width=936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507\"&gt;https://preview.redd.it/7d58igdwzhva1.png?width=936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gupdaj2vzhva1.png?width=936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=34be46932bea0ec4858325075f9501264daa00fd\"&gt;For the above structure, how to load JSON into MySQL database&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vipxn", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden-Pitch6371", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vipxn/need_suggestions_to_load_nested_json_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vipxn/need_suggestions_to_load_nested_json_to_the/", "subreddit_subscribers": 101600, "created_utc": 1682196386.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}