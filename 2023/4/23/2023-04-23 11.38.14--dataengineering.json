{"kind": "Listing", "data": {"after": null, "dist": 15, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I use Pandas pretty much daily and except from the usual head(), keys(), dtypes etc, I always have to Google things like groupby to remember the syntax. I know how to use them all but does this syndrome disappear as you get more experienced or does everyone Google these things too? SQL commands I remember a lot as it's plain English but Pandas, no.", "author_fullname": "t2_wqszb", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it normal to not remember Pandas commands and need to constantly Google them?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v9d3v", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 177, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 177, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682177321.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I use Pandas pretty much daily and except from the usual head(), keys(), dtypes etc, I always have to Google things like groupby to remember the syntax. I know how to use them all but does this syndrome disappear as you get more experienced or does everyone Google these things too? SQL commands I remember a lot as it&amp;#39;s plain English but Pandas, no.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12v9d3v", "is_robot_indexable": true, "report_reasons": null, "author": "miridian19", "discussion_type": null, "num_comments": 83, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v9d3v/is_it_normal_to_not_remember_pandas_commands_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v9d3v/is_it_normal_to_not_remember_pandas_commands_and/", "subreddit_subscribers": 101705, "created_utc": 1682177321.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I may be getting an offer soon for a Lead Data Engineer position, and the more I dwell on it, the more daunting it seems. I have 5 years of building pipelines with everything from Sqoop, Nifi, Python, Informatica, and MySQL but I'm not sure I'm up to the task of what they need. On the other hand, every Lead Data Engineer started somewhere, I guess.\n\nHere's the job description if you're morbidly curious:\n\n[https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/](https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/)\n\nI've always wanted to work with Azure stuff, but my coding is pretty meh and my proficiency with Spark/Databricks is lacking. I would hate to sign on and then get canned because I can't keep up.", "author_fullname": "t2_1663zr", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Am I ready to be a \"Lead\" Data Engineer?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vl2p5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 35, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 35, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682201014.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I may be getting an offer soon for a Lead Data Engineer position, and the more I dwell on it, the more daunting it seems. I have 5 years of building pipelines with everything from Sqoop, Nifi, Python, Informatica, and MySQL but I&amp;#39;m not sure I&amp;#39;m up to the task of what they need. On the other hand, every Lead Data Engineer started somewhere, I guess.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the job description if you&amp;#39;re morbidly curious:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/\"&gt;https://careers.auroramj.com/job/Ontario-Lead-Data-Engineer-Remote-Onta/568769017/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always wanted to work with Azure stuff, but my coding is pretty meh and my proficiency with Spark/Databricks is lacking. I would hate to sign on and then get canned because I can&amp;#39;t keep up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12vl2p5", "is_robot_indexable": true, "report_reasons": null, "author": "lengthy_preamble", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vl2p5/am_i_ready_to_be_a_lead_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vl2p5/am_i_ready_to_be_a_lead_data_engineer/", "subreddit_subscribers": 101705, "created_utc": 1682201014.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm working on a task to generate a daily sales report for each of our 100k products. Currently, we're using an SQL database to store the data. However, we're running into issues with the database being a bottleneck for our reporting workflow on Kubernetes, as it can only support a few hundred concurrent connections.\n\nWe can't scale up the database as it's expensive and out of my control. So I'm exploring alternative solutions to increase our read and write throughput while reducing database costs. One idea is to save the sales data to blob storage and partition it by product ID, with each product having its own parquet file.\n\nTo read the data, I can use DuckDB or even Pandas since each file will be very small (in KBs when saved in parquet). This can allows us to scale up to thousands of concurrent reads without putting pressure on the database.\n\nHowever, updating or inserting new sales data will be a challenge, as it's not efficient to make frequent small changes to parquet files. To address this, we have to use a temporary file to save new data and changes. Then, every night, we'll run a cronjob to merge the new data and overwrite existing parquet files.\n\nI've heard of Hive or Impala, which support SQL syntax and can save data to parquet files. But I'm not sure if switching to big data solutions is necessary, as our dataset is only a few hundred GBs.\n\nWhat do you guys think of this approach? Do you have any better ideas to increase our read and write throughput while reducing costs? Thank you!", "author_fullname": "t2_3g19ujvq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Moving data from SQL database to blob storage to increase read and write throughput", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vw320", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.92, "author_flair_background_color": null, "subreddit_type": "public", "ups": 9, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 9, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682225420.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a task to generate a daily sales report for each of our 100k products. Currently, we&amp;#39;re using an SQL database to store the data. However, we&amp;#39;re running into issues with the database being a bottleneck for our reporting workflow on Kubernetes, as it can only support a few hundred concurrent connections.&lt;/p&gt;\n\n&lt;p&gt;We can&amp;#39;t scale up the database as it&amp;#39;s expensive and out of my control. So I&amp;#39;m exploring alternative solutions to increase our read and write throughput while reducing database costs. One idea is to save the sales data to blob storage and partition it by product ID, with each product having its own parquet file.&lt;/p&gt;\n\n&lt;p&gt;To read the data, I can use DuckDB or even Pandas since each file will be very small (in KBs when saved in parquet). This can allows us to scale up to thousands of concurrent reads without putting pressure on the database.&lt;/p&gt;\n\n&lt;p&gt;However, updating or inserting new sales data will be a challenge, as it&amp;#39;s not efficient to make frequent small changes to parquet files. To address this, we have to use a temporary file to save new data and changes. Then, every night, we&amp;#39;ll run a cronjob to merge the new data and overwrite existing parquet files.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve heard of Hive or Impala, which support SQL syntax and can save data to parquet files. But I&amp;#39;m not sure if switching to big data solutions is necessary, as our dataset is only a few hundred GBs.&lt;/p&gt;\n\n&lt;p&gt;What do you guys think of this approach? Do you have any better ideas to increase our read and write throughput while reducing costs? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vw320", "is_robot_indexable": true, "report_reasons": null, "author": "harrisonjjhh", "discussion_type": null, "num_comments": 10, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vw320/moving_data_from_sql_database_to_blob_storage_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vw320/moving_data_from_sql_database_to_blob_storage_to/", "subreddit_subscribers": 101705, "created_utc": 1682225420.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "In my current job, I often need to work with backend team where turnover ratio is pretty high due to various reasons. The main problem, as a data engineer, I could see in the team lacks the structure and responsibility for data engineers because almost 95% of the codes are written in .NET for the batch jobs and OLTP kind of workload. The modeling is all over the place with no structure, sql queries with complex joins all over the places in c# code and biggest issue is the guy running the show do not want to offload those workload into data engineer and strictly against creating stored procedures or views. Because of these lots of bugs are piling up and everyone is rushing to try to fix the bug rather than fixing the underlying root issue. It has been happening for quite some time. I have raised this issue multiple different times even with proposed solution but pure denial.  I am thinking to quit, what do you think I should do in this scenario?", "author_fullname": "t2_hnxu1e2d", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do I handle this situation?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vwv1r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682227311.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my current job, I often need to work with backend team where turnover ratio is pretty high due to various reasons. The main problem, as a data engineer, I could see in the team lacks the structure and responsibility for data engineers because almost 95% of the codes are written in .NET for the batch jobs and OLTP kind of workload. The modeling is all over the place with no structure, sql queries with complex joins all over the places in c# code and biggest issue is the guy running the show do not want to offload those workload into data engineer and strictly against creating stored procedures or views. Because of these lots of bugs are piling up and everyone is rushing to try to fix the bug rather than fixing the underlying root issue. It has been happening for quite some time. I have raised this issue multiple different times even with proposed solution but pure denial.  I am thinking to quit, what do you think I should do in this scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vwv1r", "is_robot_indexable": true, "report_reasons": null, "author": "boogie_woogie_100", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vwv1r/how_do_i_handle_this_situation/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vwv1r/how_do_i_handle_this_situation/", "subreddit_subscribers": 101705, "created_utc": 1682227311.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "https://www.makingmeaning.info/post/llms-generative-ai-comes-to-databricks", "author_fullname": "t2_a1z6eog1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "LLMs and Generative AI comes to Databricks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w2gww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682241642.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.makingmeaning.info/post/llms-generative-ai-comes-to-databricks\"&gt;https://www.makingmeaning.info/post/llms-generative-ai-comes-to-databricks&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?auto=webp&amp;v=enabled&amp;s=c6c011d5cad9c522dcddd45aa1e7d61f56bb04e8", "width": 1000, "height": 606}, "resolutions": [{"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=42979bf884023d945787bca37677f7c39ed6579c", "width": 108, "height": 65}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c36d10dba0aaea2fc85ddf22624574ee953ee6a3", "width": 216, "height": 130}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=123348506a8c3dbfdf495ff63edbc5298080a89e", "width": 320, "height": 193}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5ce7df8f8208c50a669d3d9d0cc715fb3ebd95d0", "width": 640, "height": 387}, {"url": "https://external-preview.redd.it/XyLCeW-vBXfwa6wV1u86axJYvD-skOF1byhkndJiDXo.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa5f0d78536edc3a4d47d2a7687e8b13a858abff", "width": 960, "height": 581}], "variants": {}, "id": "-oI9nzQ8WeRrnbIoYsG4Ofr6SE3w0haEM2R3dsV27-Q"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12w2gww", "is_robot_indexable": true, "report_reasons": null, "author": "DataAnalyticsDude", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w2gww/llms_and_generative_ai_comes_to_databricks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w2gww/llms_and_generative_ai_comes_to_databricks/", "subreddit_subscribers": 101705, "created_utc": 1682241642.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi Folks,\n\nJust trying to build an EDW and looking for advice on design as this is my first time. There are multiple data sources involved. Here is what I am thinking and want to validate if this is a good design/practice.\n\n1. Download all sources into staging landing tables as-is.\n2. Move data from staging to core tables which will have nearly identical table design. During this process, clean, transform data etc..\n3. Move data from core tables to final fact/dimension tables\n4. Build views on these fact/dimension tables to allow for Power BI reporting\n5. Clear staging tables before next load \n\nI know this is high level, but my main confusion is whether I need \"core tables\" or simply keep the data in staging indefinitely and not clear staging tables. Transfer between staging -&gt; core seems redundant.\n\nIs this how people typically do EDW process?\n\nThanks in advance.", "author_fullname": "t2_8kbtrdm4s", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for advice on EDW", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vaprs", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.81, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682179961.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;/p&gt;\n\n&lt;p&gt;Just trying to build an EDW and looking for advice on design as this is my first time. There are multiple data sources involved. Here is what I am thinking and want to validate if this is a good design/practice.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Download all sources into staging landing tables as-is.&lt;/li&gt;\n&lt;li&gt;Move data from staging to core tables which will have nearly identical table design. During this process, clean, transform data etc..&lt;/li&gt;\n&lt;li&gt;Move data from core tables to final fact/dimension tables&lt;/li&gt;\n&lt;li&gt;Build views on these fact/dimension tables to allow for Power BI reporting&lt;/li&gt;\n&lt;li&gt;Clear staging tables before next load &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I know this is high level, but my main confusion is whether I need &amp;quot;core tables&amp;quot; or simply keep the data in staging indefinitely and not clear staging tables. Transfer between staging -&amp;gt; core seems redundant.&lt;/p&gt;\n\n&lt;p&gt;Is this how people typically do EDW process?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vaprs", "is_robot_indexable": true, "report_reasons": null, "author": "alphaqu22vice", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vaprs/looking_for_advice_on_edw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vaprs/looking_for_advice_on_edw/", "subreddit_subscribers": 101705, "created_utc": 1682179961.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is exploring auditing options for our BI datamarts so that our BI and DA teams can use them directly for building user facing dashboards and ad hoc visualisations.\n\nOnly thing is that we've been told that we would need to have some kind of auditing ability where the \"purpose/usecase\" behind a query is logged.\n\nBut, if we end up needing devs to build out some kind of REST API and interface then I have no idea how we would incorporate those into Tableau and our other BI tools.\n\nMy idea for Tableau users is creating a 'published data source's fed by a Prep flow which includes a SQL query and a Tableau Parameter that's within a single or multiline comment (these are all Impala tables which support comments). Our BI creators would then add in the ticket number for their request which would subsequently add that in to every query made on the datamart through that dashboard. Any auditing on the logs could then be pulled from Impala if needed.\n\nOnly issue is security concerns. User inputted parameters are disabled by default for Tableau servers, I guess to prevent injecting unauthorized database commands from a user. \n\nHowever, my proposal here would be only to put these user inputted Parameters within a comment block (either after '--' or strictly in-between '*/', not sure pros/cons for which yet). Hard to find anything on Google because it's bringing up general SQL injection attacks but nothing about whether there's a known SQL injection attack that can escape a SQL comment block to run unauthorized commands. \n\nSo my question here is whether or not this is a dumb idea? Are there any known SQL injection attacks that could use an entry point that's *within* a \"hard-coded\" (i.e. SQL SELECT * FROM table */ &lt;Parameter&gt; */ )  comment block where it could escape the comment block and run unauthorized commands? Note this would be in an on-prem Tableau server only used by on-boarded company users (maybe a few hundred?). \n\nOtherwise I fear the devs will be forced to go the API route which will then mean either developing a Tableau web connector for it, or maybe using the same Parameter concept with a TabPy script call to an API.\n\nThe alternative to user inputted parameters would be a dropdown list of parameters (these are enabled by default on Tableau).\n\n Users would select from some kind of list of codes representing a 'purpose', but I fear going down this path would be modelling this list to be granular enough for compliance's liking.\n\nOtherwise should we consider an entirely different route here for BI data warehouses?   How do people typically approach auditing requirements in DWH environments which could let end users use self serve dashboards while still enabling this kind of auditing requirement.", "author_fullname": "t2_51nsnxi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Comments Security Concerns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v5w9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682170001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is exploring auditing options for our BI datamarts so that our BI and DA teams can use them directly for building user facing dashboards and ad hoc visualisations.&lt;/p&gt;\n\n&lt;p&gt;Only thing is that we&amp;#39;ve been told that we would need to have some kind of auditing ability where the &amp;quot;purpose/usecase&amp;quot; behind a query is logged.&lt;/p&gt;\n\n&lt;p&gt;But, if we end up needing devs to build out some kind of REST API and interface then I have no idea how we would incorporate those into Tableau and our other BI tools.&lt;/p&gt;\n\n&lt;p&gt;My idea for Tableau users is creating a &amp;#39;published data source&amp;#39;s fed by a Prep flow which includes a SQL query and a Tableau Parameter that&amp;#39;s within a single or multiline comment (these are all Impala tables which support comments). Our BI creators would then add in the ticket number for their request which would subsequently add that in to every query made on the datamart through that dashboard. Any auditing on the logs could then be pulled from Impala if needed.&lt;/p&gt;\n\n&lt;p&gt;Only issue is security concerns. User inputted parameters are disabled by default for Tableau servers, I guess to prevent injecting unauthorized database commands from a user. &lt;/p&gt;\n\n&lt;p&gt;However, my proposal here would be only to put these user inputted Parameters within a comment block (either after &amp;#39;--&amp;#39; or strictly in-between &amp;#39;*/&amp;#39;, not sure pros/cons for which yet). Hard to find anything on Google because it&amp;#39;s bringing up general SQL injection attacks but nothing about whether there&amp;#39;s a known SQL injection attack that can escape a SQL comment block to run unauthorized commands. &lt;/p&gt;\n\n&lt;p&gt;So my question here is whether or not this is a dumb idea? Are there any known SQL injection attacks that could use an entry point that&amp;#39;s &lt;em&gt;within&lt;/em&gt; a &amp;quot;hard-coded&amp;quot; (i.e. SQL SELECT * FROM table */ &amp;lt;Parameter&amp;gt; */ )  comment block where it could escape the comment block and run unauthorized commands? Note this would be in an on-prem Tableau server only used by on-boarded company users (maybe a few hundred?). &lt;/p&gt;\n\n&lt;p&gt;Otherwise I fear the devs will be forced to go the API route which will then mean either developing a Tableau web connector for it, or maybe using the same Parameter concept with a TabPy script call to an API.&lt;/p&gt;\n\n&lt;p&gt;The alternative to user inputted parameters would be a dropdown list of parameters (these are enabled by default on Tableau).&lt;/p&gt;\n\n&lt;p&gt;Users would select from some kind of list of codes representing a &amp;#39;purpose&amp;#39;, but I fear going down this path would be modelling this list to be granular enough for compliance&amp;#39;s liking.&lt;/p&gt;\n\n&lt;p&gt;Otherwise should we consider an entirely different route here for BI data warehouses?   How do people typically approach auditing requirements in DWH environments which could let end users use self serve dashboards while still enabling this kind of auditing requirement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12v5w9z", "is_robot_indexable": true, "report_reasons": null, "author": "VersatileGuru", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v5w9z/sql_comments_security_concerns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v5w9z/sql_comments_security_concerns/", "subreddit_subscribers": 101705, "created_utc": 1682170001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've recently decided that I'd like to continue with Data Engineering as my long-term career path. I graduated with a bachelor's degree in CS at the end of 2022.\n\nI've come to understand that New Grad/Junior/Entry Level Data Engineering positions are almost non-existent. Finding a new grad position in software engineering is also becoming increasingly difficult due to competition and layoffs.\n\nI know (or at least seem to think) that the work of Data Engineers and the work of Software Engineers, Data Scientists, and Data Analysts occasionally overlap.\n\nShould I continue to aim for a career in Software Engineering, or should I go for a career in Data Analytics/Data Science? If I should go for data science/data analytics, is one title more valuable than the other for moving into data engineering?", "author_fullname": "t2_hgait", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Career path decisions for future data engineer", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vnd5g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682209194.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682205740.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently decided that I&amp;#39;d like to continue with Data Engineering as my long-term career path. I graduated with a bachelor&amp;#39;s degree in CS at the end of 2022.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve come to understand that New Grad/Junior/Entry Level Data Engineering positions are almost non-existent. Finding a new grad position in software engineering is also becoming increasingly difficult due to competition and layoffs.&lt;/p&gt;\n\n&lt;p&gt;I know (or at least seem to think) that the work of Data Engineers and the work of Software Engineers, Data Scientists, and Data Analysts occasionally overlap.&lt;/p&gt;\n\n&lt;p&gt;Should I continue to aim for a career in Software Engineering, or should I go for a career in Data Analytics/Data Science? If I should go for data science/data analytics, is one title more valuable than the other for moving into data engineering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vnd5g", "is_robot_indexable": true, "report_reasons": null, "author": "AsteroidFive", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vnd5g/career_path_decisions_for_future_data_engineer/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vnd5g/career_path_decisions_for_future_data_engineer/", "subreddit_subscribers": 101705, "created_utc": 1682205740.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Background\n\nI am a 2 YOE data analyst from a finance background but I am doing lots of data engineering work, so I want to make a move on to a data engineer role. \n\nI am certified with AWS SAA and CCP, and I am going to get the Data Analytics Specialty certification. I have built some data pipelines with AWS services with python scripting.\n\nMy question is: Is it a data structure and algorithm a must for data engineering roles?  \nI had a few interviews and there were coding interviews, but I failed so badly due to the data structure and algorithm questions. It sometimes feels like a software engineer role - perhaps data engineers are really software engineers who focus on data ?\n\nAdditionally, some role require ML knowledge in the coding interview, and again I did it not very well. \n\nI am not so sure if I should slightly leave AWS certifications and focus more on data structure and algorithm knowledge.   \nIt would be highly appreciated if anyone could share your experience, please?\n\np.s. I am seeking ways to improve myself or succeed in the interview.   \nThank you so much", "author_fullname": "t2_pd2piq1f", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is it a data structure and algorithm a must for data engineering roles", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12w49y1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682246220.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Background&lt;/p&gt;\n\n&lt;p&gt;I am a 2 YOE data analyst from a finance background but I am doing lots of data engineering work, so I want to make a move on to a data engineer role. &lt;/p&gt;\n\n&lt;p&gt;I am certified with AWS SAA and CCP, and I am going to get the Data Analytics Specialty certification. I have built some data pipelines with AWS services with python scripting.&lt;/p&gt;\n\n&lt;p&gt;My question is: Is it a data structure and algorithm a must for data engineering roles?&lt;br/&gt;\nI had a few interviews and there were coding interviews, but I failed so badly due to the data structure and algorithm questions. It sometimes feels like a software engineer role - perhaps data engineers are really software engineers who focus on data ?&lt;/p&gt;\n\n&lt;p&gt;Additionally, some role require ML knowledge in the coding interview, and again I did it not very well. &lt;/p&gt;\n\n&lt;p&gt;I am not so sure if I should slightly leave AWS certifications and focus more on data structure and algorithm knowledge.&lt;br/&gt;\nIt would be highly appreciated if anyone could share your experience, please?&lt;/p&gt;\n\n&lt;p&gt;p.s. I am seeking ways to improve myself or succeed in the interview.&lt;br/&gt;\nThank you so much&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12w49y1", "is_robot_indexable": true, "report_reasons": null, "author": "uk_dataguy", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w49y1/is_it_a_data_structure_and_algorithm_a_must_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w49y1/is_it_a_data_structure_and_algorithm_a_must_for/", "subreddit_subscribers": 101705, "created_utc": 1682246220.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I need to provide data both by queries from db and through a rest api. Using Python what\u2019s the lowest cost/maintenance solution for doing this? Are there any good frameworks? \n\nI have experience with flask/fast api/sql alchemy but wanted to know if there are more frameworks.", "author_fullname": "t2_7gpue71ag", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Provide read only data using apis", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12w2t1q", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682242450.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to provide data both by queries from db and through a rest api. Using Python what\u2019s the lowest cost/maintenance solution for doing this? Are there any good frameworks? &lt;/p&gt;\n\n&lt;p&gt;I have experience with flask/fast api/sql alchemy but wanted to know if there are more frameworks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12w2t1q", "is_robot_indexable": true, "report_reasons": null, "author": "Annual_Anxiety_4457", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12w2t1q/provide_read_only_data_using_apis/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12w2t1q/provide_read_only_data_using_apis/", "subreddit_subscribers": 101705, "created_utc": 1682242450.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Pandas and NumPy take advantage of more advanced python features to construct their APIs. For example, most container types overload equality so that `df['A'] == 2` produces a sequence of booleans rather than a boolean. Therefore `if df['A'] == 2:` is almost never the thing you want to do. I understand what is going on and why the API was designed like this, but I am in a position where I need to teach biologists and chemists who want to deal with advanced programming concepts as little as possible. I quite often have to make the awkward monologue along the lines of \"Remember how I told you how equality/indexing/callables/etc work? Well, I lied. It's actually way more complicated...\" which is very demotivating for everyone involved. \n\nDo you have any advice on how to navigate these situations? I need my students to be effective and not lose interest", "author_fullname": "t2_xftal", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to teach Pandas and NumPy to python non-experts", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vox59", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682209249.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Pandas and NumPy take advantage of more advanced python features to construct their APIs. For example, most container types overload equality so that &lt;code&gt;df[&amp;#39;A&amp;#39;] == 2&lt;/code&gt; produces a sequence of booleans rather than a boolean. Therefore &lt;code&gt;if df[&amp;#39;A&amp;#39;] == 2:&lt;/code&gt; is almost never the thing you want to do. I understand what is going on and why the API was designed like this, but I am in a position where I need to teach biologists and chemists who want to deal with advanced programming concepts as little as possible. I quite often have to make the awkward monologue along the lines of &amp;quot;Remember how I told you how equality/indexing/callables/etc work? Well, I lied. It&amp;#39;s actually way more complicated...&amp;quot; which is very demotivating for everyone involved. &lt;/p&gt;\n\n&lt;p&gt;Do you have any advice on how to navigate these situations? I need my students to be effective and not lose interest&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12vox59", "is_robot_indexable": true, "report_reasons": null, "author": "drninjabatman", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vox59/how_to_teach_pandas_and_numpy_to_python_nonexperts/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vox59/how_to_teach_pandas_and_numpy_to_python_nonexperts/", "subreddit_subscribers": 101705, "created_utc": 1682209249.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm looking for examples/implementations of sampling algos (mcmc/posterior sampling) in combination with apache flink.\n\nIs there a way to implement for example JAX/ mcmcs sampling into the streaming processing without having to pull the data in a seperate script?\n\nGoal would be either to have a continous sampling happening or sample when a certain threshold is reached (think temperature of a pc part -&gt;posterior of probability of failure)\n\nI'm assuming the only way to make it near real time would be to throw GPU's or CPU's at it?\n\nI've also considered Kalmann and variational bayes, but both would either be inappropriate for the priors or adjust (vb) them in a way that would skew the posterior to be unreliable.\n\nThank you for your time", "author_fullname": "t2_5dri898p", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Mcmc/Posterior sampling on streaming data?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ven5l", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682187954.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for examples/implementations of sampling algos (mcmc/posterior sampling) in combination with apache flink.&lt;/p&gt;\n\n&lt;p&gt;Is there a way to implement for example JAX/ mcmcs sampling into the streaming processing without having to pull the data in a seperate script?&lt;/p&gt;\n\n&lt;p&gt;Goal would be either to have a continous sampling happening or sample when a certain threshold is reached (think temperature of a pc part -&amp;gt;posterior of probability of failure)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m assuming the only way to make it near real time would be to throw GPU&amp;#39;s or CPU&amp;#39;s at it?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve also considered Kalmann and variational bayes, but both would either be inappropriate for the priors or adjust (vb) them in a way that would skew the posterior to be unreliable.&lt;/p&gt;\n\n&lt;p&gt;Thank you for your time&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ven5l", "is_robot_indexable": true, "report_reasons": null, "author": "Nokita_is_Back", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ven5l/mcmcposterior_sampling_on_streaming_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ven5l/mcmcposterior_sampling_on_streaming_data/", "subreddit_subscribers": 101705, "created_utc": 1682187954.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am a soon to be 4th year cs student who has recently met with their summer internship manager to discuss projects. \n\nAs it was briefly described to me, I would creating \u201ccheckpoints\u201d within our data system and displaying results with PowerBI. the only tools explicitly mentioned were: SQL, Snowflake, Azure DevOps, and PowerBI. \n\nI was curious about how this very general description of a project sounds to you all. This is my only relevant internship experience before I graduate, and I was really hoping for more of a development/engineering role. I\u2019m just not sure about this project, particularly because of the PowerBI aspect. Does this closer to the analytics realm? or DE?\n\nThank you for any reassurance", "author_fullname": "t2_466tn173", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is my role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v813x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682174661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am a soon to be 4th year cs student who has recently met with their summer internship manager to discuss projects. &lt;/p&gt;\n\n&lt;p&gt;As it was briefly described to me, I would creating \u201ccheckpoints\u201d within our data system and displaying results with PowerBI. the only tools explicitly mentioned were: SQL, Snowflake, Azure DevOps, and PowerBI. &lt;/p&gt;\n\n&lt;p&gt;I was curious about how this very general description of a project sounds to you all. This is my only relevant internship experience before I graduate, and I was really hoping for more of a development/engineering role. I\u2019m just not sure about this project, particularly because of the PowerBI aspect. Does this closer to the analytics realm? or DE?&lt;/p&gt;\n\n&lt;p&gt;Thank you for any reassurance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12v813x", "is_robot_indexable": true, "report_reasons": null, "author": "slurpadurpblurp", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v813x/what_is_my_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v813x/what_is_my_role/", "subreddit_subscribers": 101705, "created_utc": 1682174661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am trying to load a nested json file with into mysql with proper table structure. Json is nested and one json itself can be loaded into multiple rows. Any suggestions would be great\n\nhttps://preview.redd.it/7d58igdwzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507\n\n[For the above structure, how to load JSON into MySQL database](https://preview.redd.it/gupdaj2vzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=34be46932bea0ec4858325075f9501264daa00fd)", "author_fullname": "t2_gzg2l4p2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions to load nested JSON to the database", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 55, "top_awarded_type": null, "hide_score": false, "media_metadata": {"7d58igdwzhva1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 42, "x": 108, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d623346cf1302d34e87c882bcd778762cdcac3c9"}, {"y": 85, "x": 216, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=e76666d51934c93cf0defa5737a91240ff5ec482"}, {"y": 127, "x": 320, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=eb8eeadd83e5934de70d854533e598e9f8912ee3"}, {"y": 254, "x": 640, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2094286440e9f73d6e6d79c8539a66f1186b37a4"}], "s": {"y": 372, "x": 936, "u": "https://preview.redd.it/7d58igdwzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507"}, "id": "7d58igdwzhva1"}, "gupdaj2vzhva1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 37, "x": 108, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=38f397291a37cc778a6bf89322e7e55fa3ca2fbf"}, {"y": 75, "x": 216, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ea724296814bf6e02d734ad30f47142edaf53095"}, {"y": 111, "x": 320, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7f8347635d15a2e0c892a1ec694030b85d98279c"}, {"y": 222, "x": 640, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2f2495fa7b26c87f0b72fd7a559be602b5ec67cc"}], "s": {"y": 326, "x": 936, "u": "https://preview.redd.it/gupdaj2vzhva1.png?width=936&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=34be46932bea0ec4858325075f9501264daa00fd"}, "id": "gupdaj2vzhva1"}}, "name": "t3_12vipxn", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/jguMdnolWl6JjWHBCQnrACbVgBq4-42bYM7oodXZKck.jpg", "edited": 1682196841.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682196386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to load a nested json file with into mysql with proper table structure. Json is nested and one json itself can be loaded into multiple rows. Any suggestions would be great&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7d58igdwzhva1.png?width=936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507\"&gt;https://preview.redd.it/7d58igdwzhva1.png?width=936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=bc83498d2dbb9506a21cc785ca3c10c70fc5e507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gupdaj2vzhva1.png?width=936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=34be46932bea0ec4858325075f9501264daa00fd\"&gt;For the above structure, how to load JSON into MySQL database&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vipxn", "is_robot_indexable": true, "report_reasons": null, "author": "Sudden-Pitch6371", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vipxn/need_suggestions_to_load_nested_json_to_the/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vipxn/need_suggestions_to_load_nested_json_to_the/", "subreddit_subscribers": 101705, "created_utc": 1682196386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi everyone,\n\nNew to all of this. I've been working on a project where I aggregate streaming data using Confluent's ksqlDB in Confluent CLI. \n\nI would like to pipe my tables into an s3 bucket or a relational database but have been struggling to figure out how exactly I can create a connector. \n\nI followed this guide provided by Confluent - [https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector](https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector) \\- but it seems to be outdated because \"confluent connect cluster create --config-file &lt;file-name&gt;.json\"  shows as 'deprecated' for me in CLI.\n\nAny thoughts how I can work around this? \n\nThanks!", "author_fullname": "t2_96memylv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Struggling to create a S3 Sink Connector with Confluent CLI (Kafka)", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12vo2mm", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682207305.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;New to all of this. I&amp;#39;ve been working on a project where I aggregate streaming data using Confluent&amp;#39;s ksqlDB in Confluent CLI. &lt;/p&gt;\n\n&lt;p&gt;I would like to pipe my tables into an s3 bucket or a relational database but have been struggling to figure out how exactly I can create a connector. &lt;/p&gt;\n\n&lt;p&gt;I followed this guide provided by Confluent - &lt;a href=\"https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector\"&gt;https://docs.confluent.io/cloud/current/connectors/cc-s3-sink.html#step-4-load-the-properties-file-and-create-the-connector&lt;/a&gt; - but it seems to be outdated because &amp;quot;confluent connect cluster create --config-file &amp;lt;file-name&amp;gt;.json&amp;quot;  shows as &amp;#39;deprecated&amp;#39; for me in CLI.&lt;/p&gt;\n\n&lt;p&gt;Any thoughts how I can work around this? &lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12vo2mm", "is_robot_indexable": true, "report_reasons": null, "author": "jazzopardi203", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12vo2mm/struggling_to_create_a_s3_sink_connector_with/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12vo2mm/struggling_to_create_a_s3_sink_connector_with/", "subreddit_subscribers": 101705, "created_utc": 1682207305.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}