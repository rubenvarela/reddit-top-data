{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is quite poor practice, no? I received a job offer from a health tech company and I\u2019d be working on pipelines touching PHI data. They do not supply a laptop and I\u2019d have to use a personal device. \n\nI\u2019m quite surprised in general that a tech company wouldn\u2019t provide computers but especially shocked that a health tech company would not. Just want to sanity check that this is unacceptable, or if I\u2019m missing something (coming from a bigger company and looking for a startup role so I know there will be changes, but was not expecting this).", "author_fullname": "t2_odulp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal laptop required at health tech company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xfvnc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 45, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 45, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682343029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is quite poor practice, no? I received a job offer from a health tech company and I\u2019d be working on pipelines touching PHI data. They do not supply a laptop and I\u2019d have to use a personal device. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m quite surprised in general that a tech company wouldn\u2019t provide computers but especially shocked that a health tech company would not. Just want to sanity check that this is unacceptable, or if I\u2019m missing something (coming from a bigger company and looking for a startup role so I know there will be changes, but was not expecting this).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12xfvnc", "is_robot_indexable": true, "report_reasons": null, "author": "ihaveanideer", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xfvnc/personal_laptop_required_at_health_tech_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xfvnc/personal_laptop_required_at_health_tech_company/", "subreddit_subscribers": 102302, "created_utc": 1682343029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been writing a lot of SQL lately but I get errors like mismatching no. of columns in a row during an insert more so because I'm dealing with huge tables and the editors never show me an error like that. I understand that it's like a compile time error but is there any workaround that you guys use? TIA.", "author_fullname": "t2_q0hjyvx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What editors do you use for SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xfoqw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 40, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 40, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682342644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing a lot of SQL lately but I get errors like mismatching no. of columns in a row during an insert more so because I&amp;#39;m dealing with huge tables and the editors never show me an error like that. I understand that it&amp;#39;s like a compile time error but is there any workaround that you guys use? TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xfoqw", "is_robot_indexable": true, "report_reasons": null, "author": "Ready--Aim--Fire", "discussion_type": null, "num_comments": 78, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xfoqw/what_editors_do_you_use_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xfoqw/what_editors_do_you_use_for_sql/", "subreddit_subscribers": 102302, "created_utc": 1682342644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a Data Engineer, which Python packages and tools do you use in your work and for what purposes?", "author_fullname": "t2_uel8wujv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python usage in Data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xj7tf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 21, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 21, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682348364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a Data Engineer, which Python packages and tools do you use in your work and for what purposes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xj7tf", "is_robot_indexable": true, "report_reasons": null, "author": "Own-Guava-2015", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xj7tf/python_usage_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xj7tf/python_usage_in_data_engineering/", "subreddit_subscribers": 102302, "created_utc": 1682348364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I come from a math and data science background and I really enjoyed reading white papers about current trends in the industry. \n\nDoes data engineering have that? I've seen the duckdb white paper but it seems like DE has GitHub release pages and product websites instead of academic papers. \n\nIs this correct or am I missing out on an entire world of papers?", "author_fullname": "t2_cpnmh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for DE white papers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xqa8s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 15, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 15, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682357975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from a math and data science background and I really enjoyed reading white papers about current trends in the industry. &lt;/p&gt;\n\n&lt;p&gt;Does data engineering have that? I&amp;#39;ve seen the duckdb white paper but it seems like DE has GitHub release pages and product websites instead of academic papers. &lt;/p&gt;\n\n&lt;p&gt;Is this correct or am I missing out on an entire world of papers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12xqa8s", "is_robot_indexable": true, "report_reasons": null, "author": "DoingItForGiggles", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xqa8s/looking_for_de_white_papers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xqa8s/looking_for_de_white_papers/", "subreddit_subscribers": 102302, "created_utc": 1682357975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m building out our Databricks deployment and related DE infrastructure (new start up, greenfield). As the only DE, I\u2019m using Airbyte for raw extraction and load into our S3 data lake. \n\nI like the idea of only having to use one tool for all our DE needs. The only thing that comes to mind would be manually building out extractors to our data sources (CRMs, DBs, Tools, etc) or running  python based ETL libraries like Meltano in our notebooks. \n\nWith Databricks workflows and orchestrators, this could consolidate tooling. \n\nI will keep using airbyte as time is of the essence and the libraries help with the lift. \n\nHowever, I\u2019d love to have a discussion around projects or ideas with this type of infrastructure. \nThoughts?", "author_fullname": "t2_4rrqnyd4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Curious if anyone has adopted a stack to do raw data ingestion in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y8yu2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682399487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building out our Databricks deployment and related DE infrastructure (new start up, greenfield). As the only DE, I\u2019m using Airbyte for raw extraction and load into our S3 data lake. &lt;/p&gt;\n\n&lt;p&gt;I like the idea of only having to use one tool for all our DE needs. The only thing that comes to mind would be manually building out extractors to our data sources (CRMs, DBs, Tools, etc) or running  python based ETL libraries like Meltano in our notebooks. &lt;/p&gt;\n\n&lt;p&gt;With Databricks workflows and orchestrators, this could consolidate tooling. &lt;/p&gt;\n\n&lt;p&gt;I will keep using airbyte as time is of the essence and the libraries help with the lift. &lt;/p&gt;\n\n&lt;p&gt;However, I\u2019d love to have a discussion around projects or ideas with this type of infrastructure. \nThoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12y8yu2", "is_robot_indexable": true, "report_reasons": null, "author": "deep-data-diver", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y8yu2/curious_if_anyone_has_adopted_a_stack_to_do_raw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y8yu2/curious_if_anyone_has_adopted_a_stack_to_do_raw/", "subreddit_subscribers": 102302, "created_utc": 1682399487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does the following architecture make sense? I'm looking for the ability to swap DWH in the future if needed and I also want some flexibility to work with fivetran/glue.\n\nData Sources (Postgres, csv, dynamo, etc) to S3 Parquet files through fivetran. I'm calling this the Raw Layer.\n\nWe can enrich the raw data with glue as needed. In the \"Enrichment Layer\".\n\nSince the files are stored as Parquet Files in S3, we could use Iceberg tables as well.\n\nFrom the Raw/Enriched we can load the Raw data so snowflake using Fivetran/Snowflake tasks.\n\nNow that the data landed in snowflake we can transform it using self hosted DBT running on fargate.  \n\n\n**TL;DR :**   \nDatasources -- fivetran --&gt; S3 (Parquet) &lt;--&gt; Enrichment Layer With Glue --&gt; Fivetran/Snowflake tasks to load Raw data into snowflake --&gt; Transform with dbt .", "author_fullname": "t2_9rv5oe9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Architecture Review", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y1d42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682380749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does the following architecture make sense? I&amp;#39;m looking for the ability to swap DWH in the future if needed and I also want some flexibility to work with fivetran/glue.&lt;/p&gt;\n\n&lt;p&gt;Data Sources (Postgres, csv, dynamo, etc) to S3 Parquet files through fivetran. I&amp;#39;m calling this the Raw Layer.&lt;/p&gt;\n\n&lt;p&gt;We can enrich the raw data with glue as needed. In the &amp;quot;Enrichment Layer&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Since the files are stored as Parquet Files in S3, we could use Iceberg tables as well.&lt;/p&gt;\n\n&lt;p&gt;From the Raw/Enriched we can load the Raw data so snowflake using Fivetran/Snowflake tasks.&lt;/p&gt;\n\n&lt;p&gt;Now that the data landed in snowflake we can transform it using self hosted DBT running on fargate.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR :&lt;/strong&gt;&lt;br/&gt;\nDatasources -- fivetran --&amp;gt; S3 (Parquet) &amp;lt;--&amp;gt; Enrichment Layer With Glue --&amp;gt; Fivetran/Snowflake tasks to load Raw data into snowflake --&amp;gt; Transform with dbt .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y1d42", "is_robot_indexable": true, "report_reasons": null, "author": "jr_acc", "discussion_type": null, "num_comments": 9, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y1d42/de_architecture_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y1d42/de_architecture_review/", "subreddit_subscribers": 102302, "created_utc": 1682380749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_20tfe7ur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering and DataOps: A Beginner's Guide to Building Data Solutions and Solving Real-World Challenges", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_12ybcqo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JspNOjLI_4EFW2mFS5IQe8mgrct34voOY_55DCLLYrg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682406275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "chaosgenius.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.chaosgenius.io/blog/data-engineering-and-dataops-beginners-guide-to-building-data-solutions-and-solving-real-world-challenges/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?auto=webp&amp;v=enabled&amp;s=0c672d9f9def968617df5e594108289933769bcc", "width": 4000, "height": 2400}, "resolutions": [{"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94de9e67438d561dac53f6aa67f74ed81686c74a", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b39fc1e891c97928b67b768cfb2a44364d9581b", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1409b7ec75b01fb8e58450c13bb2c52986bf9b7e", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c9ccd575d261751cc267eea527f39ba69aeff8c", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa23b5cc27d8fca03ea360638f9a1ac90eecd9ee", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98e38fea795867bfeae0e7c41c32c6ab31afa893", "width": 1080, "height": 648}], "variants": {}, "id": "eN31HKOXDX_DK4Vg5fj3aDAKHFmxW72mKecuYjio5Wo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ybcqo", "is_robot_indexable": true, "report_reasons": null, "author": "pramit_marattha", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ybcqo/data_engineering_and_dataops_a_beginners_guide_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.chaosgenius.io/blog/data-engineering-and-dataops-beginners-guide-to-building-data-solutions-and-solving-real-world-challenges/", "subreddit_subscribers": 102302, "created_utc": 1682406275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today from an X company approached me for a freelance data engineer. I have no degree in CS and it was going to be my first interview. They asked me how much rate I am looking to pay around and I said 130 euros per day. Now I can't reach them. Is it too much? What should I say?\n\nEdit: Company in the Netherlands I am in Poland", "author_fullname": "t2_2vc85g2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xokfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.73, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682354397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today from an X company approached me for a freelance data engineer. I have no degree in CS and it was going to be my first interview. They asked me how much rate I am looking to pay around and I said 130 euros per day. Now I can&amp;#39;t reach them. Is it too much? What should I say?&lt;/p&gt;\n\n&lt;p&gt;Edit: Company in the Netherlands I am in Poland&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12xokfb", "is_robot_indexable": true, "report_reasons": null, "author": "Blazey25", "discussion_type": null, "num_comments": 26, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xokfb/i_need_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xokfb/i_need_help/", "subreddit_subscribers": 102302, "created_utc": 1682354397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering! I\u2019m finishing up my 3rd year of college and had no luck landing DE/SWE or any other data related internship this year due to uncertainty in the job market. I did, however, end up with DevOps Engineer internship opportunity, which I took since I have no prior internship experience. The tech stack will be Python, AWS, Terraform, Docker and potentially SQL (will have to clarify that with my team). I\u2019m not sure it\u2019s DevOps is for me (maybe it will be) since my goal was always Data Engineering since I like working with data and mess around with cloud services. Therefore, I\u2019m curious, will DevOps internship help me with my Data Engineer job search when I look for new grad jobs? Thanks!", "author_fullname": "t2_mwot5ip2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will this help me break into DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xq5nf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682357972.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682357716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! I\u2019m finishing up my 3rd year of college and had no luck landing DE/SWE or any other data related internship this year due to uncertainty in the job market. I did, however, end up with DevOps Engineer internship opportunity, which I took since I have no prior internship experience. The tech stack will be Python, AWS, Terraform, Docker and potentially SQL (will have to clarify that with my team). I\u2019m not sure it\u2019s DevOps is for me (maybe it will be) since my goal was always Data Engineering since I like working with data and mess around with cloud services. Therefore, I\u2019m curious, will DevOps internship help me with my Data Engineer job search when I look for new grad jobs? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12xq5nf", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting_Chard138", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xq5nf/will_this_help_me_break_into_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xq5nf/will_this_help_me_break_into_de/", "subreddit_subscribers": 102302, "created_utc": 1682357716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3w9fap27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategies for Data Quality with Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "name": "t3_12xtqww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ktdbj-H0oPXCGQ1G7Ie4PUMwq03TISRX_q1eilD5Ddw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682365177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ssmertin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ssmertin.com/articles/strategies-for-data-quality-with-apache-spark/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?auto=webp&amp;v=enabled&amp;s=eaf53de9c8a032dfa74a7e08975986102447ed13", "width": 2440, "height": 1420}, "resolutions": [{"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=540d2fcd467f8e2af55d57ac36c966d7f5bbc2fb", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55063ee95015c689e9ee437276386c5d47afc242", "width": 216, "height": 125}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50524d3c81ab61eecd37b2fa01149fa4f8362d6a", "width": 320, "height": 186}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4cf4fbbc4686d7199312c70d89744f8293ce8ea", "width": 640, "height": 372}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=843512a59bf908e1f0afd7097843e0bbe103005d", "width": 960, "height": 558}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8699a58ae68b47e11f808f83b19bf94b2331bc16", "width": 1080, "height": 628}], "variants": {}, "id": "Y2HgNpSbvchEOXTQxd0aGkItWRie2rBtSyqZXOzYwVc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12xtqww", "is_robot_indexable": true, "report_reasons": null, "author": "Practical-Ad-3928", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xtqww/strategies_for_data_quality_with_apache_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ssmertin.com/articles/strategies-for-data-quality-with-apache-spark/", "subreddit_subscribers": 102302, "created_utc": 1682365177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4adbh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Testing PySpark Applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 34, "top_awarded_type": null, "hide_score": false, "name": "t3_12ye4l1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QjLXhadh-xvOrYNnWYyVW2RRmD1MWktV_rke3is5-iU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682415084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/task-group/how-to-trust-your-data-pipelines-best-practices-for-testing-pyspark-applications-5ba05a36aa00", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?auto=webp&amp;v=enabled&amp;s=27424ecddb2e238018a2cc5020714c27f8f8b363", "width": 1170, "height": 288}, "resolutions": [{"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0513419deb358ee72bd0acb12e168240aa6a57c", "width": 108, "height": 26}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fac812cf850bae4f91a32d31f9c04efb5d70991e", "width": 216, "height": 53}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8eef8d3fa75ea5e0575d649ab41b004d7f54498c", "width": 320, "height": 78}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6396cd57992c0cd585f0ed05a217363ac0568ec2", "width": 640, "height": 157}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0654666e06f321826ebc19488fb6a9eee6cb7d3", "width": 960, "height": 236}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90fb38384969478b4d5aa3eb78e9deb1adfa1b5f", "width": 1080, "height": 265}], "variants": {}, "id": "wor6e_3wpAKnOt_wKnLAlSwcO58lUWH1czL9DxiFiX8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ye4l1", "is_robot_indexable": true, "report_reasons": null, "author": "alfet", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ye4l1/best_practices_for_testing_pyspark_applications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/task-group/how-to-trust-your-data-pipelines-best-practices-for-testing-pyspark-applications-5ba05a36aa00", "subreddit_subscribers": 102302, "created_utc": 1682415084.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello.  Soon I will start my first project as a sort of de (I dont want to  overestimate my duties, will no code I guess, use only Azure?) with  components of Azure Data Factory, Azure Synapse, Data Lake, Blob Storage  and Databricks.\n\nWe will be taking  data from on prem I guess or they will already sort of deliver it in  any format, we will be moving these into external database, I hope its  going to be Azure SQL, forget the graphical representation.\n\nI  would like to ask you for all of the tips of what to look after / for,  possible traps, obvious rookie mistakes and any protips how to handle  the project overall. Will have no help in that part (ingestion and  transformation). Currently running through documentation and some  youtube tutorials\n\nThank you kindly in advance!", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First project - tips from more experienced folks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yd8a8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682412236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.  Soon I will start my first project as a sort of de (I dont want to  overestimate my duties, will no code I guess, use only Azure?) with  components of Azure Data Factory, Azure Synapse, Data Lake, Blob Storage  and Databricks.&lt;/p&gt;\n\n&lt;p&gt;We will be taking  data from on prem I guess or they will already sort of deliver it in  any format, we will be moving these into external database, I hope its  going to be Azure SQL, forget the graphical representation.&lt;/p&gt;\n\n&lt;p&gt;I  would like to ask you for all of the tips of what to look after / for,  possible traps, obvious rookie mistakes and any protips how to handle  the project overall. Will have no help in that part (ingestion and  transformation). Currently running through documentation and some  youtube tutorials&lt;/p&gt;\n\n&lt;p&gt;Thank you kindly in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yd8a8", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yd8a8/first_project_tips_from_more_experienced_folks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yd8a8/first_project_tips_from_more_experienced_folks/", "subreddit_subscribers": 102302, "created_utc": 1682412236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've tried using springml SFTP and file transfer libraries but they are failing. The size of them will be around 2 to 3 gb", "author_fullname": "t2_4s5y1tbq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Load large XML files from SFTP server to pyspark datafram", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y7v6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682401764.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682396383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried using springml SFTP and file transfer libraries but they are failing. The size of them will be around 2 to 3 gb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y7v6o", "is_robot_indexable": true, "report_reasons": null, "author": "deadly_commander", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y7v6o/how_to_load_large_xml_files_from_sftp_server_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y7v6o/how_to_load_large_xml_files_from_sftp_server_to/", "subreddit_subscribers": 102302, "created_utc": 1682396383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I just changed teams internally in my company, and want to know which is the best practice to follow here. In my previous team, we use local development for our DEV env, then we followed usual steps of CICD, raising a PR will run pipeline on STG env and finally merging to master would run pipelines on PRD. \n\nNow in my new team, for dev we use dev server which is basically a clone of STG and PRD, just pointing to a SANDBOX database. \n\nI honestly don't mind either, using a dev server seems more \"clean\" but it's basically the same. The only problem here is that my DE team plans to open this repo to analysts and data scientists to create dbt models. In this case, the dev server env seems a bit too complicated for them to set up (exporting a bunch of credentials, setting up venv) every time, specially also because they all have windows computers. I was thinking on just using vscode devcontainers to set up an easy local dev experience for analysts and data scientists but my DE peers don't seem happy about this for some reason. They want the analysts and everyone else to do what they do to set up their dev server which honestly, it's a bit too much even for me, I could only imagine analysts reactions. \n\nAnyone has any ideas on best practices here and also best way to help analysts development of dbt models easier.", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dev server vs local development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yd3fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682411804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just changed teams internally in my company, and want to know which is the best practice to follow here. In my previous team, we use local development for our DEV env, then we followed usual steps of CICD, raising a PR will run pipeline on STG env and finally merging to master would run pipelines on PRD. &lt;/p&gt;\n\n&lt;p&gt;Now in my new team, for dev we use dev server which is basically a clone of STG and PRD, just pointing to a SANDBOX database. &lt;/p&gt;\n\n&lt;p&gt;I honestly don&amp;#39;t mind either, using a dev server seems more &amp;quot;clean&amp;quot; but it&amp;#39;s basically the same. The only problem here is that my DE team plans to open this repo to analysts and data scientists to create dbt models. In this case, the dev server env seems a bit too complicated for them to set up (exporting a bunch of credentials, setting up venv) every time, specially also because they all have windows computers. I was thinking on just using vscode devcontainers to set up an easy local dev experience for analysts and data scientists but my DE peers don&amp;#39;t seem happy about this for some reason. They want the analysts and everyone else to do what they do to set up their dev server which honestly, it&amp;#39;s a bit too much even for me, I could only imagine analysts reactions. &lt;/p&gt;\n\n&lt;p&gt;Anyone has any ideas on best practices here and also best way to help analysts development of dbt models easier.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yd3fw", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yd3fw/dev_server_vs_local_development/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yd3fw/dev_server_vs_local_development/", "subreddit_subscribers": 102302, "created_utc": 1682411804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have numerous old offline hard drives in my storage with years of my professional photo and video work backed up on them some with overlap. Is there a database program that can record file names, folder trees, and other Metadata possibly even a small thumbnail for these thousands of images and files and record what drive they are on? Would like to be able to search through the drives contents without having to plug them in unless I need to actually retrieve the file I'm looking for.", "author_fullname": "t2_3jd22sfn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Files database for offline drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y9lil", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682401162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have numerous old offline hard drives in my storage with years of my professional photo and video work backed up on them some with overlap. Is there a database program that can record file names, folder trees, and other Metadata possibly even a small thumbnail for these thousands of images and files and record what drive they are on? Would like to be able to search through the drives contents without having to plug them in unless I need to actually retrieve the file I&amp;#39;m looking for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y9lil", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrovester", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y9lil/files_database_for_offline_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y9lil/files_database_for_offline_drives/", "subreddit_subscribers": 102302, "created_utc": 1682401162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm relatively new to data engineering. We are building models which use images stored in an Azure Blob Storage Account. The volume of images could range from 300k to 900k. Should I be using spark to do image processing?\n\nI'm also open to using some other methods to make the image processing faster and more efficient. Please help if you have worked on a similar scenario. Could really use some suggestions. Thanks for any support.", "author_fullname": "t2_lpzq0jz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions to improve data processing for large volumes of images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y6d36", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682392314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m relatively new to data engineering. We are building models which use images stored in an Azure Blob Storage Account. The volume of images could range from 300k to 900k. Should I be using spark to do image processing?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also open to using some other methods to make the image processing faster and more efficient. Please help if you have worked on a similar scenario. Could really use some suggestions. Thanks for any support.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y6d36", "is_robot_indexable": true, "report_reasons": null, "author": "ProduceObjective1766", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y6d36/need_suggestions_to_improve_data_processing_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y6d36/need_suggestions_to_improve_data_processing_for/", "subreddit_subscribers": 102302, "created_utc": 1682392314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a medium-size company with some sensitive data. The data is not huge and won\u2019t grow anytime soon.\n\nWe have on-premise SQL Server and use SSIS packages to extract from our transactional DB\u2019s. We also use SSIS for transformations and load to a warehouse. This has been ok until recently because of how bad SSIS is at using web services as a source. Now we\u2019re looking at other options for ETL. Everyone on our team is comfortable with Python (and has personal/professional projects under their belt). Cloud solutions are not an option and open-core tools are preferred.\n\nMy proposal is to self-host a Prefect server in the simplest way possible. Handle ingestion with Python scripts and still use existing SSIS for the transformations when needed (just executing the packages). This would enable us to get going quickly but still allow us to expand on it with other tools in the future.\n\nDo you see any issues with this arrangement? Would you use a different orchestrator?\n\nDo you see any downsides to executing packages with Python (instead of just rebuilding transformations in Python)?\n\nWhat would you use for new transformations under this arrangement?\n\nThanks in advance for any advice/perspectives.", "author_fullname": "t2_qvk6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice for on-premise stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xh9xx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682345823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a medium-size company with some sensitive data. The data is not huge and won\u2019t grow anytime soon.&lt;/p&gt;\n\n&lt;p&gt;We have on-premise SQL Server and use SSIS packages to extract from our transactional DB\u2019s. We also use SSIS for transformations and load to a warehouse. This has been ok until recently because of how bad SSIS is at using web services as a source. Now we\u2019re looking at other options for ETL. Everyone on our team is comfortable with Python (and has personal/professional projects under their belt). Cloud solutions are not an option and open-core tools are preferred.&lt;/p&gt;\n\n&lt;p&gt;My proposal is to self-host a Prefect server in the simplest way possible. Handle ingestion with Python scripts and still use existing SSIS for the transformations when needed (just executing the packages). This would enable us to get going quickly but still allow us to expand on it with other tools in the future.&lt;/p&gt;\n\n&lt;p&gt;Do you see any issues with this arrangement? Would you use a different orchestrator?&lt;/p&gt;\n\n&lt;p&gt;Do you see any downsides to executing packages with Python (instead of just rebuilding transformations in Python)?&lt;/p&gt;\n\n&lt;p&gt;What would you use for new transformations under this arrangement?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any advice/perspectives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12xh9xx", "is_robot_indexable": true, "report_reasons": null, "author": "nnulll", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xh9xx/need_advice_for_onpremise_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xh9xx/need_advice_for_onpremise_stack/", "subreddit_subscribers": 102302, "created_utc": 1682345823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "How far are we from a company putting all the pieces together and selling an AI agent that can be installed on empty servers. They can scan the input source systems,be assigned an output, and in a matter of hours create all needed tables, etl, scheduling, monitoring, CI/CD, contenarization, orchestration and everything else we currently do.\nGPT-4 already does everything super well, but in small steps as needed. The way I see it, the scenario I just described is inevitable.\nAs a DE I've never been so confused about the future as I am today.", "author_fullname": "t2_8lszlkal", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "AI Agent to do it all", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12yfkns", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.25, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682419300.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How far are we from a company putting all the pieces together and selling an AI agent that can be installed on empty servers. They can scan the input source systems,be assigned an output, and in a matter of hours create all needed tables, etl, scheduling, monitoring, CI/CD, contenarization, orchestration and everything else we currently do.\nGPT-4 already does everything super well, but in small steps as needed. The way I see it, the scenario I just described is inevitable.\nAs a DE I&amp;#39;ve never been so confused about the future as I am today.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yfkns", "is_robot_indexable": true, "report_reasons": null, "author": "70sechoes", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yfkns/ai_agent_to_do_it_all/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yfkns/ai_agent_to_do_it_all/", "subreddit_subscribers": 102302, "created_utc": 1682419300.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When not to use SQL - Sean J Taylor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12xsak2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": "#46d160", "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When not to use SQL -  Sean J  Taylor", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "author_name": "NormConf", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/t6m9z874SLM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@normconf"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12xsak2", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NE71c6PwxV93Xv7OAGF-Dk_U6CmPWI7ySluTm_MQZIE.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682362081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/t6m9z874SLM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?auto=webp&amp;v=enabled&amp;s=cfb6853e5442024c25ac71e030cf0049a30cb99e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09a1a34fb853459ad5e6225f41b6d06a40154ec9", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b994adadf444dc5999fab2c3302bfa0e0960945", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=726ab0c5faaf3d24f9022ee50a75e98edf406191", "width": 320, "height": 240}], "variants": {}, "id": "ZNhBXxcMk4GzO9HgVXgF8OvnRRXdp8Q0b9wiiUUNzAg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12xsak2", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/12xsak2/when_not_to_use_sql_sean_j_taylor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/t6m9z874SLM", "subreddit_subscribers": 102302, "created_utc": 1682362081.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When not to use SQL -  Sean J  Taylor", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "author_name": "NormConf", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/t6m9z874SLM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@normconf"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " In this article, we\u2019ll explore how to visualize the exchange rate of Bitcoin to USD using FastAPI, Prometheus, Grafana, and Docker. We will create a simple FastAPI application to import exchange rate data from an API, store it in a database, and expose it as metrics using Prometheus. Then, we\u2019ll use Grafana to create dashboards that visualize the data, and deploy the whole setup using Docker and Jenkins \n\n&amp;#x200B;\n\nhttps://preview.redd.it/4ly1yiz9duva1.png?width=1908&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b9feb3b6aa2f06a467ab1a04abe4b74d1b77f90b\n\n[https://medium.com/@stefentaime\\_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1](https://medium.com/@stefentaime_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualizing Bitcoin to USD Exchange Rates using FastAPI, Prometheus, Grafana, Deploy with jenkins On Localhost Ubuntu Server 20.04", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4ly1yiz9duva1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 49, "x": 108, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5f38632ac81800dd66ce1f274c1d4e7ff25ea1d"}, {"y": 98, "x": 216, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1a55a40011e4ac0b5bd56023bcf7db511aff279"}, {"y": 146, "x": 320, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac2bf4f70624f6e19416845ba5df936cfbd3ca31"}, {"y": 292, "x": 640, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50266b961cbbc96922797db1ab88e0b05ac6fbca"}, {"y": 438, "x": 960, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca0560f4043f9470a40abaed0bb52519d06e50cf"}, {"y": 493, "x": 1080, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=564a2b1b4ff06ddef05cc10d9486d8e404596c53"}], "s": {"y": 871, "x": 1908, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=1908&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b9feb3b6aa2f06a467ab1a04abe4b74d1b77f90b"}, "id": "4ly1yiz9duva1"}}, "name": "t3_12xhg6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3P96OjhtaYUeU83f0Rxql-N6G2jDxCTHRmHaT2xVzQU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1682346152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this article, we\u2019ll explore how to visualize the exchange rate of Bitcoin to USD using FastAPI, Prometheus, Grafana, and Docker. We will create a simple FastAPI application to import exchange rate data from an API, store it in a database, and expose it as metrics using Prometheus. Then, we\u2019ll use Grafana to create dashboards that visualize the data, and deploy the whole setup using Docker and Jenkins &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4ly1yiz9duva1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b9feb3b6aa2f06a467ab1a04abe4b74d1b77f90b\"&gt;https://preview.redd.it/4ly1yiz9duva1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b9feb3b6aa2f06a467ab1a04abe4b74d1b77f90b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1\"&gt;https://medium.com/@stefentaime_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?auto=webp&amp;v=enabled&amp;s=ddcfcdb685931ca0bb14cd2c5a1675a026d09f3e", "width": 1200, "height": 548}, "resolutions": [{"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8641d98b45ca52920014a514e4473f80826d8fb3", "width": 108, "height": 49}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49a0caebda53c8af125a3e189f4a2be631601e48", "width": 216, "height": 98}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3fbde850dcaf378e7238abd3ab0c33f93ee8afed", "width": 320, "height": 146}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16c0eb2cf12cd8810663609fe8d99d30d4c63b8c", "width": 640, "height": 292}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f907d8f721df86fa4092358c4146374b327fe039", "width": 960, "height": 438}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b489c6310fdd90236978fdafdf20170ac4f7afaa", "width": 1080, "height": 493}], "variants": {}, "id": "pf6hn7xKOsckRNWPkeTcnBYa98kAE9DOlpcpbku2FUY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12xhg6o", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xhg6o/visualizing_bitcoin_to_usd_exchange_rates_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xhg6o/visualizing_bitcoin_to_usd_exchange_rates_using/", "subreddit_subscribers": 102302, "created_utc": 1682346152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in the process of transitioning my business from Google Drive + Microsoft Office to a modern, integrated back-end; and have a high-level understanding of modern tech stacks, but am by no means a professional. I'd like to consult with a professional to discuss system architecture from the ground up, but trying to vet data- and full-stack engineers on Upwork and Fiverr has been overwhelming due to the variability of talent (even among the 'well-rated' freelancers). \n\nWondering if there are any resources for consulting with quality DEs on system architecture, data infrastructure, recommended tech stacks, etc.?", "author_fullname": "t2_vou11bf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to find quality DE consultant for building my tech stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xvray", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682369035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the process of transitioning my business from Google Drive + Microsoft Office to a modern, integrated back-end; and have a high-level understanding of modern tech stacks, but am by no means a professional. I&amp;#39;d like to consult with a professional to discuss system architecture from the ground up, but trying to vet data- and full-stack engineers on Upwork and Fiverr has been overwhelming due to the variability of talent (even among the &amp;#39;well-rated&amp;#39; freelancers). &lt;/p&gt;\n\n&lt;p&gt;Wondering if there are any resources for consulting with quality DEs on system architecture, data infrastructure, recommended tech stacks, etc.?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xvray", "is_robot_indexable": true, "report_reasons": null, "author": "pg_archipelago", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xvray/where_to_find_quality_de_consultant_for_building/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xvray/where_to_find_quality_de_consultant_for_building/", "subreddit_subscribers": 102302, "created_utc": 1682369035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Relatively new to modern data engineering, having come from a software development background.\n\nPython is notorious of its slowness in my background, with the exception of many well optimised lower level functions.\n\nIn DE, when talking about huge quantities of data, does python\u2019s performance become a bottleneck? Is data serialisation or transformation\u2026\n\na) even performed using python at large data scale?\nb) hampered by reckless use of slower aspects of python?\n\nDISCLAIMER: Still figuring out what python\u2019s footprint in DE is (eg. Dagster), given my team has leant on low-code ETL tools before.\n\nThanks!", "author_fullname": "t2_4z0wqsio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Python a bottleneck?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y4g9n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682387686.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Relatively new to modern data engineering, having come from a software development background.&lt;/p&gt;\n\n&lt;p&gt;Python is notorious of its slowness in my background, with the exception of many well optimised lower level functions.&lt;/p&gt;\n\n&lt;p&gt;In DE, when talking about huge quantities of data, does python\u2019s performance become a bottleneck? Is data serialisation or transformation\u2026&lt;/p&gt;\n\n&lt;p&gt;a) even performed using python at large data scale?\nb) hampered by reckless use of slower aspects of python?&lt;/p&gt;\n\n&lt;p&gt;DISCLAIMER: Still figuring out what python\u2019s footprint in DE is (eg. Dagster), given my team has leant on low-code ETL tools before.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12y4g9n", "is_robot_indexable": true, "report_reasons": null, "author": "4mpig", "discussion_type": null, "num_comments": 19, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y4g9n/is_python_a_bottleneck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y4g9n/is_python_a_bottleneck/", "subreddit_subscribers": 102302, "created_utc": 1682387686.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}