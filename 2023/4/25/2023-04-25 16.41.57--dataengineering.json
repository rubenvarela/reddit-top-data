{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m building out our Databricks deployment and related DE infrastructure (new start up, greenfield). As the only DE, I\u2019m using Airbyte for raw extraction and load into our S3 data lake. \n\nI like the idea of only having to use one tool for all our DE needs. The only thing that comes to mind would be manually building out extractors to our data sources (CRMs, DBs, Tools, etc) or running  python based ETL libraries like Meltano in our notebooks. \n\nWith Databricks workflows and orchestrators, this could consolidate tooling. \n\nI will keep using airbyte as time is of the essence and the libraries help with the lift. \n\nHowever, I\u2019d love to have a discussion around projects or ideas with this type of infrastructure. \nThoughts?", "author_fullname": "t2_4rrqnyd4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Curious if anyone has adopted a stack to do raw data ingestion in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y8yu2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 25, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 25, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682399487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building out our Databricks deployment and related DE infrastructure (new start up, greenfield). As the only DE, I\u2019m using Airbyte for raw extraction and load into our S3 data lake. &lt;/p&gt;\n\n&lt;p&gt;I like the idea of only having to use one tool for all our DE needs. The only thing that comes to mind would be manually building out extractors to our data sources (CRMs, DBs, Tools, etc) or running  python based ETL libraries like Meltano in our notebooks. &lt;/p&gt;\n\n&lt;p&gt;With Databricks workflows and orchestrators, this could consolidate tooling. &lt;/p&gt;\n\n&lt;p&gt;I will keep using airbyte as time is of the essence and the libraries help with the lift. &lt;/p&gt;\n\n&lt;p&gt;However, I\u2019d love to have a discussion around projects or ideas with this type of infrastructure. \nThoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12y8yu2", "is_robot_indexable": true, "report_reasons": null, "author": "deep-data-diver", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y8yu2/curious_if_anyone_has_adopted_a_stack_to_do_raw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y8yu2/curious_if_anyone_has_adopted_a_stack_to_do_raw/", "subreddit_subscribers": 102373, "created_utc": 1682399487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I come from a math and data science background and I really enjoyed reading white papers about current trends in the industry. \n\nDoes data engineering have that? I've seen the duckdb white paper but it seems like DE has GitHub release pages and product websites instead of academic papers. \n\nIs this correct or am I missing out on an entire world of papers?", "author_fullname": "t2_cpnmh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for DE white papers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xqa8s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.96, "author_flair_background_color": null, "subreddit_type": "public", "ups": 17, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 17, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682357975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from a math and data science background and I really enjoyed reading white papers about current trends in the industry. &lt;/p&gt;\n\n&lt;p&gt;Does data engineering have that? I&amp;#39;ve seen the duckdb white paper but it seems like DE has GitHub release pages and product websites instead of academic papers. &lt;/p&gt;\n\n&lt;p&gt;Is this correct or am I missing out on an entire world of papers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12xqa8s", "is_robot_indexable": true, "report_reasons": null, "author": "DoingItForGiggles", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xqa8s/looking_for_de_white_papers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xqa8s/looking_for_de_white_papers/", "subreddit_subscribers": 102373, "created_utc": 1682357975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_20tfe7ur", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Engineering and DataOps: A Beginner's Guide to Building Data Solutions and Solving Real-World Challenges", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 84, "top_awarded_type": null, "hide_score": false, "name": "t3_12ybcqo", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "ups": 16, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 16, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/JspNOjLI_4EFW2mFS5IQe8mgrct34voOY_55DCLLYrg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682406275.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "chaosgenius.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.chaosgenius.io/blog/data-engineering-and-dataops-beginners-guide-to-building-data-solutions-and-solving-real-world-challenges/", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?auto=webp&amp;v=enabled&amp;s=0c672d9f9def968617df5e594108289933769bcc", "width": 4000, "height": 2400}, "resolutions": [{"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=94de9e67438d561dac53f6aa67f74ed81686c74a", "width": 108, "height": 64}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b39fc1e891c97928b67b768cfb2a44364d9581b", "width": 216, "height": 129}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1409b7ec75b01fb8e58450c13bb2c52986bf9b7e", "width": 320, "height": 192}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8c9ccd575d261751cc267eea527f39ba69aeff8c", "width": 640, "height": 384}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=aa23b5cc27d8fca03ea360638f9a1ac90eecd9ee", "width": 960, "height": 576}, {"url": "https://external-preview.redd.it/cHgEYqCB1Jnlcv7y8mtmmKTYsrHHtHERMcwcj_7Aogc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=98e38fea795867bfeae0e7c41c32c6ab31afa893", "width": 1080, "height": 648}], "variants": {}, "id": "eN31HKOXDX_DK4Vg5fj3aDAKHFmxW72mKecuYjio5Wo"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ybcqo", "is_robot_indexable": true, "report_reasons": null, "author": "pramit_marattha", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ybcqo/data_engineering_and_dataops_a_beginners_guide_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.chaosgenius.io/blog/data-engineering-and-dataops-beginners-guide-to-building-data-solutions-and-solving-real-world-challenges/", "subreddit_subscribers": 102373, "created_utc": 1682406275.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_4adbh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Best Practices for Testing PySpark Applications", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 34, "top_awarded_type": null, "hide_score": false, "name": "t3_12ye4l1", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 18, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 18, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/QjLXhadh-xvOrYNnWYyVW2RRmD1MWktV_rke3is5-iU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682415084.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://medium.com/task-group/how-to-trust-your-data-pipelines-best-practices-for-testing-pyspark-applications-5ba05a36aa00", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?auto=webp&amp;v=enabled&amp;s=27424ecddb2e238018a2cc5020714c27f8f8b363", "width": 1170, "height": 288}, "resolutions": [{"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f0513419deb358ee72bd0acb12e168240aa6a57c", "width": 108, "height": 26}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=fac812cf850bae4f91a32d31f9c04efb5d70991e", "width": 216, "height": 53}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8eef8d3fa75ea5e0575d649ab41b004d7f54498c", "width": 320, "height": 78}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6396cd57992c0cd585f0ed05a217363ac0568ec2", "width": 640, "height": 157}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0654666e06f321826ebc19488fb6a9eee6cb7d3", "width": 960, "height": 236}, {"url": "https://external-preview.redd.it/tPIEUyVGbws5zGkmZnToOwL1kc88VkfWjikVBEuhynI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=90fb38384969478b4d5aa3eb78e9deb1adfa1b5f", "width": 1080, "height": 265}], "variants": {}, "id": "wor6e_3wpAKnOt_wKnLAlSwcO58lUWH1czL9DxiFiX8"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12ye4l1", "is_robot_indexable": true, "report_reasons": null, "author": "alfet", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ye4l1/best_practices_for_testing_pyspark_applications/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://medium.com/task-group/how-to-trust-your-data-pipelines-best-practices-for-testing-pyspark-applications-5ba05a36aa00", "subreddit_subscribers": 102373, "created_utc": 1682415084.0, "num_crossposts": 1, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does the following architecture make sense? I'm looking for the ability to swap DWH in the future if needed and I also want some flexibility to work with fivetran/glue.\n\nData Sources (Postgres, csv, dynamo, etc) to S3 Parquet files through fivetran. I'm calling this the Raw Layer.\n\nWe can enrich the raw data with glue as needed. In the \"Enrichment Layer\".\n\nSince the files are stored as Parquet Files in S3, we could use Iceberg tables as well.\n\nFrom the Raw/Enriched we can load the Raw data so snowflake using Fivetran/Snowflake tasks.\n\nNow that the data landed in snowflake we can transform it using self hosted DBT running on fargate.  \n\n\n**TL;DR :**   \nDatasources -- fivetran --&gt; S3 (Parquet) &lt;--&gt; Enrichment Layer With Glue --&gt; Fivetran/Snowflake tasks to load Raw data into snowflake --&gt; Transform with dbt .", "author_fullname": "t2_9rv5oe9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Architecture Review", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y1d42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682380749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does the following architecture make sense? I&amp;#39;m looking for the ability to swap DWH in the future if needed and I also want some flexibility to work with fivetran/glue.&lt;/p&gt;\n\n&lt;p&gt;Data Sources (Postgres, csv, dynamo, etc) to S3 Parquet files through fivetran. I&amp;#39;m calling this the Raw Layer.&lt;/p&gt;\n\n&lt;p&gt;We can enrich the raw data with glue as needed. In the &amp;quot;Enrichment Layer&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Since the files are stored as Parquet Files in S3, we could use Iceberg tables as well.&lt;/p&gt;\n\n&lt;p&gt;From the Raw/Enriched we can load the Raw data so snowflake using Fivetran/Snowflake tasks.&lt;/p&gt;\n\n&lt;p&gt;Now that the data landed in snowflake we can transform it using self hosted DBT running on fargate.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR :&lt;/strong&gt;&lt;br/&gt;\nDatasources -- fivetran --&amp;gt; S3 (Parquet) &amp;lt;--&amp;gt; Enrichment Layer With Glue --&amp;gt; Fivetran/Snowflake tasks to load Raw data into snowflake --&amp;gt; Transform with dbt .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y1d42", "is_robot_indexable": true, "report_reasons": null, "author": "jr_acc", "discussion_type": null, "num_comments": 11, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y1d42/de_architecture_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y1d42/de_architecture_review/", "subreddit_subscribers": 102373, "created_utc": 1682380749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Hello.  Soon I will start my first project as a sort of de (I dont want to  overestimate my duties, will no code I guess, use only Azure?) with  components of Azure Data Factory, Azure Synapse, Data Lake, Blob Storage  and Databricks.\n\nWe will be taking  data from on prem I guess or they will already sort of deliver it in  any format, we will be moving these into external database, I hope its  going to be Azure SQL, forget the graphical representation.\n\nI  would like to ask you for all of the tips of what to look after / for,  possible traps, obvious rookie mistakes and any protips how to handle  the project overall. Will have no help in that part (ingestion and  transformation). Currently running through documentation and some  youtube tutorials\n\nThank you kindly in advance!", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First project - tips from more experienced folks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yd8a8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682412236.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello.  Soon I will start my first project as a sort of de (I dont want to  overestimate my duties, will no code I guess, use only Azure?) with  components of Azure Data Factory, Azure Synapse, Data Lake, Blob Storage  and Databricks.&lt;/p&gt;\n\n&lt;p&gt;We will be taking  data from on prem I guess or they will already sort of deliver it in  any format, we will be moving these into external database, I hope its  going to be Azure SQL, forget the graphical representation.&lt;/p&gt;\n\n&lt;p&gt;I  would like to ask you for all of the tips of what to look after / for,  possible traps, obvious rookie mistakes and any protips how to handle  the project overall. Will have no help in that part (ingestion and  transformation). Currently running through documentation and some  youtube tutorials&lt;/p&gt;\n\n&lt;p&gt;Thank you kindly in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yd8a8", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yd8a8/first_project_tips_from_more_experienced_folks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yd8a8/first_project_tips_from_more_experienced_folks/", "subreddit_subscribers": 102373, "created_utc": 1682412236.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering! I\u2019m finishing up my 3rd year of college and had no luck landing DE/SWE or any other data related internship this year due to uncertainty in the job market. I did, however, end up with DevOps Engineer internship opportunity, which I took since I have no prior internship experience. The tech stack will be Python, AWS, Terraform, Docker and potentially SQL (will have to clarify that with my team). I\u2019m not sure it\u2019s DevOps is for me (maybe it will be) since my goal was always Data Engineering since I like working with data and mess around with cloud services. Therefore, I\u2019m curious, will DevOps internship help me with my Data Engineer job search when I look for new grad jobs? Thanks!", "author_fullname": "t2_mwot5ip2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will this help me break into DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xq5nf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682357972.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682357716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! I\u2019m finishing up my 3rd year of college and had no luck landing DE/SWE or any other data related internship this year due to uncertainty in the job market. I did, however, end up with DevOps Engineer internship opportunity, which I took since I have no prior internship experience. The tech stack will be Python, AWS, Terraform, Docker and potentially SQL (will have to clarify that with my team). I\u2019m not sure it\u2019s DevOps is for me (maybe it will be) since my goal was always Data Engineering since I like working with data and mess around with cloud services. Therefore, I\u2019m curious, will DevOps internship help me with my Data Engineer job search when I look for new grad jobs? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12xq5nf", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting_Chard138", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xq5nf/will_this_help_me_break_into_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xq5nf/will_this_help_me_break_into_de/", "subreddit_subscribers": 102373, "created_utc": 1682357716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today from an X company approached me for a freelance data engineer. I have no degree in CS and it was going to be my first interview. They asked me how much rate I am looking to pay around and I said 130 euros per day. Now I can't reach them. Is it too much? What should I say?\n\nEdit: Company in the Netherlands I am in Poland", "author_fullname": "t2_2vc85g2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xokfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.71, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682354397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today from an X company approached me for a freelance data engineer. I have no degree in CS and it was going to be my first interview. They asked me how much rate I am looking to pay around and I said 130 euros per day. Now I can&amp;#39;t reach them. Is it too much? What should I say?&lt;/p&gt;\n\n&lt;p&gt;Edit: Company in the Netherlands I am in Poland&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12xokfb", "is_robot_indexable": true, "report_reasons": null, "author": "Blazey25", "discussion_type": null, "num_comments": 30, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xokfb/i_need_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xokfb/i_need_help/", "subreddit_subscribers": 102373, "created_utc": 1682354397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3w9fap27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategies for Data Quality with Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "name": "t3_12xtqww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ktdbj-H0oPXCGQ1G7Ie4PUMwq03TISRX_q1eilD5Ddw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682365177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ssmertin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ssmertin.com/articles/strategies-for-data-quality-with-apache-spark/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?auto=webp&amp;v=enabled&amp;s=eaf53de9c8a032dfa74a7e08975986102447ed13", "width": 2440, "height": 1420}, "resolutions": [{"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=540d2fcd467f8e2af55d57ac36c966d7f5bbc2fb", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55063ee95015c689e9ee437276386c5d47afc242", "width": 216, "height": 125}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50524d3c81ab61eecd37b2fa01149fa4f8362d6a", "width": 320, "height": 186}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4cf4fbbc4686d7199312c70d89744f8293ce8ea", "width": 640, "height": 372}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=843512a59bf908e1f0afd7097843e0bbe103005d", "width": 960, "height": 558}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8699a58ae68b47e11f808f83b19bf94b2331bc16", "width": 1080, "height": 628}], "variants": {}, "id": "Y2HgNpSbvchEOXTQxd0aGkItWRie2rBtSyqZXOzYwVc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12xtqww", "is_robot_indexable": true, "report_reasons": null, "author": "Practical-Ad-3928", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xtqww/strategies_for_data_quality_with_apache_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ssmertin.com/articles/strategies-for-data-quality-with-apache-spark/", "subreddit_subscribers": 102373, "created_utc": 1682365177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I currently work with a colleague who I swear to fucking god has the worst practices ever.  The stuff they do should be considered war crimes (in the data world, of course).\n\nThe person in question has never done CI/CD before and has been copying and pasting blocks of code into the next environment when they want to \"promote\" a change.  Naturally, this has left environments being massively out of sync.  \n\nSince then, we've implemented CI/CD to move database meta data and schemas between environments.  It's consistent, it works, it's better than copying and pasting.  The person in question, however, always rejects the idea of CI/CD with the following talking point:\n\nThem: \"We can't guarantee consistency between environments\"\n\nMe: \"We can because that's sorted through CI/CD\"\n\nThem: \"Yeah, but when I want to make emergency changes to production, it won't align\"\n\nMe: \"But why are we making changes in production directly? They'll get erased next time we release because they don't exist in the previous environments.  For us to be consistent, we have to be in the mindset of working through environments\"\n\nThem: \"I don't want to have to go through every single environment to make a single change\"\n\nThey will then continue to argue forever.  For some strange reason, this happens a lot and I feel like I'm going insane because I feel like it shouldn't be up for debate.  I feel like the person I'm arguing with wants to patch everything, I want to do everything correctly.  They want to apply ad hoc fixes as and when they feel like it, I want to be consistent so when something breaks we can test it at a lower level before releasing it. \n\nI'm 100% ready to be wrong.  Do people regularly makes production only changes?  Am I misunderstanding the whole point of CI/CD? How do you guys facilitate emergency changes with CI/CD?\n\nThank you.", "author_fullname": "t2_anttcncw", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How should production changes be handled? Rant/debate", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yj6uv", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.88, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682428694.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently work with a colleague who I swear to fucking god has the worst practices ever.  The stuff they do should be considered war crimes (in the data world, of course).&lt;/p&gt;\n\n&lt;p&gt;The person in question has never done CI/CD before and has been copying and pasting blocks of code into the next environment when they want to &amp;quot;promote&amp;quot; a change.  Naturally, this has left environments being massively out of sync.  &lt;/p&gt;\n\n&lt;p&gt;Since then, we&amp;#39;ve implemented CI/CD to move database meta data and schemas between environments.  It&amp;#39;s consistent, it works, it&amp;#39;s better than copying and pasting.  The person in question, however, always rejects the idea of CI/CD with the following talking point:&lt;/p&gt;\n\n&lt;p&gt;Them: &amp;quot;We can&amp;#39;t guarantee consistency between environments&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Me: &amp;quot;We can because that&amp;#39;s sorted through CI/CD&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Them: &amp;quot;Yeah, but when I want to make emergency changes to production, it won&amp;#39;t align&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Me: &amp;quot;But why are we making changes in production directly? They&amp;#39;ll get erased next time we release because they don&amp;#39;t exist in the previous environments.  For us to be consistent, we have to be in the mindset of working through environments&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Them: &amp;quot;I don&amp;#39;t want to have to go through every single environment to make a single change&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;They will then continue to argue forever.  For some strange reason, this happens a lot and I feel like I&amp;#39;m going insane because I feel like it shouldn&amp;#39;t be up for debate.  I feel like the person I&amp;#39;m arguing with wants to patch everything, I want to do everything correctly.  They want to apply ad hoc fixes as and when they feel like it, I want to be consistent so when something breaks we can test it at a lower level before releasing it. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m 100% ready to be wrong.  Do people regularly makes production only changes?  Am I misunderstanding the whole point of CI/CD? How do you guys facilitate emergency changes with CI/CD?&lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yj6uv", "is_robot_indexable": true, "report_reasons": null, "author": "average_ukpf_user", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yj6uv/how_should_production_changes_be_handled/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yj6uv/how_should_production_changes_be_handled/", "subreddit_subscribers": 102373, "created_utc": 1682428694.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all, \nI have started a new role recently where I am at a large legacy manufacturing company that is trying to get into analytics (lol). I was hired to do data engineering but it seems like most of what i will be doing to start off is working on getting their data policies in order. They have been using SAP for 30 years and are trying to modernize somewhat.  Currently at their plants they have analytics on site that are running based on production data and from my view those are all fine. However in their administration building is where they need a shitload of work. Basically they have always had web devs or SAP devs and never really had anyone in admin who was a DBA or trained in that regard. From just my first month i have a few things that I know need to change and was wondering if anyone had advice.\n\n1. Every project was made as its own database. Is there a way to quickly go back and put all of these projects into one database but under different schemas? To put it in perspective one of their sql servers has 55 databases.\n\n2. How do you get people to buy into data governance as a policy when it really hasnt been enforced before? Right now everyone has access to everything and its kind of a wild west. \n\n3. They currently have a few people who are very good at getting data out of SAP BW for queries. However this creates problems because the data analytics people arent really SAP trained and the data gets lost in translation over time, not to mention the bottleneck of always having to ask one of the couple people who understand SAP BW. Does anyone have experience in creating an analytics database in a third party program off of an SAP ERP system? There is really no getting rid of SAP in any capacity when its been used this long.", "author_fullname": "t2_6ra3o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Advice for new role", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yl02m", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682432808.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all, \nI have started a new role recently where I am at a large legacy manufacturing company that is trying to get into analytics (lol). I was hired to do data engineering but it seems like most of what i will be doing to start off is working on getting their data policies in order. They have been using SAP for 30 years and are trying to modernize somewhat.  Currently at their plants they have analytics on site that are running based on production data and from my view those are all fine. However in their administration building is where they need a shitload of work. Basically they have always had web devs or SAP devs and never really had anyone in admin who was a DBA or trained in that regard. From just my first month i have a few things that I know need to change and was wondering if anyone had advice.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Every project was made as its own database. Is there a way to quickly go back and put all of these projects into one database but under different schemas? To put it in perspective one of their sql servers has 55 databases.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How do you get people to buy into data governance as a policy when it really hasnt been enforced before? Right now everyone has access to everything and its kind of a wild west. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;They currently have a few people who are very good at getting data out of SAP BW for queries. However this creates problems because the data analytics people arent really SAP trained and the data gets lost in translation over time, not to mention the bottleneck of always having to ask one of the couple people who understand SAP BW. Does anyone have experience in creating an analytics database in a third party program off of an SAP ERP system? There is really no getting rid of SAP in any capacity when its been used this long.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yl02m", "is_robot_indexable": true, "report_reasons": null, "author": "deemerritt", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yl02m/advice_for_new_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yl02m/advice_for_new_role/", "subreddit_subscribers": 102373, "created_utc": 1682432808.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "We're looking at stuff like Synapse/trino/presto for the engine but for now we're just loading raw json + converting to parquet.\n\nIt's time series data so it partitions nicely along an hourly cadence. This creates files that are about 100Mb. We are about to expand by quite a lot (acquisition) so we think this will ~5x soon enough. \n\nFrom those raw json (500Mb) files we can convert to parquet for the query layer but unsure of how big we ought to go here. We can leave as is and rely on parquets/compression efficiency or compact several hours into a single file. \n\nDon't have a lot of know-how for this so we appreciate any tips. Also, we know a simple set up with a DB might do for this volume of data but the chosen paradigm for the company is data lake + query engine, and our department is just the first to build it out. Thanks.", "author_fullname": "t2_ut20i32q", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How big are your data lake raw files? Parquet files?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yknp3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682432040.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re looking at stuff like Synapse/trino/presto for the engine but for now we&amp;#39;re just loading raw json + converting to parquet.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s time series data so it partitions nicely along an hourly cadence. This creates files that are about 100Mb. We are about to expand by quite a lot (acquisition) so we think this will ~5x soon enough. &lt;/p&gt;\n\n&lt;p&gt;From those raw json (500Mb) files we can convert to parquet for the query layer but unsure of how big we ought to go here. We can leave as is and rely on parquets/compression efficiency or compact several hours into a single file. &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t have a lot of know-how for this so we appreciate any tips. Also, we know a simple set up with a DB might do for this volume of data but the chosen paradigm for the company is data lake + query engine, and our department is just the first to build it out. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yknp3", "is_robot_indexable": true, "report_reasons": null, "author": "alex_o_h", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yknp3/how_big_are_your_data_lake_raw_files_parquet_files/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yknp3/how_big_are_your_data_lake_raw_files_parquet_files/", "subreddit_subscribers": 102373, "created_utc": 1682432040.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I recently interviewed for a role and the hiring manager mentioned there would be system-design interview questions. I have traditionally been more of an analytically focused DE (close to an analytics engineer role) and have little experience with system design questions. Does anyone have advice on where I should start for interview prep? Is going through Grokking the System Design Interview enough? Does anyone have any experience ramping up on this topic within 3 weeks? For context, I am still early in my career, so this would likely be a junior-level role since I have less than 3 YOE.", "author_fullname": "t2_6wqif47u", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Interview Prep Advice - System Design - Where to start?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yk9r6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682431175.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently interviewed for a role and the hiring manager mentioned there would be system-design interview questions. I have traditionally been more of an analytically focused DE (close to an analytics engineer role) and have little experience with system design questions. Does anyone have advice on where I should start for interview prep? Is going through Grokking the System Design Interview enough? Does anyone have any experience ramping up on this topic within 3 weeks? For context, I am still early in my career, so this would likely be a junior-level role since I have less than 3 YOE.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12yk9r6", "is_robot_indexable": true, "report_reasons": null, "author": "tagavor_", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yk9r6/interview_prep_advice_system_design_where_to_start/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yk9r6/interview_prep_advice_system_design_where_to_start/", "subreddit_subscribers": 102373, "created_utc": 1682431175.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have a bunch of sites across the country collecting data.\n\nEach piece of equipment has a battery and a .txt file recording the battery level every minute, appending the record to the site .txt file. So, a file that is growing one record every minute.\n\nThe equipment is managed by an external company, so i don't have control to change the way the .txt add records.\n\nMy team want to write this data to a database (BigQuery) and i can't think of an efficient way to do this other than searching and deleting all the records for the site and re-writing them to the database. As each file may contains 200k records and doing a query to write only new records will, i guess, be slow.\n\nAny suggestions or am patterns i could use? And suggestions would be awesome. Thanks :-)", "author_fullname": "t2_qd6ssd6j", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What's an efficient write to DB a file that's continuously being appended to?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yjk8d", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682429548.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a bunch of sites across the country collecting data.&lt;/p&gt;\n\n&lt;p&gt;Each piece of equipment has a battery and a .txt file recording the battery level every minute, appending the record to the site .txt file. So, a file that is growing one record every minute.&lt;/p&gt;\n\n&lt;p&gt;The equipment is managed by an external company, so i don&amp;#39;t have control to change the way the .txt add records.&lt;/p&gt;\n\n&lt;p&gt;My team want to write this data to a database (BigQuery) and i can&amp;#39;t think of an efficient way to do this other than searching and deleting all the records for the site and re-writing them to the database. As each file may contains 200k records and doing a query to write only new records will, i guess, be slow.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or am patterns i could use? And suggestions would be awesome. Thanks :-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12yjk8d", "is_robot_indexable": true, "report_reasons": null, "author": "Careful-Doughnut-59", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yjk8d/whats_an_efficient_write_to_db_a_file_thats/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yjk8d/whats_an_efficient_write_to_db_a_file_thats/", "subreddit_subscribers": 102373, "created_utc": 1682429548.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I just changed teams internally in my company, and want to know which is the best practice to follow here. In my previous team, we use local development for our DEV env, then we followed usual steps of CICD, raising a PR will run pipeline on STG env and finally merging to master would run pipelines on PRD. \n\nNow in my new team, for dev we use dev server which is basically a clone of STG and PRD, just pointing to a SANDBOX database. \n\nI honestly don't mind either, using a dev server seems more \"clean\" but it's basically the same. The only problem here is that my DE team plans to open this repo to analysts and data scientists to create dbt models. In this case, the dev server env seems a bit too complicated for them to set up (exporting a bunch of credentials, setting up venv) every time, specially also because they all have windows computers. I was thinking on just using vscode devcontainers to set up an easy local dev experience for analysts and data scientists but my DE peers don't seem happy about this for some reason. They want the analysts and everyone else to do what they do to set up their dev server which honestly, it's a bit too much even for me, I could only imagine analysts reactions. \n\nAnyone has any ideas on best practices here and also best way to help analysts development of dbt models easier.", "author_fullname": "t2_4s2dogl9", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "dev server vs local development", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yd3fw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682411804.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I just changed teams internally in my company, and want to know which is the best practice to follow here. In my previous team, we use local development for our DEV env, then we followed usual steps of CICD, raising a PR will run pipeline on STG env and finally merging to master would run pipelines on PRD. &lt;/p&gt;\n\n&lt;p&gt;Now in my new team, for dev we use dev server which is basically a clone of STG and PRD, just pointing to a SANDBOX database. &lt;/p&gt;\n\n&lt;p&gt;I honestly don&amp;#39;t mind either, using a dev server seems more &amp;quot;clean&amp;quot; but it&amp;#39;s basically the same. The only problem here is that my DE team plans to open this repo to analysts and data scientists to create dbt models. In this case, the dev server env seems a bit too complicated for them to set up (exporting a bunch of credentials, setting up venv) every time, specially also because they all have windows computers. I was thinking on just using vscode devcontainers to set up an easy local dev experience for analysts and data scientists but my DE peers don&amp;#39;t seem happy about this for some reason. They want the analysts and everyone else to do what they do to set up their dev server which honestly, it&amp;#39;s a bit too much even for me, I could only imagine analysts reactions. &lt;/p&gt;\n\n&lt;p&gt;Anyone has any ideas on best practices here and also best way to help analysts development of dbt models easier.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12yd3fw", "is_robot_indexable": true, "report_reasons": null, "author": "rudboi12", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yd3fw/dev_server_vs_local_development/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yd3fw/dev_server_vs_local_development/", "subreddit_subscribers": 102373, "created_utc": 1682411804.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've tried using springml SFTP and file transfer libraries but they are failing. The size of them will be around 2 to 3 gb", "author_fullname": "t2_4s5y1tbq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Load large XML files from SFTP server to pyspark datafram", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y7v6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682401764.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682396383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried using springml SFTP and file transfer libraries but they are failing. The size of them will be around 2 to 3 gb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y7v6o", "is_robot_indexable": true, "report_reasons": null, "author": "deadly_commander", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y7v6o/how_to_load_large_xml_files_from_sftp_server_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y7v6o/how_to_load_large_xml_files_from_sftp_server_to/", "subreddit_subscribers": 102373, "created_utc": 1682396383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Morning guys,\n\nI\u2019m not very knowledgeable on CS and needed all of your opinions/expertise:\n\nI have a grad course or two that requires a windows OS. I only have a MacBook (M2 max) and a windows PC (AMD/GTX3070).\n\nI don\u2019t want to purchase a windows laptop just for this course. Is remoting into my home desktop a terrible idea? What are the setbacks? \n\n(I\u2019ve heard internet speed could be a bottle neck. But honestly, what speed would I want for this? I\u2019m looking for specifics so I can be the most prepared for this)", "author_fullname": "t2_3pqlnc5y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "RDP MacBook to Windows PC", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12yhkpc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682424757.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Morning guys,&lt;/p&gt;\n\n&lt;p&gt;I\u2019m not very knowledgeable on CS and needed all of your opinions/expertise:&lt;/p&gt;\n\n&lt;p&gt;I have a grad course or two that requires a windows OS. I only have a MacBook (M2 max) and a windows PC (AMD/GTX3070).&lt;/p&gt;\n\n&lt;p&gt;I don\u2019t want to purchase a windows laptop just for this course. Is remoting into my home desktop a terrible idea? What are the setbacks? &lt;/p&gt;\n\n&lt;p&gt;(I\u2019ve heard internet speed could be a bottle neck. But honestly, what speed would I want for this? I\u2019m looking for specifics so I can be the most prepared for this)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12yhkpc", "is_robot_indexable": true, "report_reasons": null, "author": "RealTrashyC", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yhkpc/rdp_macbook_to_windows_pc/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yhkpc/rdp_macbook_to_windows_pc/", "subreddit_subscribers": 102373, "created_utc": 1682424757.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm relatively new to data engineering. We are building models which use images stored in an Azure Blob Storage Account. The volume of images could range from 300k to 900k. Should I be using spark to do image processing?\n\nI'm also open to using some other methods to make the image processing faster and more efficient. Please help if you have worked on a similar scenario. Could really use some suggestions. Thanks for any support.", "author_fullname": "t2_lpzq0jz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions to improve data processing for large volumes of images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y6d36", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682392314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m relatively new to data engineering. We are building models which use images stored in an Azure Blob Storage Account. The volume of images could range from 300k to 900k. Should I be using spark to do image processing?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also open to using some other methods to make the image processing faster and more efficient. Please help if you have worked on a similar scenario. Could really use some suggestions. Thanks for any support.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y6d36", "is_robot_indexable": true, "report_reasons": null, "author": "ProduceObjective1766", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y6d36/need_suggestions_to_improve_data_processing_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y6d36/need_suggestions_to_improve_data_processing_for/", "subreddit_subscribers": 102373, "created_utc": 1682392314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When not to use SQL - Sean J Taylor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12xsak2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": "#46d160", "ups": 1, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When not to use SQL -  Sean J  Taylor", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "author_name": "NormConf", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/t6m9z874SLM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@normconf"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12xsak2", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NE71c6PwxV93Xv7OAGF-Dk_U6CmPWI7ySluTm_MQZIE.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682362081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/t6m9z874SLM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?auto=webp&amp;v=enabled&amp;s=cfb6853e5442024c25ac71e030cf0049a30cb99e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09a1a34fb853459ad5e6225f41b6d06a40154ec9", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b994adadf444dc5999fab2c3302bfa0e0960945", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=726ab0c5faaf3d24f9022ee50a75e98edf406191", "width": 320, "height": 240}], "variants": {}, "id": "ZNhBXxcMk4GzO9HgVXgF8OvnRRXdp8Q0b9wiiUUNzAg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12xsak2", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/12xsak2/when_not_to_use_sql_sean_j_taylor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/t6m9z874SLM", "subreddit_subscribers": 102373, "created_utc": 1682362081.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When not to use SQL -  Sean J  Taylor", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "author_name": "NormConf", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/t6m9z874SLM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@normconf"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "It lets data engineers automate documentation, find what they\u2019re looking for, and answer any data question in plain english. Take the grunt work out of your day-to-day.\n\nYou can have it:\n\n* Tell you what things mean across your database, dashboards, metrics, queries etc.\n* Ask it to write queries to certain tables and explain queries\n* Tell you what are relationships between tables for natural language lineage analysis\n* Write dbt / airflow / sql / LookML code with context about your data stack\n* Tell you additional context about your data (ie. what has PII, what is owned by who etc.)\n* Summarize docs, questions and dictionary terms across your assets.\n* Helping you find the right asset through any search method:\n* \u201cHelp me find the best table to calculate MRR\u201d\n* \u201cHelp me find the best dashboard to view customer churn\u201d\n* Summarize docs / tables / dictionary terms / tables\n\nMore videos and example use cases here [https://www.secoda.co/blog/transforming-data-discovery-using-secoda-ai](https://www.secoda.co/blog/transforming-data-discovery-using-secoda-ai)\n\n[ChatGPT for your data stack](https://reddit.com/link/12yljn3/video/j89dl181m1wa1/player)", "author_fullname": "t2_ek1nknwy", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "TLDR: we launched ChatGPT for your metadata and it\u2019s free to use", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": true, "media_metadata": {"j89dl181m1wa1": {"status": "valid", "e": "RedditVideo", "dashUrl": "https://v.redd.it/link/12yljn3/asset/j89dl181m1wa1/DASHPlaylist.mpd?a=1685032917%2CYTlmYTU2MWQzY2NmNmFlODg0YzhlZmRjZTI3MTljMTk2YTVlYzI2YjZjYjA4NjM1MmUzNzlhNDBlYmQwMmM1Ng%3D%3D&amp;v=1&amp;f=sd", "x": 1152, "y": 720, "hlsUrl": "https://v.redd.it/link/12yljn3/asset/j89dl181m1wa1/HLSPlaylist.m3u8?a=1685032917%2CZTc1YmM5ZmU1OWYzZjc1MjRiMGY0MjE2MDMxNzMwODU4NjBmNzYxZjM3YWM5M2Q5MjhkMzI5ODNjMmRkMDA2OQ%3D%3D&amp;v=1&amp;f=sd", "id": "j89dl181m1wa1", "isGif": false}}, "name": "t3_12yljn3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.33, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/IQfjLqMmbbXFkcpZ1RkHU3l-l9Q8bx_HDhWFHSL4JSg.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1682434004.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It lets data engineers automate documentation, find what they\u2019re looking for, and answer any data question in plain english. Take the grunt work out of your day-to-day.&lt;/p&gt;\n\n&lt;p&gt;You can have it:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Tell you what things mean across your database, dashboards, metrics, queries etc.&lt;/li&gt;\n&lt;li&gt;Ask it to write queries to certain tables and explain queries&lt;/li&gt;\n&lt;li&gt;Tell you what are relationships between tables for natural language lineage analysis&lt;/li&gt;\n&lt;li&gt;Write dbt / airflow / sql / LookML code with context about your data stack&lt;/li&gt;\n&lt;li&gt;Tell you additional context about your data (ie. what has PII, what is owned by who etc.)&lt;/li&gt;\n&lt;li&gt;Summarize docs, questions and dictionary terms across your assets.&lt;/li&gt;\n&lt;li&gt;Helping you find the right asset through any search method:&lt;/li&gt;\n&lt;li&gt;\u201cHelp me find the best table to calculate MRR\u201d&lt;/li&gt;\n&lt;li&gt;\u201cHelp me find the best dashboard to view customer churn\u201d&lt;/li&gt;\n&lt;li&gt;Summarize docs / tables / dictionary terms / tables&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;More videos and example use cases here &lt;a href=\"https://www.secoda.co/blog/transforming-data-discovery-using-secoda-ai\"&gt;https://www.secoda.co/blog/transforming-data-discovery-using-secoda-ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://reddit.com/link/12yljn3/video/j89dl181m1wa1/player\"&gt;ChatGPT for your data stack&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/4bqSio6vZqaxdgicgjqL9C76302vVS35s10ezP7Gmbc.jpg?auto=webp&amp;v=enabled&amp;s=65cb088f5011a9103da975f4510a6f33e4583425", "width": 1200, "height": 600}, "resolutions": [{"url": "https://external-preview.redd.it/4bqSio6vZqaxdgicgjqL9C76302vVS35s10ezP7Gmbc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=609a323574c080603708928f111797d8f3e448ec", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/4bqSio6vZqaxdgicgjqL9C76302vVS35s10ezP7Gmbc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=318502790e3fab14bad246f450598f462e4a0874", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/4bqSio6vZqaxdgicgjqL9C76302vVS35s10ezP7Gmbc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=9ccacc8fd5d21970397776e2b82592fa77888a98", "width": 320, "height": 160}, {"url": "https://external-preview.redd.it/4bqSio6vZqaxdgicgjqL9C76302vVS35s10ezP7Gmbc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d8afac531c3798757e0d5a4ba8f77ed3b74b7de9", "width": 640, "height": 320}, {"url": "https://external-preview.redd.it/4bqSio6vZqaxdgicgjqL9C76302vVS35s10ezP7Gmbc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=42e0d7f4ef5aa771f544bc7d685561ce20f16f4a", "width": 960, "height": 480}, {"url": "https://external-preview.redd.it/4bqSio6vZqaxdgicgjqL9C76302vVS35s10ezP7Gmbc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=39d79a85fd1683dd710bed68e852dc627e90fcf0", "width": 1080, "height": 540}], "variants": {}, "id": "dHysJ_BOvCew3y4t-Ncfne1TGx8TOcRIJM5pgTMT1ac"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12yljn3", "is_robot_indexable": true, "report_reasons": null, "author": "MissionAd7864", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12yljn3/tldr_we_launched_chatgpt_for_your_metadata_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12yljn3/tldr_we_launched_chatgpt_for_your_metadata_and/", "subreddit_subscribers": 102373, "created_utc": 1682434004.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Relatively new to modern data engineering, having come from a software development background.\n\nPython is notorious of its slowness in my background, with the exception of many well optimised lower level functions.\n\nIn DE, when talking about huge quantities of data, does python\u2019s performance become a bottleneck? Is data serialisation or transformation\u2026\n\na) even performed using python at large data scale?\nb) hampered by reckless use of slower aspects of python?\n\nDISCLAIMER: Still figuring out what python\u2019s footprint in DE is (eg. Dagster), given my team has leant on low-code ETL tools before.\n\nThanks!", "author_fullname": "t2_4z0wqsio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Python a bottleneck?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y4g9n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682387686.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Relatively new to modern data engineering, having come from a software development background.&lt;/p&gt;\n\n&lt;p&gt;Python is notorious of its slowness in my background, with the exception of many well optimised lower level functions.&lt;/p&gt;\n\n&lt;p&gt;In DE, when talking about huge quantities of data, does python\u2019s performance become a bottleneck? Is data serialisation or transformation\u2026&lt;/p&gt;\n\n&lt;p&gt;a) even performed using python at large data scale?\nb) hampered by reckless use of slower aspects of python?&lt;/p&gt;\n\n&lt;p&gt;DISCLAIMER: Still figuring out what python\u2019s footprint in DE is (eg. Dagster), given my team has leant on low-code ETL tools before.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12y4g9n", "is_robot_indexable": true, "report_reasons": null, "author": "4mpig", "discussion_type": null, "num_comments": 23, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y4g9n/is_python_a_bottleneck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y4g9n/is_python_a_bottleneck/", "subreddit_subscribers": 102373, "created_utc": 1682387686.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in the process of transitioning my business from Google Drive + Microsoft Office to a modern, integrated back-end; and have a high-level understanding of modern tech stacks, but am by no means a professional. I'd like to consult with a professional to discuss system architecture from the ground up, but trying to vet data- and full-stack engineers on Upwork and Fiverr has been overwhelming due to the variability of talent (even among the 'well-rated' freelancers). \n\nWondering if there are any resources for consulting with quality DEs on system architecture, data infrastructure, recommended tech stacks, etc.?", "author_fullname": "t2_vou11bf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to find quality DE consultant for building my tech stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xvray", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.4, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682369035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the process of transitioning my business from Google Drive + Microsoft Office to a modern, integrated back-end; and have a high-level understanding of modern tech stacks, but am by no means a professional. I&amp;#39;d like to consult with a professional to discuss system architecture from the ground up, but trying to vet data- and full-stack engineers on Upwork and Fiverr has been overwhelming due to the variability of talent (even among the &amp;#39;well-rated&amp;#39; freelancers). &lt;/p&gt;\n\n&lt;p&gt;Wondering if there are any resources for consulting with quality DEs on system architecture, data infrastructure, recommended tech stacks, etc.?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xvray", "is_robot_indexable": true, "report_reasons": null, "author": "pg_archipelago", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xvray/where_to_find_quality_de_consultant_for_building/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xvray/where_to_find_quality_de_consultant_for_building/", "subreddit_subscribers": 102373, "created_utc": 1682369035.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}