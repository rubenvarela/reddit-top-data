{"kind": "Listing", "data": {"after": null, "dist": 22, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello guys, I have 6+ years of exp in data engineering. Started with sql and python, moved to hadoop ecosystem , learned and worked on Spark (on premise), Also worked on AWS not much though, used big query and GCS for a project.  \nNow I feel anxious by never ending technology stack. When you start getting deeper into one, something new comes up.    \nFor jobs, everyone wants the expertise on that new tech like, specific cloud service, snowflake, databricks, dbt, kafka etc. I know what there tools are and what they do but not in detail since haven't work on these.   \nA lot of time I feel imposter syndrome, jack of all master of none, like I wont be able to do it long enough, And Its impacting my mental health.  \nNeed honest advice and how you guys think about it.  \n.\n\nI thought I may not be the only one facing this issue, may be worth to have a discussion.", "author_fullname": "t2_u1vbo568", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Getting overwhelmed by wide and ever-changing tech stack.", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x895j", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.97, "author_flair_background_color": null, "subreddit_type": "public", "ups": 147, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 147, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682323806.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I have 6+ years of exp in data engineering. Started with sql and python, moved to hadoop ecosystem , learned and worked on Spark (on premise), Also worked on AWS not much though, used big query and GCS for a project.&lt;br/&gt;\nNow I feel anxious by never ending technology stack. When you start getting deeper into one, something new comes up.&lt;br/&gt;\nFor jobs, everyone wants the expertise on that new tech like, specific cloud service, snowflake, databricks, dbt, kafka etc. I know what there tools are and what they do but not in detail since haven&amp;#39;t work on these.&lt;br/&gt;\nA lot of time I feel imposter syndrome, jack of all master of none, like I wont be able to do it long enough, And Its impacting my mental health.&lt;br/&gt;\nNeed honest advice and how you guys think about it.&lt;br/&gt;\n.&lt;/p&gt;\n\n&lt;p&gt;I thought I may not be the only one facing this issue, may be worth to have a discussion.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12x895j", "is_robot_indexable": true, "report_reasons": null, "author": "manu13891", "discussion_type": null, "num_comments": 39, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12x895j/getting_overwhelmed_by_wide_and_everchanging_tech/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12x895j/getting_overwhelmed_by_wide_and_everchanging_tech/", "subreddit_subscribers": 102237, "created_utc": 1682323806.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "This is quite poor practice, no? I received a job offer from a health tech company and I\u2019d be working on pipelines touching PHI data. They do not supply a laptop and I\u2019d have to use a personal device. \n\nI\u2019m quite surprised in general that a tech company wouldn\u2019t provide computers but especially shocked that a health tech company would not. Just want to sanity check that this is unacceptable, or if I\u2019m missing something (coming from a bigger company and looking for a startup role so I know there will be changes, but was not expecting this).", "author_fullname": "t2_odulp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Personal laptop required at health tech company?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xfvnc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.91, "author_flair_background_color": null, "subreddit_type": "public", "ups": 38, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 38, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682343029.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is quite poor practice, no? I received a job offer from a health tech company and I\u2019d be working on pipelines touching PHI data. They do not supply a laptop and I\u2019d have to use a personal device. &lt;/p&gt;\n\n&lt;p&gt;I\u2019m quite surprised in general that a tech company wouldn\u2019t provide computers but especially shocked that a health tech company would not. Just want to sanity check that this is unacceptable, or if I\u2019m missing something (coming from a bigger company and looking for a startup role so I know there will be changes, but was not expecting this).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12xfvnc", "is_robot_indexable": true, "report_reasons": null, "author": "ihaveanideer", "discussion_type": null, "num_comments": 38, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xfvnc/personal_laptop_required_at_health_tech_company/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xfvnc/personal_laptop_required_at_health_tech_company/", "subreddit_subscribers": 102237, "created_utc": 1682343029.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've been writing a lot of SQL lately but I get errors like mismatching no. of columns in a row during an insert more so because I'm dealing with huge tables and the editors never show me an error like that. I understand that it's like a compile time error but is there any workaround that you guys use? TIA.", "author_fullname": "t2_q0hjyvx4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What editors do you use for SQL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xfoqw", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.94, "author_flair_background_color": null, "subreddit_type": "public", "ups": 34, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 34, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682342644.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been writing a lot of SQL lately but I get errors like mismatching no. of columns in a row during an insert more so because I&amp;#39;m dealing with huge tables and the editors never show me an error like that. I understand that it&amp;#39;s like a compile time error but is there any workaround that you guys use? TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xfoqw", "is_robot_indexable": true, "report_reasons": null, "author": "Ready--Aim--Fire", "discussion_type": null, "num_comments": 68, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xfoqw/what_editors_do_you_use_for_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xfoqw/what_editors_do_you_use_for_sql/", "subreddit_subscribers": 102237, "created_utc": 1682342644.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "As a Data Engineer, which Python packages and tools do you use in your work and for what purposes?", "author_fullname": "t2_uel8wujv", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Python usage in Data engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xj7tf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.8, "author_flair_background_color": null, "subreddit_type": "public", "ups": 13, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 13, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682348364.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As a Data Engineer, which Python packages and tools do you use in your work and for what purposes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xj7tf", "is_robot_indexable": true, "report_reasons": null, "author": "Own-Guava-2015", "discussion_type": null, "num_comments": 25, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xj7tf/python_usage_in_data_engineering/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xj7tf/python_usage_in_data_engineering/", "subreddit_subscribers": 102237, "created_utc": 1682348364.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I come from a math and data science background and I really enjoyed reading white papers about current trends in the industry. \n\nDoes data engineering have that? I've seen the duckdb white paper but it seems like DE has GitHub release pages and product websites instead of academic papers. \n\nIs this correct or am I missing out on an entire world of papers?", "author_fullname": "t2_cpnmh", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Looking for DE white papers", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xqa8s", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682357975.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come from a math and data science background and I really enjoyed reading white papers about current trends in the industry. &lt;/p&gt;\n\n&lt;p&gt;Does data engineering have that? I&amp;#39;ve seen the duckdb white paper but it seems like DE has GitHub release pages and product websites instead of academic papers. &lt;/p&gt;\n\n&lt;p&gt;Is this correct or am I missing out on an entire world of papers?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12xqa8s", "is_robot_indexable": true, "report_reasons": null, "author": "DoingItForGiggles", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xqa8s/looking_for_de_white_papers/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xqa8s/looking_for_de_white_papers/", "subreddit_subscribers": 102237, "created_utc": 1682357975.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am currently creating PySpark jobs on AWS/Glue environment. Moving forward with my team we decided to make our \"catalog\" -&gt; \"transactional\"  using one between Apache Hudi and Delta Lake. I have been spiking both technologies (using from a few MBs to TBs of dummy data) and they both fulfil our requirements. However a decision has not been taken yet so I've extended my research online and I wrote down a few bullet points that look more \"significative\":\n\n* Both well maintained projects based on Github stats\n* Apache Hudi has more features and support for integration with external tools than Delta Lake\n* Delta Lake is generally \"faster\" than Apache Hudi\n* Both are supported by AWS/Glue (for my case I refer only to AWS/Glue V4)\n* Not major differences for the syntax and code produced\n\nI guess all those points are arguably. Do you have any point to help the decision? Thank you!", "author_fullname": "t2_utk7y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Indecision of choosing between Apache Hudi and Delta Lake on AWS/Glue environment", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12x8u1i", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 11, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 11, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682325902.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682325555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently creating PySpark jobs on AWS/Glue environment. Moving forward with my team we decided to make our &amp;quot;catalog&amp;quot; -&amp;gt; &amp;quot;transactional&amp;quot;  using one between Apache Hudi and Delta Lake. I have been spiking both technologies (using from a few MBs to TBs of dummy data) and they both fulfil our requirements. However a decision has not been taken yet so I&amp;#39;ve extended my research online and I wrote down a few bullet points that look more &amp;quot;significative&amp;quot;:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Both well maintained projects based on Github stats&lt;/li&gt;\n&lt;li&gt;Apache Hudi has more features and support for integration with external tools than Delta Lake&lt;/li&gt;\n&lt;li&gt;Delta Lake is generally &amp;quot;faster&amp;quot; than Apache Hudi&lt;/li&gt;\n&lt;li&gt;Both are supported by AWS/Glue (for my case I refer only to AWS/Glue V4)&lt;/li&gt;\n&lt;li&gt;Not major differences for the syntax and code produced&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I guess all those points are arguably. Do you have any point to help the decision? Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12x8u1i", "is_robot_indexable": true, "report_reasons": null, "author": "df016", "discussion_type": null, "num_comments": 12, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12x8u1i/indecision_of_choosing_between_apache_hudi_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12x8u1i/indecision_of_choosing_between_apache_hudi_and/", "subreddit_subscribers": 102237, "created_utc": 1682325555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Does the following architecture make sense? I'm looking for the ability to swap DWH in the future if needed and I also want some flexibility to work with fivetran/glue.\n\nData Sources (Postgres, csv, dynamo, etc) to S3 Parquet files through fivetran. I'm calling this the Raw Layer.\n\nWe can enrich the raw data with glue as needed. In the \"Enrichment Layer\".\n\nSince the files are stored as Parquet Files in S3, we could use Iceberg tables as well.\n\nFrom the Raw/Enriched we can load the Raw data so snowflake using Fivetran/Snowflake tasks.\n\nNow that the data landed in snowflake we can transform it using self hosted DBT running on fargate.  \n\n\n**TL;DR :**   \nDatasources -- fivetran --&gt; S3 (Parquet) &lt;--&gt; Enrichment Layer With Glue --&gt; Fivetran/Snowflake tasks to load Raw data into snowflake --&gt; Transform with dbt .", "author_fullname": "t2_9rv5oe9l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "DE Architecture Review", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y1d42", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682380749.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does the following architecture make sense? I&amp;#39;m looking for the ability to swap DWH in the future if needed and I also want some flexibility to work with fivetran/glue.&lt;/p&gt;\n\n&lt;p&gt;Data Sources (Postgres, csv, dynamo, etc) to S3 Parquet files through fivetran. I&amp;#39;m calling this the Raw Layer.&lt;/p&gt;\n\n&lt;p&gt;We can enrich the raw data with glue as needed. In the &amp;quot;Enrichment Layer&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Since the files are stored as Parquet Files in S3, we could use Iceberg tables as well.&lt;/p&gt;\n\n&lt;p&gt;From the Raw/Enriched we can load the Raw data so snowflake using Fivetran/Snowflake tasks.&lt;/p&gt;\n\n&lt;p&gt;Now that the data landed in snowflake we can transform it using self hosted DBT running on fargate.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR :&lt;/strong&gt;&lt;br/&gt;\nDatasources -- fivetran --&amp;gt; S3 (Parquet) &amp;lt;--&amp;gt; Enrichment Layer With Glue --&amp;gt; Fivetran/Snowflake tasks to load Raw data into snowflake --&amp;gt; Transform with dbt .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y1d42", "is_robot_indexable": true, "report_reasons": null, "author": "jr_acc", "discussion_type": null, "num_comments": 6, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y1d42/de_architecture_review/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y1d42/de_architecture_review/", "subreddit_subscribers": 102237, "created_utc": 1682380749.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m building out our Databricks deployment and related DE infrastructure (new start up, greenfield). As the only DE, I\u2019m using Airbyte for raw extraction and load into our S3 data lake. \n\nI like the idea of only having to use one tool for all our DE needs. The only thing that comes to mind would be manually building out extractors to our data sources (CRMs, DBs, Tools, etc) or running  python based ETL libraries like Meltano in our notebooks. \n\nWith Databricks workflows and orchestrators, this could consolidate tooling. \n\nI will keep using airbyte as time is of the essence and the libraries help with the lift. \n\nHowever, I\u2019d love to have a discussion around projects or ideas with this type of infrastructure. \nThoughts?", "author_fullname": "t2_4rrqnyd4", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Curious if anyone has adopted a stack to do raw data ingestion in Databricks?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12y8yu2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 7, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 7, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682399487.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m building out our Databricks deployment and related DE infrastructure (new start up, greenfield). As the only DE, I\u2019m using Airbyte for raw extraction and load into our S3 data lake. &lt;/p&gt;\n\n&lt;p&gt;I like the idea of only having to use one tool for all our DE needs. The only thing that comes to mind would be manually building out extractors to our data sources (CRMs, DBs, Tools, etc) or running  python based ETL libraries like Meltano in our notebooks. &lt;/p&gt;\n\n&lt;p&gt;With Databricks workflows and orchestrators, this could consolidate tooling. &lt;/p&gt;\n\n&lt;p&gt;I will keep using airbyte as time is of the essence and the libraries help with the lift. &lt;/p&gt;\n\n&lt;p&gt;However, I\u2019d love to have a discussion around projects or ideas with this type of infrastructure. \nThoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12y8yu2", "is_robot_indexable": true, "report_reasons": null, "author": "deep-data-diver", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y8yu2/curious_if_anyone_has_adopted_a_stack_to_do_raw/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y8yu2/curious_if_anyone_has_adopted_a_stack_to_do_raw/", "subreddit_subscribers": 102237, "created_utc": 1682399487.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hi r/dataengineering! I\u2019m finishing up my 3rd year of college and had no luck landing DE/SWE or any other data related internship this year due to uncertainty in the job market. I did, however, end up with DevOps Engineer internship opportunity, which I took since I have no prior internship experience. The tech stack will be Python, AWS, Terraform, Docker and potentially SQL (will have to clarify that with my team). I\u2019m not sure it\u2019s DevOps is for me (maybe it will be) since my goal was always Data Engineering since I like working with data and mess around with cloud services. Therefore, I\u2019m curious, will DevOps internship help me with my Data Engineer job search when I look for new grad jobs? Thanks!", "author_fullname": "t2_mwot5ip2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Will this help me break into DE?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xq5nf", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.72, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682357972.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682357716.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi &lt;a href=\"/r/dataengineering\"&gt;r/dataengineering&lt;/a&gt;! I\u2019m finishing up my 3rd year of college and had no luck landing DE/SWE or any other data related internship this year due to uncertainty in the job market. I did, however, end up with DevOps Engineer internship opportunity, which I took since I have no prior internship experience. The tech stack will be Python, AWS, Terraform, Docker and potentially SQL (will have to clarify that with my team). I\u2019m not sure it\u2019s DevOps is for me (maybe it will be) since my goal was always Data Engineering since I like working with data and mess around with cloud services. Therefore, I\u2019m curious, will DevOps internship help me with my Data Engineer job search when I look for new grad jobs? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12xq5nf", "is_robot_indexable": true, "report_reasons": null, "author": "Interesting_Chard138", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xq5nf/will_this_help_me_break_into_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xq5nf/will_this_help_me_break_into_de/", "subreddit_subscribers": 102237, "created_utc": 1682357716.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Today from an X company approached me for a freelance data engineer. I have no degree in CS and it was going to be my first interview. They asked me how much rate I am looking to pay around and I said 130 euros per day. Now I can't reach them. Is it too much? What should I say?\n\nEdit: Company in the Netherlands I am in Poland", "author_fullname": "t2_2vc85g2g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "I need help", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xokfb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.87, "author_flair_background_color": null, "subreddit_type": "public", "ups": 6, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Interview", "can_mod_post": false, "score": 6, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682354397.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today from an X company approached me for a freelance data engineer. I have no degree in CS and it was going to be my first interview. They asked me how much rate I am looking to pay around and I said 130 euros per day. Now I can&amp;#39;t reach them. Is it too much? What should I say?&lt;/p&gt;\n\n&lt;p&gt;Edit: Company in the Netherlands I am in Poland&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "0922f6d6-a952-11eb-91e4-0e23043eebfb", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ffb000", "id": "12xokfb", "is_robot_indexable": true, "report_reasons": null, "author": "Blazey25", "discussion_type": null, "num_comments": 17, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xokfb/i_need_help/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xokfb/i_need_help/", "subreddit_subscribers": 102237, "created_utc": 1682354397.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I've tried using springml SFTP and file transfer libraries but they are failing. The size of them will be around 2 to 3 gb", "author_fullname": "t2_4s5y1tbq", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to Load large XML files from SFTP server to pyspark datafram", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y7v6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682401764.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682396383.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried using springml SFTP and file transfer libraries but they are failing. The size of them will be around 2 to 3 gb&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y7v6o", "is_robot_indexable": true, "report_reasons": null, "author": "deadly_commander", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y7v6o/how_to_load_large_xml_files_from_sftp_server_to/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y7v6o/how_to_load_large_xml_files_from_sftp_server_to/", "subreddit_subscribers": 102237, "created_utc": 1682396383.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_3w9fap27", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Strategies for Data Quality with Apache Spark", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 81, "top_awarded_type": null, "hide_score": false, "name": "t3_12xtqww", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.76, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/Ktdbj-H0oPXCGQ1G7Ie4PUMwq03TISRX_q1eilD5Ddw.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682365177.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "ssmertin.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://ssmertin.com/articles/strategies-for-data-quality-with-apache-spark/", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?auto=webp&amp;v=enabled&amp;s=eaf53de9c8a032dfa74a7e08975986102447ed13", "width": 2440, "height": 1420}, "resolutions": [{"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=540d2fcd467f8e2af55d57ac36c966d7f5bbc2fb", "width": 108, "height": 62}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=55063ee95015c689e9ee437276386c5d47afc242", "width": 216, "height": 125}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50524d3c81ab61eecd37b2fa01149fa4f8362d6a", "width": 320, "height": 186}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f4cf4fbbc4686d7199312c70d89744f8293ce8ea", "width": 640, "height": 372}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=843512a59bf908e1f0afd7097843e0bbe103005d", "width": 960, "height": 558}, {"url": "https://external-preview.redd.it/VX7xvo6mUt5DWcIzYtfIAeiheyNNUwDlJzapQs603JA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8699a58ae68b47e11f808f83b19bf94b2331bc16", "width": 1080, "height": 628}], "variants": {}, "id": "Y2HgNpSbvchEOXTQxd0aGkItWRie2rBtSyqZXOzYwVc"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12xtqww", "is_robot_indexable": true, "report_reasons": null, "author": "Practical-Ad-3928", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xtqww/strategies_for_data_quality_with_apache_spark/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://ssmertin.com/articles/strategies-for-data-quality-with-apache-spark/", "subreddit_subscribers": 102237, "created_utc": 1682365177.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I have numerous old offline hard drives in my storage with years of my professional photo and video work backed up on them some with overlap. Is there a database program that can record file names, folder trees, and other Metadata possibly even a small thumbnail for these thousands of images and files and record what drive they are on? Would like to be able to search through the drives contents without having to plug them in unless I need to actually retrieve the file I'm looking for.", "author_fullname": "t2_3jd22sfn", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Files database for offline drives", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12y9lil", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682401162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have numerous old offline hard drives in my storage with years of my professional photo and video work backed up on them some with overlap. Is there a database program that can record file names, folder trees, and other Metadata possibly even a small thumbnail for these thousands of images and files and record what drive they are on? Would like to be able to search through the drives contents without having to plug them in unless I need to actually retrieve the file I&amp;#39;m looking for.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y9lil", "is_robot_indexable": true, "report_reasons": null, "author": "TheGrovester", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y9lil/files_database_for_offline_drives/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y9lil/files_database_for_offline_drives/", "subreddit_subscribers": 102237, "created_utc": 1682401162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm relatively new to data engineering. We are building models which use images stored in an Azure Blob Storage Account. The volume of images could range from 300k to 900k. Should I be using spark to do image processing?\n\nI'm also open to using some other methods to make the image processing faster and more efficient. Please help if you have worked on a similar scenario. Could really use some suggestions. Thanks for any support.", "author_fullname": "t2_lpzq0jz8", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need suggestions to improve data processing for large volumes of images", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y6d36", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682392314.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m relatively new to data engineering. We are building models which use images stored in an Azure Blob Storage Account. The volume of images could range from 300k to 900k. Should I be using spark to do image processing?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also open to using some other methods to make the image processing faster and more efficient. Please help if you have worked on a similar scenario. Could really use some suggestions. Thanks for any support.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12y6d36", "is_robot_indexable": true, "report_reasons": null, "author": "ProduceObjective1766", "discussion_type": null, "num_comments": 4, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y6d36/need_suggestions_to_improve_data_processing_for/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y6d36/need_suggestions_to_improve_data_processing_for/", "subreddit_subscribers": 102237, "created_utc": 1682392314.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello. Soon I will start my first project as a sort of de (I dont want to overestimate my duties, will no code I guess, use only Azure?) with components of Azure Data Factory, Azure Synapse, Data Lake, Blob Storage and Databricks. \n\nWe will be taking data from on prem I guess or they will already sort of deliver it in any format, we will be moving these into external database, I hope its going to be Azure SQL, forget the graphical representation. \n\nI would like to ask you for all of the tips of what to look after / for, possible traps, obvious rookie mistakes and any protips how to handle the project overall. Will have no help in that part (ingestion and transformation). Currently running through documentation and some youtube tutorials\n\nThank you kindly in advance!", "author_fullname": "t2_ajcdrgrx", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "First even project as a sort of DE", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xmatb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682351033.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. Soon I will start my first project as a sort of de (I dont want to overestimate my duties, will no code I guess, use only Azure?) with components of Azure Data Factory, Azure Synapse, Data Lake, Blob Storage and Databricks. &lt;/p&gt;\n\n&lt;p&gt;We will be taking data from on prem I guess or they will already sort of deliver it in any format, we will be moving these into external database, I hope its going to be Azure SQL, forget the graphical representation. &lt;/p&gt;\n\n&lt;p&gt;I would like to ask you for all of the tips of what to look after / for, possible traps, obvious rookie mistakes and any protips how to handle the project overall. Will have no help in that part (ingestion and transformation). Currently running through documentation and some youtube tutorials&lt;/p&gt;\n\n&lt;p&gt;Thank you kindly in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12xmatb", "is_robot_indexable": true, "report_reasons": null, "author": "Commercial-Ask971", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xmatb/first_even_project_as_a_sort_of_de/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xmatb/first_even_project_as_a_sort_of_de/", "subreddit_subscribers": 102237, "created_utc": 1682351033.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I work at a medium-size company with some sensitive data. The data is not huge and won\u2019t grow anytime soon.\n\nWe have on-premise SQL Server and use SSIS packages to extract from our transactional DB\u2019s. We also use SSIS for transformations and load to a warehouse. This has been ok until recently because of how bad SSIS is at using web services as a source. Now we\u2019re looking at other options for ETL. Everyone on our team is comfortable with Python (and has personal/professional projects under their belt). Cloud solutions are not an option and open-core tools are preferred.\n\nMy proposal is to self-host a Prefect server in the simplest way possible. Handle ingestion with Python scripts and still use existing SSIS for the transformations when needed (just executing the packages). This would enable us to get going quickly but still allow us to expand on it with other tools in the future.\n\nDo you see any issues with this arrangement? Would you use a different orchestrator?\n\nDo you see any downsides to executing packages with Python (instead of just rebuilding transformations in Python)?\n\nWhat would you use for new transformations under this arrangement?\n\nThanks in advance for any advice/perspectives.", "author_fullname": "t2_qvk6b", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Need advice for on-premise stack", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xh9xx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": true, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682345823.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a medium-size company with some sensitive data. The data is not huge and won\u2019t grow anytime soon.&lt;/p&gt;\n\n&lt;p&gt;We have on-premise SQL Server and use SSIS packages to extract from our transactional DB\u2019s. We also use SSIS for transformations and load to a warehouse. This has been ok until recently because of how bad SSIS is at using web services as a source. Now we\u2019re looking at other options for ETL. Everyone on our team is comfortable with Python (and has personal/professional projects under their belt). Cloud solutions are not an option and open-core tools are preferred.&lt;/p&gt;\n\n&lt;p&gt;My proposal is to self-host a Prefect server in the simplest way possible. Handle ingestion with Python scripts and still use existing SSIS for the transformations when needed (just executing the packages). This would enable us to get going quickly but still allow us to expand on it with other tools in the future.&lt;/p&gt;\n\n&lt;p&gt;Do you see any issues with this arrangement? Would you use a different orchestrator?&lt;/p&gt;\n\n&lt;p&gt;Do you see any downsides to executing packages with Python (instead of just rebuilding transformations in Python)?&lt;/p&gt;\n\n&lt;p&gt;What would you use for new transformations under this arrangement?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any advice/perspectives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12xh9xx", "is_robot_indexable": true, "report_reasons": null, "author": "nnulll", "discussion_type": null, "num_comments": 5, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xh9xx/need_advice_for_onpremise_stack/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xh9xx/need_advice_for_onpremise_stack/", "subreddit_subscribers": 102237, "created_utc": 1682345823.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_20a1cwjg", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Creating Map Visualizations with Wikidata and Python", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 70, "top_awarded_type": null, "hide_score": false, "name": "t3_12x8nqb", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.67, "author_flair_background_color": null, "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/udnkcjVCosMgwiUtKFTLcxOMhtsAJIjzVj-DNombFxs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682325017.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "link.medium.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://link.medium.com/kKIkuldPfzb", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?auto=webp&amp;v=enabled&amp;s=def798be933b7245c1866fe9ccf6539b62f83a7e", "width": 1200, "height": 604}, "resolutions": [{"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=2583932a77a8a88dfdda4833e2efa4e4c89beaa6", "width": 108, "height": 54}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=56f0ee48ea86326d8a6c050c63ef55013be21f90", "width": 216, "height": 108}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=85a5ef80305143c3b934503f6b9d453a1321f977", "width": 320, "height": 161}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=afa9792f954872cd445f477f7631d966f803725a", "width": 640, "height": 322}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=df665efdc0663f406bd22f9b3c0e621e26ae1dc9", "width": 960, "height": 483}, {"url": "https://external-preview.redd.it/nXSRYeJoLB5JpSwhdAz4rM9ooAMi131DpnTaEZ4TOPI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5e35a9e14de5f7cda63d26ebd5539f85a55c2412", "width": 1080, "height": 543}], "variants": {}, "id": "rjVs8rT6pWH7ForqHakf1ssj7iplJ1LxDWZHnVDWbQk"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12x8nqb", "is_robot_indexable": true, "report_reasons": null, "author": "jkspiderdog", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12x8nqb/creating_map_visualizations_with_wikidata_and/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://link.medium.com/kKIkuldPfzb", "subreddit_subscribers": 102237, "created_utc": 1682325017.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Relatively new to modern data engineering, having come from a software development background.\n\nPython is notorious of its slowness in my background, with the exception of many well optimised lower level functions.\n\nIn DE, when talking about huge quantities of data, does python\u2019s performance become a bottleneck? Is data serialisation or transformation\u2026\n\na) even performed using python at large data scale?\nb) hampered by reckless use of slower aspects of python?\n\nDISCLAIMER: Still figuring out what python\u2019s footprint in DE is (eg. Dagster), given my team has leant on low-code ETL tools before.\n\nThanks!", "author_fullname": "t2_4z0wqsio", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Is Python a bottleneck?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12y4g9n", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.44, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682387686.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Relatively new to modern data engineering, having come from a software development background.&lt;/p&gt;\n\n&lt;p&gt;Python is notorious of its slowness in my background, with the exception of many well optimised lower level functions.&lt;/p&gt;\n\n&lt;p&gt;In DE, when talking about huge quantities of data, does python\u2019s performance become a bottleneck? Is data serialisation or transformation\u2026&lt;/p&gt;\n\n&lt;p&gt;a) even performed using python at large data scale?\nb) hampered by reckless use of slower aspects of python?&lt;/p&gt;\n\n&lt;p&gt;DISCLAIMER: Still figuring out what python\u2019s footprint in DE is (eg. Dagster), given my team has leant on low-code ETL tools before.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12y4g9n", "is_robot_indexable": true, "report_reasons": null, "author": "4mpig", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12y4g9n/is_python_a_bottleneck/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12y4g9n/is_python_a_bottleneck/", "subreddit_subscribers": 102237, "created_utc": 1682387686.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I am in the process of transitioning my business from Google Drive + Microsoft Office to a modern, integrated back-end; and have a high-level understanding of modern tech stacks, but am by no means a professional. I'd like to consult with a professional to discuss system architecture from the ground up, but trying to vet data- and full-stack engineers on Upwork and Fiverr has been overwhelming due to the variability of talent (even among the 'well-rated' freelancers). \n\nWondering if there are any resources for consulting with quality DEs on system architecture, data infrastructure, recommended tech stacks, etc.?", "author_fullname": "t2_vou11bf1", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Where to find quality DE consultant for building my tech stack?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xvray", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682369035.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am in the process of transitioning my business from Google Drive + Microsoft Office to a modern, integrated back-end; and have a high-level understanding of modern tech stacks, but am by no means a professional. I&amp;#39;d like to consult with a professional to discuss system architecture from the ground up, but trying to vet data- and full-stack engineers on Upwork and Fiverr has been overwhelming due to the variability of talent (even among the &amp;#39;well-rated&amp;#39; freelancers). &lt;/p&gt;\n\n&lt;p&gt;Wondering if there are any resources for consulting with quality DEs on system architecture, data infrastructure, recommended tech stacks, etc.?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xvray", "is_robot_indexable": true, "report_reasons": null, "author": "pg_archipelago", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xvray/where_to_find_quality_de_consultant_for_building/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xvray/where_to_find_quality_de_consultant_for_building/", "subreddit_subscribers": 102237, "created_utc": 1682369035.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_2tv9i42n", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "When not to use SQL - Sean J Taylor", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12xsak2", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": "#46d160", "ups": 0, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": "fd5b074e-239e-11e8-a28b-0e0f8d9eda5a", "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When not to use SQL -  Sean J  Taylor", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "author_name": "NormConf", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/t6m9z874SLM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@normconf"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12xsak2", "height": 200}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/NE71c6PwxV93Xv7OAGF-Dk_U6CmPWI7ySluTm_MQZIE.jpg", "edited": false, "author_flair_css_class": "mod", "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682362081.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtu.be", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://youtu.be/t6m9z874SLM", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?auto=webp&amp;v=enabled&amp;s=cfb6853e5442024c25ac71e030cf0049a30cb99e", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=09a1a34fb853459ad5e6225f41b6d06a40154ec9", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3b994adadf444dc5999fab2c3302bfa0e0960945", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/9BIqEl2puffh7lDYCF39bnRE5KwXaU338vXNsWGDh3o.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=726ab0c5faaf3d24f9022ee50a75e98edf406191", "width": 320, "height": 240}], "variants": {}, "id": "ZNhBXxcMk4GzO9HgVXgF8OvnRRXdp8Q0b9wiiUUNzAg"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": "mod | Sr. Data Engineer", "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12xsak2", "is_robot_indexable": true, "report_reasons": null, "author": "theporterhaus", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": "light", "permalink": "/r/dataengineering/comments/12xsak2/when_not_to_use_sql_sean_j_taylor/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://youtu.be/t6m9z874SLM", "subreddit_subscribers": 102237, "created_utc": 1682362081.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "When not to use SQL -  Sean J  Taylor", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/t6m9z874SLM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"When not to use SQL -  Sean J  Taylor\"&gt;&lt;/iframe&gt;", "author_name": "NormConf", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/t6m9z874SLM/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@normconf"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " In this article, we\u2019ll explore how to visualize the exchange rate of Bitcoin to USD using FastAPI, Prometheus, Grafana, and Docker. We will create a simple FastAPI application to import exchange rate data from an API, store it in a database, and expose it as metrics using Prometheus. Then, we\u2019ll use Grafana to create dashboards that visualize the data, and deploy the whole setup using Docker and Jenkins \n\n&amp;#x200B;\n\nhttps://preview.redd.it/4ly1yiz9duva1.png?width=1908&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b9feb3b6aa2f06a467ab1a04abe4b74d1b77f90b\n\n[https://medium.com/@stefentaime\\_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1](https://medium.com/@stefentaime_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1)", "author_fullname": "t2_7sisbd20", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Visualizing Bitcoin to USD Exchange Rates using FastAPI, Prometheus, Grafana, Deploy with jenkins On Localhost Ubuntu Server 20.04", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 63, "top_awarded_type": null, "hide_score": false, "media_metadata": {"4ly1yiz9duva1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 49, "x": 108, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d5f38632ac81800dd66ce1f274c1d4e7ff25ea1d"}, {"y": 98, "x": 216, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a1a55a40011e4ac0b5bd56023bcf7db511aff279"}, {"y": 146, "x": 320, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ac2bf4f70624f6e19416845ba5df936cfbd3ca31"}, {"y": 292, "x": 640, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=50266b961cbbc96922797db1ab88e0b05ac6fbca"}, {"y": 438, "x": 960, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ca0560f4043f9470a40abaed0bb52519d06e50cf"}, {"y": 493, "x": 1080, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=564a2b1b4ff06ddef05cc10d9486d8e404596c53"}], "s": {"y": 871, "x": 1908, "u": "https://preview.redd.it/4ly1yiz9duva1.png?width=1908&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b9feb3b6aa2f06a467ab1a04abe4b74d1b77f90b"}, "id": "4ly1yiz9duva1"}}, "name": "t3_12xhg6o", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/3P96OjhtaYUeU83f0Rxql-N6G2jDxCTHRmHaT2xVzQU.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "subreddit_type": "public", "created": 1682346152.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In this article, we\u2019ll explore how to visualize the exchange rate of Bitcoin to USD using FastAPI, Prometheus, Grafana, and Docker. We will create a simple FastAPI application to import exchange rate data from an API, store it in a database, and expose it as metrics using Prometheus. Then, we\u2019ll use Grafana to create dashboards that visualize the data, and deploy the whole setup using Docker and Jenkins &lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4ly1yiz9duva1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b9feb3b6aa2f06a467ab1a04abe4b74d1b77f90b\"&gt;https://preview.redd.it/4ly1yiz9duva1.png?width=1908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=b9feb3b6aa2f06a467ab1a04abe4b74d1b77f90b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@stefentaime_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1\"&gt;https://medium.com/@stefentaime_10958/visualizing-bitcoin-to-usd-exchange-rates-using-fastapi-prometheus-grafana-deploy-on-jenkins-76c7e3aa30e1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?auto=webp&amp;v=enabled&amp;s=ddcfcdb685931ca0bb14cd2c5a1675a026d09f3e", "width": 1200, "height": 548}, "resolutions": [{"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=8641d98b45ca52920014a514e4473f80826d8fb3", "width": 108, "height": 49}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=49a0caebda53c8af125a3e189f4a2be631601e48", "width": 216, "height": 98}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3fbde850dcaf378e7238abd3ab0c33f93ee8afed", "width": 320, "height": 146}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=16c0eb2cf12cd8810663609fe8d99d30d4c63b8c", "width": 640, "height": 292}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f907d8f721df86fa4092358c4146374b327fe039", "width": 960, "height": 438}, {"url": "https://external-preview.redd.it/77EA-NwiwKM45Su7FffgPsPY4vL_i_FgnBaoQrUcdDc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=b489c6310fdd90236978fdafdf20170ac4f7afaa", "width": 1080, "height": 493}], "variants": {}, "id": "pf6hn7xKOsckRNWPkeTcnBYa98kAE9DOlpcpbku2FUY"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12xhg6o", "is_robot_indexable": true, "report_reasons": null, "author": "Jealous_Ad6059", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xhg6o/visualizing_bitcoin_to_usd_exchange_rates_using/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xhg6o/visualizing_bitcoin_to_usd_exchange_rates_using/", "subreddit_subscribers": 102237, "created_utc": 1682346152.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "What are some developed or coming Reverse ETL frameworks in market right now??\n\nGrouproo was one which has been acquired by Airbyte recently and feels like this feature/framework will closed sourced.", "author_fullname": "t2_1pye2bsf", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Open Source Reverse ETL", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12xbq2r", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682334024.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are some developed or coming Reverse ETL frameworks in market right now??&lt;/p&gt;\n\n&lt;p&gt;Grouproo was one which has been acquired by Airbyte recently and feels like this feature/framework will closed sourced.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12xbq2r", "is_robot_indexable": true, "report_reasons": null, "author": "piyushsingariya", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12xbq2r/open_source_reverse_etl/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12xbq2r/open_source_reverse_etl/", "subreddit_subscribers": 102237, "created_utc": 1682334024.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}