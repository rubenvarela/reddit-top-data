{"kind": "Listing", "data": {"after": null, "dist": 20, "modhash": "", "geo_filter": "", "children": [{"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hearing from a friend who still works there that they just had another massive layoff (following another one back in January) including many engineers this time. Didn\u2019t see any mention of this online so wanted to share the news here\n\nWe don\u2019t use astronomer at my current company (it\u2019s stupid expensive) but curious to hear thoughts from those that do, on your experience and future plans given this news. Seems like over 50% laid off this year is bad news for their success\n\nI won\u2019t out my friend but sounds like pretty awful leadership and erratic changes in company direction have been happening for quite a while", "author_fullname": "t2_981fn786m", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Astronomer (the airflow company) laid off another 100 last week", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ug5fp", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.98, "author_flair_background_color": null, "subreddit_type": "public", "ups": 78, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 78, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682106846.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hearing from a friend who still works there that they just had another massive layoff (following another one back in January) including many engineers this time. Didn\u2019t see any mention of this online so wanted to share the news here&lt;/p&gt;\n\n&lt;p&gt;We don\u2019t use astronomer at my current company (it\u2019s stupid expensive) but curious to hear thoughts from those that do, on your experience and future plans given this news. Seems like over 50% laid off this year is bad news for their success&lt;/p&gt;\n\n&lt;p&gt;I won\u2019t out my friend but sounds like pretty awful leadership and erratic changes in company direction have been happening for quite a while&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12ug5fp", "is_robot_indexable": true, "report_reasons": null, "author": "Poopsydaisy123", "discussion_type": null, "num_comments": 45, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ug5fp/astronomer_the_airflow_company_laid_off_another/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ug5fp/astronomer_the_airflow_company_laid_off_another/", "subreddit_subscribers": 101461, "created_utc": 1682106846.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Over the years people have started realising you don't need a distributed framework if you're not operating on that scale. SQL-first tooling such as DBT and others have also improved SQL-based workflows.\n\nHowever as much as I like SQL before I start a project I always reflect on whether or not it's a good fit. Yes you can do everything with SQL but *should* you? There's times where queries are so far removed from intentions which is a no-go in most other places in software. Sometimes imperative paradigms are a better fit. \n\nDo you go for Python in these cases or does your shop stick to SQL for all tabular data? What are your opinions?", "author_fullname": "t2_8rjci796o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How do you feel about the return to SQL?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v2lcx", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.9, "author_flair_background_color": null, "subreddit_type": "public", "ups": 41, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 41, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682161281.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the years people have started realising you don&amp;#39;t need a distributed framework if you&amp;#39;re not operating on that scale. SQL-first tooling such as DBT and others have also improved SQL-based workflows.&lt;/p&gt;\n\n&lt;p&gt;However as much as I like SQL before I start a project I always reflect on whether or not it&amp;#39;s a good fit. Yes you can do everything with SQL but &lt;em&gt;should&lt;/em&gt; you? There&amp;#39;s times where queries are so far removed from intentions which is a no-go in most other places in software. Sometimes imperative paradigms are a better fit. &lt;/p&gt;\n\n&lt;p&gt;Do you go for Python in these cases or does your shop stick to SQL for all tabular data? What are your opinions?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12v2lcx", "is_robot_indexable": true, "report_reasons": null, "author": "Odd-One8023", "discussion_type": null, "num_comments": 32, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v2lcx/how_do_you_feel_about_the_return_to_sql/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v2lcx/how_do_you_feel_about_the_return_to_sql/", "subreddit_subscribers": 101461, "created_utc": 1682161281.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I'm talking about the trendy AI firms that are clearly doing mad stuff at the moment. OpenAI, DeepMind, FAIR, Anthropic, I'm sure there's tons of others.\n\nI've always had the feeling they must need data engineers, both low level to do the massive amounts of data processing before we get to the model training part, and the higher level stuff, about analysing results, doing measurements, making research reproducible and so on.\n\nBut I'm not really aware of DE roles in those spaces, wondering if anyone here has any experience with it or is working at one of those places.", "author_fullname": "t2_wphrs", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do the AI research companies hire DEs?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ux14g", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.93, "author_flair_background_color": null, "subreddit_type": "public", "ups": 24, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 24, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682144571.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m talking about the trendy AI firms that are clearly doing mad stuff at the moment. OpenAI, DeepMind, FAIR, Anthropic, I&amp;#39;m sure there&amp;#39;s tons of others.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve always had the feeling they must need data engineers, both low level to do the massive amounts of data processing before we get to the model training part, and the higher level stuff, about analysing results, doing measurements, making research reproducible and so on.&lt;/p&gt;\n\n&lt;p&gt;But I&amp;#39;m not really aware of DE roles in those spaces, wondering if anyone here has any experience with it or is working at one of those places.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ux14g", "is_robot_indexable": true, "report_reasons": null, "author": "nesh34", "discussion_type": null, "num_comments": 8, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ux14g/do_the_ai_research_companies_hire_des/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ux14g/do_the_ai_research_companies_hire_des/", "subreddit_subscribers": 101461, "created_utc": 1682144571.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I\u2019m a Data Analyst that writes MS SQL views and reports in Power BI. \n\nMy company is at the final stage of rolling out a data warehouse where we will be using for reporting (right now it\u2019s off the prod DB). \n\nAnyway I was asked the other day if I was interested in moving roles from reporting to learning Data Factory and maintaining and creating new ETL\u2019s and no longer work on reports. \n\nI guess my question is, is there a demand for people with Data Factory knowledge? Not that I\u2019m thinking about leaving my company but should anything happen my skill set will be needed.", "author_fullname": "t2_j52ax", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Data Factory", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ur5oi", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.85, "author_flair_background_color": null, "subreddit_type": "public", "ups": 14, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 14, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682131913.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682129386.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I\u2019m a Data Analyst that writes MS SQL views and reports in Power BI. &lt;/p&gt;\n\n&lt;p&gt;My company is at the final stage of rolling out a data warehouse where we will be using for reporting (right now it\u2019s off the prod DB). &lt;/p&gt;\n\n&lt;p&gt;Anyway I was asked the other day if I was interested in moving roles from reporting to learning Data Factory and maintaining and creating new ETL\u2019s and no longer work on reports. &lt;/p&gt;\n\n&lt;p&gt;I guess my question is, is there a demand for people with Data Factory knowledge? Not that I\u2019m thinking about leaving my company but should anything happen my skill set will be needed.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ur5oi", "is_robot_indexable": true, "report_reasons": null, "author": "lez_s", "discussion_type": null, "num_comments": 18, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ur5oi/data_factory/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ur5oi/data_factory/", "subreddit_subscribers": 101461, "created_utc": 1682129386.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_io93l", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Manage database schemas with Terraform in plain SQL | Atlas", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 78, "top_awarded_type": null, "hide_score": false, "name": "t3_12uywhq", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/1s8UyyjnFQq02Em7LWQi2oi3R2KXeW2LZx8BKtFqjDI.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "link", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682150162.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "atlasgo.io", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://atlasgo.io/blog/2023/04/21/terraform-v050", "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?auto=webp&amp;v=enabled&amp;s=bdfe189fcf3539f968ba1ad6d6560385b5df6863", "width": 1600, "height": 900}, "resolutions": [{"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=36f51c01b871d14cca0ecde7182dc7586bee6934", "width": 108, "height": 60}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65853ab6a9b7990eb10f3bc7861b98b4a37c9901", "width": 216, "height": 121}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=7a036214f5d2f95cb7529b9b46f217802b73184b", "width": 320, "height": 180}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=3ca75ab823edc8985fc45494413ffda88d0073c2", "width": 640, "height": 360}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=82be9967d84d4e8f251aaeaecdedaf43789adb6f", "width": 960, "height": 540}, {"url": "https://external-preview.redd.it/MAUKlqy4LHgGan2Jjz_LaHYze-qDc-83DdPgbWfTEho.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=adcd8a82f8cbdc8eace9b65b74fedcc700cefda8", "width": 1080, "height": 607}], "variants": {}, "id": "GGnS-T6QHI8DM_NzmZLXRxvo00Kg2yFySzJfHJqnodA"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12uywhq", "is_robot_indexable": true, "report_reasons": null, "author": "rotemtam", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12uywhq/manage_database_schemas_with_terraform_in_plain/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://atlasgo.io/blog/2023/04/21/terraform-v050", "subreddit_subscribers": 101461, "created_utc": 1682150162.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, our database is deployed on AWS RDS in a VPC and can only be accessed via bastion and ssh. \n\nCurrently, we run migrations manually. \n\nHow do you do this on a CI/CD pipeline?", "author_fullname": "t2_6542em0g", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How to run DB migrations in CICD", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u6y53", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 10, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 10, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682090331.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, our database is deployed on AWS RDS in a VPC and can only be accessed via bastion and ssh. &lt;/p&gt;\n\n&lt;p&gt;Currently, we run migrations manually. &lt;/p&gt;\n\n&lt;p&gt;How do you do this on a CI/CD pipeline?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12u6y53", "is_robot_indexable": true, "report_reasons": null, "author": "EngrRhys", "discussion_type": null, "num_comments": 13, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u6y53/how_to_run_db_migrations_in_cicd/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u6y53/how_to_run_db_migrations_in_cicd/", "subreddit_subscribers": 101461, "created_utc": 1682090331.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Most current OLAP databases are built with a columnar storage engine to process huge data volumes. They take pride in their high throughput, but often underperform in high-concurrency scenarios. As a complement, many data engineers invite Key-Value stores like Apache HBase for point queries, and Redis as a cache layer to ease the burden. The downside is redundant storage and high maintenance costs.\n\nApache Doris has been striving to become a unified database for data queries of all sizes, including ad-hoc queries and point queries. Till now, we have already taken down the monster of high-throughput OLAP scenarios. In the upcoming Apache Doris 2.0, we have optimized it for high-concurrency point queries. Long story short, it can achieve over 30,000 QPS for a single node (a 20-time increase in concurrency), by methods of **partitioning and bucketing, indexing, materialized view, runtime filter, TOP-N optimization, row storage format, short-circuit, prepared statement, and row storage cache.**\n\n# 1. Partioning and Bucketing\n\nApache Doris shards data into a two-tiered structure: Partition and Bucket. You can use time information as the Partition Key. As for bucketing, you distribute the data into various nodes after data hashing. A wise bucketing plan can largely increase concurrency and throughput in data reading, because the system only needs to scan one bucket in one partition before it can locate the needed data.\n\n# 2. Index\n\nApache Doris uses various data indexes to speed up data reading and filtering, including smart indexes and secondary indexes. Smart indexes are auto-generated by Doris upon data ingestion, which requires no action from the user\u2019s side.There are two types of smart indexes:\n\n* **Sorted Index**: Apache Doris stores data in an orderly way. It creates a sorted index for every 1024 rows of data. The Key in the index is the value of the sorted column in the first row of the current 1024 rows. If the query involves the sorted column, the system will locate the first row of the relevant 1024 row group and start scanning there.\n* **ZoneMap Index**: These are indexes on the Segment and Page level. The maximum and minimum values of each column within a Page will be recorded, so are those within a Segment. Hence, in equivalence queries and range queries, the system can narrow down the filter range with the help of the MinMax indexes.\n\nSecondary indexes are created by users. These include Bloom Filter indexes, Bitmap indexes, [Inverted indexes](https://doris.apache.org/docs/dev/data-table/index/inverted-index/), and [NGram Bloom Filter indexes](https://doris.apache.org/docs/dev/data-table/index/ngram-bloomfilter-index/). \n\nExample: `select * from user_table where id &gt; 10 and id &lt; 1024`\n\nSuppose that the user has designated `id` as the Key during table creation, the data will be sorted by `id` on Memtable and the disks. So any queries involving `id` as a filter condition will be executed much faster with the aid of sorted indexes. Specifically, the data in storage will be put into multiple ranges based on `id` , and the system will implement binary search to locate the exact range according to the sorted indexes. But that could still be a large range since the sorted indexes are sparse. You can further narrow it down based on ZoneMap indexes, Bloom Filter indexes, and Bitmap indexes.This is another way to reduce data scanning and improve overall concurrency of the system.\n\n# 3. Materialized View\n\nThe idea of materialized view is to trade space for time: You execute pre-computation with pre-defined SQL statements, and perpetuate the results in a table that is visible to users but occupies some storage space. In this way, Apache Doris can respond much faster to queries for aggregated data and breakdown data and those involve the matching of sorted indexes once it hits a materialized view. This is a good way to lessen computation, improve query performance, and reduce resource consumption.\n\n# 4. Runtime Filter\n\nIn multi-table Join queries, the left table is usually called ProbeTable while the right one is called BuildTable, with the former much bigger than the latter. In query execution, firstly, the system reads the right table and creates a HashTable (Build) in the memory. Then, it starts reading the left table row by row, during which it also compares data between the left table and the HashTable and returns the matched data (Probe).\n\nDuring the creation of HashTable, Apache Doris generates a filter for the columns. It can be a Min/Max filter or an IN filter. Then it pushes down the filter to the left table, which can use the filter to screen out data and thus reduces the amount of data that the Probe node has to transfer and compare.This is how the Runtime Filter works. In most Join queries, the Runtime Filter can be automatically pushed down to the most underlying scan nodes or to the distributed Shuffle Join. In other words, Runtime Filter is able to reduce data reading and shorten response time for most Join queries.\n\n# 5. TOP-N Optimization\n\nTOP-N query is a frequent scenario in data analysis. For example, users want to fetch the most recent 100 orders, or the 5 highest/lowest priced products. For such queries, Apache Doris implements TOP-N optimization:\n\n1. Apache Doris reads the sorted fields and query fields from the Scanner layer, reserves only the TOP-N pieces of data by means of Heapsort, updates the real-time TOP-N results as it continues reading, and dynamically pushes them down to the Scanner.\n2. Combing the received TOP-N range and the indexes, the Scanner can skip a large proportion of irrelevant files and data chunks and only read a small number of rows.\n3. Queries on flat tables usually mean the need to scan massive data, but TOP-N queries only retrieve a small amount of data. The strategy here is to divide the data reading process into two stages. In stage one, the system sorts the data based on a few columns (sorted column, or condition column) and locates the TOP-N rows. In stage two, it fetches the TOP-N rows of data after data sorting, and then it retrieves the target data according to the row numbers.\n\n# 6. Row Storage Format\n\nAs we know, row storage is much more efficient when the user only queries for a single row of data. So we introduced row storage format in Apache Doris 2.0. We chose JSONB as the encoding format for row storage for three reasons:\n\n* **Flexible schema change**: If a user has added or deleted a field, or modified the type of a field, these changes must be updated in row storage in real time. So we choose to adopt the JSONB format and encode columns into JSONB fields. This makes changes in fields very easy.\n* **High performance**: Accessing rows in row-oriented storage is much faster than doing that in columnar storage, and it requires much less disk access in high-concurrency scenarios. Also, in some cases, you can map the column ID to the corresponding JSONB value so you can quickly access a certain column.\n* **Less storage space**: JSONB is a compacted binary format. It consumes less space on the disk and is more cost-effective.\n\nIn the storage engine, row storage will be stored as a hidden column (DORIS\\_ROW\\_STORE\\_COL). During Memtable Flush, the columns will be encoded into JSONB and cached into this hidden column. In data reading, the system uses the Column ID to locate the column, finds the target row based on the row number, and then deserializes the relevant columns.\n\n# 7. Short-Circuit\n\nNormally, an SQL statement is executed in three steps:\n\n1. SQL Parser parses the statement to generate an abstract syntax tree (AST).\n2. The Query Optimizer produces an executable plan.\n3. Execute the plan and return the results.\n\nFor complex queries on massive data, it is better to follow the plan created by the Query Optimizer. However, for high-concurrency point queries requiring low latency, that plan is not only unnecessary but also brings extra overheads. That\u2019s why we implement a short-circuit plan for point queries.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/40g6ww69d9va1.png?width=1606&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ada4306ffe3cc6f9874fba1777d35d32b5c0e1ea\n\nOnce the FE receives a point query request, a short-circuit plan will be produced. It is a lightweight plan that involves no equivalent transformation, logic optimization or physical optimization. Instead, it conducts some basic analysis on the AST, creates a fixed plan accordingly, and finds ways to reduce overhead of the optimizer.\n\nFor a simple point query involving primary keys, such as `select * from tbl where pk1 = 123 and pk2 = 456,` since it only involves one single Tablet, it is better to use a lightweight RPC interface for interaction with the Storage Engine. This avoids the creation of a complicated Fragment Plan and eliminates the performance overhead brought by the scheduling under the MPP query framework.\n\nDetails of the RPC interface are as follows:\n\n    message PTabletKeyLookupRequest {\n      required int64 tablet_id = 1;\n      repeated KeyTuple key_tuples = 2;\n      optional Descriptor desc_tbl = 4;\n      optional ExprList  output_expr = 5;\n    }\n    message PTabletKeyLookupResponse {\n      required PStatus status = 1;\n      optional bytes row_batch = 5;\n    optional bool\n    empty_batch = 6;\n    }\n    rpc tablet_fetch_data(PTabletKeyLookupRequest) returns (PTabletKeyLookupResponse);\n\n`tablet_id` is calculated based on the primary key column, while `key_tuples` is the string format of the primary key. In this example, the `key_tuples` is similar to \\['123', '456'\\]. As BE receives the request, `key_tuples` will be encoded into primary key storage format. Then, it will locate the corresponding row number of the Key in the Segment File with the help of the primary key index, and check if that row exists in `delete bitmap`. If it does, the row number will be returned; if not, the system returns NotFound. The returned row number will be used for point query on `__DORIS_ROW_STORE_COL__`. That means we only need to locate one row in that column, fetch the original value of the JSONB format, and deserialize it.\n\n# 8. Prepared Statement\n\nThe idea of prepared statements is to cache precomputed SQL and expressions in HashMap in memory, so they can be directly used in queries when applicable.\n\nPrepared statements adopt MySQL binary protocol for transmission. The protocol is implemented in the mysql\\_row\\_buffer.\\[h|cpp\\] file, and uses MySQL binary encoding. Under this protocol, the client (for example, JDBC Client) sends a pre-compiled statement to FE via `PREPARE` MySQL Command. Next, FE will parse and analyze the statement and cache it in the HashMap as shown in the figure above. Next, the client, using `EXECUTE` MySQL Command, will replace the placeholder, encode it into binary format, and send it to FE. Then, FE will perform deserialization to obtain the value of the placeholder, and generate query conditions.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/nwmpb1d2e9va1.png?width=1134&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=856b9884206b9fd8628224c1bb5ad30e7cfc5105\n\nApart from caching prepared statements in FE, we also cache reusable structures in BE. These structures include pre-allocated computation blocks, query descriptors, and output expressions. Serializing and deserializing these structures often cause a CPU hotspot, so it makes more sense to cache them. The prepared statement for each query comes with a UUID named CacheID. So when BE executes the point query, it will find the corresponding class based on the CacheID, and then reuse the structure in computation.\n\n# 9. Row Storage Cache\n\nApache Doris has a Page Cache feature, where each page caches the data of one column. \n\nhttps://preview.redd.it/v6fvikn8e9va1.png?width=568&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=833ea248bb86a5af2a2917fc319cd81c7894833f\n\nAs mentioned above, we have introduced row storage in Doris. The problem with this is, one row of data consists of multiple columns, so in the case of big queries, the cached data might be erased. Thus, we also introduced row cache to increase row cache hit rate.\n\nRow cache reuses the LRU Cache mechanism in Apache Doris. When the caching starts, the system will initialize a threshold value. If that threshold is hit, the old cached rows will be phased out. For a primary key query statement, the performance gap between cache hit and cache miss can be huge (we are talking about dozens of times less disk I/O and memory access here). So the introduction of row cache can remarkably enhance point query performance.\n\nFull post link: [https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3](https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3)", "author_fullname": "t2_no0j2ndo", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "How We Increase Database Query Concurrency by 20 Times", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 67, "top_awarded_type": null, "hide_score": false, "media_metadata": {"40g6ww69d9va1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 52, "x": 108, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=243dd96f95fb86e20ced85be8324937398397de6"}, {"y": 104, "x": 216, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=d51cfeff1ac8692dd77bf482316569a57ca19cf6"}, {"y": 155, "x": 320, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=608adef3e0df5912de83af4dbe5dd0ba497dd050"}, {"y": 310, "x": 640, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6db26b77609a90c2678d422c74f351d02f4b4f07"}, {"y": 465, "x": 960, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=62dbab27cf9bef5156d719cdd0e61cce86d81a98"}, {"y": 523, "x": 1080, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4d286a4d8150be3f2c8e484c05889549ab052b95"}], "s": {"y": 778, "x": 1606, "u": "https://preview.redd.it/40g6ww69d9va1.png?width=1606&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ada4306ffe3cc6f9874fba1777d35d32b5c0e1ea"}, "id": "40g6ww69d9va1"}, "v6fvikn8e9va1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 102, "x": 108, "u": "https://preview.redd.it/v6fvikn8e9va1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a0892aa7ecebe4ebc62a6ad5974acb7c7f3c72b4"}, {"y": 205, "x": 216, "u": "https://preview.redd.it/v6fvikn8e9va1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=f92f965796971b4eaa30217324030fc660f9f76d"}, {"y": 304, "x": 320, "u": "https://preview.redd.it/v6fvikn8e9va1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4ba831050cad4be44a2753d6bc35fa766ad94c0e"}], "s": {"y": 540, "x": 568, "u": "https://preview.redd.it/v6fvikn8e9va1.png?width=568&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=833ea248bb86a5af2a2917fc319cd81c7894833f"}, "id": "v6fvikn8e9va1"}, "nwmpb1d2e9va1": {"status": "valid", "e": "Image", "m": "image/png", "p": [{"y": 86, "x": 108, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=4b7c7b5015e9ea0be2fc89a2d64ddf800c0c30c6"}, {"y": 172, "x": 216, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=ecdf1be3c197fd3dfad10a23b9afb1df1d10515e"}, {"y": 255, "x": 320, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=334c311f365e0becf291b2c97202d659d68f1515"}, {"y": 510, "x": 640, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=640&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=65496580bb5c00058cbcf1791532313864f99cff"}, {"y": 765, "x": 960, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=960&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a57f03f7187d9710839f1de787b4a0d07795d84c"}, {"y": 860, "x": 1080, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=6feef10f64c71454c294184c02c5e5de44753236"}], "s": {"y": 904, "x": 1134, "u": "https://preview.redd.it/nwmpb1d2e9va1.png?width=1134&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=856b9884206b9fd8628224c1bb5ad30e7cfc5105"}, "id": "nwmpb1d2e9va1"}}, "name": "t3_12u8ske", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.79, "author_flair_background_color": null, "subreddit_type": "public", "ups": 8, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 8, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/4rBvxpZtYRMws1zKv6LL5HGgeOjaGQCesHdLymCRJBs.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682092445.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most current OLAP databases are built with a columnar storage engine to process huge data volumes. They take pride in their high throughput, but often underperform in high-concurrency scenarios. As a complement, many data engineers invite Key-Value stores like Apache HBase for point queries, and Redis as a cache layer to ease the burden. The downside is redundant storage and high maintenance costs.&lt;/p&gt;\n\n&lt;p&gt;Apache Doris has been striving to become a unified database for data queries of all sizes, including ad-hoc queries and point queries. Till now, we have already taken down the monster of high-throughput OLAP scenarios. In the upcoming Apache Doris 2.0, we have optimized it for high-concurrency point queries. Long story short, it can achieve over 30,000 QPS for a single node (a 20-time increase in concurrency), by methods of &lt;strong&gt;partitioning and bucketing, indexing, materialized view, runtime filter, TOP-N optimization, row storage format, short-circuit, prepared statement, and row storage cache.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;h1&gt;1. Partioning and Bucketing&lt;/h1&gt;\n\n&lt;p&gt;Apache Doris shards data into a two-tiered structure: Partition and Bucket. You can use time information as the Partition Key. As for bucketing, you distribute the data into various nodes after data hashing. A wise bucketing plan can largely increase concurrency and throughput in data reading, because the system only needs to scan one bucket in one partition before it can locate the needed data.&lt;/p&gt;\n\n&lt;h1&gt;2. Index&lt;/h1&gt;\n\n&lt;p&gt;Apache Doris uses various data indexes to speed up data reading and filtering, including smart indexes and secondary indexes. Smart indexes are auto-generated by Doris upon data ingestion, which requires no action from the user\u2019s side.There are two types of smart indexes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Sorted Index&lt;/strong&gt;: Apache Doris stores data in an orderly way. It creates a sorted index for every 1024 rows of data. The Key in the index is the value of the sorted column in the first row of the current 1024 rows. If the query involves the sorted column, the system will locate the first row of the relevant 1024 row group and start scanning there.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;ZoneMap Index&lt;/strong&gt;: These are indexes on the Segment and Page level. The maximum and minimum values of each column within a Page will be recorded, so are those within a Segment. Hence, in equivalence queries and range queries, the system can narrow down the filter range with the help of the MinMax indexes.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Secondary indexes are created by users. These include Bloom Filter indexes, Bitmap indexes, &lt;a href=\"https://doris.apache.org/docs/dev/data-table/index/inverted-index/\"&gt;Inverted indexes&lt;/a&gt;, and &lt;a href=\"https://doris.apache.org/docs/dev/data-table/index/ngram-bloomfilter-index/\"&gt;NGram Bloom Filter indexes&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Example: &lt;code&gt;select * from user_table where id &amp;gt; 10 and id &amp;lt; 1024&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Suppose that the user has designated &lt;code&gt;id&lt;/code&gt; as the Key during table creation, the data will be sorted by &lt;code&gt;id&lt;/code&gt; on Memtable and the disks. So any queries involving &lt;code&gt;id&lt;/code&gt; as a filter condition will be executed much faster with the aid of sorted indexes. Specifically, the data in storage will be put into multiple ranges based on &lt;code&gt;id&lt;/code&gt; , and the system will implement binary search to locate the exact range according to the sorted indexes. But that could still be a large range since the sorted indexes are sparse. You can further narrow it down based on ZoneMap indexes, Bloom Filter indexes, and Bitmap indexes.This is another way to reduce data scanning and improve overall concurrency of the system.&lt;/p&gt;\n\n&lt;h1&gt;3. Materialized View&lt;/h1&gt;\n\n&lt;p&gt;The idea of materialized view is to trade space for time: You execute pre-computation with pre-defined SQL statements, and perpetuate the results in a table that is visible to users but occupies some storage space. In this way, Apache Doris can respond much faster to queries for aggregated data and breakdown data and those involve the matching of sorted indexes once it hits a materialized view. This is a good way to lessen computation, improve query performance, and reduce resource consumption.&lt;/p&gt;\n\n&lt;h1&gt;4. Runtime Filter&lt;/h1&gt;\n\n&lt;p&gt;In multi-table Join queries, the left table is usually called ProbeTable while the right one is called BuildTable, with the former much bigger than the latter. In query execution, firstly, the system reads the right table and creates a HashTable (Build) in the memory. Then, it starts reading the left table row by row, during which it also compares data between the left table and the HashTable and returns the matched data (Probe).&lt;/p&gt;\n\n&lt;p&gt;During the creation of HashTable, Apache Doris generates a filter for the columns. It can be a Min/Max filter or an IN filter. Then it pushes down the filter to the left table, which can use the filter to screen out data and thus reduces the amount of data that the Probe node has to transfer and compare.This is how the Runtime Filter works. In most Join queries, the Runtime Filter can be automatically pushed down to the most underlying scan nodes or to the distributed Shuffle Join. In other words, Runtime Filter is able to reduce data reading and shorten response time for most Join queries.&lt;/p&gt;\n\n&lt;h1&gt;5. TOP-N Optimization&lt;/h1&gt;\n\n&lt;p&gt;TOP-N query is a frequent scenario in data analysis. For example, users want to fetch the most recent 100 orders, or the 5 highest/lowest priced products. For such queries, Apache Doris implements TOP-N optimization:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Apache Doris reads the sorted fields and query fields from the Scanner layer, reserves only the TOP-N pieces of data by means of Heapsort, updates the real-time TOP-N results as it continues reading, and dynamically pushes them down to the Scanner.&lt;/li&gt;\n&lt;li&gt;Combing the received TOP-N range and the indexes, the Scanner can skip a large proportion of irrelevant files and data chunks and only read a small number of rows.&lt;/li&gt;\n&lt;li&gt;Queries on flat tables usually mean the need to scan massive data, but TOP-N queries only retrieve a small amount of data. The strategy here is to divide the data reading process into two stages. In stage one, the system sorts the data based on a few columns (sorted column, or condition column) and locates the TOP-N rows. In stage two, it fetches the TOP-N rows of data after data sorting, and then it retrieves the target data according to the row numbers.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;6. Row Storage Format&lt;/h1&gt;\n\n&lt;p&gt;As we know, row storage is much more efficient when the user only queries for a single row of data. So we introduced row storage format in Apache Doris 2.0. We chose JSONB as the encoding format for row storage for three reasons:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Flexible schema change&lt;/strong&gt;: If a user has added or deleted a field, or modified the type of a field, these changes must be updated in row storage in real time. So we choose to adopt the JSONB format and encode columns into JSONB fields. This makes changes in fields very easy.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt;: Accessing rows in row-oriented storage is much faster than doing that in columnar storage, and it requires much less disk access in high-concurrency scenarios. Also, in some cases, you can map the column ID to the corresponding JSONB value so you can quickly access a certain column.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Less storage space&lt;/strong&gt;: JSONB is a compacted binary format. It consumes less space on the disk and is more cost-effective.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;In the storage engine, row storage will be stored as a hidden column (DORIS_ROW_STORE_COL). During Memtable Flush, the columns will be encoded into JSONB and cached into this hidden column. In data reading, the system uses the Column ID to locate the column, finds the target row based on the row number, and then deserializes the relevant columns.&lt;/p&gt;\n\n&lt;h1&gt;7. Short-Circuit&lt;/h1&gt;\n\n&lt;p&gt;Normally, an SQL statement is executed in three steps:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;SQL Parser parses the statement to generate an abstract syntax tree (AST).&lt;/li&gt;\n&lt;li&gt;The Query Optimizer produces an executable plan.&lt;/li&gt;\n&lt;li&gt;Execute the plan and return the results.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For complex queries on massive data, it is better to follow the plan created by the Query Optimizer. However, for high-concurrency point queries requiring low latency, that plan is not only unnecessary but also brings extra overheads. That\u2019s why we implement a short-circuit plan for point queries.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/40g6ww69d9va1.png?width=1606&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ada4306ffe3cc6f9874fba1777d35d32b5c0e1ea\"&gt;https://preview.redd.it/40g6ww69d9va1.png?width=1606&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=ada4306ffe3cc6f9874fba1777d35d32b5c0e1ea&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Once the FE receives a point query request, a short-circuit plan will be produced. It is a lightweight plan that involves no equivalent transformation, logic optimization or physical optimization. Instead, it conducts some basic analysis on the AST, creates a fixed plan accordingly, and finds ways to reduce overhead of the optimizer.&lt;/p&gt;\n\n&lt;p&gt;For a simple point query involving primary keys, such as &lt;code&gt;select * from tbl where pk1 = 123 and pk2 = 456,&lt;/code&gt; since it only involves one single Tablet, it is better to use a lightweight RPC interface for interaction with the Storage Engine. This avoids the creation of a complicated Fragment Plan and eliminates the performance overhead brought by the scheduling under the MPP query framework.&lt;/p&gt;\n\n&lt;p&gt;Details of the RPC interface are as follows:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;message PTabletKeyLookupRequest {\n  required int64 tablet_id = 1;\n  repeated KeyTuple key_tuples = 2;\n  optional Descriptor desc_tbl = 4;\n  optional ExprList  output_expr = 5;\n}\nmessage PTabletKeyLookupResponse {\n  required PStatus status = 1;\n  optional bytes row_batch = 5;\noptional bool\nempty_batch = 6;\n}\nrpc tablet_fetch_data(PTabletKeyLookupRequest) returns (PTabletKeyLookupResponse);\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;code&gt;tablet_id&lt;/code&gt; is calculated based on the primary key column, while &lt;code&gt;key_tuples&lt;/code&gt; is the string format of the primary key. In this example, the &lt;code&gt;key_tuples&lt;/code&gt; is similar to [&amp;#39;123&amp;#39;, &amp;#39;456&amp;#39;]. As BE receives the request, &lt;code&gt;key_tuples&lt;/code&gt; will be encoded into primary key storage format. Then, it will locate the corresponding row number of the Key in the Segment File with the help of the primary key index, and check if that row exists in &lt;code&gt;delete bitmap&lt;/code&gt;. If it does, the row number will be returned; if not, the system returns NotFound. The returned row number will be used for point query on &lt;code&gt;__DORIS_ROW_STORE_COL__&lt;/code&gt;. That means we only need to locate one row in that column, fetch the original value of the JSONB format, and deserialize it.&lt;/p&gt;\n\n&lt;h1&gt;8. Prepared Statement&lt;/h1&gt;\n\n&lt;p&gt;The idea of prepared statements is to cache precomputed SQL and expressions in HashMap in memory, so they can be directly used in queries when applicable.&lt;/p&gt;\n\n&lt;p&gt;Prepared statements adopt MySQL binary protocol for transmission. The protocol is implemented in the mysql_row_buffer.[h|cpp] file, and uses MySQL binary encoding. Under this protocol, the client (for example, JDBC Client) sends a pre-compiled statement to FE via &lt;code&gt;PREPARE&lt;/code&gt; MySQL Command. Next, FE will parse and analyze the statement and cache it in the HashMap as shown in the figure above. Next, the client, using &lt;code&gt;EXECUTE&lt;/code&gt; MySQL Command, will replace the placeholder, encode it into binary format, and send it to FE. Then, FE will perform deserialization to obtain the value of the placeholder, and generate query conditions.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nwmpb1d2e9va1.png?width=1134&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=856b9884206b9fd8628224c1bb5ad30e7cfc5105\"&gt;https://preview.redd.it/nwmpb1d2e9va1.png?width=1134&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=856b9884206b9fd8628224c1bb5ad30e7cfc5105&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Apart from caching prepared statements in FE, we also cache reusable structures in BE. These structures include pre-allocated computation blocks, query descriptors, and output expressions. Serializing and deserializing these structures often cause a CPU hotspot, so it makes more sense to cache them. The prepared statement for each query comes with a UUID named CacheID. So when BE executes the point query, it will find the corresponding class based on the CacheID, and then reuse the structure in computation.&lt;/p&gt;\n\n&lt;h1&gt;9. Row Storage Cache&lt;/h1&gt;\n\n&lt;p&gt;Apache Doris has a Page Cache feature, where each page caches the data of one column. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v6fvikn8e9va1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=833ea248bb86a5af2a2917fc319cd81c7894833f\"&gt;https://preview.redd.it/v6fvikn8e9va1.png?width=568&amp;amp;format=png&amp;amp;auto=webp&amp;amp;v=enabled&amp;amp;s=833ea248bb86a5af2a2917fc319cd81c7894833f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As mentioned above, we have introduced row storage in Doris. The problem with this is, one row of data consists of multiple columns, so in the case of big queries, the cached data might be erased. Thus, we also introduced row cache to increase row cache hit rate.&lt;/p&gt;\n\n&lt;p&gt;Row cache reuses the LRU Cache mechanism in Apache Doris. When the caching starts, the system will initialize a threshold value. If that threshold is hit, the old cached rows will be phased out. For a primary key query statement, the performance gap between cache hit and cache miss can be huge (we are talking about dozens of times less disk I/O and memory access here). So the introduction of row cache can remarkably enhance point query performance.&lt;/p&gt;\n\n&lt;p&gt;Full post link: &lt;a href=\"https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3\"&gt;https://medium.com/geekculture/how-we-increase-database-query-concurrency-by-20-times-440f8b772fe3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12u8ske", "is_robot_indexable": true, "report_reasons": null, "author": "ApacheDoris", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u8ske/how_we_increase_database_query_concurrency_by_20/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u8ske/how_we_increase_database_query_concurrency_by_20/", "subreddit_subscribers": 101461, "created_utc": 1682092445.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Ive been chipping away at a CI/CD pipeline for snowflake. Ive been using schemachange and works as an MVP but would not be manageable at scale and there is no rollback feature i have found that would retain data from other pipelines. \n\nId love for people to share their experience with CI/CD for stateful data warehouses, even better if its a large enterprise client. Would like to write in VS code, deploy to a branch, and have that deployment roll out changes. This is happening now but want to hear about more robust solutions that have worked in the past, it seems like theres not much standardization in this space. \n\nP.S. mainly working on changes to DDL on snowflake internal objects", "author_fullname": "t2_3j972yz2", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "CI/CD for data warehouse", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12uzsui", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.84, "author_flair_background_color": null, "subreddit_type": "public", "ups": 4, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 4, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682153169.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ive been chipping away at a CI/CD pipeline for snowflake. Ive been using schemachange and works as an MVP but would not be manageable at scale and there is no rollback feature i have found that would retain data from other pipelines. &lt;/p&gt;\n\n&lt;p&gt;Id love for people to share their experience with CI/CD for stateful data warehouses, even better if its a large enterprise client. Would like to write in VS code, deploy to a branch, and have that deployment roll out changes. This is happening now but want to hear about more robust solutions that have worked in the past, it seems like theres not much standardization in this space. &lt;/p&gt;\n\n&lt;p&gt;P.S. mainly working on changes to DDL on snowflake internal objects&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12uzsui", "is_robot_indexable": true, "report_reasons": null, "author": "lturanski", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12uzsui/cicd_for_data_warehouse/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12uzsui/cicd_for_data_warehouse/", "subreddit_subscribers": 101461, "created_utc": 1682153169.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Data folk: do you use the [write-audit-publish (WAP) pattern](https://www.youtube.com/watch?v=fXHdeBnpXrg&amp;t=1001s) in your pipelines? \n\nIt seems like a great idea\u2014and Netflix has been using it for years\u2014but I'm curious if it's more widely adopted than that. Both Apache Iceberg and Apache Hudi support it. \n\nWould also love to hear comments on how you've done it, successes, problems - and also reasons why you haven't, etc \ud83d\udc47\ud83c\udffb\n\n[View Poll](https://www.reddit.com/poll/12u6oqk)", "author_fullname": "t2_bvkm0", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Do *you* use the Write-Audit-Publish (WAP) pattern in your data pipelines?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u6oqk", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.86, "author_flair_background_color": null, "subreddit_type": "public", "ups": 5, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 5, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682090100.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data folk: do you use the &lt;a href=\"https://www.youtube.com/watch?v=fXHdeBnpXrg&amp;amp;t=1001s\"&gt;write-audit-publish (WAP) pattern&lt;/a&gt; in your pipelines? &lt;/p&gt;\n\n&lt;p&gt;It seems like a great idea\u2014and Netflix has been using it for years\u2014but I&amp;#39;m curious if it&amp;#39;s more widely adopted than that. Both Apache Iceberg and Apache Hudi support it. &lt;/p&gt;\n\n&lt;p&gt;Would also love to hear comments on how you&amp;#39;ve done it, successes, problems - and also reasons why you haven&amp;#39;t, etc \ud83d\udc47\ud83c\udffb&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/12u6oqk\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?auto=webp&amp;v=enabled&amp;s=e636ce7d35a6ffe63bd8d2bcefd07cf7d55c6392", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=1257c3e138f6335e24a1a4cee0d488d27364532d", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=078eeabf7b25c2fe02e02714669f3b6cda833417", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/lBzKL8wuzi-AmMtzoRtl8Zy870JLdZuuHy7Mf3IGLF8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=5282a32789acaac7bd0c2567442756e2aeb3bcc0", "width": 320, "height": 240}], "variants": {}, "id": "YzD2yhQk3wTSSmdUqerRI6LFu8Dk_iCDwBZvhHo8aek"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12u6oqk", "is_robot_indexable": true, "report_reasons": null, "author": "rmoff", "discussion_type": null, "num_comments": 2, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "poll_data": {"prediction_status": null, "total_stake_amount": null, "voting_end_timestamp": 1682349300982, "options": [{"text": "\u2705 Yep!", "id": "22673764"}, {"text": "\ud83d\ude45\ud83c\udffb Nope", "id": "22673765"}, {"text": "\ud83e\udd14 What even is WAP?", "id": "22673766"}], "vote_updates_remained": null, "is_prediction": false, "resolved_option_id": null, "user_won_amount": null, "user_selection": null, "total_vote_count": 95, "tournament_id": null}, "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u6oqk/do_you_use_the_writeauditpublish_wap_pattern_in/", "parent_whitelist_status": "all_ads", "stickied": false, "mod_reports": [], "url": "https://old.reddit.com/r/dataengineering/comments/12u6oqk/do_you_use_the_writeauditpublish_wap_pattern_in/", "subreddit_subscribers": 101461, "created_utc": 1682090100.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Python + dagster: So I want to get a list of failed jobs that happen in the last hour. But I dont seem to be able to find an API that allows me to do a query like this... Do you guys know how to do it? \n\nNot querying by run Id or job name... Just by status... Thanks in advance!", "author_fullname": "t2_37bvgi4r", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Accessing run log in dagster", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ufyo5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682106464.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Python + dagster: So I want to get a list of failed jobs that happen in the last hour. But I dont seem to be able to find an API that allows me to do a query like this... Do you guys know how to do it? &lt;/p&gt;\n\n&lt;p&gt;Not querying by run Id or job name... Just by status... Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ufyo5", "is_robot_indexable": true, "report_reasons": null, "author": "fukkingcake", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ufyo5/accessing_run_log_in_dagster/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ufyo5/accessing_run_log_in_dagster/", "subreddit_subscribers": 101461, "created_utc": 1682106464.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Working with a client who is pushing ~25m records to Salesforce (~100 cols)\n\nIt\u2019s taking 6hrs to complete\u2026 would you consider this slow or fast? And what would you say is a decent load time for kind of data?\n\nEdit:sorry I meant 100 cols", "author_fullname": "t2_2mc0740t", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is fast?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u9s8u", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 3, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 3, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": 1682101600.0, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682094315.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": true, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working with a client who is pushing ~25m records to Salesforce (~100 cols)&lt;/p&gt;\n\n&lt;p&gt;It\u2019s taking 6hrs to complete\u2026 would you consider this slow or fast? And what would you say is a decent load time for kind of data?&lt;/p&gt;\n\n&lt;p&gt;Edit:sorry I meant 100 cols&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12u9s8u", "is_robot_indexable": true, "report_reasons": null, "author": "justbane", "discussion_type": null, "num_comments": 16, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u9s8u/what_is_fast/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u9s8u/what_is_fast/", "subreddit_subscribers": 101461, "created_utc": 1682094315.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "My team is exploring auditing options for our BI datamarts so that our BI and DA teams can use them directly for building user facing dashboards and ad hoc visualisations.\n\nOnly thing is that we've been told that we would need to have some kind of auditing ability where the \"purpose/usecase\" behind a query is logged.\n\nBut, if we end up needing devs to build out some kind of REST API and interface then I have no idea how we would incorporate those into Tableau and our other BI tools.\n\nMy idea for Tableau users is creating a 'published data source's fed by a Prep flow which includes a SQL query and a Tableau Parameter that's within a single or multiline comment (these are all Impala tables which support comments). Our BI creators would then add in the ticket number for their request which would subsequently add that in to every query made on the datamart through that dashboard. Any auditing on the logs could then be pulled from Impala if needed.\n\nOnly issue is security concerns. User inputted parameters are disabled by default for Tableau servers, I guess to prevent injecting unauthorized database commands from a user. \n\nHowever, my proposal here would be only to put these user inputted Parameters within a comment block (either after '--' or strictly in-between '*/', not sure pros/cons for which yet). Hard to find anything on Google because it's bringing up general SQL injection attacks but nothing about whether there's a known SQL injection attack that can escape a SQL comment block to run unauthorized commands. \n\nSo my question here is whether or not this is a dumb idea? Are there any known SQL injection attacks that could use an entry point that's *within* a \"hard-coded\" (i.e. SQL SELECT * FROM table */ &lt;Parameter&gt; */ )  comment block where it could escape the comment block and run unauthorized commands? Note this would be in an on-prem Tableau server only used by on-boarded company users (maybe a few hundred?). \n\nOtherwise I fear the devs will be forced to go the API route which will then mean either developing a Tableau web connector for it, or maybe using the same Parameter concept with a TabPy script call to an API.\n\nThe alternative to user inputted parameters would be a dropdown list of parameters (these are enabled by default on Tableau).\n\n Users would select from some kind of list of codes representing a 'purpose', but I fear going down this path would be modelling this list to be granular enough for compliance's liking.\n\nOtherwise should we consider an entirely different route here for BI data warehouses?   How do people typically approach auditing requirements in DWH environments which could let end users use self serve dashboards while still enabling this kind of auditing requirement.", "author_fullname": "t2_51nsnxi6", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "SQL Comments Security Concerns?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v5w9z", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682170001.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My team is exploring auditing options for our BI datamarts so that our BI and DA teams can use them directly for building user facing dashboards and ad hoc visualisations.&lt;/p&gt;\n\n&lt;p&gt;Only thing is that we&amp;#39;ve been told that we would need to have some kind of auditing ability where the &amp;quot;purpose/usecase&amp;quot; behind a query is logged.&lt;/p&gt;\n\n&lt;p&gt;But, if we end up needing devs to build out some kind of REST API and interface then I have no idea how we would incorporate those into Tableau and our other BI tools.&lt;/p&gt;\n\n&lt;p&gt;My idea for Tableau users is creating a &amp;#39;published data source&amp;#39;s fed by a Prep flow which includes a SQL query and a Tableau Parameter that&amp;#39;s within a single or multiline comment (these are all Impala tables which support comments). Our BI creators would then add in the ticket number for their request which would subsequently add that in to every query made on the datamart through that dashboard. Any auditing on the logs could then be pulled from Impala if needed.&lt;/p&gt;\n\n&lt;p&gt;Only issue is security concerns. User inputted parameters are disabled by default for Tableau servers, I guess to prevent injecting unauthorized database commands from a user. &lt;/p&gt;\n\n&lt;p&gt;However, my proposal here would be only to put these user inputted Parameters within a comment block (either after &amp;#39;--&amp;#39; or strictly in-between &amp;#39;*/&amp;#39;, not sure pros/cons for which yet). Hard to find anything on Google because it&amp;#39;s bringing up general SQL injection attacks but nothing about whether there&amp;#39;s a known SQL injection attack that can escape a SQL comment block to run unauthorized commands. &lt;/p&gt;\n\n&lt;p&gt;So my question here is whether or not this is a dumb idea? Are there any known SQL injection attacks that could use an entry point that&amp;#39;s &lt;em&gt;within&lt;/em&gt; a &amp;quot;hard-coded&amp;quot; (i.e. SQL SELECT * FROM table */ &amp;lt;Parameter&amp;gt; */ )  comment block where it could escape the comment block and run unauthorized commands? Note this would be in an on-prem Tableau server only used by on-boarded company users (maybe a few hundred?). &lt;/p&gt;\n\n&lt;p&gt;Otherwise I fear the devs will be forced to go the API route which will then mean either developing a Tableau web connector for it, or maybe using the same Parameter concept with a TabPy script call to an API.&lt;/p&gt;\n\n&lt;p&gt;The alternative to user inputted parameters would be a dropdown list of parameters (these are enabled by default on Tableau).&lt;/p&gt;\n\n&lt;p&gt;Users would select from some kind of list of codes representing a &amp;#39;purpose&amp;#39;, but I fear going down this path would be modelling this list to be granular enough for compliance&amp;#39;s liking.&lt;/p&gt;\n\n&lt;p&gt;Otherwise should we consider an entirely different route here for BI data warehouses?   How do people typically approach auditing requirements in DWH environments which could let end users use self serve dashboards while still enabling this kind of auditing requirement.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12v5w9z", "is_robot_indexable": true, "report_reasons": null, "author": "VersatileGuru", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v5w9z/sql_comments_security_concerns/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v5w9z/sql_comments_security_concerns/", "subreddit_subscribers": 101461, "created_utc": 1682170001.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "So I was watching a bunch of conferences and videos about Kappa architecture and Confluent when I learned that Kafka had tiered storage. Did anyone actually used this in production ?\n\nIn one of the talks I've seen, Netflix was talking about how they cooled down every events they received for backfilling just in case. Cooling was required in order to decrease storage costs and avoid long retention periods in Kafka. Then they'd have Flink apps to switch between streaming and batch mode depending on the need. Pretty clever.\n\nAnyway, my question is : now can't they just use Kafka tiered storage to solve the issue of backfilling for their streaming apps ? Has anyone here ever used that ? Any pitfalls ?", "author_fullname": "t2_bja0o", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Has anyone tried Kafka tiered storage ?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12v1yf3", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682159555.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was watching a bunch of conferences and videos about Kappa architecture and Confluent when I learned that Kafka had tiered storage. Did anyone actually used this in production ?&lt;/p&gt;\n\n&lt;p&gt;In one of the talks I&amp;#39;ve seen, Netflix was talking about how they cooled down every events they received for backfilling just in case. Cooling was required in order to decrease storage costs and avoid long retention periods in Kafka. Then they&amp;#39;d have Flink apps to switch between streaming and batch mode depending on the need. Pretty clever.&lt;/p&gt;\n\n&lt;p&gt;Anyway, my question is : now can&amp;#39;t they just use Kafka tiered storage to solve the issue of backfilling for their streaming apps ? Has anyone here ever used that ? Any pitfalls ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": false, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12v1yf3", "is_robot_indexable": true, "report_reasons": null, "author": "Shinosha", "discussion_type": null, "num_comments": 1, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v1yf3/has_anyone_tried_kafka_tiered_storage/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v1yf3/has_anyone_tried_kafka_tiered_storage/", "subreddit_subscribers": 101461, "created_utc": 1682159555.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "", "author_fullname": "t2_32lec", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Noah Gift - Speed Up Python dramatically With CUDA GPU", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": 105, "top_awarded_type": null, "hide_score": false, "name": "t3_12uklg5", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.63, "author_flair_background_color": null, "ups": 2, "total_awards_received": 0, "media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/cXlz-hZ_bgU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Speed Up Python DRAMATICALLY With CUDA GPU\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "height": 200}, "thumbnail_width": 140, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Speed Up Python DRAMATICALLY With CUDA GPU", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/cXlz-hZ_bgU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Speed Up Python DRAMATICALLY With CUDA GPU\"&gt;&lt;/iframe&gt;", "author_name": "Pragmatic AI Labs", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/cXlz-hZ_bgU/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@pragmaticai"}}, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {"content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/cXlz-hZ_bgU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Speed Up Python DRAMATICALLY With CUDA GPU\"&gt;&lt;/iframe&gt;", "width": 356, "scrolling": false, "media_domain_url": "https://www.redditmedia.com/mediaembed/12uklg5", "height": 200}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "https://b.thumbs.redditmedia.com/rXav7xM3n6pfuyc3LXrprG3I2pf9AwGm1U4JtOjebLo.jpg", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "rich:video", "content_categories": null, "is_self": false, "subreddit_type": "public", "created": 1682115579.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "youtube.com", "allow_live_comments": false, "selftext_html": null, "likes": null, "suggested_sort": null, "banned_at_utc": null, "url_overridden_by_dest": "https://www.youtube.com/watch?v=cXlz-hZ_bgU", "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/XgjOCQCzYUDhHL58_2oZtLf949oMC7ZdbBbb_QkrVSw.jpg?auto=webp&amp;v=enabled&amp;s=44986109a35c5b966179c8ae2d2af152f7c98907", "width": 480, "height": 360}, "resolutions": [{"url": "https://external-preview.redd.it/XgjOCQCzYUDhHL58_2oZtLf949oMC7ZdbBbb_QkrVSw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=c08c239d7e228aed4ff26d9699968589084eb3ed", "width": 108, "height": 81}, {"url": "https://external-preview.redd.it/XgjOCQCzYUDhHL58_2oZtLf949oMC7ZdbBbb_QkrVSw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=a2f74b0bb40c0103db3dab736643ae8df9ccec8e", "width": 216, "height": 162}, {"url": "https://external-preview.redd.it/XgjOCQCzYUDhHL58_2oZtLf949oMC7ZdbBbb_QkrVSw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;v=enabled&amp;s=770925c8329c094ca930e491d3cde0147881e750", "width": 320, "height": 240}], "variants": {}, "id": "utQRdZLRCjBEre0BeZSglZIsl17UH7hFWPFok3Ge8FI"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "mod_note": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "num_reports": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12uklg5", "is_robot_indexable": true, "report_reasons": null, "author": "RichKatz", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12uklg5/noah_gift_speed_up_python_dramatically_with_cuda/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://www.youtube.com/watch?v=cXlz-hZ_bgU", "subreddit_subscribers": 101461, "created_utc": 1682115579.0, "num_crossposts": 0, "media": {"type": "youtube.com", "oembed": {"provider_url": "https://www.youtube.com/", "version": "1.0", "title": "Speed Up Python DRAMATICALLY With CUDA GPU", "type": "video", "thumbnail_width": 480, "height": 200, "width": 356, "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/cXlz-hZ_bgU?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"Speed Up Python DRAMATICALLY With CUDA GPU\"&gt;&lt;/iframe&gt;", "author_name": "Pragmatic AI Labs", "provider_name": "YouTube", "thumbnail_url": "https://i.ytimg.com/vi/cXlz-hZ_bgU/hqdefault.jpg", "thumbnail_height": 360, "author_url": "https://www.youtube.com/@pragmaticai"}}, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello all,\n\nFirst off, apologies if this is the wrong forum for this kind of post. I considered posting on r/dataengineeringjobs but it did not seem to be the appropriate forum.\n\nBackground (feel free to skip):\n\nI am a data analyst with a passion for data science and an interest in data engineering. In my current position, I do a lot of reporting and automation-type tasks using SQL, python, power bi, tableau, etc. I enjoy my position to an extent, but it is difficult as I am most likely the most technical person in my team and I, myself, don't even feel like I am well-versed with the modern tech stack. In my off time, I have been trying to upskill myself and learn how to think like a data scientist by doing a number of personal side projects. In an old position, I did a little of data engineering via building pipelines and managing triggers in Azure DF/Databricks but that is the utmost extent of my knowledge/experience in this field.\n\nQuestion:\n\nThe reason I am posting here is that I was offered a DE position with a fully-remote software company that is heavily involved in a variety of industries. I was very upfront in the interview that my skills align more with DA and DS, and I also did a somewhat poor job on the tech side of the interview, but I apparently made a good enough impact to be offered the job. On one hand, I am very excited as the job description mentions that I will be doing some ml/ai tasks, building pipelines, developing software, etc etc etc; and it is, by all accounts, a dream position to 'beef up my skills'.  They also work with a wide variety of tools from what I understand. My concern is that I will be in over my head, and will quickly be let go on account of being a fraud. My current position is also with the government so going to the private sector is also a little nerve-wracking as I will be losing some job security.\n\n&amp;#x200B;\n\nThis post is already long enough so I will end it here, but I would be eternally grateful for some advice. As I mentioned, this is a very exciting opportunity but I worry that my anxiety will get the best of me.\n\n&amp;#x200B;\n\nThanks,", "author_fullname": "t2_10cte7ke", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Weighing the jump from Data Analytics to Data Engineering", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ubjlh", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.75, "author_flair_background_color": null, "subreddit_type": "public", "ups": 2, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Career", "can_mod_post": false, "score": 2, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682097652.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;First off, apologies if this is the wrong forum for this kind of post. I considered posting on &lt;a href=\"/r/dataengineeringjobs\"&gt;r/dataengineeringjobs&lt;/a&gt; but it did not seem to be the appropriate forum.&lt;/p&gt;\n\n&lt;p&gt;Background (feel free to skip):&lt;/p&gt;\n\n&lt;p&gt;I am a data analyst with a passion for data science and an interest in data engineering. In my current position, I do a lot of reporting and automation-type tasks using SQL, python, power bi, tableau, etc. I enjoy my position to an extent, but it is difficult as I am most likely the most technical person in my team and I, myself, don&amp;#39;t even feel like I am well-versed with the modern tech stack. In my off time, I have been trying to upskill myself and learn how to think like a data scientist by doing a number of personal side projects. In an old position, I did a little of data engineering via building pipelines and managing triggers in Azure DF/Databricks but that is the utmost extent of my knowledge/experience in this field.&lt;/p&gt;\n\n&lt;p&gt;Question:&lt;/p&gt;\n\n&lt;p&gt;The reason I am posting here is that I was offered a DE position with a fully-remote software company that is heavily involved in a variety of industries. I was very upfront in the interview that my skills align more with DA and DS, and I also did a somewhat poor job on the tech side of the interview, but I apparently made a good enough impact to be offered the job. On one hand, I am very excited as the job description mentions that I will be doing some ml/ai tasks, building pipelines, developing software, etc etc etc; and it is, by all accounts, a dream position to &amp;#39;beef up my skills&amp;#39;.  They also work with a wide variety of tools from what I understand. My concern is that I will be in over my head, and will quickly be let go on account of being a fraud. My current position is also with the government so going to the private sector is also a little nerve-wracking as I will be losing some job security.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;This post is already long enough so I will end it here, but I would be eternally grateful for some advice. As I mentioned, this is a very exciting opportunity but I worry that my anxiety will get the best of me.&lt;/p&gt;\n\n&lt;p&gt;&amp;#x200B;&lt;/p&gt;\n\n&lt;p&gt;Thanks,&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "069dd614-a7dc-11eb-8e48-0e90f49436a3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#349e48", "id": "12ubjlh", "is_robot_indexable": true, "report_reasons": null, "author": "KinglyOyster", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ubjlh/weighing_the_jump_from_data_analytics_to_data/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ubjlh/weighing_the_jump_from_data_analytics_to_data/", "subreddit_subscribers": 101461, "created_utc": 1682097652.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello, I am a soon to be 4th year cs student who has recently met with their summer internship manager to discuss projects. \n\nAs it was briefly described to me, I would creating \u201ccheckpoints\u201d within our data system and displaying results with PowerBI. the only tools explicitly mentioned were: SQL, Snowflake, Azure DevOps, and PowerBI. \n\nI was curious about how this very general description of a project sounds to you all. This is my only relevant internship experience before I graduate, and I was really hoping for more of a development/engineering role. I\u2019m just not sure about this project, particularly because of the PowerBI aspect. Does this closer to the analytics realm? or DE?\n\nThank you for any reassurance", "author_fullname": "t2_466tn173", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What is my role?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": true, "name": "t3_12v813x", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682174661.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, I am a soon to be 4th year cs student who has recently met with their summer internship manager to discuss projects. &lt;/p&gt;\n\n&lt;p&gt;As it was briefly described to me, I would creating \u201ccheckpoints\u201d within our data system and displaying results with PowerBI. the only tools explicitly mentioned were: SQL, Snowflake, Azure DevOps, and PowerBI. &lt;/p&gt;\n\n&lt;p&gt;I was curious about how this very general description of a project sounds to you all. This is my only relevant internship experience before I graduate, and I was really hoping for more of a development/engineering role. I\u2019m just not sure about this project, particularly because of the PowerBI aspect. Does this closer to the analytics realm? or DE?&lt;/p&gt;\n\n&lt;p&gt;Thank you for any reassurance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12v813x", "is_robot_indexable": true, "report_reasons": null, "author": "slurpadurpblurp", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12v813x/what_is_my_role/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12v813x/what_is_my_role/", "subreddit_subscribers": 101461, "created_utc": 1682174661.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": " Data Engineering peeps, if you move data around (whether it be Modern Data Stack or something else), can you let me know.\n\nDo you use ETL or ELT logic?\n\nWhat tools do you for each of the E, L &amp; T steps?\n\n\\#datafam #bigdata #dataengineering", "author_fullname": "t2_353ucr1h", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "What do you use?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12uvhve", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.55, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Discussion", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682140223.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Data Engineering peeps, if you move data around (whether it be Modern Data Stack or something else), can you let me know.&lt;/p&gt;\n\n&lt;p&gt;Do you use ETL or ELT logic?&lt;/p&gt;\n\n&lt;p&gt;What tools do you for each of the E, L &amp;amp; T steps?&lt;/p&gt;\n\n&lt;p&gt;#datafam #bigdata #dataengineering&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "92b74b58-aaca-11eb-b160-0e6181e3773f", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ff4500", "id": "12uvhve", "is_robot_indexable": true, "report_reasons": null, "author": "cmcau", "discussion_type": null, "num_comments": 7, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12uvhve/what_do_you_use/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12uvhve/what_do_you_use/", "subreddit_subscribers": 101461, "created_utc": 1682140223.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Do you use any platform to share python/scripting notebooks, and run other people's notebooks? Wondering the best way to share them and have people re-use them.", "author_fullname": "t2_vikcbs0a", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Sharing python notebooks", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12uh1t6", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682108606.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Do you use any platform to share python/scripting notebooks, and run other people&amp;#39;s notebooks? Wondering the best way to share them and have people re-use them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12uh1t6", "is_robot_indexable": true, "report_reasons": null, "author": "RespondOk3068", "discussion_type": null, "num_comments": 3, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12uh1t6/sharing_python_notebooks/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12uh1t6/sharing_python_notebooks/", "subreddit_subscribers": 101461, "created_utc": 1682108606.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "Hello everyone,   \nIn Medium I wrote 3 articles regarding [\\#pandas](https://www.linkedin.com/feed/hashtag/?keywords=pandas&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640) vs [\\#polars](https://www.linkedin.com/feed/hashtag/?keywords=polars&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640) in [\\#python](https://www.linkedin.com/feed/hashtag/?keywords=python&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640)   \n **#1 - Pandas vs Polars vs Pandas 2.0 \u2026. FIGHT - Testing an ETL process**  \n[https://lnkd.in/dst7b7bk](https://lnkd.in/dst7b7bk)  \n\n\n**#2 - Pandas vs Polars vs Pandas 2.0\u00a0\u2026 ROUND 2 - An article developed after some ideas from the first article**  \n[https://lnkd.in/dae42ePe](https://lnkd.in/dae42ePe)  \n\n\n**#3 - From Pandas to Polars - How to Extract, Transform, and Load in Python - Cheatsheet for Polars**  \n[https://lnkd.in/daESeWe8](https://lnkd.in/daESeWe8): \n\nI thinking of doing two more in this area:  \n1- Pandas 2.0 (it is in stable version) vs Polars - Focus on Transformation  \n2- Polars vs PySpark - \"Major League\" fight  \n\n\nAny extra idea you would like me to test on this theme? \ud83d\ude09  \nSome code I could develop and test?   \nThank you very much for your help!   \n[\\#programming](https://www.linkedin.com/feed/hashtag/?keywords=programming&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640) [\\#dataengineer](https://www.linkedin.com/feed/hashtag/?keywords=dataengineer&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640) [\\#dataengineering](https://www.linkedin.com/feed/hashtag/?keywords=dataengineering&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640) [\\#datascience](https://www.linkedin.com/feed/hashtag/?keywords=datascience&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640)", "author_fullname": "t2_7dy3sswp", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Ideas for the next performance tests", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12u9flc", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 1.0, "author_flair_background_color": null, "subreddit_type": "public", "ups": 1, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Blog", "can_mod_post": false, "score": 1, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "post_hint": "self", "content_categories": null, "is_self": true, "mod_note": null, "created": 1682093648.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;br/&gt;\nIn Medium I wrote 3 articles regarding &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=pandas&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640\"&gt;#pandas&lt;/a&gt; vs &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=polars&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640\"&gt;#polars&lt;/a&gt; in &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=python&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640\"&gt;#python&lt;/a&gt;&lt;br/&gt;\n &lt;strong&gt;#1 - Pandas vs Polars vs Pandas 2.0 \u2026. FIGHT - Testing an ETL process&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://lnkd.in/dst7b7bk\"&gt;https://lnkd.in/dst7b7bk&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;#2 - Pandas vs Polars vs Pandas 2.0\u00a0\u2026 ROUND 2 - An article developed after some ideas from the first article&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://lnkd.in/dae42ePe\"&gt;https://lnkd.in/dae42ePe&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;#3 - From Pandas to Polars - How to Extract, Transform, and Load in Python - Cheatsheet for Polars&lt;/strong&gt;&lt;br/&gt;\n&lt;a href=\"https://lnkd.in/daESeWe8\"&gt;https://lnkd.in/daESeWe8&lt;/a&gt;: &lt;/p&gt;\n\n&lt;p&gt;I thinking of doing two more in this area:&lt;br/&gt;\n1- Pandas 2.0 (it is in stable version) vs Polars - Focus on Transformation&lt;br/&gt;\n2- Polars vs PySpark - &amp;quot;Major League&amp;quot; fight  &lt;/p&gt;\n\n&lt;p&gt;Any extra idea you would like me to test on this theme? \ud83d\ude09&lt;br/&gt;\nSome code I could develop and test?&lt;br/&gt;\nThank you very much for your help!&lt;br/&gt;\n&lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=programming&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640\"&gt;#programming&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataengineer&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640\"&gt;#dataengineer&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=dataengineering&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640\"&gt;#dataengineering&lt;/a&gt; &lt;a href=\"https://www.linkedin.com/feed/hashtag/?keywords=datascience&amp;amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7055209522918768640\"&gt;#datascience&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "preview": {"images": [{"source": {"url": "https://external-preview.redd.it/5ZI7oL3JTPPt59G0vTfOaQMHvka17QCAdFnF87leUeA.jpg?auto=webp&amp;v=enabled&amp;s=751b05e77b1c50dfc8477f4c599cb33affc7e2fc", "width": 64, "height": 64}, "resolutions": [], "variants": {}, "id": "QqSY3F9i2BgB-OdT_JpQr1vBqr2oq4spYNzkghHXwCM"}], "enabled": false}, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "eb739554-a7db-11eb-95d7-0ec0f8f30313", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#0079d3", "id": "12u9flc", "is_robot_indexable": true, "report_reasons": null, "author": "Asleep-Organization7", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12u9flc/ideas_for_the_next_performance_tests/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12u9flc/ideas_for_the_next_performance_tests/", "subreddit_subscribers": 101461, "created_utc": 1682093648.0, "num_crossposts": 0, "media": null, "is_video": false}}, {"kind": "t3", "data": {"approved_at_utc": null, "subreddit": "dataengineering", "selftext": "I came across one that had all the SQL commented out but it was still \"running\". What does that mean?", "author_fullname": "t2_bpb6o8y", "saved": false, "mod_reason_title": null, "gilded": 0, "clicked": false, "title": "Does anyone have any experience with Kinesis SLQ (Legacy) applications?", "link_flair_richtext": [], "subreddit_name_prefixed": "r/dataengineering", "hidden": false, "pwls": 6, "link_flair_css_class": "", "downs": 0, "thumbnail_height": null, "top_awarded_type": null, "hide_score": false, "name": "t3_12ulgi8", "quarantine": false, "link_flair_text_color": "light", "upvote_ratio": 0.5, "author_flair_background_color": null, "subreddit_type": "public", "ups": 0, "total_awards_received": 0, "media_embed": {}, "thumbnail_width": null, "author_flair_template_id": null, "is_original_content": false, "user_reports": [], "secure_media": null, "is_reddit_media_domain": false, "is_meta": false, "category": null, "secure_media_embed": {}, "link_flair_text": "Help", "can_mod_post": false, "score": 0, "approved_by": null, "is_created_from_ads_ui": false, "author_premium": false, "thumbnail": "self", "edited": false, "author_flair_css_class": null, "author_flair_richtext": [], "gildings": {}, "content_categories": null, "is_self": true, "mod_note": null, "created": 1682117306.0, "link_flair_type": "text", "wls": 6, "removed_by_category": null, "banned_by": null, "author_flair_type": "text", "domain": "self.dataengineering", "allow_live_comments": false, "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across one that had all the SQL commented out but it was still &amp;quot;running&amp;quot;. What does that mean?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;", "likes": null, "suggested_sort": null, "banned_at_utc": null, "view_count": null, "archived": false, "no_follow": true, "is_crosspostable": false, "pinned": false, "over_18": false, "all_awardings": [], "awarders": [], "media_only": false, "link_flair_template_id": "2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3", "can_gild": false, "spoiler": false, "locked": false, "author_flair_text": null, "treatment_tags": [], "visited": false, "removed_by": null, "num_reports": null, "distinguished": null, "subreddit_id": "t5_36en4", "author_is_blocked": false, "mod_reason_by": null, "removal_reason": null, "link_flair_background_color": "#ea0027", "id": "12ulgi8", "is_robot_indexable": true, "report_reasons": null, "author": "data_addict", "discussion_type": null, "num_comments": 0, "send_replies": true, "whitelist_status": "all_ads", "contest_mode": false, "mod_reports": [], "author_patreon_flair": false, "author_flair_text_color": null, "permalink": "/r/dataengineering/comments/12ulgi8/does_anyone_have_any_experience_with_kinesis_slq/", "parent_whitelist_status": "all_ads", "stickied": false, "url": "https://old.reddit.com/r/dataengineering/comments/12ulgi8/does_anyone_have_any_experience_with_kinesis_slq/", "subreddit_subscribers": 101461, "created_utc": 1682117306.0, "num_crossposts": 0, "media": null, "is_video": false}}], "before": null}}